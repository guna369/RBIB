Arnold Beckmann Costas Dimitracopoulos Benedikt Lo¨we (Editors)
Logic and Theory of Algorithms
Fourth Conference on Computability in Europe, CiE 2008 Local Proceedings

June 15–20, 2008

University of Athens

Preface
CiE 2008: Logic and Theory of Algorithms Athens, Greece, June 15–20, 2008
Computability in Europe (CiE) is an informal network of European scientists working on computability theory, including its foundations, technical development, and applications. Among the aims of the network is to advance our theoretical understanding of what can and cannot be computed, by any means of computation. Its scientiﬁc vision is broad: computations may be performed with discrete or continuous data by all kinds of algorithms, programs, and machines. Computations may be made by experimenting with any sort of physical system obeying the laws of a physical theory such as Newtonian mechanics, quantum theory, or relativity. Computations may be very general, depending on the foundations of set theory; or very speciﬁc, using the combinatorics of ﬁnite structures. CiE also works on subjects intimately related to computation, especially theories of data and information, and methods for formal reasoning about computations. The sources of new ideas and methods include practical developments in areas such as neural networks, quantum computation, natural computation, molecular computation, computational learning. Applications are everywhere, especially, in algebra, analysis and geometry, or data types and programming. Within CiE there is general recognition of the underlying relevance of computability to physics and a broad range of other sciences, providing as it does a basic analysis of the causal structure of dynamical systems.
The conference was based on invited tutorials and lectures, a set of special sessions on a range of subjects as well as contributed papers and informal presentations. The formal proceedings volume appeared in the Springer LNCS series, Vol. 5028 and contains 25 of the invited lectures and 36 of the submitted contributed papers. The present volume represents the informal abstract booklet which contains another 86 of the contributed talks and two invited plenary talks as well as 23 abstracts of informal presentations. The informal abstract booklet is not a scholarly publication, and papers appearing in it should not be considered published or as selected in a peer review process. There will be a number of post-proceedings publications, including special issues of Theory of Computing Systems, Archive for Mathematical Logic, and Journal of Algorithms.

VI

The ﬁrst three meetings of CiE were at the University of Amsterdam in 2005, at the University of Wales Swansea in 2006, and at the University of Siena in 2007. The large number of mathematicians and computer scientists attending these conference had their view of computability theory enlarged and transformed: they discovered that its foundations were deeper and more mysterious, its technical development more vigorous, its applications wider and more challenging than they had known. The Athens meeting promised to extend and enrich that process.
The annual CiE conference, based on the Computability in Europe network, has become a major event, and is the largest international meeting focused on computability theoretic issues. The series is coordinated by the CiE Conference Series Steering Committee:

Arnold Beckmann (Swansea) Paola Bonizzoni (Milan) S. Barry Cooper (Leeds) Benedikt L¨owe (Amsterdam, Chair)

Elvira Mayordomo (Zaragoza) Dag Normann (Oslo) Peter van Emde Boas (Amsterdam).

We will reconvene 2009 in Heidelberg and 2010 in Ponta Delgada (A¸cores).

Organization and Acknowledgements

The conference CiE 2008 was organized by: Dionysis Anapolitanos (Athens), Arnold Beckmann (Swansea), Costas Dimitracopoulos (Athens, Chair), Michael Mytilinaios (Athens) †, Thanases Pheidas (Heraklion), Stathis Zachos (Athens and New York NY).
The Programme Committee was chaired by Arnold Beckmann and Costas Dimitracopoulos:

Luigia Aiello (Rome) Thorsten Altenkirch (Nottingham) Klaus Ambos-Spies (Heidelberg) Giorgio Ausiello (Rome) Arnold Beckmann (Swansea) Lev Beklemishev (Moscow) Paola Bonizzoni (Milan) Stephen A. Cook (Toronto ON) Barry Cooper (Leeds) Costas Dimitracopoulos (Athens) Rod Downey (Wellington) Elias Koutsoupias (Athens) Orna Kupferman (Jerusalem) Sophie Laplante (Orsay) Hannes Leitgeb (Bristol) Benedikt L¨owe (Amsterdam) Elvira Mayordomo (Zaragoza)

Franco Montagna (Siena) Michael Mytilinaios (Athens) † Mogens Nielsen (Aarhus) Isabel Oitavem (Lisbon) Catuscia Palamidessi (Palaiseau) Thanases Pheidas (Heraklion) Ramanujam (Chennai) Andrea Schalk (Manchester) Uwe Sch¨oning (Ulm) Helmut Schwichtenberg (Munich) Alan Selman (Buﬀalo NY) Andrea Sorbi (Siena) Ivan Soskov (Soﬁa) Christopher Timpson (Oxford) Stathis Zachos (Athens and New York NY)

Preface VII

We are delighted to acknowledge and thank the following for their essential ﬁnancial support: City of Athens, Bank of Greece, Graduate Program in Logic, Algorithms and Computation (MPLA), Hellenic Ministry of Education, John S. Latsis Foundation, Kleos S.A., National and Kapodistrian University of Athens, Public Power Corporation S.A., Rizareio Foundation, The Elsevier Foundation.
The high scientic quality of the conference was possible through the conscientious work of the Programme Committee, and the special session organizers. While papers in this abstract booklet are not to be considered publications, some of the papers in this volume greatly beneﬁtted from comments of the referees during our reviewing process, and we would like to thank the referees for their work. The list of referees is given on pp. IX - X of the mentioned LNCS volume for this conference.
We thank Andrej Voronkov for his EasyChair system which facilitated the work of the Programme Committee and the editors considerably. We also thank Nikolaos S. Papaspyrou for his great support in producing this abstract booklet.

Swansea, Athens and Amsterdam, June 2008

Arnold Beckmann Costas Dimitracopoulos
Benedikt L¨owe

Table of Contents

Invited Talks.

The Computing Species . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Keith Devlin

Logic, Automata, Games, and Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Moshe Y. Vardi

Contributed Talks.

On the Performance of Automata Minimization Algorithms . . . . . . . . . . . . 3 Marco Almeida, Nelma Moreira, Rog´erio Reis

Decidability Results for Mobile Membranes derived from Mobile Ambients 15 Bogdan Aman, Gabriel Ciobanu

Sophisticated Inﬁnite Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Luis Antunes, Andr´e Souto

On Detecting Deadlock in the Pi-Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Tiago Azevedo, Mario Benevides, F´abio Protti, Marcelo Sihman

Algorithms for Analytic Functions and Applications to Toeplitz Operators 45 Edwin Beggs, Annelies Gerber

Triviality and Minimality in the Degrees of Monotone Complexity . . . . . . . 55 William Calhoun

Extraction of Eﬃcient Programs from Correct Proofs: The Case of Structural Induction over Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Luca Chiarabini

Phase Shifts of LFSM as Pseudorandom Number Generators for BIST for VLSI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Sung-Jin Cho, Un-Sook Choi, Han-Doo Kim, Yoon-Hee Hwang, JinGyoung Kim

77

Introducing Service Schemes and Systems Organization in the Theory of Interactive Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Antˆonio Carlos Costa, Gra¸caliz Dimuro

Online-division with Periodic Rational Numbers . . . . . . . . . . . . . . . . . . . . . . 97 Gregorio de Miguel Casado, Juan Manuel Garc´ıa Chamizo, Higinio Mora Mora

Table of Contents IX
Abstract Geometrical Computation with Accumulations: Beyond the Blum, Shub and Smale model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
J´erˆome Durand-Lose
Notions of Bisimulation for Heyting-Valued Modal Languages . . . . . . . . . . 117 Pantelis Eleftheriou, Costas Koutras, Christos Nomikos
Algorithmic Properties of Structures for Languages with Two Unary Functional Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Ekaterina Fokina
Veriﬁcation of Newman’s and Yokouchi’s Lemmas in PVS . . . . . . . . . . . . . . 137 Andr´e Luiz Galdino, Mauricio Ayala-Rinc´on
Computation over Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 Christine Gaßner
Singularities of Holomorphic Functions in Subsystems of Second Order Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
Yoshihiro Horihata, Keita Yokoyama
Modelling Linear Cellular Automata with the Minimum Stage Corresponding to CCSG based on LFSR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, Han-Doo Kim
Multitape Ordinal Machines and Primitive Recursion . . . . . . . . . . . . . . . . . . 175 Bernhard Irrgang, Benjamin Seyﬀerth
Prescribed Learning of Indexed Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 Sanjay Jain, Frank Stephan, Nan Ye
Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 195 Maurice Jansen
The Use of Information Aﬃnity in Possibilistic Decision Tree Learning and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
Ilyes Jenhani, Salem Benferhat, Zied Elouedi
Ordering Finite Labeled Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 Herman Ruge Jervell
Towards Reverse Proofs-as-Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 Reinhard Kahle
Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems . 234 Basil Kar´adais
Safety Properties Veriﬁcation for Pfaﬃan Dynamics . . . . . . . . . . . . . . . . . . . 246 Margarita Korovina, Nicolai Vorobjov

X
On Extending Wand’s Type Reconstruction Algorithm to Handle Polymorphic Let . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
Sunil Kothari, James Caldwell
Simulations Between Tilings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264 Gr´egory Laﬁtte, Michael Weiss
Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 274 St´ephane Le Roux
Combining Concept Maps and Petri Nets to Generate Intelligent Tutoring Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
Maikel Le´on, Isis Bonet, Zenaida Garc´ıa
Query-Optimal Oracle Turing Machines for Type-2 Computations . . . . . . . 294 Chung-Chih Li
From Hilbert’s Program to a Logic Toolbox . . . . . . . . . . . . . . . . . . . . . . . . . . 304 Johann Makowsky
From Program Veriﬁcation to Certiﬁed Binaries . . . . . . . . . . . . . . . . . . . . . . . 324 Angelos Manousaridis, Michalis Papakyriakou, Nikolaos Papaspyrou
Limiting Recursion, FM–Repressentability, and Hypercomputations . . . . . 332 Marcin Mostowski
Using Tables to Construct Non-Redundant Proofs . . . . . . . . . . . . . . . . . . . . . 344 Vivek Nigam
Classifying the Phase Transition Threshold for Unordered Regressive Ramsey Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
Florian Pelupessy, Andreas Weiermann
Almost Partial m-Reducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361 Katya Petrova, Boris Solon
Two-Dimensional Cellular Automata Transforms for a Novel Edge Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
Yongri Piao, Seok-Tae Kim, Sung-Jin Cho
Computable Counter-examples to the Brouwer Fixed-point Theorem . . . . 377 Petrus Potgieter
Polynomial Iterations over Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387 Mihai Prunescu
Simulations of Quantum Turing Machines by Quantum Multi-Counter Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
Daowen Qiu

Table of Contents XI
Optimal Proof Systems and Complete Languages . . . . . . . . . . . . . . . . . . . . . 407 Zenon Sadowski
On the Complexity of Computing Winning Strategies for Finite Poset Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
Michael Soltys, Craig Wilson
A Statistical Mechanical Interpretation of Algorithmic Information Theory 425 Kohtaro Tadaki
Solving Tripartite Matching by Interval-valued Computation in Polynomial Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
A´kos Tajti, Benedek Nagy
Probabilistic Machines vs. Relativized Computation . . . . . . . . . . . . . . . . . . . 445 Hayato Takahashi, Kazuyuki Aihara
Quantum Query Algorithms for AND and OR Boolean Functions . . . . . . . 453 Alina Vasilieva
Space Complexity in Ordinal Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . 463 Joost Winter
Reverse Mathematics for Fourier Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . 473 Keita Yokoyama
Induced Matchings in Graphs of Maximum Degree Three . . . . . . . . . . . . . . 483 Graz˙yna Zwo´zniak
Abstracts of Informal Presentations.
Subsystems of Iterated Inductive Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . 493 Bahareh Afshari, Michael Rathjen
Query Algorithms for Detecting Hamming and Reed-Solomon Codes . . . . 494 Ruben Agadˇzanjan
Expressive Power Of Graph Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495 Timos Antonopoulos, Anuj Dawar
The Lost Melody Theorem for Inﬁnite Time Register Machines . . . . . . . . . 496 Merlin Carl, Peter Koepke
Comparing Notions of Fractal Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497 Chris Conidis
Clockable Ordinals for Inﬁnite Time Register Machines . . . . . . . . . . . . . . . . 498 Tim Fischbach, Peter Koepke, Miriam Nasﬁ, Gregor Weckbecker
Embedding the Enumeration Degrees in the ω-Enumeration Degrees . . . . . 499 Hristo Ganchev

XII
Computable Models Spectras of Ehrenfeucht Theories . . . . . . . . . . . . . . . . . 500 Alexander Gavryushkin
Deterministic Subsequential Transducers with Additional FIFO-memory . 501 Stefan Gerdjikov
Proof Fragments and Cut-Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502 Stefan Hetzl
q-Overlaps in the Random Exact Cover Problem . . . . . . . . . . . . . . . . . . . . . . 503 Gabriel Istrate, Romeo Negrea
Inductive Deﬁnitions over Domain Representable Spaces . . . . . . . . . . . . . . . 504 Petter Kristian Køber
The Computable Dimension of Free Projective Planes . . . . . . . . . . . . . . . . . 505 Nurlan Kogabaev
A Classiﬁcation of Theories of Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506 Graham Leigh, Michael Rathjen
Monotonicity Conditions over Characterisations of PSPACE . . . . . . . . . . . . 507 Bruno Loﬀ, Isabel Oitavem
Some Results on Local LR-degree Structures . . . . . . . . . . . . . . . . . . . . . . . . . 508 Anthony Morphett
The Axiomatic Derivation of Absolute Lower Bounds . . . . . . . . . . . . . . . . . . 509 Yiannis Moschovakis
Exploring the Computational Contribution of a Non-constructive Combinatorial Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
Diana Ratiu, Trifon Trifonov
Autostability of Automatic Linear Orders . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511 Alexandra Revenko
Quantiﬁers on Automatic Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512 Sasha Rubin
The Almost Zero ω-Enumeration Degrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513 Ivan N. Soskov
A Sequent Calculus for Intersection and Union Logic . . . . . . . . . . . . . . . . . . 514 Anastasia Veneti, Yiorgos Stavrinos
Anhomomorphic Logic: The Logic of Quantum Realism . . . . . . . . . . . . . . . . 515 Petros Wallden

The Computing Species
Keith Devlin
Center for the Study of Language and Information (CSLI) Standford University
Cordura Hall, 210 Panama Street Stanford, CA 94305-4115 USA devlin@csli.stanford.edu
We are characteristically the “symbolic species,” Terrence Deacon claimed in his celebrated 1997 book of that name. Deacon’s focus was on language, a capacity unique to humans, the development of which changed in fundamental ways the way humans think and live. Computation is a particular form of symbolic processing. At four distinct stages in the development of modern society, acquisition of the ability to carry out certain new kinds of computation likewise changed - also in fundamental ways - how we humans understand the world and live our lives.
The fourth such change is taking place during our lifetime, brought about by the invention of machines that can be instructed to compute for us. The others occurred in 8,000 B.C., the 13th century, and the 17th century. I’ll look at how human life and cognition changed at each of those three stages.

Logic, Automata, Games, and Algorithms
Moshe Y. Vardi
Department of Computer Science Rice University
6100 S. Main Street Houston, TX 77005-1892, USA
vardi@cs.rice.edu
The automata-theoretic approach to decision procedures, introduced by Buechi, Elgot, and Trakhtenbrot in the late 1950s, is one of the most fundamental approaches to decision procedures. Recently, this approach has found industrial applications in formal veriﬁcation of hardware and software systems. The path from logic to practical algorithms goes not only through automata, but also through games, whose algorithmic aspects were studies by Chandra, Kozen, and Stockmeyer in the late 1970s. In this tutorial we describe the path from logic to algorithms via games and automata.

On the Performance of Automata Minimization Algorithms
Marco Almeida, Nelma Moreira, and Rog´erio Reis {mfa,nam,rvr}@ncc.up.pt
DCC-FC & LIACC, Universidade do Porto R. do Campo Alegre 1021/1055, 4169-007 Porto, Portugal
Abstract. Apart from the theoretical worst-case running time analysis not much is known about the average-case analysis or practical performance of ﬁnite automata minimization algorithms. On this paper we compare the running time of four minimization algorithms based on experimental results, and apply them to both deterministic and nondeterministic random automata. Although there is no clear winner, some conclusions can be taken for speciﬁc cases. Hopcroft’s algorithm performs better for DFAs with small alphabets, and, regardless of the alphabet size, Brzozowski’s algorithm is the clearly the fastest dealing with NFAs.
Key words: deterministic ﬁnite automata, non-deterministic ﬁnite automata, minimal automata, minimization algorithms, random generation
1 Introduction
The problem of writing eﬃcient algorithms to ﬁnd the minimal DFA equivalent to a given automaton can be traced back to the 1950’s with the works of Huﬀman [Huf55] and Moore [Moo58]. Over the years several alternative algorithms were proposed. Authors typically present the running time worst-case analysis of their algorithms, but the practical experience is sometimes diﬀerent. The comparison of algorithms performance is always a diﬃcult problem, and little is known about the practical running time performance of automata mininimization algorithms. In particular, there are no studies of average-case analysis of these algorithms, an exception being the work of Nicaud [Nic00], where it is proved that the averagecase complexity of Brzozowski’s algorithm is exponential for group automata. Lhot´ak [Lho00] presents a general data structure for DFA minimization algorithms to run in O(kn log n), where n is the number of states of the DFA and k is the size of the alphabet. Bruce Watson [Wat95] presents some experimental results but his data sets were small and biased. Tabakov and Vardi [TV05] compared Hopcroft’s and Brzozowski’s algorithms. Baclet et al. [BP06] analysed diﬀerent implementations of the Hopcroft’s algorithm. More recently, Bassino et al. [BDN07] compared Moore’s and Hopcroft’s algorithms.
Using the Python programming language, we implemented the algorithms due to Hopcroft (H) [Hop71], Brzozowski (B) [Brz63], Watson (W) [Wat95], and also using full memoization (WD) [WD03]. The choice of the algorithms

4 Marco Almeida et al.
was due to the disparate worst-case complexities and doubts about the practical behaviour of each algorithm. The input data was obtained with random automata generators. For the (initially-connected) deterministic ﬁnite automata we used a uniform random generator and thus our results are statistically accurate. Lacking an equivalent uniform random generator for NFAs, we implemented a non-uniform one. Although not statistically signiﬁcant, the results in this case are still fairly interesting.
The text is organised as follows. In Section 2 we present some deﬁnitions and the notation used throughout the paper. In Section 3 we describe each of the algorithms, brieﬂy explain them, and present the respective pseudo-code. In Section 4 we describe the generation methods of the random automata. In Section 5 we present the experimental results and in Section 6 we analyze these results. Finally, on Section 7 we summarize and compare our results with previous work.

2 Preliminaries

A deterministic ﬁnite automaton (DFA) D is a tuple (Q, Σ, δ, q0, F ) where Q is

a ﬁnite set of states, Σ is the input alphabet (any non-empty set of symbols),

δ : Q × Σ → Q is the transition function, q0 is the initial state and F ⊆ Q is the set of ﬁnal states. When the transition function is total, the automaton D is said

to be complete. Any ﬁnite sequence of alphabet symbols a ∈ Σ is a word. Let Σ

denote the set of all words over the alphabet Σ and denote the empty word. We deﬁne the extended transition function δˆ : Q × Σ → Q in the following way: δˆ(q, ) = q; δˆ(q, xa) = δ(δˆ(q, x), a). A state q ∈ Q of a DFA D = (Q, Σ, δ, q0, F ) is called accessible if δˆ(q0, w) = q for some w ∈ Σ . If all states in Q are accessible, a
complete DFA D is called (complete) initially-connected (ICDFA). The language

accepted by D, L(D), is the set of all words w ∈ Σ such that δˆ(q0, w) ∈ F . Two DFAs D and D are equivalent if and only if L(D) = L(D ). A DFA is

called minimal if there is no other equivalent DFA with fewer states. Given

a DFA D = (Q, Σ, δ, q0, F ), two states q1, q2 ∈ Q are said to be equivalent, denoted q1 ≈ q2, if for every w ∈ Σ , δˆ(q1, w) ∈ F ⇔ δˆ(q2, w) ∈ F . Two

states that are not equivalent are called distinguishable. The equivalent minimal

automaton D/ ≈ is called the quotient automaton, and its states correspond to

the equivalence classes of ≈. It is proved to be unique up to isomorphism.

A non-deterministic ﬁnite automaton (NFA) is also a tuple (Q, Σ, ∆, I, F ),

where I is a set of initial states and the transition function is deﬁned as ∆ : Q × Σ → 2Q. Just like with DFAs, we can deﬁne the extended transition function ∆ˆ : 2Q × Σ → 2Q in the following way: ∆ˆ(S, ) = S; ∆ˆ(S, xa) = q∈∆ˆ(S,x) δ(q, a).
The language accepted by N is the set of all words w ∈ Σ such that ∆ˆ(I, w) ∩

F = ∅. Every language accepted by some NFA can also be described by a DFA.

The subset construction method takes a NFA A as input and computes a DFA

D such that L(A) = L(D). This process is also referred to as determinization and has a worst-case running time complexity of O 2|Q| .

Following Leslie [Les95], we deﬁne the transition density of an automaton

A

=

(Q, Σ, ∆, I, F )

as

the

ratio

t |Q|2 |Σ |

,

where

t

is

the

number

of

transitions

in

On the Performance of Automata Minimization Algorithms

5

A. This density function is normalised, giving always a value between 0 and 1.

We also deﬁne deterministic density as the ratio of the number of transitions t

to the number of transitions of a complete DFA with the same number of states

and

symbols,

i.e.,

t |Q||Σ|

.

The reversal of a word w = a0a1 · · · an, written wR, is an · · · a1a0. The re-

versal of a language L ⊆ Σ is LR = {wR | w ∈ L}. Further details on regular

languages can be found in the usual references (Hopcroft [HMU00] or Kozen

[Koz97], for example).

3 Minimization Algorithms
Given an automaton, to obtain the associated minimal DFA we must compute the equivalence relation ≈ as deﬁned in Section 2. The computation of this relation is the key diﬀerence of the several minimization algorithms. Moore’s algorithm and its variants, for example, aim to ﬁnd pairs of distinguishable states. H, on the other hand, computes the minimal automaton by reﬁning a partition of the states’ set.
Of the three minimization algorithms we compared, H has the best worstcase running time analysis. B is simple and elegant, and, despite its exponential worst-case complexity, it is supposed to frequently outperform other algorithms (including H). B also has the particularity of being able to take both DFAs and NFAs as input. W can be halted at any time yielding a partially minimized automaton. The improved version, WD, includes the use of full memoization. Because one of our motivations was to check the minimality of a given automaton, not to obtain the equivalent minimal one, this algorithm was of particular interest.

3.1 Hopcroft’s Algorithm (H)
H [Hop71], published in 1971, achieves the best known running time worst-case complexity for minimization algorithms. It runs on O(kn log n) time for a DFA with n states and an alphabet of size k. Let D = (Q, Σ, δ, q0, F ) be a DFA. H, proceeds by reﬁning the coarsest partition until no more reﬁnements are possible. The initial partition is P = {F, Q−F } and, at each step of the algorithm, a block B ∈ P and a symbol a ∈ Σ are selected to reﬁne the partition. This reﬁnement process splits each block B of the partition according to whether the states of B , when consuming the symbol a, go to a state which is in B or not. Formally, we call this procedure split and deﬁne it by
split(B , B, a) = (B ∩ δˇ−1(B, a), B ∩ δˇ−1(B, a))
where δˇ(S, a) = q∈S δ(q, a). The algorithm terminates when there are no more blocks to reﬁne. In the
end, each block of the partition is a set of equivalent states. Because, for any two blocks B, B ∈ P , every state q ∈ B is distinguishable from any state

6 Marco Almeida et al.
q ∈ B , the elements of P represent the states of a new minimal DFA. The complete pseudo-code is presented in Algorithm 1.1.
def hopcroft ( ) : L = {} i f | F | < | Q−F | : P = {Q−F , F } ; L = {F} else : P = {F , Q−F } ; L = {Q−F} while L = ∅ : S = extract (L) for a in Σ : for B in P: ( B1 , B2 ) = s p l i t (B, S , a ) P = P − {B} ; P = P ∪ {B1 } ; P = P ∪ {B2 } i f | B1 | < | B2 | : L = L ∪ {B1 } else : L = L ∪ {B2 } return P Algorithm 1.1. Hopcroft’s algorithm (H).
The set L contains blocks of P not yet treated. The extract procedure removes one element of L to be used in the splitting process. The choice of the element does not inﬂuence the correctness of the algorithm.
3.2 Brzozowski’s Algorithm (B)
B [Brz63] is based on two successive reverse and determinization operations and the full pseudo-code is presented (in one single line!) on Algorithm 1.2.
def brzozowski ( fa ) : return det ( rev ( det ( rev ( fa ) ) ) ) Algorithm 1.2. Brzozowski’s algorithm (B).
Having to perform two determinizations, the worst-case running time complexity of B is exponential. Watson’s thesis, however, presents some surprising results about B practical performance, usually outperforming H. As for the peculiar way that this algorithm computes a minimal DFA, Watson assumed it to be unique and, in his taxonomy, placed it apart all other algorithms. Later, Champarnaud et al. [CKP02] analysed the way the sequential determinizations perform the minimization and showed that it does compute state equivalences.
3.3 An Incremental Algorithm (W)
Watson presented an incremental DFA minimization algorithm (W) [Wat95]. This algorithm can be halted at any time yielding a partially minimized DFA that recognises the same language as the input DFA. Later, Watson and Daciuk presented an improved version of the same algorithm (WD) [WD03] which makes

On the Performance of Automata Minimization Algorithms

7

use of full memoization. While the ﬁrst algorithm has a worst-case exponential running time, the memoized version yields a O(n2α((n2)) algorithm, where α is the inverse of the Ackermann function. Since α(x) ≤ 5 for any x ≤ 2216 , α(x) can be considerend a constant for all “practical” values of x. It was not clear, however, that this algorithm would outperform H as the experimental results in [WD03] seemed to point to. Since the use of memoization introduces some considerable overhead in the algorithm, we wanted to discover at what point this extra work begins to payoﬀ.
W uses an auxiliary function, equiv, to test the equivalence of two states. The third argument, an integer k, is used to control the recursion depth only for matters of eﬃciency. Also for matters of eﬃciency, a variable S that contains a set of presumably equivalent pairs of states is made global. The pseudo-code for a non-memoized, specialized for ICDFAs, implementation of equiv is presented in Algorithm 1.3. The memoized algorithm (WD) is quite extensive and can be found in Watson and Daciuk’s paper [WD03].
def equiv (p , q , k ) : if k = 0: return ( p in F and q in F) or ( not p in F and not q in F) elif (p , q) in S : return True else : eq = ( p in F and q in F) or ( not p in F and not q in F) S = S ∪ {(p,q)} for a in Σ : i f not eq : return False eq = eq and e q u i v ( δ(p, a) , δ(q, a) , k−1) S = S − {(p,q)} return eq
Algorithm 1.3. Pairwise state equivalence algorithm.
Having a method to verify pairwise state equivalence, it is possible to implement a test that calls equiv for every pair of states and returns F alse if some pair is found to be equivalent.

4 Random Automata Generation
Even if we consider only (non-isomorphic) ICDFAs, the number of automata with n states over an alphabet of k symbols grows so fast [RMA05] that trying to minimize every one is not feasible, even for small values of n and k. The same applies to NFAs. In order to compare the practical performance of the minimization algorithms, we must have an arbitrary quantity of “good” randomly generated automata available, i.e. the samples can not be unbiased. This can achieved with a uniform random generator. We used the DFA string representation and random generation method proposed by Almeida et al. [AMR07]. This approach, unlike the one by Bassino et al. [BN07], does not require a rejection

8 Marco Almeida et al.
step. The generator produces a string that is a canonical representation of an ICDFA with n states and k symbols without ﬁnal states information, as described by Reis et al. [RMA05,AMR07]. Given an order over Σ, it is possible to deﬁne a canonical order over the set of states by traversing the automaton in a breadth-ﬁrst way choosing at each node the outgoing edges using the order of Σ. In the string representation, each of the i blocks, for 1 ≤ i ≤ n, corresponds to the transitions from the state i − 1. The random generator produces the random strings from left to right, taking into account the number of ICDFAs that, at a given point, would still be possible to produce with a given preﬁx. The set of ﬁnal states is computed by generating an equiprobable bitstream of size n and considering ﬁnal all the states that correspond to a non-zero position.
Lacking a uniform random generator for NFAs, we implemented one which combines the van Zijl bit-stream method, as presented by Champarnaud et al. [CHPZ04], with one of Leslie’s approaches [Les95], which allows us both to generate initially connected NFAs (with one initial state) and to control the transition density. Leslie presents a “generate-and-test” method which may never stop, so we added some minor changes that correct this situation. A brief explanation of the random NFA generator follows. Suppose we want to generate a random NFA with n states over an alphabet of k symbols and a transition density d. Let the states (respectively the symbols) be named by the integers 0, . . . , n − 1 (respectively 0, . . . , k − 1). A sequence of n2k bits describes the transition function in the following way: the occurrence of a non-zero bit at the position ink + jk + a denotes the existence of a transition from state i to state j labelled by the symbol a.
Starting with a sequence of zero bits, the ﬁrst step of the algorithm is to create a connected structure and thus ensure that all the states of the ﬁnal NFA will be accessible. In order to do so, we deﬁne the ﬁrst state as 0, mark it as visited, generate a transition from 0 to any not-visited state i, and mark i as visited. Next, until all states are marked as visited, randomly choose an already visited state q1, randomly choose a not-visited state q2, add a transition from q1 to q2 (with a random), and mark q2 as visited. At this point we have an initially connected NFA and proceed by adding random transitions. Until the desired density is achieved, we simply select one of the bitstream’s zero bits and set it to one. By maintaining a list of visited states on the ﬁrst step and keeping record of the zero bits on the second step, we avoid generating either a disconnected NFA or a repeated transition and guarantee that the algorithm always halts. The set of ﬁnal states can be easily obtained by generating an equiprobable bitstream of size n and considering ﬁnal all the states that correspond to a non-zero position in the bitstream.
5 Experimental Results
To compare algorithms is always a diﬃcult problem. The choice of the programming language, implementation details, and the hardware used may harm the rigour of any benchmark. In order to produce realistic results, the input data

On the Performance of Automata Minimization Algorithms

9

should be random so that it represents a typical usage of the algorithm and the

test environment should be identical for all benchmarks. We implemented all the

algorithms in the Python 2.4 programming language, using similar data struc-

tures whenever possible. All the tests were executed in the same computer, an

Intel R Xeon R 5140 at 2.33GHz with 2GB of RAM. We used samples of 10.000

automata, with 5 ≤ n ≤ 100 states and alphabets with k ∈ {2, 5, 10, 20} sym-

bols. For the data sets obtained with the uniform random generator, the size of

each sample is suﬃcient to ensure results with a 95% conﬁdence level within a

1%

error

margin.

It

is

calculated

with

the

formula

n

=

(

z 2

)2,

where

z

is

obtained

from the normal distribution table such that P (−z < Z < z)) = γ, is the error

margin, and γ is the desired conﬁdence level.

5.1 Random ICDFA Minimization
On his thesis, Watson used a fairly biased sample. It consisted of 4833 DFAs of which only 7 had 23 states. As Watson himself states, being constructed from regular expressions, the automata “... are usually not very large, they have relatively sparse transition graphs, and the alphabet frequently consists of the entire ASCII character set.”. On their paper on the incremental minimization algorithm [WD03], Watson and Daciuk also present some performance comparisons of automata minimization algorithms. They used four diﬀerent data sets, one from experiments on ﬁnite-state approximation of context-free grammars and three that were automatically generated. These are not, however, uniform random samples, and thus, do not represent a typical usage of the algorithms.
The graphics on Figure 1 show the running times of the tested algorithms while minimizing samples of 10.000 random ICDFAs. This running time is the total time required for minimizing an entire sample and was limited to 24 hours. The batches which did not ﬁnish under this limit appear with the value −1, in order to be distinguished from those that actually took almost 0 seconds.
For small alphabets (k = 2), H is always the fastest. When the alphabet size grows (k ≥ 5), however, H is clearly outperformed by the WD, which was over twice as fast as H when minimizing ICDFAs with an alphabet of size k ≥ 10. W showed itself quite slow in all tests. It is important to point out that for k ≥ 5 all the automata were already minimal and so the speed of the incremental algorithm can not be justiﬁed by the possibility of halting whenever two equivalent states are found. The fact that almost all ICDFAs are minimal was observed by several authors, namely Almeida et al. [AMR07] and Bassino et al.[BDN07]. As Watson himself stated, the incremental algorithm may show exponential performance for some DFAs. This was the case in one of our tests. For the sample of 5 symbols and 15 states WD took an unusual amount of time. B is never the fastest algorithm. In fact, even for small alphabets it was not possible to use it on ICDFAs with more than 15 states.

10 Marco Almeida et al.

2 symbols time - log(s) 5 4 3 2 1 0
10 20 50
10 symbols time - log(s) 5 4 3 2 1 0
10 20 50
H

5 symbols time - log(s) 5 4 3 2 1 0 100 10 20 50 # states
20 symbols time - log(s) 5 4 3 2 1 0 100 10 20 50 # states
W WD B

100 # states
100 # states

Fig. 1. Running time results for the minimization of 10.000 ICDFAs with k ∈ {2, 5, 10, 20}.

5.2 Random NFAs Minimization
The next set of graphics shows the execution times of the three algorithms when applied to a set of 10.000 random NFAs. The running time limit for all algorithms was set to 15 hours and, again, the batches that did not ﬁnish under this time limit are shown with the value −1. It is important to note that the NFA generator we used is not a uniform one, and so we can not prove that each sample is actually a good representative of the universe. Because we are dealing with NFAs, the transition density is an important factor and so each sample was generated with three diﬀerent transition densities (d): 0.2, 0.5, and 0.8. For both the WD and H, which are only able to minimize DFAs, we also accounted for the time spent in the subset construction method. For alphabets with two symbols there are no signiﬁcant diﬀerences in any of the algorithms’ general performance, although B is usually the fastest. H outperforms B for less than 4% only when d = 0.5 and n ∈ {50, 100}. For alphabets with k = 5, B is always the fastest and, except for occasional cases, H is slightly faster than the incremental algorithm. When the alphabet size increases, B’s performance becomes quite remarkable. For an alphabet with size k ∈ {10, 20}, B is deﬁnitively the fastest, being the only algorithm to actually ﬁnish the minimization process of almost all random samples within the 15 hour limit. As for H and WD, except for two cases, there are no signiﬁcant performance diﬀerences. For d = 0.2 and

On the Performance of Automata Minimization Algorithms

11

Transition density d = 0.2

Transition density d = 0.2

time - log(s) 4

time - log(s) 4

33

22

11

0 5

10 20 50 100

0 5

# states

10 20 50 100 # states

Transition density d = 0.5

Transition density d = 0.5

time - log(s) 4

time - log(s) 4

33

22

11

0 5

10 20 50 100

0 5

# states

10 20 50 100 # states

Transition density d = 0.8

Transition density d = 0.8

time - log(s) 4

3

2

1

0 5

10

20

time - log(s) 4

3

2

1

0

50 100

5

# states

10 20

50 100 # states

H WD B

Fig. 2. Running time results for the minimization of 10.000 NFAs with k = 2 (left) and k = 5 (right).

n = 10 all the algorithms showed a particularly bad performance. For k = 5 only

B ﬁnished the minimization process (taking an unusual high amount of time)

and for k ∈ {10, 20} none of the algorithms completed the minimization process

within the 15 hour time limit. This result corroborates Leslie’s conjecture [Les95]

about the number of states obtained with the subset construction method for a

given deterministic density dd. Leslie’s conjecture states that randomly generated automata exhibit the maximum execution time and the maximum number

of states at an approximate deterministic density of 2.0. While generating the

random

NFAs,

we

considered

the

transition

density

d

=

t n2

k

,

which

is

related

to

the

deterministic

density

dd

=

t nk

by

dd

=

nd.

It

is

easy

to

see

that

in

our

case

dd = nd = 10 × 0.2 = 2.0, which will make the subset construction computation-

ally expensive. In order to achieve the same exponential behaviour in the subset

method for d ∈ {0.5, 0.8} the number of states would have to be n ∈ {4, 2.5}, but

for such a small number of states the exponential blowup is not meaningful. This

12 Marco Almeida et al.

Transition density d = 0.2

Transition density d = 0.2

time - log(s) 4

3

2

1

0 5

10

20

time - log(s) 4

3

2

1

0

50 100

5

# states

10 20

50 100 # states

Transition density d = 0.5

Transition density d = 0.5

time - log(s) 4

3

2

1

0 5

10

20

time - log(s) 4

3

2

1

50 100

0 5

# states

10 20

50 100 # states

Transition density d = 0.8

Transition density d = 0.8

time - log(s) 4

3

2

1

0 5

10

20

time - log(s) 4

3

2

1

50 100

0 5

# states

10 20

50 100 # states

H WD B

Fig. 3. Running time results for the minimization of 10.000 NFAs with k = 10 (left) and k = 20 (right).

explains why there are no similar results for the test batches with d ∈ {0.5, 0.8}. Considering we used a variation of one of Leslie’s random NFA generators, this result does not come with any surprise.
6 Analysis of the Results
In this work, we experimentally compare four automata minimization algorithms (H, W, WD and B). As data sets we use two diﬀerent types of randomly generated automata (ICDFAs and NFAs) with a range of diﬀerent number of states and symbols. The ICDFAs’ data set was obtained using a uniform random generator and is large enough to ensure a 95% conﬁdence level within a 1% error margin. For ICDFAs with only two symbols, H is the fastest algorithm. As the

On the Performance of Automata Minimization Algorithms

13

alphabet size grows, WD begins to perform better than H. With alphabets larger than 10, WD becomes clearly faster. As for W and B algorithms, we can safely conclude that neither performs well, regardless of the number of states and symbols. As for the NFAs, it is important to note that the random generator used was not a uniform one, and thus does not have the same statistical accuracy as the ﬁrst one. B is deﬁnitively the fastest algorithm. Both H and WD consistently show equally slower results.
Since all these algorithms make use of the subset construction at least once, the reason for B’s good performance is even less evident. It would be interesting to make an average-case running time complexity analysis for the DFA reversal, and thus possibly explain B’s behaviour with ICDFAs minimization.

7 Comparison with Related Work
In this ﬁnal section we summarise and compare our experiments with some of the results of the works cited before.
Bruce Watson implemented ﬁve minimization algorithms: two versions of Moore’s algorithm as well as H, W and B. As we have mentioned before, the data set then used was fairly biased and his results lead him to conclude that the two Moore based algorithms perform very poorly and that B normally outperforms the other two. Baclet et al. [BP06] implemented H with two diﬀerent queue strategies: LIFO and FIFO. The implementations were tested with several random (non-uniform) automata having thousands of states but very small alphabets (k ≤ 4), concluding that the LIFO strategy is better, at least for alphabets of one or two symbols. Bassino et al. [BDN07] used an ICDFAs’ uniform random generator based on a Boltzmann sampler to create a data set and compared the performance of the H (with the two strategies refereed above) and Moore’s algorithms. The automata generated have up to some thousands of states for alphabets of size 2. Their results are statistically accurate, and indicate that Moore’s algorithm is, for the average case, more eﬃcient than H. Moreover, no clear diﬀerence were found between the two queue strategies for H. These results are interesting because they neither corroborate the results of the works mentioned above nor the general idea that H outperforms Moore’s algorithm in practice. It remains unstudied a comparison between Moore’s algorithm with WD using a uniformly generated data set. Finally, Tabakov and Vardi [TV05] studied H and B performance with a data set of random NFAs. The random model they used is similar to the one we describe in Section 4 but considering a deterministic density, dd. They choose 0 ≤ dd ≤ 2.5 and the samples were relatively small: 200 automata with n < 50 and k = 2. The conclusion was that H is faster for low-density automata (dd < 1.5), while B is better for high-density automata. For the example studied of n = 30 the deterministic density dd < 1.5, corresponds to a normalised transition density that we used, d < 0.05. This phase transition may be due to the fact that for such a low normalised transition density the probability of a connected NFA being deterministic is very high.

14 Marco Almeida et al.
References
[AMR07] M. Almeida, N. Moreira, and R. Reis. Enumeration and generation with a string automata representation. Theoretical Computer Science, 387(2):93– 102, 2007.
[BDN07] F. Bassino, J. David, and C. Nicaud. A library to randomly and exhaustively generate automata. In Implementation and Application of Automata, volume 4783 of LNCS, pages 303–305. Springer-Verlag, 2007.
[BN07] F. Bassino and C. Nicaud. Enumeration and random generation of accessible automata. Theoretical Computer Science, 381(1-3):86–104, 2007.
[BP06] M. Baclet and C. Pagetti. Around hopcroft’s algorithm. pages 114–125, Taipei, Taiwan, 2006. Springer.
[Brz63] J. A. Brzozowski. Canonical regular expressions and minimal state graphs for deﬁnite events. In J. Fox, editor, Proc. of the Sym. on Math. Theory of Automata, volume 12 of MRI Symposia Series, pages 529–561, NY, 1963.
[CHPZ04] J.-M. Champarnaud, G. Hansel, T. Parantho¨en, and D. Ziadi. Random generation models for nfas. J. of Automata, Lang. and Comb., 9(2), 2004.
[CKP02] J.-M. Champarnaud, A. Khorsi, and T. Parantho¨en. Split and join for minimizing: Brzozowski’s algorithm. In M. Bal´ık and M. Sim´anek, editors, Proc. of PSC’02, Report DC-2002-03, pages 96–104. Czech Technical University of Prague, 2002.
[HMU00] John E. Hopcroft, Rajeev Motwani, and Jeﬀrey D. Ullman. Introduction to Automata Theory, Languages and Computation. Addison Wesley, 2000.
[Hop71] J. Hopcroft. An n log n algorithm for minimizing states in a ﬁnite automaton. In Proc. Inter. Symp. on the Theory of Machines and Computations, pages 189–196, Haifa, Israel, 1971. AP.
[Huf55] D. A. Huﬀman. The synthesis of sequential switching circuits. The Journal of Symbolic Logic, 20(1):69–70, 1955.
[Koz97] D. C. Kozen. Automata and Computability. Undergrad. Texts in Computer Science. Springer-Verlag, 1997.
[Les95] T. Leslie. Eﬃcient approaches to subset construction. Master’s thesis, University of Waterloo, Ontario, Canada, 1995.
[Lho00] O. Lhot´ak. A general data structure for eﬃcient minimization of deterministic ﬁnite automata. Technical report, University of Waterloo, 2000.
[Moo58] E. F. Moore. Gedanken-experiments on sequential machines. The Journal of Symbolic Logic, 23(1):60, 1958.
[Nic00] C. Nicaud. E´tude du comportement en moyenne des automates ﬁnis et des langages rationnels. PhD thesis, Universit´e de Paris 7, 2000.
[RMA05] R. Reis, N. Moreira, and M. Almeida. On the representation of ﬁnite automata. In C. Mereghetti, B. Palano, G. Pighizzini, and D.Wotschke, editors, Proc. of DCFS’05, pages 269–276, Como, Italy, 2005.
[TV05] D. Tabakov and M. Vardi. Evaluating classical automata-theoretic algorithms. In LPAR’05, 2005.
[Wat95] B. W. Watson. Taxonomies and toolkit of regular languages algortihms. PhD thesis, Eindhoven Univ. of Tec., 1995.
[WD03] B. W. Watson and J. Daciuk. An eﬃcient DFA minimization algorithm. Natural Language Engineering, pages 49–64, 2003.

Decidability Results for Mobile Membranes Derived from Mobile Ambients
Bogdan Aman1 and Gabriel Ciobanu1,2
1 Romanian, Academy, Institute of Computer Science Blvd. Carol I no.8, 700505 Ia¸si, Romania
2 “A.I.Cuza” University, Faculty of Computer Science Blvd. Carol I no.11, 700506 Ia¸si, Romania
baman@iit.tuiasi.ro, gabriel@info.uaic.ro
Abstract. Mobile ambients and membrane systems are two new computation models. We investigate some decidability properties in mobile membranes, using also some known decidability results of the mobile ambients.
1 Introduction
Ambients calculus [6] and membrane systems [15] represent two new computation models which are quickly evolving. During the last decade, both of them were emergent research ﬁelds in computer science. Both mobile ambients and mobile membranes (a variant of membrane systems introduced in [11]) have similar structures and common concepts. Both formalisms work with boundaries representing locations, have a hierarchical structure representing the nesting relationship among locations, and describe mobility in complex systems. We consider these new computing models, studying their computability aspects and their relationship.
Mobile ambients are designed to model both mobile computing (provided by mobile devices as laptops or PDA’s), and mobile computation (mobile code like applets or agents moving between computers and devices). An ambient is a location (bounded place) where computation happens. Ambients can be nested, having a hierarchical structure. An ambient has a name, and can move with all the subambients and processes it contains. Ambient calculus is designed to describe distributed and mobile computation. In contrast with other formalisms for mobility such as the π-calculus [13] whose computational model is based on the notion of communication, the ambient calculus is based on the notion of movement. An ambient is the unit of movement. Ambients mobility is controlled by the capabilities in, out, and open. Moreover, in an ambient we have processes which may exchange messages.
Membrane systems are introduced in [14] as a class of parallel computing devices inspired by the biological systems which are complex hierarchical structures, with a ﬂow of materials and information which underlies their functioning. Essentially, the membrane systems (also called P systems) are composed of various compartments with diﬀerent tasks, all of them working simultaneously to

16 Bogdan Aman and Gabriel Ciobanu
accomplish a more general task. There are several variants of membrane systems. The mobile membranes were introduced in [2] where the mobility is expressed through the operations of gemmation and fusion of mobile membranes. In this paper we use mobile membrane systems (also called P systems with mobile membranes) which express mobility using two biologically inspired operations: exocytosis and endocytosis.
The structure of the paper is as follows. In Section 2 we present the ambient calculus. In the last sections of the paper we present the mobile membrane systems and investigate some decidability problems (reachability, boundedness, periodicity) in mobile membranes. Conclusions and references end the paper.

2 Mobile Ambients

In this section we provide a short description of the mobile ambients; more details can be found in [6]. The following table describes the syntax of mobile ambients.

Table 1: Mobile Ambients Syntax

c channel name

n, m

ambient names

x variable

M ::=

capabilities

in n can enter n

out n

can exit n

open n

can open n

P, Q ::=

processes

0 inactivity

M.P

movement

n[P ]

ambient

P |Q

composition

(νn)P

restriction

c! m .P

output action

c?(x).P

input action

∗P replication

Process 0 is an inactive process (it does nothing). A movement M.P is provided by a capability M , followed by the execution of process P . An ambient n[P ] represents a bounded place labelled by n in which a process P is executed. P | Q is a parallel composition of processes P and Q. (νn)P creates a new unique name n within the scope of process P . An output action c! m .P releases a name m on channel c, and then behaves as process P . An input action c?(x). P captures a name from channel c, and binds it to a variable x within the scope of process P . ∗P denotes an unbounded replication of a process P , producing as many parallel copies of process P as needed.
Semantics of the ambient calculus is provided by two relations: structural congruence and reduction. The structural congruence P ≡amb Q relates diﬀerent syntactic representations of the same process; it is also used to deﬁne the reduction relation. The reduction relation P ⇒amb Q describes the evolution of ambients and processes. We denote by ⇒∗amb the reﬂexive and transitive closure of ⇒amb, and by ⇒+amb its transitive closure.
The structural congruence is deﬁned as the least relation over processes satisfying the axioms from the table below:

Decidability Results for Mobile Membranes Derived from Mobile Ambients

17

Table 2: Structural Congruence

P | Q ≡amb Q | P

(P | Q) | R ≡amb P | (Q | R)

∗P ≡amb P | ∗ P

(νn)(νm)P ≡amb (νm)(νn)P if n = m

(νn)(P | Q) ≡amb P | (νn)Q if n ∈/ f n(P )

(νn)m[P ] ≡amb m[(νn)P ] if n = m

P | 0 ≡amb P

(νn)0 ≡amb 0

∗0 ≡amb 0

P ≡amb P

P ≡amb Q implies Q ≡amb P P ≡amb Q, Q ≡amb R implies P ≡amb R P ≡amb Q implies (νn)P ≡amb (νn)Q P ≡amb Q implies P | R ≡amb Q | R P ≡amb Q implies ∗P ≡amb ∗Q P ≡amb Q implies n[P ] ≡amb n[Q] P ≡amb Q implies M.P ≡amb M.Q P ≡amb Q implies c?(x).P ≡amb c?(x).Q P ≡amb Q implies c! m .P ≡amb c! m .Q

The rules from the left side of the table describe the commutativity and associativity of the parallel components, unfolding recursion, stretching of a restriction scope, renaming of bounded names. The rules from the right side describe how structural congruence is propagated across processes. The set f n(P ) of free names of a process P is deﬁned as:

8∅ if P = 0

>

>>>f n(R) ∪ {n} if P = in n.R or P = out n.R or P = open n.R

>

> <
f n(P ) = f n(R) ∪ f n(Q)

or P = n[R] or P = c! n .R if P = R | Q

>

>>>f n(R) − {n} if P = (νn)R

>

>:f n(R)

if P = c?(x).R or P = ∗R

The reduction relation is deﬁned as the least relation over processes satisfying the following set of axioms:

Table 3: Reduction Rules

(In) (Out) (Open) (Com) (Res) (Amb) (Par) (Struct)

n[in m. P | Q] | m[R] ⇒amb m[n[P | Q] | R] m[n[out m. P | Q] | R] ⇒amb n[P | Q] | m[R] open n. P | n[Q] ⇒amb P | Q c! m . P | c?(x). P ⇒amb P | P {m/x} P ⇒amb Q implies (νn)P ⇒amb (νn)Q P ⇒amb Q implies n[P ] ⇒amb n[Q] P ⇒amb Q implies P | R →amb Q | R P ≡ P, P ⇒amb Q, Q ≡ Q implies P ⇒amb Q

The ﬁrst four rules are the one-step reductions for in, out, open and communication. In the rule (Com) we write P {m/x} for the substitution of name m for each free occurrence of variable x in process P . The next three rules propagate reductions across scopes, ambient nesting and parallel composition. The ﬁnal rule allows the use of structural congruence during the evolution given by the reduction relation.

3 Decidability Results for Mobile Membranes
A membrane system consists of a hierarchy of nested membranes, placed inside a distinguishable membrane called skin. The space outside the skin membrane is

18 Bogdan Aman and Gabriel Ciobanu
called environment. A membrane contains multisets of objects, evolution rules, and possibly other membranes. The multisets of objects from a membrane correspond to the chemicals swimming in the solution in the cell compartment, while the rules correspond to the chemical reactions possible in the same compartment. The rules contain target indications specifying the membranes where the new obtained objects are sent. The new objects either remain in the same membrane whenever they have attached a here target, or they pass through membranes in two directions: they can be sent out of the membrane, or can be sent in one of the nested membranes which is precisely identiﬁed by its label. In one step, the objects can pass only through one membrane. A membrane without any other membranes inside is called elementary, while a membrane containing other membranes is a composite one.
The membrane systems are synchronous: at each time unit of a global clock, a transformation of the system takes place by applying the rules in a nondeterministic and maximally parallel manner. This means that the objects and the rules involved in such a transformation are chosen in a nondeterministic way, and the application of rules is performed in a maximally parallel way. After a choice was made, no rule can be applied anymore in the same evolution step: there are not enough objects and membranes available for any rule to be applied. Several membranes can evolve in parallel. Many variants of this basic model are discussed in the literature [7, 15]. In this paper we use mobile membrane systems. This is a variant of membrane systems with active membranes, but having none of the features like polarizations, label change and division of non-elementary membranes.
Reachability is the problem of deciding whether a system may reach a given conﬁguration during its execution. This is one of the most critical properties in the veriﬁcation of systems; most of the safety properties of computing systems can be reduced to the problem of checking whether a system may reach a “unintended state”.
Boundedness is a property of systems whose production and consumption of resources may be bounded. From a biological point of view, boundedness can be interpreted as a property of sustainable development, in the sense that a cell can accumulate and work with only a ﬁnite amount of material.
When working with Petri nets, reachability and boundedness are two general properties of interest. Given a net with initial marking µ0, we say that the marking µ is reachable if there exists a sequence of ﬁrings µ0 → µ1 → . . . µn = µ of the net. We say that a net is bounded if the set of reachable markings is ﬁnite. A k-bounded net implies that there exists some integer k bounding the number of tokens which may be present at each place.
The boundedness and reachability problems are decidable in Petri nets, even if they tend to have a very large complexity in practice. A good survey of the known decidability issues for Petri nets is given in [9].
We use various results to investigate reachability and boundedness for mobile membranes.

Decidability Results for Mobile Membranes Derived from Mobile Ambients

19

3.1 Mobile Membranes with Replication

We prove in [1] that reachability in the class of mobile membranes deﬁned by Deﬁnition 1 can be decided by reducing it to the reachability problem of a version of pure and public ambient calculus from which the open capability has been removed. It is proven in [4] that the reachability for this fragment of ambient calculus is decidable by reducing it to marking reachability for Petri nets, which is proven to be decidable in [12]. Problems like reachability and boundedness are investigated in [8] for other classes of P systems, namely for extensions of PB systems with volatile membranes. In what follows we present a sketch of the work done in [1], and we extend it with new results.
First we deﬁne a new class of mobile membranes with replication rules.

Deﬁnition 1. A mobile membrane system with replication rules is a structure = (V ∪ V , H ∪ H, µ, w1, . . . , wn, R), where:

1. n ≥ 1 represents the initial degree of the system; 2. V ∪ V is an alphabet (its elements are called objects), where V ∩ V = ∅; 3. H ∪ H is a ﬁnite set of labels for membranes, where H ∩ H = ∅; 4. µ is a membrane structure, consisting of n membranes, labelled (not neces-
sarily in a one-to-one manner) with elements of H; 5. w1, w2, . . . , wn are multisets of objects from V ∪V placed in the n membranes
of the system; 6. R is a ﬁnite set of developmental rules, of the following forms:

(a) [a↓→ a↓ a↓]h, for h ∈ H, a↓∈ V , a↓∈ V ; replication rule The objects a↓ are used to create new objects a↓ without being consumed.

(b) [a↓ a↓→ a↓]h, for h ∈ H, a↓∈ V , a↓∈ V ; consumption rule

The objects a↓ are consumed.

(c) [a↑→ a↑ a↑]h, for h ∈ H, a↑∈ V , a↑∈ V ; replication rule The objects a↑ are used to create new objects a↑ without being consumed.

(d) [a↑ a↑→ a↑]h, for h ∈ H, a↑∈ V , a↑∈ V ; The objects a↑ are consumed.

consumption rule

(e) [ a↓ ]h [ ]a → [ [ ]h ]a, for a, h ∈ H, a↓∈ V ; endocytosis

An elementary membrane labelled h enters the adjacent membrane la-

belled a, under the control of object a↓. The labels h and a remain un-

changed during this process; however the object a↓ is consumed during

the operation. Membrane a is not necessarily elementary.

(f ) [ [ a↑ ]h ]a → [ ]h [ ]a, for a, h ∈ H, a↑∈ V ; exocytosis

An elementary membrane labelled h is sent out of a membrane labelled

a, under the control of object a↑. The labels of the two membranes re-

main unchanged; the object a↑ of membrane h is consumed during the

operation. Membrane a is not necessarily elementary.

(g) [ ]h → [ ]h[ ]h for h ∈ H, h ∈ H

division rules

An elementary membrane labelled h is divided into two membranes la-

belled by h and h having the same objects.

20 Bogdan Aman and Gabriel Ciobanu
In 2., V ∩ V = ∅ states that the objects from V can participate only in rules of type (a) and (b). Similarly in 3., H ∩ H = ∅ states that the membranes having labels from the set H can participate only in rules of type (g).
The rules are applied using the following principles:
1. In biological systems molecules are divided into classes of diﬀerent types. We make the same decision here and split the objects into four classes: a ↓ - objects which control the endocytosis, a ↑ - objects which control the exocytosis, and a ↓, a ↑ - objects which produce new objects from the ﬁrst two classes without being consumed.
2. All the rules are applied in parallel, non-deterministically choosing the rules, the membranes, and the objects in such a way that the parallelism is maximal; this means that in each step we apply a set of rules such that no further rule, no further membranes and objects can evolve at the same time.
3. A membrane a from each rule of type (e) and (f ) is said to be passive, while membrane h is said to be active. In any step of a computation, any object and any active membrane can be involved in at most one rule, but the passive membranes are not considered involved in the use of rules (hence they can be used by several rules at the same time).
4. When a membrane is moved across another membrane, by endocytosis or exocytosis, its whole content (its objects) is moved.
5. If a membrane is divided, then its content is replicated in the two new copies. 6. The skin membrane can never be divided.
According to these rules, we get transitions among the conﬁgurations of the system. For two mobile membrane systems M and N , we say that M reduces to N if there is a sequence of rules applicable in the membrane system M in order to obtain the membrane system N .
Theorem 1 ([1]). For two arbitrary mobile membrane systems M1 and M2, it is decidable whether M1 reduces to M2.
The main steps of the proof are as follows:
1. we reduce mobile membranes systems to pure and public mobile ambients without the capability open;
2. we show that the reachability problem for two arbitrary mobile membranes can be expressed as the reachability problem for the corresponding mobile ambients;
3. we use the result that the reachability problem is decidable for a fragment of pure and public mobile ambients without the capability open, using the following decidability theorem from Petri nets:
Theorem 2 ([12]). For all Petri nets P , for all markings m, m for P , one can decide whether m is reachable from m.
We deﬁne the following translation steps:

Decidability Results for Mobile Membranes Derived from Mobile Ambients

21

1. any object a↓ is translated into a capability in a; 2. any object a↑ is translated into a capability out a; 3. any object a↓ is translated into a replication !in a; 4. any object a↑ is translated into a replication !out a; 5. a membrane h is translated into an ambient h; 6. an elementary membrane h is translated into a replication !h[ ] where all the
objects inside membrane h are translated into capabilities of ambient h by using the above steps.
A correspondence exists between the rules from mobile membranes and the rules from mobile ambients:
rule (e) corresponds to rule (In); rule (f ) corresponds to rule (Out); rules (a), (b), c and (d) correspond to instances of the structural congruence rule ∗P ≡amb P | ∗ P , namely (a) and (c) for creation of new instances, while (b) and (d) for consuming existing instances. If we consider a mobile membrane system M , we denote by T (M ) the mobile ambient obtained using the above translation steps. For example, starting from the membrane system M = [m↓ m↑]n[ ]m, we obtain T (M ) = n[in m | out m] | m[ ].
Proposition 1 ([1]). Given two mobile membrane systems M and N , M reduces to N by applying one rule if and only if T (M ) reduces to T (N ) by applying only one rule.
Theorem 3 ([1]). For two arbitrary ambients A and B of the restricted fragment, it is decidable whether A reduces to B.
By restricted fragment we refer to the class of mobile ambients obtained through translation from the class of mobile membrane systems from Deﬁnition 1. As a corollary, we prove that periodicity is a decidable property. Periodicity is a central notion in biological processes which have a periodic or quasi-periodic evolution, such as the ATP cycle, for example.
Theorem 4 (Periodicity). Given a mobile membrane system and an initial conﬁguration µ0, if the system evolves it is decidable whether the system is periodic.
Proof. From Theorem 1, considering µ0 both as an initial and as a target conﬁguration.
We say that a mobile membrane system, with an initial conﬁguration µ0, is bounded if there are only ﬁnitely many conﬁgurations which are reachable starting from µ0. Since we have obtained the translation of the class of mobile membrane systems with replication into Petri nets, the following result can be viewed as a corollary.
Theorem 5 (Boundedness). Given a mobile membrane system and an initial conﬁguration µ0, it is decidable whether the system is bounded.

22 Bogdan Aman and Gabriel Ciobanu
The decidability of the deadlock problem is also a consequence of the encoding into the Petri nets.
Theorem 6 (Non-termination). Given a mobile membrane system and an initial conﬁguration µ0, it is decidable whether all computations starting with µ0 are inﬁnite.
For the decidability results from Petri nets we refer to [9].
3.2 Mobile Membrane Systems with Dissolution
A mobile membrane system with dissolution provides, in addition to the rules from Deﬁnition 1, a new rule of the form:
aδ [ ]a → ε where ε is the empty multiset. The intuitive meaning of this new rule is that after applying it, the membrane a is dissolved and its content, including the inner membranes (if any), are moved to the parent of membrane a.
In order to simulate this rule in mobile ambients we use the following translation step: the object aδ is translated into a capability open a. The class of pure mobile ambients containing the capability open is shown to be undecidable in [5]. Using this result and the fact that the mobile membrane systems from Deﬁnition 1 together with the dissolution rule can be encoded in pure mobile ambients with the capability open, we obtain the following result:
Theorem 7. Reachability is undecidable in mobile membrane systems with dissolution rules.
4 Termination in Mobile Membrane Systems
A computation terminates if all its reduction sequences are of ﬁnite length. Termination is an interesting property in concurrency. For some conﬁgurations and sets of rules, we may want to know that an answer is eventually produced. Termination alone does not guarantee that an answer is eventually produced since other properties, e.g. deadlock-free, are also involved. A ﬁrst attempt to formalize some halting conditions has been done quite recently in [10], by deﬁning certain sets of conﬁgurations which satisfy some conditions.
In order to assure that a computation terminates, the set of rules applied to a membrane system must not contain rules which produce periodicity in the system. In the rest of this section we study some of rules that can produce periodicity.
If is a membrane system, then i denoted the membrane systems obtained after i steps. Inspired from [3] we deﬁne in what follows the periodicity of a membrane system:
Deﬁnition 2. A membrane systems is periodic if i= for some i > 0.

Decidability Results for Mobile Membranes Derived from Mobile Ambients

23

Deﬁnition 3. A membrane systems is eventually periodic if i+k= k for some i, k > 0. In this case k is called the transient and i the period.
Generally this can be described as:

u

u1,1 u1,2 ...................... u1,k1

u2,1 u2,2 ...................... u2,k2 ...................... ......................
uj,k1 uj,k2...................... uj,kn ...................... ......................

u
In the ﬁgure above all the objects from line 1 are obtained using only objects from the initial multiset u, then the multisets from line 2 are obtained using only objects obtained in the ﬁrst line, and so on. A special case is when at each step we apply a single rule, namely we have
u1 → u2, u3 → u4 . . . u2n−1 → u2n, with u1, . . . , u2n ∈ V ∗, u2i+1 u2i, for i ∈ {1, . . . , n − 1} and u1 = u2n
The simplest case has the following form u → u, where u ∈ V ∗
for which if the precondition is satisﬁed, then it can be applied forever.
5 Conclusion
Ambient calculus is a process algebra using interleaving semantics, compositionality and bisimulation. Membrane computing is a branch of natural computing deﬁning computation in the Turing sense, i.e., making use of automata, formal languages and complexity tools. This paper presents some existing computability results of the mobile ambient calculus and mobile membrane systems, and emphasizes on a formal relationship between mobile ambients and mobile membranes.
We investigate the problem of reaching a certain conﬁguration of a system of mobile membranes starting from another conﬁguration. In order to do this we use a result of [4] where the reachability problem for the pure and public ambient calculus without open capability is proven to be decidable. The same problem is tackled in [5], where the authors do not take into account the replication of ambients which in our case is used to simulate the division rules in mobile

24 Bogdan Aman and Gabriel Ciobanu
membranes. We proved that the reachability can be decided by reducing this problem to the reachability problem for a fragment of ambient calculus. Starting from this approach, some other decidability results with respect to boundedness, periodicity and termination are proven.
Acknowledgments
We thank Oana Agrigoroaiei for her useful remarks and help in improving early drafts of the paper.
References
1. B. Aman, G. Ciobanu. On the Reachability Problem in P Systems with Mobile Membranes. Prooceedings of the Eight Workshop on Membrane Computing, 111121, 2007.
2. D. Besozzi, C. Zandron, G. Mauri, N. Sabadini. P Systems with Gemmation of Mobile Membranes. Italian Conference on Theoretical Computer Science, Lecture Notes in Computer Science vol.2202, Springer, 136-153, 2001.
3. L. Bianco, F. Fontana, G. Franco, V. Manca. P Systems for Biological Dynamics. Applications of Membrane Computing, Springer, 83-128, 2005.
4. I. Boneva, J.-M. Talbot. When Ambients Cannot be Opened. Foundations of Software Science and Computation Structures, Lecture Notes in Computer Science vol.2620, Springer, 169-184, 2003.
5. N. Busi, G. Zavattaro. Deciding Reachability in Mobile Ambients. Lecture Notes in Computer Science, Springer vol.3444, 248-262, 2005.
6. L. Cardelli, A. Gordon. Mobile Ambients. Theoretical Aspects of Computer Software, Lecture Notes in Computer Science vol.1378, Springer, 140-155, 1998.
7. G. Ciobanu, Gh. P˘aun, M.J. P´erez-Jim´enez. Application of Membrane Computing, Springer, 2006.
8. G. Delzanno, L. Van Begin. On the Dynamics of PB Systems with Volatile Membranes. Proceedings of the Eight Workshop on Membrane Computing, 279-300, 2007.
9. J. Esparza, M. Nielson. Decidability issues for Petri nets - a survey. Journal of Informatik Processing and Cybernetics, vol.30, 143-160, 1994.
10. R. Freund, S. Verlan. A Formal Framework for P Systems. Prooceedings of the Eight Workshop on Membrane Computing, 317-330, 2007.
11. S.N. Krishna, Gh. P˘aun. P Systems with Mobile Membranes. Natural Computing, vol.4(3), Springer, 255-274, 2005.
12. E.W. Mayr. An Algorithm for the General Petri Net Reachability Problem. SIAM Journal of Computing, vol.13(3), 441-460, 1984.
13. R. Milner. Communicating and mobile systems: the π-calculus, Cambridge University Press, 1999.
14. Gh. P˘aun. Computing with membranes. Journal of Computer and System Sciences, vol.61(1), 108-143, 2000.
15. Gh. P˘aun. Membrane Computing. An Introduction, Springer, 2002.

Sophisticated Inﬁnite Sequences
Luis Antunes and Andr´e Souto
University of Porto lfa@ncc.up.pt, andresouto@ncc.up.pt
Abstract. In this paper we revisit the notion of sophistication for inﬁnite sequences. Koppel deﬁned sophistication of an object as the length of the shortest (ﬁnite) total program (p) that with some (ﬁnite or inﬁnite) data (d) produce it and |p| + |d| is smaller than the shortest description of the object plus a constant. However the notion of “description of inﬁnite sequences” is not appropriately deﬁned. In this work, we propose a new deﬁnition of sophistication for inﬁnite sequences as the limit of the ratio of sophistication of the initial segments and its length. As the main result we prove that highly sophisticated sequences are dense when the sophistication is deﬁned with lim sup and are sparse when we consider the lim inf. We also prove that, similarly to what happens for ﬁnite strings, sophistication and depth, for inﬁnite sequences are distinct complexity measures.
Key words: Kolmogorov complexity, Sophistication, Constructive dimensions
1 Introduction
Solomonoﬀ [Sol64], Kolmogorov [Kol65] and Chaitin [Cha66] independently deﬁned the complexity of an object x as the length of the shortest program that produces x. The Kolmogorov structure function divides the smallest program producing the object into two parts: the part accounting for the useful regularity present in the object and the part accounting for the remaining accidental information. Kolmogorov suggested that the useful information is represented by a ﬁnite set in which the object is a typical member, so that the two-part description of the ﬁnite set together with the index of the object is as shortest as the one-part description. This approach has been generalized to computable probability mass functions. The combined theory has been developed in detail in [GTV01] and called “Algorithmic Statistics”. The most general way to proceed is perhaps to express the useful information as a recursive function. The resulting measure has been called the sophistication of the object in [Kop87,Kop88,AK91]. Later Antunes and Fortnow [AF01] revisited the notion of sophistication for ﬁnite strings and formalize a connection between sophistication and a variation of computational depth (intuitively the useful or nonrandom information in a string), proving the existence of strings with maximum sophistication and showing that they are the deepest of all strings.
In 1982, at a seminar in the Moscow State University (see [V’y99]), Kolmogorov raised the question if “absolutely non-random” (or non-stochastic) objects exist. G´acs et al. [GTV01] and Antunes and Fortnow [AF01], independently, proved that these (ﬁnite) objects do exist.
In this work we take a fresh look at the sophistication of inﬁnite sequences. We start by redeﬁning the notion of sophistication for inﬁnite sequences, introducing the
Departamento de Ciˆencia de Computadores, Rua Campo Alegre, 1021/1055, 4169 - 007 Porto - Portugal; The authors are partially supported by KCrypt (POSC/EIA/60819/2004) and funds granted to LIACC through the Programa de Financiamento Plurianual, Funda¸c˜ao para a Ciˆencia e Tecnologia and Programa POSI

26 Luis Antunes and Andr´e Souto
lower and upper sophistication as the lim inf and lim sup, respectively, of the ratio between the sophistication of the initial segment and the length of that segment. Then we prove that the set of sequences with upper sophistication diﬀerent from 0 is dense and the set of sequences with lower sophistication diﬀerent from 0 is sparse. So, the answer for Kolmogorov’s question for inﬁnite sequences is aﬃrmative if we use the upper sophistication and probably negative if we use the lower sophistication.
Koppel claimed the equivalence between sophistication and logical depth. However, he used a diﬀerent deﬁnition of logical depth imposing totality in the functions and deﬁning the length of a time bound by the smallest program describing it. Later Antunes and Fortnow [AF01] gave an example, for the ﬁnite case where the equivalence is not valid considering the classical deﬁnition of Kolmogorov complexity and computational depth. In this work we go further and give an example of an inﬁnite sequence, the diagonal of the Halting Problem, for which the sophistication is small and the dimensional depth is high.
The rest of this paper is organized as follows: in the next section, we give some background on the basic concepts necessary to the rest of the paper. In section 3 we formally present the new deﬁnitions of sophistications and establish a relationship with Hausdorﬀ dimension and packing dimension. In section 4 we prove that highly sophisticated sequences are dense if sophistication is deﬁned with lim sup and are rare if in the deﬁnition we consider lim inf. In section 5 we relate sophistication with dimensional depth by proving that these two complexity measures are of diﬀerent type.
2 Preliminaries
To avoid confusions any element of Σ∞ will be called a sequence and will be denoted by Greek letters and any element of Σ∗ will be called a string. We denote the initial segment of length n of a sequence α by α[1..n] and the ith bit by αi. The length of a binary string x is denoted by |x|. The function log will always mean the logarithmic function of base 2. If x and y are strings x ≤ y means that x is preﬁx of y.
2.1 Kolmogorov Complexity
We refer the reader to the book of Li and Vit´anyi [LV97] for a complete study on Kolmogorov complexity. Here we give essential deﬁnitions and basic results in Kolmogorov complexity necessary to the rest of this paper.
Deﬁnition 1. Let U be a ﬁxed universal Turing machine. The plain Kolmogorov complexity of x ∈ Σ∗ is, C(x) = min{|p| : U (p) = x}. If t is a constructible function,
p
the t-time-bounded Kolmogorov complexity of x ∈ Σ∗ is, Ct(x) = minp{|p| : U (p) = x in at most t(|x|) steps}.
Notice that the choice of Universal Turing machine aﬀects the running time of a program at most by a logarithmic factor and the program length at most a constant number of extra bits. So the Kolmogorov complexity theory is machine independent.
Deﬁnition 2. A string x is c-incompressible if C(x) ≥ |x| − c.
Proposition 1. There are at least 2n × (1 − 2 − c) + 1 binary strings x ∈ Σn that are c-incompressible.
We need preﬁx-free Kolmogorov complexity deﬁned using preﬁx free Turing machines: Turing machines with a one-way input tape (the input head can only read from left to right and crashes if it reads past the end of the input), a one-way output tape and a two-way work tape.

Sophisticated Inﬁnite Sequences

27

Deﬁnition 3. Let U be a ﬁxed preﬁx free universal Turing machine. Then for any string x ∈ Σ∗, the preﬁx free Kolmogorov complexity of x is, K(x) = minp{|p| : U (p) = x}.
For any time constructible t, the t-time-bounded preﬁx-free Kolmogorov complexity of x ∈ Σ∗ is, Kt(x) = minp{|p| : U (p) = x in at most t(|x|) steps}.
We extend the deﬁnition of Kolmogorov complexity to ﬁnite sets in the following way: the Kolmogorov complexity of a set S (denoted C(S)) is the Kolmogorov complexity of its characteristic sequence. As noted by Cover [Cov85], Kolmogorov proposed in 1973 at the Information Theory Symposium, Tallin, Estonia the following function:
Deﬁnition 4. The Kolmogorov structure function Hk(x|n) for x ∈ Σn is deﬁned by
Hk(x|n) = min{log |S| : x ∈ S and C(S|n) ≤ k}.
Of special interest is the value
C∗(x|n) = min{k : Hk(x|n) + k = C(x|n)}.
A program for x can be written in two stages:
1. Use p to print the indicator function for S. 2. The desired string is the ith string in a lexicographic ordering
of the elements of this set.
This program has length |p| + log |S| + O(1), and C∗(x|n) is the length of the shortest program p for which this two-stage description is as concise as the shortest one stage description. Note that x must be maximally random (a typical element) with respect to S otherwise the two stage description could be improved, contradicting the minimality of C(x|n). G´acs et al. [GTV01] generalize the model class from ﬁnite sets to probability distributions, where the models are computable probability density functions.
In 1982, at a seminar in the Moscow State University (see [V’y99]), Kolmogorov raised the question if “absolutely non-random” (or non-stochastic) objects exist.
Deﬁnition 5. Let α and β be natural numbers. A string x ∈ Σn is called (α, β)stochastic if there exists a ﬁnite set S such that x ∈ S, C(S) ≤ α and C(x|S) ≥ log |S| − β.
G´acs et al. [GTV01] and Antunes and Fortnow [AF01], independently, proved that (α, β)-stochastic objects do exist.

2.2 Sophistication
Koppel [Kop87] used total functions to represent the useful information, and the resulting measure has been called sophistication. The deﬁnition of sophistication is based on process (monotonic) complexity deﬁned by Schnorr. A function f : Σ∗ → Σ∗ is monotonic if x ≤ y (x is a preﬁx of y) implies that f (x) ≤ f (y) for all x and y. SΣ is a sample space consisting of all ﬁnite and inﬁnite sequences over Σ.
Deﬁnition 6. Let U be the reference monotone machine. The monotone complexity of x with respect to y, is deﬁned as,
Km(x|y) = min{|p| : U (p, y) = xω, ω ∈ SΣ}.
p

28 Luis Antunes and Andr´e Souto

Koppel [Kop87] deﬁned a description of a ﬁnite or inﬁnite binary string α as a pair (p, d) such that p is a total, i.e., U (p, d) is deﬁned for all d, p is a self-delimiting program and U (p, d) ≤ α, i.e., U (p, d) is an initial segment of α. He also deﬁned the complexity of α by

H(α) = min{|p| + |d| : (p, d) is a description of α}.

Deﬁnition 7 ([Kop87]). The c-sophistication of α ∈ Σ∞, is

sophc(α)

=

min{|p|
p

:

exists

d

s.t.

(p, d)

is

a

description

of

α

and

|p|+|d|

≤

H (α)+c}

Note that Koppel’s notion of description of inﬁnite sequences is not appropriately deﬁned. Also, as Koppel remark in [Kop88], it is not deﬁned for every sequence α. In order to avoid this problem Koppel deﬁned a weak version of sophistication based on “weak” compression programs for α. Antunes and Fortnow [AF01] later, revisited the notion of sophistication and, using the plain Kolmogorov complexity, adapted Koppel’s deﬁnition for ﬁnite sequences as:

Deﬁnition 8 ([AF01]). Let c be a constant, x ∈ Σn and U the universal reference Turing machine. The c-sophistication of x is

sophc(x)

=

min
p

|p| : p is total and exists d s.t. U (p, d) = x and |p|+|d| ≤ C(x)+c

.

Remark 1. An important observation about sophistication is the fact that it is not known rather if it is or not a robust measure. Indeed it is not known if slightly variations on the parameter c inﬂuences largely the length of sophistication program.

The deﬁnitions of sophistication for inﬁnite sequences introduced here use this notion of sophistication for ﬁnite strings. We will need the following result, on the existence of highly sophisticated ﬁnite strings.

Theorem 1 ([AF01]). Let c be a constant. There exists x ∈ Σn such that

sophc(x) > n − 2 log n − 2c.

2.3 Dimension

Hausdorﬀ [Haus79] augmented Lebesgue measure theory with a theory of dimension. It assigns to every subset X of a given metric space a real number dim(X), called the Hausdorﬀ dimension of X. Lutz [Lut00] proved a gale characterization of Hausdorﬀ dimension. This characterization gives an exact relationship between the Hausdorﬀ dimension of a set X consisting of all inﬁnite binary sequences, and the growth rates achievable by martingales betting on the sequences in X. This gale characterization of Hausdorﬀ dimension was a breakthrough since it enabled the deﬁnition of eﬀective versions of Hausdorﬀ dimension by imposing various computability and complexity constraints on the gales.
Later Mayordomo [May02] showed that constructive Hausdorﬀ dimension can be fully characterized in terms of Kolmogorov complexity.

Theorem 2 ([May02]). For every sequence α,

dim(α) = lim inf K(α[1..n])

n→∞

n

where dim(α) is the constructive Hausdorﬀ dimension.

Sophisticated Inﬁnite Sequences

29

Packing dimension was introduced independently by Tricot [Tic82] and Sullivan [Sul84]. Later, Athreya et al. [AHLM07] showed how to characterize packing dimension in terms of gales, a dual of the gale characterization of the Hausdorﬀ dimension. By imposing computational and complexity constrains on the gales Athreya et al. obtained a variety of eﬀective strong dimensions that are exactly duals of the eﬀective Hausdorﬀ dimensions. In particular, it was proved the following characterization in terms of algorithmic information theory that is the dual of the previous one.

Theorem 3 ([AHLM07]). For every sequence α,

Dim(α) = lim sup K(α[1..n])

n→∞

n

where Dim(α) is the constructive packing dimension.

3 The Sophistication for Inﬁnite Sequence Redeﬁned

Based on the strong relationship between constructive Hausdorﬀ dimension (respectively packing dimension) and the lim sup (respectively lim inf) of the ratio between the Kolmogorov complexity of the initial segment and its length, in this section we introduce a new and clean approach to the sophistication of inﬁnite sequences.

Deﬁnition 9. We deﬁne lower sophistication of a sequence α ∈ Σ∞ by

soph (α) = lim inf sophc(α[1..n]) cnn
and the upper sophistication by

sophc(α)

=

lim sup
n

sophc(α[1..n]) n

The ﬁrst observation is that the lower and the upper sophistication of a sequence are well deﬁned. Notice that the lim inf and lim sup of well deﬁned real numbers are themselves well deﬁned. Now we give some properties of the new measure and establish a connection to some known concepts, namely constructive Hausdorﬀ dimension and constructive Packing dimension.

Proposition 2. For all sequence α and constant c, soph (α) ≤ dim(α).
c
Proof.

soph (α) = lim inf sophc(α[1..n]) ≤ lim inf C(α[1..n]) + c

cnn

nn

≤ lim inf C(α[1..n]) + lim inf c ≤ lim inf K(α[1..n]) + lim inf c

nn

nn

n

n

nn

= lim inf K(α[1..n]) = dim(α)

nn

Proposition 3. For all sequence α and constant c, sophc(α) ≤ Dim(α). Proof.

sophc(α)

=

lim sup
n

sophc(α[1..n]) n

≤

lim sup
n

C(α[1..n]) + c n

≤ lim sup C(α[1..n]) + lim sup c ≤ lim sup K(α[1..n]) + lim sup c

nn

nn

n

n

nn

= lim sup K(α[1..n]) = Dim(α)

nn

30 Luis Antunes and Andr´e Souto

In the next proposition we show that the lower and upper sophistication of inﬁnite sequences is a non increasing function with c.

Proposition 4.

If c > c

then

soph (α)
c

≤

soph
c

(α)

and

sophc(α)

≤

sophc

(α).

The proof of this result follows immediately from the fact that for any ﬁnite string x, if c > c then sophc(x) ≤ sophc (x).
We can, in fact, prove a sharper result for the upper sophistication. We show
that there are sequences for which the upper sophistication is strictly smaller than
the packing dimension.

Proposition 5. There exist sequences α such that sophc(α) < Dim(α) for some constant c.

Proof. The idea is to use a sequence with high Kolmogorov complexity. Chaitin, in
[Cha66] and Martin-L¨of [ML71] observed that there exists α such that from some n0 onwards K(α[1..n]) ≥ n − log n − log log n.1 Thus

Dim(α) = lim sup K(α[1..n]) ≥ lim sup n − log n − log log n = 1

nn

n

n

On the other hand, it is known that inﬁnitely many initial segments of α satisfy

C(α[1..n]) = n − c for some constant c . Hence for some appropriate constant c, that

only depends on c sophc(α[1..n]) = O(1), for inﬁnitely many n, since the program p that prints α[1..n] when α[1..n] is given as data satisﬁes |p| + |α[1..n]| ≤ C(α[1..n]) + c.

So,

sophc(α)

=

lim sup
n

sophc(α[1..n]) n

≤

lim sup
n

O(1) n

=

0.

4 On the Existence of Highly Sophisticated Sequences

In this section we investigate the existence of highly sophisticated sequences. In particular, we show that the set of sequences with upper sophistication diﬀerent from 0 is dense and the set of sequences with lower sophistication diﬀerent from 0 is sparse. To formally present these results, we consider the standard metric in the Cantor space Σ∞ and use a known result for complete metric spaces.

Deﬁnition 10. In the Cantor set Σ∞, given α and β in Σ∞, the standard metric

is deﬁned by:

d(α, β) = max{2−i : αi = βi}
i

It is well known that (Σ∞, d) is a complete metric space, i.e., is a metric space in which every Cauchy sequence converges.

Remark 2. The less the distance between α and β, the bigger the initial segment common to α and β.

Consider the following set:

Vi,e = {α ∈ Σ∞ : (∀n ≥ i)sophc(α[1:n]) ≤ ne}

where c and e are ﬁxed constants and 0 < e < 1 is a rational number. Vi,e is the set of sequences that from its ith bit their initial segments are not highly sophisticated.
Then:

1 In fact, almost all sequences α with respect to the binary measure have this property,

since

P
n∈N

2−

log

n−log

log

n

converges.

Sophisticated Inﬁnite Sequences

31

1. Vi,e ⊂ Vi+1,e. If α ∈ Vi,e then for all n ≥ i, sophc(α[1:n]) ≤ ne. In particular, for all n ≥ i + 1, we have that sophc(α[1:n]) ≤ ne. So α ∈ Vi+1,e.
2. Vi,e is non empty.
For example the sequence such that all bits are equal to 0 has low sophistication

since it has low Kolmogorov complexity.

3. For all e and suﬃciently large i, Vi,e = Σ∞.

Set

n

>

i+1

such

that

n−2 log n−2c

>

ne.

Notice

that

lim
n

n

−

2 log n ne

−

2c

=

∞

and thus such n exists. Consider x ∈ Σn that satisﬁes sophc(x) ≥ n − 2 log n −

2c. This string exists by Theorem 1. Then the sequence α = x000... satisﬁes

sophc(α[1:n]) ≥ n − 2 log n − 2c > ne and thus α ∈ Vi,e. 4. All sets Vi,e are closed subsets of Σ∞.
To prove this fact we show that Σ∞ − Vi,e are open subsets of (Σ∞, d). This can be done by showing that given α ∈ Σ∞ − Vi,e there exists ε > 0 such that B(α, ε) = {β ∈ Σ∞ : d(α, β) < ε} ⊂ Σ∞ − Vi,e. If α ∈ Σ∞ − Vi,e then there exists n such that sophc(α[1:n]) ≥ ne. Set ε = 2n−1.
Then, if d(α, β) < ε it implies that for all i ≤ n, αi = βi. So sophc(β[1:n]) = sophc(α[1:n]) ≥ ne, which proves that β ∈ Σ∞ − Vi,e.

We now present a known result for complete metric spaces that gives a condition to prove that a certain sequence of subsets of a metric space have dense intersection.

Theorem 4 (Baire’s theorem). Let (X, m) be a complete metric space and let

(An)n∈N be a sequence of open dense subsets of X. Then An is dense.
n∈N

So if we prove that Σ∞ − Vi,e are dense then we prove that the set of all highly

sophisticated sequences is dense in Σ∞ since Σ∞ − Vi,1−1/i = Σ∞ − Vi,1−1/i.

i∈N

i∈N

Notice that α ∈ Vi,1−1/i if an only if α satisﬁes sophc(α) = 0.

i∈N
To prove that each Σ∞ − Vi,e is dense it is suﬃcient to show that given ε > 0 and α ∈ Vi,e there exists a sequence β ∈ Σ∞ − Vi,e such that d(α, β) < ε.

Intuitively this fact is true since we can consider the ﬁrst bits of α (to insure that

d(α, β) < ε) and construct a sophisticated string with that preﬁx of a reasonable

size.

Proposition 6. Each set Σ∞ − Vi,e is dense.

Proof. Let α be an element in Vi,e and ε > 0 a real number. We construct β as
follows: Let i0 be the index such that 2−i0 ≤ ε/2. Set βi = αi for all i ≤ i0. With this
condition we guarantee that d(α, β) < ε. Let n be suﬃciently large satisfying n − 2 log n − 6i0 − 2kc > (n + i0)e and
n > i. Consider x ∈ Σn such that soph3i0+kc(x) > n − 2 log n − 6i0 − 2kc where k is also a positive constant. Notice that since i0 is a constant value depending only
on ε, this string x always exists by theorem 1. We stress that y = α[1:i0]x has high sophistication.
Let p be a program corresponding to sophc(y). Then there exists data f such that |p| + |f | ≤ C(y) + c. Consider the following program q:

Algorithm 5 Given some data 1i0if :

1. If the data does not have this structure, stop outputting Otherwise
2. Run U with p and f to produce y .

the empty string.

32 Luis Antunes and Andr´e Souto

3. Print all the string except the ﬁrst i bits.
Since p is total it follows that q is also total. Notice that if f = f and i = i0, U (q, 1i0if ) = x. Notice also that:
|q| + | 1i0 0i0f | ≤ |p| + |f | + 2i0 + O(1) = sophc(y) + 2i0 + O(1) ≤ C(y) + 2i0 + O(1) ≤ C(α[1:i0]) + C(x) + 2i0 + O(1) ≤ C(x) + 3i0 + kc
So |q| is an upper bound of soph3i0+kc(x) and sophc(y) ≥ soph3i0+kc(x) > n − 2 log n − 6i0 − 2kc > (n + i0)e = |y|e
So let β be the sequence β = α[1:i0]x0000 · · ·. Since n > i the above discussion means that β ∈ Vi,e.
Thus, using Theorem 4 and Proposition 6 we have:
Theorem 6. The set of sequences α such that for some c sophc(α) = 0 is dense.
In what follows, we prove that the set of sequences with lower sophistication diﬀerent from 0 is sparse. Consider now the following set:
Ai,e = {α ∈ Σ∞ : ∃n > i such that sophc(α[1..n]) < ne}
where c and e are constants such that 0 < e < 1 and is a rational number. Ai,e is the set of sequences for which there exists a value n > i such that the
sophistication of α[1..n] is less than ne.
Proposition 7. Ai,e is open.
Proof. The idea is to use the same argument that proves that Σ∞ − Vi,e is open. Let α be a sequence in Ai,e. Let n be the index such that sophc(α[1..n]) < ne. Then taking ε = 2−n it follows that if d(α, β) < ε then βi = αi for all i ≤ n. So β ∈ Ai,e.
Proposition 8. Ai,e is dense.
Proof. Let α ∈ Σ∞ − Ai,e and ε > 0. Consider β deﬁned by: βi = αi for all i ≤ − log ε and βi = 0 otherwise. Then d(α, β) < ε and it is clear that β ∈ Ai,e.
So using again Baire’s theorem we conclude the following:

Theorem 7. The set of sequences α such that for some constant c, soph (α) = 0
c
is sparse.

Proof. By Baire’s theorem Ai,1−1/i is dense. So if α ∈ Ai,1−1/i then there ex-

i∈N

i∈N

ists

a

sequence

of

indexes

(in)n∈N

such

that

sophc(α[1..in])

≤

(in)e.

So

soph (α)
c

=

0.

Notice that the sequence (in)n∈N can be constructed inductively.

5 Sophistication vs Depth of Inﬁnite Sequences
Bennett [Ben88] formally deﬁned the s-signiﬁcant logical depth of an object x as the time required by a standard universal Turing machine to generate x by a program that is no more than s bits longer than the shortest descriptions of x. A deep string x should take a lot of eﬀort to construct from its short description. Incompressible strings are trivially constructible from their shortest description, and therefore computationally shallow.

Sophisticated Inﬁnite Sequences

33

Koppel [Kop87], claimed that sophistication and logical depth are equivalent, for all inﬁnite sequences. However the proof uses a diﬀerent deﬁnition of logical depth imposing totality in the functions deﬁning it.
The claimed equivalence between sophistication and logical depth would be, in fact, an unexpected result, as sophistication measures program length that is upper bounded by the length of the string and logical depth measures running time that can grow unbounded.
In [AF01] the authors proved that computational depth and sophistication are distinct measures of complexity for ﬁnite strings contradicting Koppel’s intuition. In this section, we reenforce the distinctness of these two measures for inﬁnite sequence by proving the existence of sequences that are deep but not very sophisticated.

Deﬁnition 11. The dimensional depth of a sequence α is deﬁned as

depthtdim(α)

=

lim inf
n→∞

δ(α[1..n]/2−Kt(α[1..n])) . n

where δ is the random deﬁciency deﬁned by δ(x | µ) =

2−K (x) log

.

µ(x)

The next example shows that the diagonal of the Halting Problem is deep but not very sophisticated.

Example 1. Let H be the diagonal of Halting problem i.e., H = {i : Mi(i) halts}. From Barzdini’s Lemma (see [LV97]) we know that for all n

C(χH [1..n]) ≤ C(n) + C(χH [1..n]|n) ≤ 2 log n.

So

soph
c

(χH

)

≤

sophc(χH )

=

lim sup
n

sophc(χH [1..n]) n

≤

lim sup
n

2 log n n

=

0

On the other hand we know that:

depthtdim(χ)

=

lim inf
n→∞

δ(χH [1..n]/2−Kt(χH [1..n])) n

log(2−K(χH [1..n])/2−Kt(χ[1..n]))

= lim inf

n→∞

n

= lim inf Kt(χH [1..n]) − K(χH [1..n])

n→∞

n

= lim inf deptht(χH [1..n]) = 1

n→∞

n

The last inequality is due to the fact that χH [1..n] is very deep, i.e., deptht(χH [1..n]) ≈ n.

Conclusions
In this paper we proposed two deﬁnitions for sophistication for inﬁnite sequences. The major improvement of this work to the existent one is the fact that sophistication is a concept well deﬁned for all sequences. The most important result proved here concerns the existence of highly sophisticated sequences. In fact, we proved that if sophistication is deﬁned with lim sup then the set of sequences that have sophistication diﬀerent from 0 is dense and if it is deﬁned with lim inf this set is rare. The last result stating that sophistication and depth are diﬀerent complexity measures strengthens a similar result already proved in [AF01] for ﬁnite strings.

34 Luis Antunes and Andr´e Souto
Acknowledgments
We thank to the CiE 2008 anonymous reviewers for pointing out some problems in the proofs of some results.
References
[AF01] L. Antunes and L. Fortnow, “Sophistication Revisited”, Proceedings of the 30th International Colloquium on Automata, Languages and Programming, Lecture Notes in Computer Science, 2719:267-277, Springer, 2003.
[AFMV06] L. Antunes, L. Fortnow, D. van Melkebeek and N. V. Vinodchandran, “Computational depth: concept and applications”, Theor. Comput. Sci., 354(3): 391-404, 2006.
[AMVS07] L. Antunes, A. Matos, A. Souto and P. Vit´anyi, “Depth as Randomness Deﬁciency”, submitted to Theory of Computing Systems.
[AHLM07] K. Athreya, J. Hitchcock, J. Lutz and E. Mayordomo, “Eﬀective Strong Dimension in Algorithmic Information and Computational Complexity”, SIAM Journal on Computing, 37(3):671-705, 2007
[AK91] H. Atlan and M. Koppel, An almost machine-independent theory of program-length complexity, sophistication, and induction, in Inf. Sci. 56(1-3):23-33, Elsevier Science Inc., 1991
[Ben88] C. H. Bennett. Logical depth and physical complexity. In R. Herken, editor, The Universal Turing Machine: A Half-Century Survey, pages 227–257. Oxford University Press, 1988.
[Cha66] G. Chaitin. On the length of programs for computing ﬁnite binary sequences. Journal of the ACM, 13:4(1966), 145-149.
[Cov85] T. M. Cover, Kolmogorov Complexity, Data Compression and Inference, Chapter in The Impact of Processing Techniques on Communications. Series E: Applied Sciences (91), Martinus Nijhoﬀ Publishers, 1985
[Gacs88] P. Gacs, “Lecture notes on descriptional complexity and randomness”, Tech. report, Boston University, Computer Science Dept., Boston, MA 02215, 1988.
[GTV01] P. G´acs, John Tromp and P. Vit´anyi. Algorithmic Statistics. In IEEE Trans. Inform. Theory, 47:6, 2443-2463, 2001
[Haus79] F. Hausdorﬀ, “Dimension und ¨außeres Maß”, in Math. Ann., 79:157–179, 1919 [Kol65] A. N. Kolmogorov. Three approaches to the quantitative deﬁnition of information.
Problemy Inform. Transmission, 1(1): 1-7,1965 [Kop87] M. Koppel, “Complexity, Depth, and Sophistication”, in Complex Systems 1,
pages = 1087-1091, 1987 [Kop88] M. Koppel, “Structure”, in “The universal Turing machine: a half-century
survey”, pages = 403–419, Springer-Verlag New York, Inc., 1988 [LV97] M. Li and P. Vit´anyi, “An introduction to Kolmogorov complexity and its applica-
tions”, Springer Verlag, 2nd edition, 1997. [Lut00] J. H. Lutz, “Dimension in complexity classes”, Proceedings of the 15th IEEE
Conference of Computational Complexity, IEEE Computer Society Press, 2000. [Lut02] J. H. Lutz, “The dimensions of individual strings and sequences”, Technical
Report cs. CC/0203017, ACM Computing Research Repository, 2002. [ML71] P. Martin-L¨of, “Complexity oscillations in inﬁnite binary sequences”, Zeitschrift
Wahrscheinlichkeitstheory und Verwandte Gabiete, 19, pages=225-230, 1971. [May02] E. Mayordromo, “A Kolmogorov complexity characterization of constructive
Hausdorﬀ dimension”. Information Processing Letters, 84:1-3, 2002. [Sol64] R. Solomonoﬀ. “A Formal Theory of Inductive Inference, Part I”, Information
and Control, 7(4):1-22, 1964 [Sul84] D. Sullivan, “Entropy, Hausdorﬀ measures old and new, and limit sets of geomet-
rically ﬁnite Kleinian groups”, in Acta Math., 153:259–277, 1984 [Tic82] C. Tricot, “Two deﬁnitions of fractional dimension” in Math. Proc. Cambridge
Philos. Soc., 91:57–74, 1982. [V’y99] V.V. V’yugin. “Algorithmic complexity and stochastic properties of ﬁnite binary
sequences.” The Computer Journal, 42(4):294-317, 1999.

On Detecting Deadlock in the π-Calculus
Tiago Azevedo1, Mario R. F. Benevides2, F´abio Protti3, and Marcelo Sihman1
1 COPPE/PESC - Universidade Federal do Rio de Janeiro Caixa Postal 68511 - 21945-970 - Rio de Janeiro - RJ - Brazil 2 IM and COPPE/PESC - Universidade Federal do Rio de Janeiro Caixa Postal 68511 - 21945-970 - Rio de Janeiro - RJ - Brazil
3 IM and NCE - Universidade Federal do Rio de Janeiro Caixa Postal 2324 - 20001-970 - Rio de Janeiro - RJ - Brazil
tiagoazevedo@cos.ufrj.br, mario@cos.ufrj.br fabiop@nce.ufrj.br, marcelosihman@ufrj.br
Abstract. In this work we deal with the notion of deadlock in the asynchronous polyadic π-calculus. Based on the proposed deﬁnition, we show that detecting deadlock in the asynchronous polyadic π-calculus is usually an undecidable problem. However, when restricting the language by forbidding replicating input preﬁxed processes, the detection turns out to be decidable, and a polynomial deadlock detection algorithm is presented.
Key words: Concurrency; Program Correctness; Programming Calculi; Program Speciﬁcation.
1 Introduction
When specifying distributed systems and concurrent programs, one crucial question is to ensure absence of deadlock and run-time errors. However, standard mechanisms for detecting automatically such situations can hardly be found in general, since those notions are usually diﬃcult to be dealt with.
In a previous work by Vasconcelos and Ravara [9], the notion of run-time (communication) errors in the asynchronous polyadic π-calculus was shown to be undecidable. Their result supports the use of type systems, a static method of analysis in preventing run-time errors. In many cases, when a language enjoys the subject reduction property, it is possible to prove type-safety, and this implies the absence of run-time errors. Therefore, the method is correct, but not complete as not all well-behaved programs are well-typed. However, the undecidability of the run-time error notion does not allow us to do better.
Although this undecidability result is not surprising, its proof uses and extends non-trivial results from the encoding of the λ-calculus into the asynchronous π-calculus described in [6, 8](see Section 2.1), and adapts a technique to transfer undecidability results: it reduces the problem to a known undecidable property in the source language, and prove that if it is decidable in the target language, it would also be in the source language, attaining a contradiction.
The motivation of this work is to extend the above result to the notion of deadlock. We believe that it is worth obtaining formal proofs of undecidability

36 Tiago Azevedo, Mario R. F. Benevides, F´abio Protti, and Marcelo Sihman
for such situations in the π-calculus, since they have remained up to now as “folklore” results. The use of the asynchronous π-calculus as the setting of this work was shown to be an easier way to give continuity to the ideas inspiring the cited paper [9]. This choice is justiﬁed by the fact that if these results hold in the asynchronous π-calculus, then they must also hold in superlanguages (e.g. the synchronous π-calculus) – while the inverse implication may not be necessarily true. It turns out more useful and simpler to prove undecidability results using this smaller language, which is the basis of several implementations.
The deﬁnition of deadlock proposed here (Section 3)intends to capture the potentiality a process possesses of reaching this situation after performing an empty experiment (a sequence of silent actions, where external agents observe nothing). A similar deﬁnition of deadlock can be found in [10–14], where the deadlock state is deﬁned through the concept of annotations.
The undecidability proof presented here (Section 4) follows the strategy employed in [9]: the problem of deciding whether a lambda term has a normal form [3] is reduced to the problem of deciding whether a process is potentially capable of reaching a deadlock state. This is done by deﬁning a computable function f from λ-terms into processes of the π-calculus such that f (M ) is a deadlocking process if and only if the λ-term M is convergent. The deﬁnition of f embodies the above-mentioned encoding of the lazy λ-calculus into the asynchronous π-calculus in [6, 8].
When restricting the asynchronous polyadic π-calculus by forbidding replicating input preﬁxed process, deadlock detection turns out to be decidable. In section 5, a polynomial-time deadlock detection algorithm is presented for this case.
2 Background
The concepts and deﬁnitions of the asynchronous polyadic π-calculus are used here as usual [5, 4]. Below, we brieﬂy present some deﬁnitions. Small letters a, b, p, q, x, y, . . . are names, v˜ is a sequence of names, and x˜ is a sequence of pairwise distinct names.
The set Π of processes of the asynchronous polyadic π-calculus is given by the following grammar: P ::= a¯[v˜] | a(x˜).P | P |Q | νxP | !a(x˜).P | 0
Here, ‘|’ is the operator for parallel composition of processes; ‘ν’ restricts the scope of a name to a process; and ‘!a(x˜).P ’ is a replicating input preﬁxed process. Preﬁxed operations (α. and νa) bind more tightly than composition.
The set of free names in a process P is denoted by fn(P ). Alpha-conversion is denoted by ≡α. If v˜ and x˜ are sequences with the same length, the process P [v˜/x˜] is obtained by substituting the names in v˜ for every free occurrence of the corresponding name in x˜.
The structural congruence, written ≡, is determined by the following equations: P ≡ Q if P ≡α Q, P |0 ≡ P, P |Q ≡ Q|P, (P |Q)|R ≡ P |(Q|R), νx0 ≡ 0, νxyP ≡ νyxP, νxP |Q ≡ νx(P |Q) if x ∈/ f n(Q), !P ≡ !P | P .

On Detecting Deadlock in the π-Calculus

37

The set of action labels is given by the following grammar, where {x˜} ⊆ {v˜}\{a}:
α ::= τ | a(v˜) | νx˜a¯[v˜]
The symbol τ denotes a silent action, an internal communication within a process. The remaining actions are observable actions (external communications between processes): the input action a(v˜) denotes the reception on a of the sequence of names v˜, and the output action νx˜a¯[v˜] denotes the emission to a of the sequence of names v˜, where some of them are bound. The sets fn(α) and bn(α) consist of the free names and the bound names in an action α, respectively. Given an observable action α = a(v˜) or α = νx˜a¯[v˜], the name a is called the subject of the action and is denoted by subj(α). The asynchronous transition relation contains exactly those transitions which can be inferred from the following rules:

(IN )

a(x˜).P −a(→v˜) P [v˜/x˜]

(OU T )

a¯[v˜] −a¯[→v˜] 0

(COM ) (P AR)
(RES)

a(x˜).P | a¯[v˜] −τ→ P [v˜/x˜]

P −α→ Q P | R −α→ Q | R

(bn(α) ∩ fn(R) = Ø)

P −α→ Q νxP −α→ νxQ

(x ∈/ bn(α) ∪ fn(α))

(OP EN )

νx˜a¯[v˜]
P −−−→ Q
νxx˜a¯[v˜]
νxP −−−→ Q

(a ∈/ {x, x˜})

(ST RU CT )

P −α→ P Q −α→ Q

if P ≡ Q and P ≡ Q

Note that there is no speciﬁc rule for replication. Its required behavior is
given by the last rule of structural congruence.
The process a(x˜).P and a¯[v˜] in rule (COM) are called component processes. The symbol =⇒ denotes the reﬂexive and transitive closure of −τ→, and =α⇒ denotes =⇒−α→=⇒. Let us simply write P −α→ (resp. P =α⇒) to mean that there exists Q such that P −α→ Q (resp. P =α⇒ Q).
Let s be a sequence s = α1, α2, . . . , αn of action labels. An experiment =⇒s
is deﬁned in the following way:

=⇒s d=ef =⇒−α→1 =⇒−α→2 =⇒ . . . =⇒−α→n =⇒ .
Again, write P =⇒s to mean that there exists Q such that P =⇒s Q. The empty experiment is deﬁned for n = 0; in this case, =⇒s is the same as =⇒, indicating that silent actions may occur even if we observe nothing.

38 Tiago Azevedo, Mario R. F. Benevides, F´abio Protti, and Marcelo Sihman
When no silent actions can occur within a process P , we shall call P stable (cfr. [7], p.35). Stability implies that P needs an observable action to proceed any further. Processes that have already ﬁnished all their possible computations and are reduced to 0 are also stable.
A relation R ⊆ Π × Π is called an expansion(we refer the reader to [8]) if P RQ implies: 1. if P −α→ P then there exists Q such that Q =α⇒ Q and P RQ ; 2. if Q −α→ Q and α is an observable action then there exists P such that
P −α→ P and P RQ ; 3. if Q −τ→ Q then either P RQ or there exists P such that P −τ→ P and
P RQ . We write P <∼ Q if P RQ for some expansion R. Relation <∼ is a preorder and a congruence.
2.1 Encoding the lazy λ-calculus into the asynchronous polyadic π-calculus.
The transfer of results from the λ-calculus to the asynchronous polyadic πcalculus is achieved by using the encoding of the lazy λ-calculus described below.
Deﬁnition 1. [2, 3, 8] The set Λ of λ-terms is deﬁned by the following grammar, where x ranges over the set of λ-calculus variables:
M ::= x | λx.M | M N
Free variables, closed terms, substitution, alpha-conversion etc. are deﬁned as usual. The reduction relation is −→, and the reﬂexive and transitive closure of −→ is =⇒. We write M ↓ if M is convergent, and M ↑ otherwise. In the lazy λ-calculus [1], the redex is always at the left extreme of a term.
The proofs in Section 5 need the following tools:
Theorem 1. [3] The predicate ‘M ↓’ is undecidable.
Deﬁnition 2. [6, 8] The encoding of the lazy λ-calculus into the asynchronous polyadic π-calculus is given by the following rules:
[[λx.M ]]p d=ef p(x, q).[[M ]]q [[x]]p d=ef x¯[p]
[[M N ]]p d=ef νuv([[M ]]u | u¯[v, p] | !v(q).[[N ]]q)
Lemma 1. [8, 9] The following properties of the above encoding are valid:
(a) If M =⇒ λx.N then [[M ]]p p=(x⇒,q) ∼>[[N ]]q (b) If M =⇒ x then [[M ]]p =x¯[⇒p] ∼>0 (c) M ↑ if and only if for no observable α [[M ]]p =α⇒

On Detecting Deadlock in the π-Calculus

39

3 Notion of deadlock

In this section we deﬁne the set DEAD of processes which are understood to be potentially deadlocked.
Informally, P is capable of deadlock if it may reach a situation in which the computation cannot evolve internally. The ﬁrst deﬁnition of this section says that such P is a deadlocking process if it reduces to a restricted composition of mutually non-communicating stable processes P1, P2, . . . , Pn.
We use the following notation: for a process P , let

obs(P ) = { subj(α) | α is observable and P −α→}.
In addition, say that two observable actions α, µ are complementary if subj(α) = subj(µ) and either α is an input action and µ is an output action, or α is an output action and µ is an input action.
Deﬁnition 3. The set DEAD of processes which are capable of deadlock is deﬁned in the following way:
DEAD = {P | P =⇒ νx˜(P1 | P2 | . . . | Pn), where: (i) Pj is stable, for all j = 1, . . . , n;
(ii) for any a and for all j = 1, . . . , n, if a ∈ obs(Pj) then a occurs in x˜; (iii) for all j, k (j = k), there are no complementary actions α, µ such
that Pj −α→ and Pk −µ→; (iv) obs(Pi) = Ø for some i ∈ {1, . . . , n}. }
Let us discuss the elements of the above deﬁnition.
– Condition (i) says that every Pj needs an observable action (with respect to itself) to proceed. (Note that if some silent actions can still occur within some Pj, we can not properly speak of a ’deadlocking state’.)
– Condition (ii) means that the process νx˜(P1 | P2 | . . . | Pn) cannot establish communication with the external world. By excluding this condition, it would be possible to continue the computation via non-restricted names.
– Condition (iii) ensures that P1, . . . , Pn are mutually non-communicating. – Condition (iv) excludes from the deﬁnition of DEAD those processes that
terminate for all possible computations, e.g. Q1 d=ef 0. The case n = 1 covers examples such as Q2 d=ef νa(a().0), which is considered a deadlocking process.

4 Undecidability proof
Now we are ready to present the undecidability result. Theorem 2. The predicate ‘P ∈ DEAD’ is undecidable.

40 Tiago Azevedo, Mario R. F. Benevides, F´abio Protti, and Marcelo Sihman
Proof. Let us show that if the predicate ‘P ∈ DEAD’ is decidable, then the predicate ‘M ↓’ is decidable, leading to a contradiction.
First, let f : Λ −→ Π be such that
f (M ) d=ef νpx1x2 . . . xn([[M ]]p)
where p is a new name and x1, x2, . . . , xn are the free variables occurring in M . The function f is clearly computable by the encoding of the lazy λ-calculus into the asynchronous polyadic π-calculus.
Now, given M ∈ Λ, let us show that f (M ) ∈ DEAD if and only if M ↓. Assume ﬁrst that f (M ) ∈ DEAD. Under these assumption, observe that process [[M ]]p, after zero or more silent actions, must reach a point in which it depends on an observable action to proceed further, since condition (iv) in the deﬁnition of DEAD must be satisﬁed. This means that [[M ]]p =α⇒, for some observable α. Therefore, by Lemma 1(c), M ↓. On the other hand, assume that f (M ) ∈ DEAD. Since f (M ) trivially satisﬁes conditions (ii) and (iii) in the deﬁnition of DEAD, it is mandatory that process [[M ]]p keeps continuously performing internal computations, in order to invalidate either condition (i) or condition (iv). This means that [[M ]]p does not perform any observable action after any sequence of silent actions. Therefore, by Lemma 1(c) again, M ↑. To conclude the proof, assume that ‘P ∈ DEAD’ is decidable, that is, given P ∈ Π, there is a ﬁnite time algorithm that decides whether P belongs to DEAD or not. Then the following algorithm decides whether a λ-term M is convergent or not: compute f (M ) and verify whether f (M ) ∈ DEAD; if f (M ) ∈ DEAD then the answer is ‘yes’, otherwise the answer is ‘no’. This contradicts the result of Theorem 1.
5 An algorithm for deadlock detection in the restricted π-calculus
Now we deﬁne the new set Π of processes of the restricted asynchronous polyadic π-calculus, given by the following grammar:
P ::= νxQ Q ::= a¯[v˜] | a(x˜).Q | Q|Q | 0
In other words, there is a single scope restriction acting on a parallel composition of processes (also called component processes). This simpliﬁcation is reasonable since it is easy to see that every process containing no replication can be rewritten to the above form, via alpha-conversions and structural congruences.
The algorithm for deadlock detection in the restricted π-calculus receives as input a process and then simulates it, step by step, testing all the execution paths. In case there is some possibility of deadlock, it is detected and the location

On Detecting Deadlock in the π-Calculus

41

of the deadlock is determined. The algorithm uses the fact that the deadlock state of a process can be identiﬁed by a knot in the waiting graph (see these deﬁnitions below). In addition, the algorithm shows how to locate it.
A description of the algorithm is given below.

Algorithm 1 Deadlock Detection
1: /* External call, P is the input process*/ 2: V (G) ← {P } /* Vertex set of Global State Graph*/ 3: E(G) ← {} /* Edge set of Global State Graph*/ 4: NewGlobalState(P)
5: procedure NewGlobalState(P ) 6: for all A occurring in P do 7: Build GA,P 8: /*GA,P is the waiting graph for A in P */ 9: if GA,P does not contain knot then 10: for all possible handshake h of A in P do 11: Let Ph be the new global state 12: if Ph ∈ N then 13: N ← N S{Ph} 14: E ← E S{P, Ph} 15: NewGlobalState(Ph) 16: end if 17: if (P, Ph) ∈ E then 18: E ← E S{P, Ph} 19: end if 20: end for 21: else 22: if current action of A is restricted AND 23: not passed as a parameter by another process then 24: Deadlock! 25: end if 26: end if 27: end for 28: end procedure

We use the term handshake to mean a communication over some action x between two component processes A and B, according to
rule (COM).
A global state is the state reached by the component processes after a sequence of handshakes starting at the input process P (which is the initial global state). Each global state represents a single situation, i.e., there cannot exist two identical global states.
The global state graph G = (N, E) is the (directed) graph where each node in N represents a global state, two nodes P, P ∈ N being connected by a directed

42 Tiago Azevedo, Mario R. F. Benevides, F´abio Protti, and Marcelo Sihman
edge (P, P ) ∈ E labelled h whenever global state P is reached from P after executing the handshake h. In this case, we denote P by Ph.
The waiting graph of a component process A in P is the (directed) graph GA,P = (NP , EA,P ), where NP is the set of component processes occurring in P, and EA,P is a set of labelled edges. If an edge (A, B) ∈ EA,P is labelled x, then it means that process A is waiting for a handshake over action x which can be provided by process B.
A knot in a directed graph G is a strong connected component C of G such that either (i) C consists of a single node and is equal to G or (ii) C contains at least two nodes and no directed edge leaves it.
A scope extrusion occurs when n process A contains a private action x shared with a process B, and intends to send this action (as a parameter) to another process C. When this action is exported to C, the scope of the restriction is extended to C, characterizing the scope extrusion.
Now we will describe the execution of the algorithm. First, for every component process A in global state P , we build GA,P . Next, we check whether P is in deadlock or not. Each component process is tested separately, each one with its corresponding waiting graph.
To build GA,P (line 7 of the algorithm), we look for the complement of the current action x of A in other component process (uniﬁcation algorithm), and for each complementary action found in a component process B we add to EA,P a directed edge (A, B) (if x is an input action) or (B, A) (if x is an output action), labelled with x. If the complement of x is the current action in B, then no other edge is added from/to this node, because this action will be the next to execute. Otherwise, we recursively repeat the search for complements of the current action in B, adding labelled edges to GA,P .The construction of GA,P ﬁnishes when no more edges can be added in any of the nodes, meaning that all the paths (sequences of edges that are incrementally added) have ended. Note that some paths may have turned out to be cycles.
After constructing GA,P , our goal is to detect whether A will succeed to communicate its current action with another process in the sequel of the simulation, i.e., if A is in deadlock or not. This is equivalent to checking whether GA,P contains a knot (line 8 of the algorithm). For this purpose we use a knot detection algorithm, which computes strongly connected components of GA,P .
In case that no knot is found in GA,P , the simulation continues from the current global state P . For each possible handshake h between A and another component process in P , we add a new global state Ph to N (if Ph does not exist yet) and an edge (P, Ph) to E (if it has not been added yet). See lines 9 − 19 of the algorithm. If Ph is a pre-existing global state, the simulation of this path in G ﬁnishes, because Ph has already generated its associated simulation branches; otherwise, the simulation continues until we get to the ﬁrst case (coming back to a pre-existing global state) or until a deadlock is detected (lines 21 − 23). The information on which handshakes can be executed in a step, from the current global state to the next one, is obtained from the waiting graphs of the component processes.

On Detecting Deadlock in the π-Calculus

43

In case that GA,P contains a knot C, we have to check whether C cannot be repaired by a scope extrusion (lines 21 − 22). If so, a deadlock is detected. (line
23)

5.1 Complexity of the algorithm
In this section we calculate the complexity of the deadlock detection algorithm. The number of process is denoted by n, and the number of actions in the process by m.
The time needed to build the waiting graph for some process is in the worst case O(n) for creating a vertex to each process, and O(n2) to create all edges, because each of these edges always leaves a vertex representing a process and reaches another vertex. Thus the overall complexity is O(n2).
To search for complementary ports, we use the uniﬁcation algorithm. This algorithm has a linear time complexity on the size of the input process, thus the complexity of this task takes O(n + m) time.
The complexity to determine whether the waiting graph contains a knot is O(n2) (linear on the size of the waiting graph), via a depth-ﬁrst search.
Finally as we build a waiting graph for each process in each created global state, in the worst case we spend O(n) time (processes) times O(n2) (waiting graphs), which gives an O(n3) time. Moreover, we spend O(n3) time to check whether these waiting graphs contain knots, and in the aﬃrmative case, we search for complementary ports in O(n + m) time. As we have O(m) handshakes (because there are m actions) and therefore O(m) global states (because there is a recursive call to each global state), the complexity for the deadlock detection algorithm wis O(m(n3 + n3 + m + n)) = O(m(2n3 + n + m)) = O(n3m + m2)).

5.2 The CCS Case
In the CCS case, processes cannot exchange actions with other process. The algorithm for deadlock detection is thus simpler, because we can determine a priori which processes can execute a handshake, preventing the search for complementary ports; moreover we only build waiting graphs for restricted process. Thus the complexity for the CCS case is O(m(n3 + n3)) = O(2mn3) = O(mn3).

References
1. S. Abramsky. The lazy lambda calculus. Research Topics in Functional Programming, pp. 65–116. Addison-Wesley, 1989.
2. H. Barendregt. The Lambda Calculus: Its Syntax and Semantics. Studies in Logic, vol. 103. North Holland, 1984. Revised edition.
3. J. R. Hindley and J. P. Seldin. Introduction to Combinators and λ-Calculus. Cambridge University Press, 1986, p. 63.
4. K. Honda and M. Tokoro. An object calculus for asynchronous communication. 5th European Conference on Object-Oriented Programming, LNCS 512 (1991) 141–162. Springer-Verlag.

44 Tiago Azevedo, Mario R. F. Benevides, F´abio Protti, and Marcelo Sihman
5. R. Milner. The polyadic π-calculus: a tutorial. In F. L. Bauer, W. Brauer, and H. Schwichtenberg, eds. Logic and Algebra of Speciﬁcation, Springer-Verlag, 1993.
6. R. Milner. Functions as processes. Journal of Mathematical Structures in Computer Science 2(2) (1992) 119–141.
7. R. Milner. Communicating and Mobile Systems: the π-Calculus. Cambridge University Press, 1999.
8. D. Sangiorgi and D. Walker. The Pi-Calculus: A Theory of Mobile Processes. Cambridge University Press, 2001.
9. V. T. Vasconcelos and A. Ravara. Communication errors in the π-calculus are undecidable. Information Processing Letters 71(5-6) (1999), 229–233.
10. Naoki Kobayashi A New Type System for Deadlock-Free Processes CONCUR 233-247, Springer, Lecture Notes in Computer Science, 2006
11. Naoki Kobayashi Type-based information ﬂow analysis for the pi-calculus Acta Informatica, Vol. 42, No 4-5 pp. 291-347, 2005
12. Naoki Kobayashi and Shin Saito and Eijiro Sumii An Implicitly-Typed DeadlockFree Process Calculus Lecture Notes in Computer Science, 1877, pp. 489–??,2000
13. Naoki Kobayashi A Partially Deadlock-Free Typed ProcessCalculus ACM Transactions on Programming Languages and Systems Volume 20, pp. 436–482, 1998
14. Eijoro Kobayashi and Naoki Kobayashi A Generalized Deadlock-Free Process Calculus HLCL, 1998

Algorithms for Analytic Functions and Applications to Toeplitz Operators
E. J. Beggs and A. Gerber
Mathematics Department, Swansea University, Singleton Park, Swansea SA2 8PP, Wales
E.J.Beggs@Swansea.ac.uk, A.Gerber@Swansea.ac.uk
Abstract. In this paper we present two algorithms for analytic functions. They are based on a quadtree datatype and we use methods from object oriented programming to describe them. The ﬁrst algorithm decides at each level n whether a given square of length 2−n lies further than a minimal distance away from the graph of a function f (z) and then computes the winding number for the centre of this square with respect to f (z). The second algorithm computes zeros of an analytic function f (z). Many algorithms for computing zeros of analytic functions exist already. Our algorithm explicitly deals with the problem of when zeros occur on the boundaries of squares and when the domain of the function is potentially complicated. We perturbing the relevant edges of such squares to guarantee the method working. Then, we brieﬂy explain how we can apply these algorithms to particular linear operators on Hilbert spaces called Toeplitz operators. The ﬁrst algorithm allows us to draw the spectrum of such a Toeplitz operator T (a). The second algorithm is used to determine the spectrum of a perturbed Toeplitz operator.
Key words: computable analysis, real computation, zeros of analytic functions, Toeplitz operators
1 Introduction
Computing the number of zeros of an analytic function in a given region by sub-dividing that region into smaller and smaller squares or rectangles is a wellknown procedure. Several algorithms for locating the zeros of complex analytic functions f : C → C exist. In [9] a derivative-free method for computing zeros of analytic functions speciﬁcally taylored to certain problems from physics is given. Dellnitz et al. [8] present an argument based algorithm that sub-divides a given region into rectangles, where any rectangle, which does not contain a zero, is discarded. They point out that zeros could be on the boundary of a rectangle and assign the value ∞ to these rectangles but do not explicitly deal with them.
In [11], [12] Kravanja and Van Barel present a derivative-free algorithm based on winding numbers for computing zeros of analytic functions. The package ZEAL [13] written in Fortran 90 computes the actual zeros. If zeros occur on the initial outer boundary this boundary is perturbed by ZEAL. If an inner edge is too close to a zero, this edge is shifted. The guiding principle is that

46 E. J. Beggs and A. Gerber

subdivisions of rectangles are chosen randomly and in an asymmetric way so

that the probability of a zero lying on an inner edge is eﬀectively zero. The total

number of zeros in a (perturbed) square is obtained in the same way as in the

package ZEBEC [14] (see also [16]).

Here, we propose an algorithm to detect the zeros of an analytic function

within a given, possibly complicated domain. We use a quadtree data structure

that allows us to keep track of all rectangles examined and we also move edges

to within a maximal threshold distance should there be a possibility of a zero

occurring on the that edge. The diﬀerence between our algorithm and the one

presented in [15] is similar in nature to the diﬀerence between a computable

subset of N and a computably enumerable subset. For a computable subset of N

the algorithm computing that set can ﬁnd all elements in an initial segment of N

in ﬁnite time. The algorithm we present can determine, up to a given error ε > 0,

all zeros of the function more than a given distance r > 0 from the boundary of

a bounded domain, and terminate in ﬁnite time.

We consider the Hilbert space H = l2(N), i.e. the space of square summable

sequences ||x|| =

∞ n=1

|xn

|2

<

∞

over

C

(for

further

details

consult

for

instance [7] or [18]). Banded Toeplitz operators on H are particular bounded

operators. Work by B¨ottcher et al. regarding Toeplitz operators such as in [2–4]

proved particularly useful for our purposes.

2 Computability
Turing [20] introduced what is now called a Turing machine and computability theory is based on Turing machines. One approach to computability in analysis taken by Weihrauch and others is based on TTE (type-2 theory of eﬀectivity) [21]. For real numbers all deﬁnitions of computability are equivalent. We now give the following version:
Deﬁnition 1. A real number x is computable if there exists a computable sequence {rk} of rationals which converges eﬀectively to x. Such a sequence {rk} converges eﬀectively to a real number x if there exists a recursive function e : N → N such that for all n ∈ N: k ≥ e(n) implies |rk − x| ≤ 2−n.
We will work with eﬀective or computably Cauchy sequences {xi}. This means that there is a computable function e : N → N so that for all n ∈ N we have
∀i, j ≥ e(n) |xi − xj| < 2−n .
In the applications we present later in the paper we will make use of a computability structure on a separable Hilbert space H and in particular of computable linear operators on H. The ﬁnite dimensional case was discussed in detail in two articles by Ziegler and Brattka [23, 24]. In a further article [6] the computability of the spectrum of self-adjoint operators is examined. Pour-El and Richards [17] present three important results concerned with computability on Banach spaces and introduce notions such as that of an eﬀectively separable Hilbert space.

Algorithms for Analytic Functions and Applications to Toeplitz Operators

47

It may also be of interest to know when the inverse of a computable linear operator is computable. In [5] Brattka proves a number of results regarding the computability of inverses of linear bijective operators on Banach spaces. He uses the compact-open topology whereas our results are based on the operator topology.

3 Algorithm for Winding Numbers Using Quadtrees

In this section we are going to present a method that ﬁrst checks whether a given square is on the graph of a given computable function f : S1 → C (here S1 is the unit circle in the complex numbers). Then (if the square is not on the curve) the winding number of the curve given by f around the square is calculated. We use an algorithm for the winding number of a curve, and the reader can ﬁnd other results on this in [8, 11, 12]. However, we will give some details of the algorithm we use now, as the algorithm for zeros will refer back to this.
We denote the winding number of a point λ ∈ C with respect to the function f (z), meromorphic inside γ, by wind(f, λ). For λ = 0 the winding number of f (z) along a closed curve γ is given by

f

(t) dt

=

2πi(N

−

P)

=

2πi

·

wind(f,

0)

,

γ f (t)

(1)

where N is the number of zeros and P the number of poles inside γ. We are going to use an object-oriented approach and introduce the two classes
Graph and Square. We also assume that the curve is contained in the square [0, 1]2 for convenience - it is not hard to remove this condition by scaling.
Graph - Describes a continuous function f from the unit circle to the complex numbers. Methods: ﬁndValue(x, ) - Returns a value of f (e2πix) (for dyadic x ∈ [0, 1]) within an error of > 0. ﬁndDelta( ) - Given > 0 returns a value of δ > 0 for which, for all z, w ∈ S1 if |z − w| < δ then |f (z) − f (w)| < .
Square - Describes a square in the quadtree data structure on [0, 1]2. Variables: level - gives the depth of the object in the quadtree data structure. center - gives the coordinates of the center of the square. north, south, east, west - gives the squares in those directions. onGraph - value ‘no’ (in which case distanceFromGraph is set) or ‘don’t know’. distanceFromGraph - a lower bound on the distance from the square to the graph. windingNumber - integer - the winding number of the graph around the square. Methods: ﬁndDistance(graph, ) - returns a lower bound on the distance from the square to the graph or ‘don’t know’. Is guaranteed to return a bound if the actual distance

48 E. J. Beggs and A. Gerber

is greater than > 0. ﬁndWindingNumber(graph) - uses distanceFromGraph (and is thus only called if onGraph=‘no’). Returns the windingNumber for the graph.

Now start at level zero (the square [0, 1]2), where the square at level n has side 2−n.
Main routine: enmumerate over level n squares

call ﬁndDistance(graph, 2−n) and set onGraph † If onGraph=‘no’ set distanceFromGraph If onGraph=‘no’ call ﬁndWindingNumber(graph) terminate (according to a given resolution) or move to next level

The important fact is that in the line † the method ﬁndDistance(graph, 2−n)

can be written to return a value in ﬁnite time because of the limit on the res-

olution 2−n and the data on uniform continuity contained in the object graph.

The price paid is that certain squares are recorded as ‘don’t know’ even if they

are actually a distance > 0 from the graph. But this will be remedied at a

lower level with the subsquares of these squares. The procedure could be made

faster if we let subsquares inherit variables from their parent, if the parent had

onGraph=‘no’.

We will now give a description of the method ﬁndWindingNumber. For sim-

plicity we assume that the center of the square is at the origin. The winding

number

is

1 2π

times

the

change

in

the

argument

(or

angle)

of

the

complex

num-

ber f (e2πix) as x moves from 0 to 1. The critical point here is that for complex

numbers

z, w

with

|z|, |w|

≥

r,

we

have

|

z |z|

−

w |w|

|

≤

2|z

−

w|/r.

If

a

square

of

side 2−n is known not to intersect the graph, then we know that every point on

the graph is distance ≥ 2−n−1 = 2r from the center. Now set = 2−n−6, and

we are guaranteed that |ﬁndValue(x, )| ≥ 2−n−2 = r. The previous inequality

gives

ﬁndValue(x, |ﬁndValue(x,

) )|

−

f (e2πix) |f (e2πix)|

≤2 r

1 =.
8

From this data we can ﬁnd an approximation to the argument of f (e2πix). However this angle is only known up to adding a multiple of 2π, and in moving round the circle in a ﬁnite number of steps we need to be careful to avoid the angle changing by more than half this amount in one step. Using the inequality again, for all x, y ∈ [0, 1],

ﬁndValue(x, |ﬁndValue(x,

) − ﬁndValue(y, )| |ﬁndValue(y,

) )|

≤4 r

2 |f (e2πix) − f (e2πiy)| +.
r

We call ﬁndDelta( ), set δ to be the value returned, and if |x − y| < δ we have

ﬁndValue(x, |ﬁndValue(x,

) )|

−

ﬁndValue(y, |ﬁndValue(y,

) )|

≤6 r

3 =.
8

(2)

Now step from x = 0 to x = 1 in a ﬁnite number of steps of length less than δ, and calculate the argument at each point. By (2) we can avoid skipping multiples

Algorithms for Analytic Functions and Applications to Toeplitz Operators

49

of 2π and get the winding number up to an error. But the winding number must be an integer, so we take the closest integer. Figures 3.1 and 3.2 illustrate this procedure.

000000

00

0 0 0 0 0 0 +1 +1 0 0

0 0 0 0 0 0 +1 +1 0 0

0 0 0 0 0 0 +1 +1 0 0

000000

00

000000 E
0 0 0 -1 -1 -1 -1 -1

0

00

0000

000

Fig 3.1

f (tx5) #   r

tf (x6)

"!

t f (x4)

t f (x3)

tf (x2)
t f (x7)

tf$(x1)
T tf (x8) % Fig 3.2

4 Computing the Zeros of Holomorphic Functions

The total number of zeros (counted with multiplicity) of a complex analytic function f inside a given contour γ can be calculated by evaluating the winding number of f (z) as z moves counter-clockwise once round γ. We are going to make use of this and also of the important principle of isolated zeros for any analytic function [19]. This means that in any compact subset of the domain of the function, there can only be ﬁnitely many zeros. In the example we shall give later, the domain of the function is itself quite complicated, it is the set of points not on the graph where the winding number of the graph is zero. To describe such complicated domains, we will use a quadtree datatype. Then, given that a particular square is in the domain, we just calculate the winding number of f round its edges to ﬁnd the number of zeros in the square. Unfortunately there is a complication - the edge may contain a zero of the function.
Consider a complex analytic function f deﬁned on an open set (its domain) in the square [0, 1]2 in C. We assume that f has no zeros on the edges of [0, 1]2. The domain of f is deﬁned by a quadtree data structure beginning with the level

50 E. J. Beggs and A. Gerber
0 square [0, 1]2. A square at level n has a variable inDomain. If this takes the value ‘true’ then a slightly bigger square (for example, say the same center, but double the sides) lies in the domain of f . The other value of inDomain is ‘don’t know’. The union of all the squares for which inDomain=‘true’ is the domain of the function. Each level n square in the quadtree has nominal edges of side 2−n, but for our purposes these are not the actual edges that we choose. We choose edges of length 5/2n+2 at a position shifted away from the center by an amount between 0 and 2−n−3 from the nominal side. Fig. 4.1 shows a nominal square (with thick lines) and examples of the shifted edges. We only allow horizontal edges to move East and vertical edges to move South, which is suﬃcient.

Fig. 4.1

Since the possible set occupied by any one shifted edge is a compact subset of

the domain, by the principle of isolated zeros there can only be ﬁnitely many such

shifted edges containing a zero of the function. We keep testing edges until we

have one without a zero. Repeat for all four edges, then calculate the winding

number. Of course, it is not quite that simple. First we can’t have adjacent

squares disagreeing about what the common edge is. We take a class Edge

and each edge is shifted once, so each object of class Edge contains a variable

recording this shift. Squares query the objects of class Edge, referenced by the

labels North, South, East and West.

Secondly, we can’t actually test if the function vanishes on a given shifted

edge, as such a program may never terminate. What we can do is to run a test for

an error > 0, which is guaranteed to return a strictly positive lower bound for the function along the shifted edge if |f (z)| > on the shifted edge, but may also

return a ‘don’t know’ result. We test a list of trial shifted edges at successively

reducing errors (say

=

1 m

on

the

mth

trial).

The

list

includes

inﬁnitely

many

diﬀerent edges, but also mentions each of these diﬀerent edges inﬁnitely many

times. Then we must have a result in ﬁnite time giving a shifted edge and a strictly positive lower bound for |f (z)| on that edge.

The last complication is that the shifted edges are unlikely to form a simple

grid pattern, but will have more complicated intersections at the corners. There

are four possibilities, illustrated by the following diagrams in Fig. 4.2.

Fig 4.2
In each of the four possible cases we add any zeros occurring in the small rectangle to the perturbed, adjacent square to the North-West of it. It is impor-

Algorithms for Analytic Functions and Applications to Toeplitz Operators

51

tant to evaluate the integrals along the contours in each possible case detailed by Fig. 4.2 in the appropriate direction so as not to doubly count or omit zeros.

4.1 Classes for Computing Zeros

Function - Describes an analytic function f deﬁned on the domain. Methods: ﬁndValue(z, ) - returns a value of f (z) for z in the domain within an error of
> 0. ﬁndDelta( , edge) - given > 0 returns a value of δ > 0 for which, for all z, w in the Edge edge if |z − w| < δ then |f (z) − f (w)| < .

Square - a quadtree datastructure square Variables: level, center, northSquare, southSquare, eastSquare, westSquare, northEdge, southEdge, eastEdge, westEdge, inDomain, numberOfZeros Methods: ﬁndInDomain - determines if the enlarged square is in the domain ﬁndNumberOfZeros(f unction) - determines the winding number by calling methods from the surrounding edges.

HorizontalEdge - a quadtree datastructure edge

Variables:

edgePosition, level, northSquare, southSquare, inDomain, lowerBound

Methods:

ﬁndEdgePosition(f unction) - ﬁnds a shifted edgePosition where f (z) is guaran-

teed not to be zero, and a strictly positive lowerBound on |f (z)| there.

changeInArgument(f unction, x, y) - ﬁnds the change in argument of f (z) along

the

shifted

edge

from

real

part

x

to

real

part

y

to

an

accuracy

of

1 16

of

a

revo-

lution.

VerticalEdge - a quadtree datastructure edge - similar to HorizontalEdge

Main routine: enmumerate over level n squares call ﬁndInDomain and set inDomain for square and surrounding edges
enmumerate over level n edges with inDomain=true call ﬁndEdgePosition and set edgePosition and lowerBound
enmumerate over level n squares use edgePosition for surrounding edges to determine which of the corner cases applies call changeInArgument(f unction, x, y) for surrounding edges and set numberOfZeros
We do not go into details of the calculation of the winding numbers, since this is similar to the previous discussion. The only diﬀerence is that we use

52 E. J. Beggs and A. Gerber
squares not circles, and do the calculation separately for each edge, only taking the closest integer on combining the data for the edges of the square.
There are a couple of points we should point out to the reader. The ﬁrst is that the -δ uniform continuity estimates for the function are carried out separately for the compact set of possible shifted edges in each Edge object. The reason is quite simple - the function is not analytic in the whole level 0 square, but on some open subset, the domain. Typically the function may become very badly behaved towards the boundary of the domain, so probably there is no global choice of δ because the function is not globally uniformly continuous.
Secondly the output does not count the zeros in the original nominal squares, but in some slightly enlarged squares. However we have taken care not to double count any zeros, so a zero near an original nominal edge may end up counted in one or other of the neighbouring squares, but not both.
Thirdly we count zeros multiply, so f (z) = (z − a)2 counts as having two zeros at z = a. However the data given cannot distinguish between an actual double zero and two single zeros too close to separate at the given resolution.

5 Applications to Banded Toeplitz Operators

In a chosen basis (ei) on a separable Hilbert space H, following the notational conventions in [2], a Toeplitz operator T (a) is a linear operator given by

 a0 a−1 a−2 . . . 

 a1 a0

 

a2

a1

a−1 a0

...

.

.

.

 

,

... ... ... ...

where ai ∈ C for i ∈ Z. We can associate a function a(t) : S1 → C with each Toeplitz operator consisting of the Laurent series with coeﬃcients ai. Here, we are interested in banded Toeplitz operators, which have a ﬁnite number of entries {a−m, a−m+1, . . . , a0, . . . , am} only. In this case the associated function a(t) is simply a Laurent polynomial

m

a(t) =

aktk .

k=−m

(3)

Criteria for the invertibility of Toeplitz operators are given in [2]. It is known [10] that the spectrum sp(T ) of a Toeplitz operator T (a) is given by

sp(T ) = a(S1) ∪ {λ ∈ C\a(S1) : wind(a, λ) = 0} .

(4)

If T (a) is simply a shift operator then sp(T ) consists of the closed unit disk.

B¨ottcher uses MATLAB [22] to compute sp(T ) for some examples in [4].

We now state how sp(T ) can be computed by using the algorithm from

Section 3. For that we assume that all entries of a Toeplitz operator are dyadic

fractions,

i.e.

of

the

form

a 2b

,

where

a

and

b

are

non-negative

integers.

In

order

to

Algorithms for Analytic Functions and Applications to Toeplitz Operators

53

compute sp(T ) we simply need to use the methods for Graph for the Laurent polynomial (3) and then the main routine in Section 3. This will compute the complement of sp(T ) up to a given accuracy depending on the level n of the squares in the quadtree.
From a private communication with B¨ottcher we learned that the spectrum of a Toeplitz operator T with a ﬁnite rank perturbation K, denoted by A = T + K and where K is contained in the ﬁrst n rows and columns, is given by

sp(A) = sp(T ) ∪ {λ | f (λ) := det(I + Pl(T − λI)−1KPl) = 0} .

(5)

Here Pl is the projection operator onto the ﬁrst l basis vectors. Since f (λ) is analytic and not identically zero, this means that only countably many λ at most will be added to sp(T ) to make up sp(A).
Now, we brieﬂy explain how sp(A) can be computed. The continuous part of sp(A) is simply sp(T ) and can be computed as just stated. For the second, discrete part of sp(A) we retrieve all squares with centre λ for which windN umber = 0. Then, we check whether these λ are in the domain of f (λ) and run the main routine from Section 4 to determine the zeros of f (λ). If a square with centre λ does not contain any zeros then it belongs to sp(A)C. We subdivide all squares containing zeros and so on. This allows us to approximate sp(A)C up to an accuracy depending on the level n of the quadtree structure.
Further results on computability of perturbed Toeplitz operators and a detailed account of the functional analysis behind the theory of Toeplitz operators are being written up in [1].

Acknowledgements
The authors are indepted to Professor A B¨ottcher for numerous valuable discussions on Toeplitz operators and for providing very helpful advice on the literature. We also would like to thank Professor V Brattka for discussions relating to computable functional analysis. Both authors would like to thank EPSRC for supporting this work by the grant EP/C525361/1 .

References
1. Beggs, E.J., Gerber, A.: Computability of banded Toeplitz Operators with Finite Rank Perturbations. In preparation
2. B¨ottcher, A., Grudsky, S.M.: Spectral properties of Toeplitz operators. SIAM (2005) 3. B¨ottcher, A., Embree, M., Sokolov, V.I.: Inﬁnite Toeplitz and Laurent matrices with
localised impurities. Lin. Alg. Appl. 343-344 (2002) 101-118 4. B¨ottcher, A., Embree, M., Sokolov, V.I.: On large Toeplitz band matrices with an
uncertain block. Lin. Alg. Appl. 366 (2003) 87-97 5. Brattka, V.: The inversion problem for computable linear operators. STACS 2003.
LNCS 2607 Springer, Berlin (2003), 391-402 6. Brattka, V., Dillhage, R.: Computability of the spectrum of self-adjoint operators.
J. Univ. Comput. Sci. 11 (2005) 1884-1900

54 E. J. Beggs and A. Gerber
7. Debnath, L., Mikusinski, P.: Introduction to Hilbert spaces with applications. 2nd edition, Academic Press, (1999)
8. Dellnitz, M., Schu¨tze, O., Zheng, Q.: Locating all the zeros of an analytic function in one complex variable. J. Comput. Appl. Math. 138 (2002) 325-333
9. Gillan, C.J., Schuchinsky, A., Spence, I.: Computing zeros of analytic functions in the complex plane without using derivatives. Comp. Phys. Comm. 175 (2006) 304-313
10. Gohberg, I.C.: On an application of the theory of normed rings to singular integral equations. Uspekhi Matem. Nauk SSSR 112 (1952) 149-156
11. Kravanja, P., Van Barel, M.: A derivative-free algorithm for computing zeros of analytic functions. Computing 63 (1999) 69-91
12. Kravanja, P., Van Barel, M.: Computing the zeros of analytic functions. Lecture Notes in Mathematics 1727 Springer, Berlin (2000)
13. Kravanja, P., Van Barel, M., Ragos, O., Vrahatis, M.N., Zaﬁropoulos, F.A.: ZEAL: A mathematical software package for computing zeros of analytic functions. Comp. Phys. Comm. 124 (2000) 212-232
14. Kravanja, P., Ragos, O., Vrahatis, M.N., Zaﬁropoulos, F.A.: ZEBEC: A mathematical software package for computing zeros of Bessel functions of real order and complex argument. Comp. Phys. Comm. 113 (1998) 220-238
15. Matheson, A., McNicholl, T.H.: Computable analysis and Blaschke products. Proc. AMS 136 (2008) 321-332
16. Piessens, R., de Doncker-Kapenga, E., U¨ berhuber, C.W., Kahaner, D.K.: QUADPACK: A Subroutine package for automatic integration. Springer Series in Computational Mathematics Vol. 1 Springer, Berlin (1983)
17. Pour-El, M.B., Richards, J.I.: Computability in analysis and physics. Springer, (1989)
18. Rudin, W.: Functional analysis. 2nd edition, McGraw-Hill, (1991) 19. Rudin, W.: Principles of mathematical analysis. 3rd edition, McGraw-Hill, (1976) 20. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proc. Lond. Math. Soc. Series 2 42 (1936/37) 230-265 21. Weihrauch, K.: Computable analysis. Springer, Berlin (2000) 22. Wright, T.G.: MATLAB Pseudospectra GUI, 2000-2001. Available online at
http://www.comlab.ox.ac.uk/pseudospectra/psagui 23. Ziegler, M., Brattka, V.: Computability in linear algebra. Theor. Comput. Sci. 326
(2004) 187-211 24. Ziegler, M., Brattka, V.: A computable spectral theorem. CCA 2000, Swansea,
LNCS 2064 (2001) 378-388

Triviality and Minimality in the Degrees of Monotone Complexity
William C. Calhoun
Department of Mathematics, Computer Science and Statistics Bloomsburg University, Bloomsburg, PA 17815, USA wcalhoun@bloomu.edu
Abstract. This work extends the author’s article Degrees of Monotone Complexity (2006). Monotone complexity, Km, is a variant of Kolmogorov complexity that was introduced independently by Levin and Schnorr. Monotone complexity was used in the previous article to deﬁne the relative complexity (or randomness) of reals. Equivalence classes of reals under monotone complexity are the Km-degrees, similar to the Kdegrees deﬁned via preﬁx-free complexity. In this paper, the Km-trivial reals are deﬁned and shown to be equivalent to the (Km, K)-trivial reals. A nondecreasing, function f : ω → ω is deﬁned to be computably inﬁnitesimal if it is dominated by every computable, nondecreasing, unbounded function. It is shown that if a real is Km-trivial then its monotone complexity is computably inﬁnitesimal. If a Km-minimal real exists, it is Km-trivial. The operation ⊗, deﬁned previously, horizontally stretches the complexity graph of a real α by a strictly increasing computable function f . Here, α is deﬁned to be invariant under computable stretching if α ⊗ f ≡Km α for any such f . It is shown that any Km-minimal real must be invariant under computable stretching.
Key words: Kolmogorov complexity; monotone complexity; randomness; trivial reals
1 Introduction
Kolmogorov complexity was introduced by Solomonoﬀ [17], Kolmogorov [7] and Chaitin [2] [3] in the 1960’s and has been studied by many mathematicians and computer scientists since then. The present work is a continuation of the mathematical investigation of Kolmogorov complexity, in this case applied to the relative complexity (or relative randomness) of reals (binary sequences). We will use monotone complexity, a variant of Kolmogorov complexity deﬁned by Levin [8]. Schnorr [13] independently deﬁned a similar notion called process complexity that he later made equivalent to monotone complexity by slightly adjusting the deﬁnition [14]. In a previous article [1], the author studied the relative complexity of reals derived from monotone complexity for strings. Monotone complexity is arguably more appropriate for deﬁning the complexity of a real than the more commonly used preﬁx-free complexity, K(σ). Monotone programs can describe both ﬁnite strings and computable reals, and the bottom monotone degree is

56 William C. Calhoun
the set of computable reals. In contrast, a preﬁx-free program can only describe a ﬁnite string and the bottom preﬁx-free degree is the set of K-trivial reals. See the book by Li and Vita´nyi [9] for additional background on monotone and preﬁx-free complexity.
Here we will focus on reals with very little complexity: Km-trivial and Kmminimal reals. We will deﬁne them formally in the following sections. Before delving into the mathematical details, it might be appropriate, in the spirit of the Computability in Europe conferences, to comment brieﬂy on a potential application of Kolmogorov complexity. The close connection between Shannon’s notion of entropy in information theory [15] and Kolmogorov complexity is well known. One example of a practical application of entropy is in computer data recovery and computer forensics [16]. Computing the entropy of recovered data can help determine the source of the data when directory information about the source ﬁle has been lost. High entropy indicates (seemingly) random data that is likely to come from a compressed or encrypted ﬁle. Low entropy indicates the data is redundant, possibly coming from a text or bitmap graphics ﬁle. Since the entropy is based on the frequencies of bytes in a ﬁle, it does not take into account the order of the bytes. Some researchers in computer forensics have used randomness measures based on Kolmogorov complexity to take the byte order into account. [19] In the setting of computer forensics, information may be sparsely scattered in data via steganography, a method of hiding messages in other data. One might think of the Km-trivial reals informally as data in which information is sparsely scattered. The complexity of the initial segments grows so slowly that the real might be said to be “almost” computable. Nevertheless, Km-trivial reals are noncomputable reals containing inﬁnitely much information.
Of course our primary reason for studying Km-trivial and Km-minimal reals is to understand them as mathematical objects, rather than for any direct applications to computer forensics. The Km-trivial reals are deﬁned similarly to the intensively studied K-trivial reals. The connections and contrasts between Km-trivial and K-trivial reals will be explored in the next section. The main question about Km-minimal reals is whether they exist. The existence of minimal elements in a degree structure is one way the structure can fail to be dense. Density is considered an important property in the study of degree structures.
We close the introduction with some basic deﬁnitions that will be used in the analysis of trivial and minimal reals. A monotone machine M is a computably enumerable set of pairs p, σ where p, σ ∈ 2<ω and for every p, σ , q, τ ∈ M , p ⊆ q implies σ ⊆ τ or τ ⊆ σ. (It is important to note that, p and q may be comparable or even equal.) We deﬁne the monotone complexity of a string σ with respect to M to be KmM (σ) = min{|p| : p, τ ∈ M for some τ ⊇ σ}. One can show there is an optimal monotone machine U : for any other monotone machine M , there is a constant c such that KmU (σ) ≤ KmM (σ) + c for all σ. For any string σ deﬁne Km(σ) = KmU (σ). For any α ∈ 2<ω we deﬁne an optimal monotone description of α to be a program p of minimal length such that p, β ∈ U for some β ⊇ α. In some constructions we approximate Km by the computable functions Kms(σ) = min{|p| : p, τ ∈ Us and σ ⊆ τ }.

Monotone Triviality and Minimality

57

A useful method for constructing preﬁx-free machines is given by the KraftChaitin Theorem.
Theorem 1. (Kraft-Chaitin) From a computable sequence of pairs ( ni, σi )i∈ω (known as axioms) such that Σi∈ω2−ni ≤ 1, we can eﬀectively obtain a preﬁxfree machine M such that for each i there is a τi of length ni with M (τi) ↓= σi, and M (µ) ↑ unless µ = τi, for some i.
When applying the Kraft-Chaitin theorem we shall refer to Σσi=µ2−ni as the measure assigned to µ. We will refer to Σi∈ω2−ni as the total measure assigned. Note that the total measure assigned is the sum over all strings µ of the measure assigned to µ. Using this terminology, the hypothesis of the Kraft-Chaitin theorem may be restated as follows: the total measure assigned is less than or equal to one.
As is done with preﬁx-free complexity, monotone complexity for binary strings may be used to deﬁne the complexity of reals via the complexity functions on initial segments. We will consider two complexity functions to be equivalent if their diﬀerence is bounded. We use the notation f g or g f to mean there is a constant c ∈ ω such that f (n) ≤ g(n) + c for all n ∈ ω. We use the notation f g to mean that f g and g f . We write α≤Km β or β≥Km α if Km(α n) Km(β n). We write α≡Km β if α≤Km β and β≤Km α. The equivalence classes of ≡Km are the degrees of monotone complexity. See the book by Downey and Hirschfeldt [4] for additional background on relative randomness.

2 Km-Trivial Reals
Solovay [18] was the ﬁrst to construct a noncomputable K-trivial real. (See also [5], [10] and [11].) The following deﬁnition was used in [1] to provide a notion of triviality for the monotone degrees.
Deﬁnition 2. A real α is (Km, K)-trivial if Km(α n) K(n).
Here we will more simply deﬁne triviality for the monotone degrees using only monotone complexity. First, we deﬁne the complexity of an integer n to be the complexity of a particular string of length n. For preﬁx-free complexity, we may deﬁne K(n) = K(1n). For monotone complexity, we must use a family of uniformly “self-delimiting” strings so that the length of the string can be computed from the pattern of bits in the string. A simple choice for this family is λn = 0n−1 1 for n > 0. (Let λ0 be the empty string.)
Deﬁnition 3. The monotone complexity of a positive integer n is deﬁned by Km(n) = Km(λn)
We can now deﬁne a real to be Km-trivial if the monotone complexity of its initial segments is dominated by the monotone complexity of the lengths of the initial segments.
Deﬁnition 4. A real α is Km-trivial if Km(α n) Km(n).

58 William C. Calhoun
The deﬁnitions (Km, K)-trivial and Km-trivial are actually equivalent as we will show. We begin with a theorem that shows that the Km and K complexities are equivalent when applied to any computable set of incomparable strings.
Theorem 5. If {αn}n∈ω is a computable set of pairwise incomparable binary strings, then Km(αn) K(αn).
Proof. Since K dominates Km, it is obvious that Km(αn) K(αn). For the other direction, we will use the Kraft-Chaitin Theorem to deﬁne a preﬁx-free machine M such that KM (αn) Km(αn). Since K(αn) KM (αn), it follows that K(αn) Km(αn). Let Sn = {s : Kms(αn) < Kms−1(αn). (For this deﬁnition we set Kms(αn) = ∞ when Kms(αn) ↑.) Note that for each n there is a stage tn at which Kmtn (αn) = Km(αn). Therefore, Sn is ﬁnite for each n. To deﬁne M , we enumerate an axiom Kms(αn) + 1, αn for each n and s where s ∈ Sn. Note that in the axioms ( ni, σi )i∈ω assigned for each n, the ni are distinct, and the least such ni is Km(αn) + 1. Then, for each n, the measure assigned to αn is Σs∈Sn 2−(Kms(αn)+1) < 2(2−(Km(αn)+1)) = 2−Km(αn). So the total measure assigned is less than Σn∈ω2−Km(αn). Since the αn are pairwise incomparable, it follows that the total measure assigned is less than one. By the Kraft-Chaitin theorem, there is a preﬁx-free machine M such that KM (αn) = Km(αn) + 1 for all n. Noting that KM (αn) Km(αn), the proof is complete.
As a corollary, we may deduce that K(n) and Km(n) are equivalent functions.
Corollary 6. K(n) Km(n).
Proof. First, it is easy to see that K(1n) K(λn). Then we use the previous theorem to conclude that K(λn) Km(λn).
We can conclude that (Km, K)-triviality is equivalent to Km-triviality.
Corollary 7. A real is (Km, K)-trivial iﬀ it is Km-trivial.
As noted in [1] (for (Km, K)-triviality), it is obvious that every K-trivial real is Km-trivial, and hence that there are noncomputable Km-trivial reals. D. Hirschfeldt and A. Nies have shown that every K-trivial real is Turing incomplete. [5] [10] On the other hand, F. Stephan has shown that the real α deﬁned by α(n) = 1 ⇐⇒ ∀m > n(K(m) > K(n)) is Km-trivial and Turing complete. [12] (The argument uses the fact that a universal machine with special properties can be constructed. [6]) Thus α is a Km-trivial real that is not K-trivial.
Since Km(n) and K(n) are equivalent functions, we may deduce the asymptotic behavior of Km(n) from known results for K(n).
Corollary 8.
1. limn→∞ Km(n) = ∞. 2. There is no upper bound on Km(n1) − Km(n2) for n1 < n2.
Proof. The corresponding statements are true of K(n).

Monotone Triviality and Minimality

59

For any real α it is clear that K(|α|) K(α). Hence, if α is K-trivial, we have K(α n) K(n). However, the corresponding fact does not hold for monotone complexity as indicated by the following theorem.
Theorem 9. For any real α, Km(α n) Km(n).
Proof. Suppose for a contradiction that Km(α n) Km(n) for some real α. Then for some constant b, Km(α n) − b ≤ Km(n) ≤ Km(α n) + b. But by Corollary 8 we may choose n1 < n2 such that Km(n1) − Km(n2) > 2b . Combining the inequalities, we obtain Km(α n1) + b ≥ Km(n1) > Km(n2) + 2b ≥ Km(α n2) + b. But then Km(α n1) > Km(α n2), which is impossible since Km(α n) is nondecreasing.
It follows immediately that Km-trivial reals must have monotone complexity strictly below the complexity of the length.
Corollary 10. If α is a Km-trivial real, then Km(α n) ≺ Km(n).
However, it should be understood that the previous result only means that Km(n) − Km(α n) is bounded below and unbounded above. The diﬀerence can oscillate so that Km(n) − Km(α n) is small (less than a ﬁxed bound) inﬁnitely many times. This raises the question of whether a noncomputable real can have monotone complexity unboundedly less than the complexity of the length. We now state this question more precisely.
Deﬁnition 11. We write f (n) ≺≺ g(n) if limn→∞ g(n) − f (n) = ∞.
Question 12. Is there a noncomputable real α such that Km(α n) ≺≺ Km(n)?
We now show that Km-trivial reals must have extremely slow-growing complexities. To state this precisely, we give a deﬁnition for nondecreasing functions that are below any computable, nondecreasing, unbounded function.
Deﬁnition 13. A nondecreasing, function f : ω → ω is computably inﬁnitesimal if for every computable, nondecreasing, unbounded function g, f (n) g(n).
Note that a computably inﬁnitesimal function must grow more slowly than such slow-growing computable functions as logk for any k or the inverse Ackermann function. We now show the monotone complexity of any Km-trivial real is computably inﬁnitesimal, although it is unbounded.
Theorem 14. If α is a Km-trivial real, then Km(α n) is computably inﬁnitesimal.
Proof. Let g be a computable, nondecreasing, unbounded function. We wish to show that Km(α n) g(n). We may assume that g(0) = 0 and g(n+1)−g(n) ≤ 1 for all n, since otherwise we can replace g with a function h(n) g(n) that satisﬁes those properties. We deﬁne a computable sequence {ni}i∈ω by ni = the greatest n such that g(n) = i. The deﬁnition provides a description of ni of size

60 William C. Calhoun
depending only on the size of i. Hence Km(ni) i = g(ni). Since α is Kmtrivial, Km(α ni) Km(ni) g(ni). Since Km(α n) and g(n) are monotone functions and g(ni+1) − g(ni) = 1, it follows that Km(α n) g(n).

As mentioned above, there is a Km-trivial real that is not K-trivial. However, as a corollary to the previous result, we can show that every Km-trivial real is “almost” K-trivial, in the sense that the diﬀerence between the preﬁx-free complexity of a Km-trivial real and K(n) is computably inﬁnitesimal.
Corollary 15. If α is Km-trivial then K(α n) − K(n) is computably inﬁnitesimal.
Proof. Let α be a Km-trivial real. Then K(α n) K(n) + 2Km(α n). Hence K(α n) − K(n) 2Km(α n). Since Km(α n) is computably inﬁnitesimal, so is 2Km(α n).

3 Km-Minimal Reals

We know turn our attention to Km-minimal reals.
Deﬁnition 16. A real α is Km-minimal if α>Km 0 and for every real β≤Km α either β≡Km 0 or β≡Km α.
It is not known whether a Km-minimal real exists. We will prove several properties that such a real would have to satisfy. The following theorem connecting Km-triviality and Km-minimality is from [1].
Theorem 17. If a real is Km-minimal then it is Km-trivial.
As an immediate corollary, a Km-minimal real must satisfy all the properties of Km-trivial reals from the previous section.
Corollary 18. If a real α is Km-minimal then
1. Km(α n) ≺ Km(n), 2. Km(α n) is computably inﬁnitesimal 3. and K(α n) − K(n) is computably inﬁnitesimal.

We will close with a discussion of an additional property of Km-minimal reals. An operation ⊗ was introduced in [1] to “horizontally stretch” the graph
of a real α by a strictly increasing function f .

Deﬁnition 19. Given any real α and a strictly increasing function f : ω → ω, let α ⊗ f be the real deﬁned by

(α ⊗ f )(n) =

α(f −1(n)) if n ∈ range(f ) 0 otherwise

Monotone Triviality and Minimality

61

Theorem 20. If α is any real and f : ω → ω is a strictly increasing computable function, then Km(α ⊗ f n) Km(α f −1[n]) where f −1[n] = max{k : f (k) ≤ n}.
It is conceivable that Km(α n) could grow so slowly that α ⊗ f has monotone complexity equivalent to that of α, for any strictly increasing computable function f .
Deﬁnition 21. A real α is invariant under computable stretching if for all strictly increasing computable functions f , α ⊗ f ≡Km α.
The monotone complexity of a real that is invariant under computable stretching must grow extremely slowly, as indicated by the next theorem.
Theorem 22. If a real α is invariant under computable stretching then for any strictly increasing computable function f , Km(α f (n))−Km(α n) is bounded.
Proof. Let α be invariant under computable stretching and f be a strictly increasing computable function. Since α ⊕ f ≡Km α, we have Km(α ⊕ f f (n)) Km(α f (n)). By Theorem 20 Km(α f (n)) Km(α f −1[f (n)]) = Km(α n). Therefore Km(α f (n)) Km(α n) and the result follows.
If a real α is invariant under computable stretching, then by Theorem 20 its monotone complexity g(n) = Km(α n) satisﬁes g(f −1[n]) g(n) for any strictly increasing computable function f . It is not hard to construct a function g that satisﬁes that property.
Theorem 23. There is a function g : ω → ω such that g(f −1[n]) g(n) for any strictly increasing computable function f .
Proof. Let {fi}i∈ω be a list of a strictly increasing computable functions. Deﬁne a sequence {an}n∈ω by a0 = 0 and, for n > 0, an = max{fi(an−1) : i < n} + 1. Let g(x) = max{n : an ≤ x}. Now we verify that g satisﬁes the desired property. For any strictly increasing computable function f , f = fi for some i. For any integer x ≥ i there is a unique integer n ≥ i such that an ≤ x < an+1. By deﬁnition an > f (an−1). So an−1 ≤ f −1[an] ≤ f −1[x] ≤ x. Since g is increasing, g(an−1) ≤ g(f −1[x]) ≤ g(x). By deﬁnition of g, we have g(an−1) = n − 1 and g(x) = n. Therefore, g(x) − 1 ≤ g(f −1[x]) ≤ g(x).
The function g constructed in the previous theorem is not necessarily the monotone complexity of a real, leaving open the question of whether a real exists that is invariant under computable stretching. However, if a Km-minimal real exists, it must be invariant under computable stretching.
Theorem 24. Every Km-minimal real is invariant under computable stretching.
Proof. Let α be a Km-minimal real and let f be a strictly increasing computable function. We have Km(α ⊗ f n) Km(α f −1[n]) by Theorem 20. From α>Km 0 we may conclude that limn→∞ Km(α f −1[n]) = ∞. Therefore 0<Km α ⊗ f ≤Km α. Since α is Km-minimal, α ⊗ f ≡Km α.

62 William C. Calhoun
Perhaps the results of this section will be useful in settling the main open question about Km-minimal reals.
Question 25. Is there a Km-minimal real?
Acknowledgments: The author would like to thank Frank Stephan for correspondence regarding the existence of Km-trivial reals that are not K-trivial. The author would also like to thank the anonymous referees for detailed helpful comments.
References
1. Calhoun, W.C.: Degrees of Monotone Complexity, J. Symbolic Logic 71, 1327–1341 (2006).
2. Chaitin, G.J.: On the Length of Programs for Computing Finite Binary Sequences, J. ACM 13, 547–569 (1966).
3. Chaitin, G.J.: On the Length of Programs for Computing Finite Binary Sequences: Statistical Considerations, J. ACM 16, 145–159 (1969).
4. Downey, R.G., Hirschfeldt, D.R.: Algorithmic Randomness and Complexity, Springer-Verlag (to appear).
5. Downey, R.G., Hirschfeldt, D.R., Nies, A., Stephan, F.: Trivial Reals. In: Downey, R.G., Hui, Q.Y., Ping, T.S., Decheng, D., Yasugi, Y. (eds.) Proceedings of the 7th and 8th Asian Logic Conferences, pp. 103-131. World Scientiﬁc (2003).
6. Figueira, S., Stephan, F., Wu, G.: Randomness and Universal Machines, J. Complexity 22, 738–751 (2006).
7. Kolmogorov, A.N.: Three Approaches to the Quantitative Deﬁnition of Information, Problems Inform. Transmission 1, 1–7 (1965).
8. Levin, L.A.: On the Notion of a Random Sequence, Soviet Math. Dokl. 14 1413–1416 (1973).
9. Li, M., Vit´anyi, P.: An Introduction to Kolmogorov Complexity and Its Applications (2nd ed.). Springer-Verlag, New York (1997).
10. Nies, A., Lowness Properties and Randomness, Advances in Math. 197, 274– 305 (2005).
11. Nies, A., Stephan, F., Terwijn, S.: Randomness, Relativization and Turing Degrees, J. Symbolic Logic 70, 515–535 (2005).
12. Stephan, F., personal correspondence, (2006, 2008). 13. Schnorr, C.P.: Process Complexity and Eﬀective Random Tests, J. Comput.
System Sci. 7, 376–388 (1973). 14. Schnorr, C.P.: A Survey of the Theory of Random Sequences. In: Butts,
R.E., Hintikka, J. (eds.) Basic Problems in Methodology and Linguistics, pp. 193–210. D. Reidel (1977). 15. Shannon, C.E.: The Mathematical Theory of Communication, Bell System Tech. J. 27, 379–423, 623–656 (1948). 16. Shannon, M.M.: Forensic Relative Strength Scoring: ASCII and Entropy Scoring, Int. J. Digital Evidence 2 , 1–19 (2004). 17. Solomonoﬀ, R.J.: A Formal Theory of Inductive Inference (Part 1 and Part 2), Inform. Contr. 7, 1–22, 224–254 (1964).

Monotone Triviality and Minimality

63

18. Solovay, R.M.: Draft of a Paper (or Series of Papers) on Chaitin’s Work (Unpublished notes), IBM Thomas J. Watson Research Center, Yorktown Heights, NY (1975).
19. Veenman, C.J.: Statistical Disk Cluster Classiﬁcation for File Carving. In: IEEE Third International Symposium on Information Assurance and Security, 393–398 (2007).

Extraction of Eﬃcient Programs from Proofs: The Case of Structural Induction over Natural
Numbers
Luca Chiarabini
LMU Mathematisches Institut, Theresienstrasse 39, D-80333 Mu¨nchen, Germany chiarabi@mathematik.uni-muenchen.de
Abstract. Transforming a recursive procedure into a tail recursive one brings many computational beneﬁts; in particular in each recursive call there is no context information to store. In this paper we consider the particularly simple induction schema over natural numbers, and we propose two methods to automatically turn it into another proof with tail recursive content: one continuation and one accumulator based.
Key words: Functional Programming, Program extraction from constructive proofs, Program development by proof transformation, CPSTransformation, Defunctionalization.
1 Introduction
Let M be a proof by induction over n (natural number) of the property ∀n.ϕ(n), and let, by the Proofs-as-Program paradigm, [[M ]] be the (recursive) content of M . In this paper we will try to answer the following question: If [[M ]] is not tail recursive, is it possible (and if yes, how) to turn automatically M into another proof, say N , with tail recursive content? During an informal chat in the ﬁrst MATHLOGAPS meeting in Fischbachau, Germany, Andrej Bauer suggested to me an interesting idea[1] to mimic the behavior of a let-expression by a pure λ-calculus function. Here we present and develop his idea in a formal setting, which will provide a solution for the problem proposed above. Just to give an intuition of what we will see, let
define FACT = fun n -> (if n=0 then 1 else (n* (FACT (n-1))))
be the factorial function, written in an Ocaml-like syntax. FACT is not tail recursive because in each step of the computation the compiler has to store the context (n∗[]), evaluate FACT (n-1) → v, and returns (n∗v). We turn FACT into a simpler function where it is not necessary to store any context information:
define FACT’ = fun n -> (define FACT’’= fun n,m,y -> (if n=0 then y else FACT’’(n-1)(m+1)((m+1)*y))) n 0 1

Extraction of Eﬃcient Programs from Proofs

65

Now suppose FACT to be the computational content of the proof by induction M , with end formula ∀n.ϕ(n), that states that for each natural n there exists its factorial. Given a natural n, (FACT’’n) is a function that takes the natural m, the witness y for ϕ(m) and returns a witness for ϕ(n + m). One of the two proofs transformations proposed in the present paper (the accumulator based one) will regards how to turn M into a new proof M ′ with end formula (∀n, m.ϕ(m) → ϕ(n + m)) and computational content equal to Fact’’. It is clear that given n, (FACT’’n 0 1) is the witness for ϕ(n) as desired.
The paper is organized as follows: section two is a short introduction to minimal logic and program extraction. In section three we address two transformations over proofs in order extract continuation and accumulator based programs. The last section regards future works. All the proofs presented in the paper were developed with the MINLOG proof assistant[2].

2 Modiﬁed Realizability for First Order Minimal Logic
2.1 Go¨del’s T
Types are built from base types N (Naturals) , L(ρ) (lists with elements of type ρ) and B (booleans) by function (→) and pair (×) formation. The Terms of G¨odel’s T[3] are simply typed λ-calculus terms with pairs, projections (πi) and constants (constructors and recursive operators for the basic types)
T ypes ρ, σ ::= N | B | L(ρ) | ρ → σ | ρ × σ Const c ::= 0N | SuccN→N | ttB | ﬀB | []L(ρ) | ::ρ→L(ρ)→L(ρ) |RσN| RσL(ρ)| RσB
T erms r, s, t ::= c |xρ|(λxρrσ)ρ→σ|(rρ→σsρ)σ|(π0tρ×σ)ρ| (π1tρ×σ)σ | (rρ, sσ)ρ×σ
The types and conversion rules for the recursive operators, applications and projections are:

RσN : σ → (N → σ → σ) → N → σ
(RσN b f ) 0 −→ b (RσN b f ) (n + 1) −→ f n ((RσN b f ) n)

RσL(ρ) : σ → (L(ρ) → σ → σ) → L(ρ) → σ (RσL(ρ) b f ) [] −→ b (RσL(ρ) b f ) (a :: l) −→ f l ((RσL(ρ) b f ) l)

RσB : σ → σ → B → σ (RσB r s)tt −→ r (RσB r s)ﬀ −→ s

π0(r, s) −→β r π1(r, s) −→β s (λx.r)s −→β r[x := s]

The term (RσB r s)t is normally printed as (if t r s). By −→Rηβ we indicate the union of −→β, −→, and −→η the η-reduction deﬁned as λx.rs −→η r if x ∈ FV(r). Finally we deﬁne the extensional equality relation =Rηβ, as the least equivalence relation that contain −→Rηβ. The extensional equality relation capture the idea that two functions should be considered equal if they yield equal
results whenever applied to equal arguments.

66 Luca Chiarabini
2.2 Heyting Arithmetic
We deﬁne Heyting Arithmetic HAω for our language based on G¨odel’s T, which is ﬁnitely typed. For falsity we can take the atomic formula F := atom(ﬀ) – called arithmetical falsity – built from the boolean constant ﬀ and the predicate operator on booleans atom. Below we will also need the (logical) falsity ⊥, which we can view as just a particular propositional symbol. We deﬁne negation ¬ϕ by ϕ → F or ϕ → ⊥ (depending on the context).
Formulas: Atomic formulas (Ptρ) (P a predicate symbol, t, ρ lists of terms and types), ϕ → ψ, ∀xρϕ, ∃xρϕ, ϕ ∧ ψ.
Derivations: In spite of the Curry-Howard correspondence it is convenient to write derivations as terms: we deﬁne λ-terms M ϕ for natural deduction proofs of formulas ϕ together with the set OA(M ) of free assumptions in M :
(ass) uϕ, OA(u)={u} (∧+) ( M ϕ, N ψ ϕ∧ψ), OA( M, N )=OA(M ) ∪ OA(N ) (∧−0 ) (M ϕ∧ψ0)ϕ , OA(M 0)=OA(M ) (∧−1 ) (N ϕ∧ψ1)ψ , OA(N 1)=OA(N ) (→+) (λuϕ.M ψ)ϕ→ψ , OA(λu.M )=OA(M )\{u} (→−) (M ϕ→ψN ϕ)ψ, OA(M N )=OA(M ) ∪ OA(N ) (∀+) (λxρ.M ϕ)∀xρ.ϕ, OA(λx.M )=OA(M )
provided xρ ∈ FV(ϕ), for any uϕ ∈OA(M ) (∀−) (M ∀xρϕtρ)ϕ, OA(M t)=OA(M )
Usually we will omit type and formula indices in derivations if they are uniquely determined by the context or if they are not relevant and sometimes we will write M : ϕ instead of M ϕ. In the above deﬁnition of derivations as terms we have left out the standard connectives ∃ and ∨ because for simplicity we want our derivation terms to be pure lambda terms formed just by lambda abstraction, application, pairing and projections. In spite of this omission we can use ∃ and ∨ in our logic, if we allow appropriate axioms as constant derivation terms, e.g. for ∃:
∃+xρ,ϕ : ∀xρ(ϕ → ∃xρϕ) ∃−xρ,ϕ,ψ : ∃xρϕ → (∀xρϕ → ψ) → ψ
with the usual proviso x ∈ F V (ψ). For ∨ we could introduce similar axioms, but we do not do here, since we can deﬁne ∨ from ∃ via:
ϕ ∨ σ ∃pB.(p → ϕ) ∧ ((p → ⊥) → ψ)
Finally the inductions axioms associated to the types N,B and L(ρ) are:
Indn,ϕ(n) : ϕ(0) → (∀n.ϕ(n) → ϕ(n + 1)) → ∀nN.ϕ(n) Indt,ϕ(t) : ϕ(tt) → ϕ(ﬀ) → ∀tB.ϕ(t) Indl,ϕ(l) : ϕ([]) → (∀a, l.ϕ(l) → ϕ(a :: l)) → ∀lL(ρ).ϕ(l)

Extraction of Eﬃcient Programs from Proofs 2.3 Modiﬁed Realizability

67

Clearly proper existence proofs have computational content. A well-known and natural way to deﬁne this concept is the notion of realizability, which can be seen as an incarnation of the Brouwer-Heyting-Kolmogorov interpretation of proofs.

Type of a Formula We deﬁne τ (ϕ) as the type of the term (or “program”) to be extracted from a proof of ϕ. More precisely, we assign to every formula ϕ an object τ (ϕ) (a type or the “nulltype” symbol ε). In case τ (ϕ) = ε proofs of ϕ have no computational content; such formulas ϕ are called Harrop formulas. The deﬁnition can be conveniently written if we extend the use of ρ → σ and ρ × σ, to the nulltype symbol: (ρ → ε) := ε, (ε → σ) := σ, (ε → ε) := ε, (ρ × ε) := ρ, (ε × σ) := σ, (ε × ε) := ε:

τ (P tρ)

:= ε

τ (ϕ → ψ) := (τ (ϕ) → τ (ψ))

τ (ϕ ∧ ψ) := (τ (ϕ) × τ (ψ))

τ (∃xρ.ϕ(xρ)) := ρ × τ (ϕ)

τ (∀xρ.ϕ) := (ρ → τ (ϕ))

Realize a formula For a convenient deﬁnition we extend the use of term application and projection to the nullterm symbol: εt := ε, tε := t, εε := ε, π0ε := ε, π1ε := ε. We deﬁne the formula t mr ϕ, to be read t realizes ϕ:

t mr P t

:= P t

t mr ∃x.ϕ(x) := π0t mrϕ(π1t) t mr (ϕ → ψ) := ∀x.(x mr ϕ → tx mr ψ)

t mr (ϕ ∧ ψ) := (π0t mr ϕ ∧ π1t mr ψ)

t mr (∀x.ϕ) := ∀x.txmrϕ

Formulas which do not contain ∃ play a special role in this context; we call them negative. Their crucial property is (ε mr ϕ) = ϕ. Clearly every formula of the form (t mr ϕ) is negative.

Program Extraction We now deﬁne the extracted term [[M ]] of a derivation M [4]. For derivations M ϕ where τ (ϕ) = ε (i.e., ϕ is a Harrop formula) let
[[M ]] := ε (the nullterm symbol). We extend the use of terms in the form (t1, t2) to nullterm symbol: (ε, t2) := t2, (t1, ε) := t1, (ε, ε) := ε. Moreover in case τ (ϕ) = ε, xuτ(ϕ) := ε and (λε.[[M ]]) means just [[M ]]. Now assume that M derives
a formula ϕ with τ (ϕ) = ε. Then

68 Luca Chiarabini

[[uϕ]]

:= xuτ(ϕ) (xuτ(ϕ) uniquely associated with uϕ)

[[(λuϕ.N ψ)ϕ→ψ]] := λxuτ(ϕ).[[N ]]

[[(M ϕ→ψN ϕ)ψ]] := [[M ]][[N ]]

[[ M ϕ, N ψ ϕ∧ψ]] := ([[M ]], [[N ]])

[[(M ϕ∧ψi)]]

:= πi[[M ]]

[[(λxρ.M ϕ)∀xϕ]] := λxρ.[[M ]]

[[(M ∀xρϕtρ)ϕx[t]]] := [[M ]]t

We also need to deﬁne extracted terms for our axioms:

[[∃−xρ,ϕ,ψ]] := λpρ×τ(ϕ), f ρ→τ(ϕ)→τ(ψ).(f (π0p) (π1p)), assuming τ (ϕ) = ε

[[∃+xρ,ϕ]] := λxρ, yτ(ϕ).(x, y)

, assuming τ (ϕ) = ε

[[IFϕ]]

:= λbB, lτ(ϕ), rτ(ϕ).(if b l r)

, assuming τ (ϕ) = ε

[[Indn,ϕ(n)]] := RσN

[[Indl,ϕ(l)]] := RσL(ρ)

[[Indt,ϕ(t)]] := RσB

Theorem 2.1 (Soundness) Let M be a derivation of a formula ϕ from assumptions ui : ϕi. Then we can ﬁnd a derivation of the formula ([[M ]] mr ϕ) from assumptions u¯i: xui mr ϕi.

Proof. Induction on M .

3 Proof Manipulation

Deﬁnition 3.1 (Tail Expressions [5]) The tail expressions of t ∈ T erms, are deﬁned inductively as follows:
1. If t ≡ (λx.e) then e is a tail expression. 2. If t ≡ (if t r s) is a tail expression, then both r and s are tail expressions. 3. If t ≡ (Rι r s) is a tail expression, then r and s are tail expressions. 4. Nothing else is a tail expression.

With ι ∈ {N, L(ρ)}.

Deﬁnition 3.2 A tail call is a tail expression that is a procedure call.

Deﬁnition 3.3 (Tail Recursion [6]) A recursive procedure is said tail recursive when it tail calls itself or calls itself indirectly through a series of tail calls.

Now, Let consider F be the following induction proof over N:

|M |N

Indn,ϕ(n)

ϕ(0) ∀n.ϕ(n) → ϕ(n + 1) ∀n.ϕ(n)

The content of F is (RσN b f ) with b and f base and step case of the recursion operator, content of the proofs M and N .

Extraction of Eﬃcient Programs from Proofs

69

3.1 Continuation based Tail Recursion
Given the procedure (RσN b f ) deﬁned in the previous section, let Λ be the term:
RN(σ→σ′)→σ′ (λk.kb)(λn, p, k. p λu.k(f n u))
In Λ we name continuation the input parameter of type (σ → σ′). Λ is a function with just one tail recursive call and a functional accumulator parameter k with the follow property: for each n, at the i-th (0 < i ≤ n) step of the computation of (Λ n (λx.x)) the continuation has the form λu.(f (n − 1) (. . . (f (n − i) u) . . .). At the n-th step the continuation λu.(f (n − 1) (. . . (f 0 u) . . .) is applyed to the term b and returned. We see that such returned valued correspond to (RσN b f )n. This fact is stated formally in the following,

Theorem 3.1 For each natural n:

Λ n =Rηβ λkσ→σ′ . k((RσN b f )n)

Proof. Appendix A

By Theorem 3.1, (λn.Λ n (λx.x)) and (RσN b f ) are extensionally equals and by deﬁnition 3.3 Λ is tail recursive. The procedure call in (λn.Λ n (λx.x)) is tail being the body of a lambda abstraction. The question that motivated the writing

of this paper is: being

[[F ]] = (RσN b f )

then it is possible to turn F into F ′ such that

[[F ′]] = (λn.Λ n (λx.x))?

The answer is “yes”. The key point is understand the logical role of the continuation parameter in Λ: given a natural n, at each step i : n, . . . , 0 in computing (Λ n (λx.x)), the continuation is a function that takes the witness for ϕ(i) and returns the witness for ϕ(i + m), for m such that i + m = n. So we expect Λ to be the computational content of a proof with end formula:

∀n∀ncm.(ϕ(n) → ϕ(n + m)) → ϕ(n + m)

(1)

We observe that the counter m is introduced to count the “decrease” of n during the computation. It plays a “logical” role, but at programming level it is irrelevant. As we expect to extract programs from proofs, we explicitly underline the “hidden” role of m quantifying over it by the special non-computational quantiﬁer ∀nc [7, page 47]. Supposing to know the derivation terms M : ϕ(0) and N : ∀n.ϕ(n) → ϕ(n + 1), let’s prove (1):
Proposition 3.1 ϕ(0) → (∀n.(ϕ(n) → ϕ(n + 1))) → ∀n∀ncm.(ϕ(n) → ϕ(n + m)) → ϕ(n + m)

Proof. Assume b : ϕ(0) and f : ∀n.(ϕ(n) → ϕ(n + 1)). By induction on n.

70 Luca Chiarabini

n = 0 We have to prove ∀ncm.(ϕ(0) → ϕ(m)) → ϕ(m)

So assume m and k : (ϕ(0) → ϕ(m)). Apply k to b : ϕ(0). n + 1 Assume n, the recursive call p : ∀ncm.(ϕ(n) → ϕ(n + m)) → ϕ(n + m),
m, and the continuation k : ϕ(n + 1) → ϕ(n + m + 1). We have to prove:

ϕ(n + m + 1)

Apply p to (m + 1) obtaining (p (m + 1)) : (ϕ(n) → ϕ(n + m + 1)) → ϕ(n + m + 1). So, if we are able to prove the formula ϕ(n) → ϕ(n + m + 1), by some proof t, we can just apply (p (m + 1)) to t and we are done. So let’s prove
ϕ(n) → ϕ(n + m + 1)

Assume v : ϕ(n). We apply k to (f n v).

⊓⊔

For a formal proof in Natural Deduction style of Proposition 3.1 the reader is invited to consult the Appendix C. Now we are able to provide a new proof of the induction principle on natural numbers:

Proposition 3.2 ϕ(0) → (∀n.(ϕ(n) → ϕ(n + 1))) → ∀n.ϕ(n).

Proof. Assume b : ϕ(0), f : ∀n.(ϕ(n) → ϕ(n + 1)) and n. To prove ϕ(n), instantiate the formula proved in Proposition 3.1 on b, f , n, 0 and ϕ(n) → ϕ(n). ⊓⊔
The term extracted from the previous proof is the following:

[b,f,n].(Rec nat => (sigma => sigma’) => sigma’) [k](k b) [n,p,k]p([u] k(f n u))
n ([x]x)

Here ([x1,...,xN]t) stands for (λx1, . . . , xN .t) and ((Rec nat=>sigma) r s) for (RσN r s).
Remark: Although the functional parameter in Λ is named as continuation, Λ is not the CPS-transformed scheme of the recursion over naturals. In fact f and b are not altered in our transformation and they could contain bad expressions, like not tail calls. The formula (1) could be substituted by the more general ∀n.(ϕ(n) → ⊥) → ⊥, but the author think that the approach proposed in this paper supply a clearer idea of the logical property the continuation parameter is supposed to satisfy. Moreover, such approach represent a not trivial usage of the not computational quantiﬁers ∀nc.

3.2 Accumulator based tail recursion
Here we present the essence of the Bauer’s[1] original idea. Given the procedure (RσN b f ) deﬁned in the last section, let Π be:
RNN→σ→σ (λm, y.y) (λn, p, m, y. p (m + 1) (f m y))

Extraction of Eﬃcient Programs from Proofs

71

In Π there are two accumulators parameters: a natural and parameter of type σ where are stored partial results. For each natural n, at the i-th (0 < i ≤ n) step of the computation of (Π n 0 b) the accumulator of the partial results will be equal to the expression (f (i − 1) (. . . (f 0 b) . . .)). At the n-th step (base case of Π) the accumulator of the partial results is returned and it correspond to (RσN b f )n. This fact is stated in the Theorem below.
Deﬁnition 3.4 For all n,m, let f N→N→σ→σ be a function such that:

f m n = f (n + m) Proposition 3.3 For all naturals n and m:

(RσN (f m 0 b) f m+1) n =Rηβ (RσN b f m) (n + 1) Proof. By induction on n.

⊓⊔

Theorem 3.2 For all natural n,

Π n =Rηβ λm, y.(RσN y f m) n
Proof. Appendix B.
By Theorem 3.2, λn.(Π n 0 b) and (RσN b f ) are extensionally equals, moreover by deﬁnition 3.3, Π is tail recursive and the procedure application in λn.(Π n 0 b) is a tail call being the body of a lambda abstraction . As done in the last section we address the following question: being

[[F ]] = (RσN b f ) then how to turn F into F ′ such that:

[[F ′]] = λn.(Π n 0 b)?

We note that given a natural n:

(λn.(Π n 0 b))n =Rηβ (Π n 0 b) ...
=Rηβ (f (n − 1) (. . . (f 0 b) . . .))

So given two natural indexes i , j, with i + j = n, (Π i j) is a function that takes the witness for ϕ(j) and returns the witness for ϕ(i + j). So we expect Π to be the computational content of a proof with end formula:

∀n, m.ϕ(m) → ϕ(n + m)

that use the proofs terms M ϕ(0) and N ∀n.ϕ(n)→ϕ(n+1) as assumptions. We prove this claim in the following,

72 Luca Chiarabini

Proposition 3.4 ϕ(0) → (∀n.ϕ(n) → ϕ(n + 1)) → ∀n, m.ϕ(m) → ϕ(n + m)

Proof. Assume b : ϕ(0) and f : ∀n.ϕ(n) → ϕ(n + 1). By induction on n:

n = 0 We have to prove

∀m.ϕ(m) → ϕ(m)

this is trivially proved by (λ m, u.u). n + 1 Let’s assume n, the recursive call p : ∀m.ϕ(m) → ϕ(n + m), m and the
accumulator y : ϕ(m). We have to prove

ϕ(n + m + 1)

Apply f to m and y obtaining (f m y) : ϕ(m + 1). Now apply p to (m + 1)

and (f m y).

⊓⊔

See Appendix D for the formal proof in Natural Deduction style. The accumulatorbased program transformation provides us with a new proof of the induction principle over natural numbers:

Proposition 3.5 ϕ(0) → (∀n.ϕ(n) → ϕ(n + 1)) → ∀n.ϕ(n).

Proof. Assume b : ϕ(0), f : (∀n.ϕ(n) → ϕ(n + 1)) and n. To prove ϕ(n): instan-

tiate the formula proved in Proposition 3.4 on n, 0 and b : ϕ(0)

⊓⊔

The term extracted from the previous proof is the following:

[b,f,n].(Rec nat => nat => sigma => sigma) [m,y]y [n,p,m,y]p (m+1)(f m u))
n0b

4 Conclusions and future work
The expression Π (section 3.2) represent a way to mimic a let expression just by a pure lambda calculus expression (original goal of Bauer in designing it). Moreover, while Π is tail recursive the recursive schema over natural written by a let would not. The terms Λ and Π compute, applied to opportune input parameters, the same function (RσN b f ). But we note that Λ is in some way more general than Π. The modiﬁcation of Λ in order to make it working on lists (let’s name it ΛL(ρ)) instead of naturals is straightforward; but more important, the proof from which ΛL(ρ) can be extracted is obtained by a slightly modiﬁcation of the proof from which Λ is extracted. In the case of lists the end formula to prove should be: ∀lL(ρ).(P (l) → ⊥) → ⊥. Unfortunately we can not extend in the same way Π and it’s proof: Π looks intrinsically dependent from the algebra of natural numbers. Our current work regard the study of such extensions. Which is the formal connection between Λ and Π? We claim is possible transform Λ into Π by the so called defunctionalization technique (Reynolds [8], Danvy and others [9]) a whole program transformation to turn higher-order into

Extraction of Eﬃcient Programs from Proofs

73

ﬁrst-order functional programs. But also this aspect need a deeper investigation. Possible applications of Λ and Π go beyond the tail recursion. We noted that there exists proofs from which are extracted programs that run in exponential time that can be turned (by the proofs transformations proposed here) in new proofs from which is possible to extract polynomial time algorithms. This can appear pretty amazing and we are currently working in order to state such result more precisely. Another application of the proofs transformations proposed here is an extension of the CPS-transformation over formal proofs (Schwichtenberg[10] and Griﬃn[11]) but this time concerning the induction axiom. The proposal is perform CPS over proofs in two stages: a preprocessing step where are transformed all the proofs by induction, and a second stage where the canonical CPS-transformation is applied. Currently we are studying also this aspect but it need a deeper investigation.

Acknowledgments
I really thank Helmut Schwichtenberg, Philippe Audebaud, Amr Sabry Stefan Schimanski, Freiric Barral, Volker Heun, Andrej Bauer for the helpful discussions and feedback. Thank’s to the anonymous referee for their useful comments.

References

1. http://math.andrej.com/2005/09/16/proof-hacking/.

2. http://www.minlog-system.de/.

3. M.H. Sørensen and P.Urzyczyn. Lectures on the Curry-Howard Isomorphism, vol-

ume 149 of Studies in Logic and the Foundations of Mathematics. Elsevier, 2006.

4. G. Kreisel. Interpretation of Analysis by means of Functionals of Finite Type. In

A. Heyting, editor, Constructivity in Mathematics, 1959. 5. R. Kelsey, W. Clinger, and J. Rees (eds.). Revised5 report on the algorithmic

language scheme. Higher-Order and Symbolic Computation, 11(1), August, 1998.

6. K. Kent Dybvig. The Scheme Programming Language. Mit Press, 1996.

7. Helmut Schwichtenberg. Minimal Logic for Computable Functionals.

http://www.mathematik.uni-muenchen.de/~chiarabi/mlcf.pdf,

December

2008.

8. John C. Reynolds. Deﬁnitional interpreters for higher-order programming lan-

guages. Higher-Order and Symbolic Computation, 11(4):363–397, 1998. Reprinted

from the proceedings of the 25th ACM National Conference (1972).

9. Olivier Danvy and Lasse R.Nielsen. Defunctionalization at work. In editor Har-

ald Søndergaard, editor, Proceedings of the Third International Conference of Prin-

ciples and Practice of Declarative Programming, pages 162–174, Firenze, Italy,

September 2001. ACM Press. Extended version available as the technical report

BRICS RS-01-23.

10. Helmut Schwichtenberg. Proofs, lambda terms and control operators. In Logic of

computation. Proceedings of the NATO ASI.Marktoberdorf, Germany, 1995.

11. Timothy G. Griﬃn. A formulae-as-types notion of control. In 17th Annual ACM

Symp. on Principles of Programming Languages, POPL’90, San Francisco, CA,

USA, 1990.

74 Luca Chiarabini

A

Theorem A.1 For each natural n: Λ n =Rηβ λkσ→σ′ . k((RσN b f )n)
Proof. By induction over n: n=0

n+1

Λ 0 =Rηβ λk.kb =Rηβ λk. k((RσN b f )0)

Λ (n + 1) =Rηβ (λn, p, k. p λu.k(f n u)) n (Λ n)
=Rηβ λk.(Λ n)λu.k(f n u) =Rηβ λk.(λk.k((RσN b f )n))λu.k(f n u) =Rηβ λk.(λu.k(f n u))((RσN b f )n) =Rηβ λk.k(f n ((RσN b f )n)) =Rηβ λk.k((RσN b f )(n + 1))

(By IH)

⊓⊔

B

Theorem B.1 For all natural n,

Π n =Rηβ λm, y.(RσN y f m) n Proof. By induction on n: n=0

n+1

Π 0 =Rηβ λm, y.y =Rηβ λm, y.(RσN y f m) 0

Π (n + 1) =Rηβ (λn, p, m, y.p(m + 1)(f m y)) n (Πn)

=Rηβ λm, y.(Πn) (m + 1) (f m y)

=Rηβ λm, y.(λm, y.(RσN y f m )n) (m + 1) (f m y) By IH =Rηβ λm, y.(RσN (f m y) f m+1) n

=Rηβ λm, y.(RσN (f m 0 y) f m+1) n

By Def.3.4

=Rηβ λm, y.(RσN y f m )(n + 1)

By Prop. 3.3

⊓⊔

Extraction of Eﬃcient Programs from Proofs

75

C
Proposition C.1 ϕ(0) → ∀n.ϕ(n) → ϕ(n + 1)) → ∀n∀ncm.(ϕ(n) → ϕ(n + m)) → ϕ(n + m)
Proof. Assume b : ϕ(0) and f : ∀n.ϕ(n) → ϕ(n + 1)). By induction on n. n=0
[k : ϕ(0) → ϕ(m)] [b : ϕ(0)] (k b) : ϕ(m)
(λ k.k b) : (ϕ(0) → ϕ(m)) → ϕ(m) (λ m, k.k b) : ∀ncm.(ϕ(0) → ϕ(m)) → ϕ(m)
n>0

[f : ∀n.ϕ(n) → ϕ(n + 1)] n

(f n) : ϕ(n) → ϕ(n + 1)

[k : ϕ(n + 1) → ϕ(n + m + 1)]

(f n v) : ϕ(n + 1)

(k (f n v)) : ϕ(n + m + 1)

(λ v.k (f n v)) : ϕ(n) → ϕ(n + m + 1)

[v : ϕ(n)]

[p : ∀ncm.(ϕ(n) → ϕ(n + m)) → ϕ(n + m)] (m + 1) (p(m + 1)) : (ϕ(n) → ϕ(n + m + 1)) → ϕ(n + m + 1)
(p (m + 1) (λ v.k (f n v))) : ϕ(n + m + 1) (λ k.p (m + 1) (λ v.k (f n v))) : (ϕ(n + 1) → ϕ(n + m + 1)) → ϕ(n + m + 1) (λ m, k.p (m + 1) (λ v.k (f n v))) : ∀ncm.(ϕ(n + 1) → ϕ(n + m + 1)) → A(n + m + 1) (λ p, m, k. p (m + 1) (λ v.k (f n v))) : (∀ncm.(ϕ(n) → ϕ(n + m)) → ϕ(n + m)) →
∀ncm.(ϕ(n + 1) → ϕ(n + m + 1)) → ϕ(n + m + 1) (λ n, p, m, k. p (m + 1) (λ v.k (f n v))) : ∀n.(∀ncm.(ϕ(n) → ϕ(n + m)) → ϕ(n + m)) →
∀ncm.(ϕ(n + 1) → ϕ(n + m + 1)) → ϕ(n + m + 1)
D
Proposition D.1 ϕ(0) → (∀n.ϕ(n) → ϕ(n + 1)) → ∀n, m.ϕ(m) → ϕ(n + m)
Proof. Assume b : ϕ(0), f : (∀n.ϕ(n) → ϕ(n + 1)). By induction on n: n=0
[u : ϕ(m)] (λ u.u) : ϕ(m) → ϕ(m) (λ m, u.u) : ∀m.ϕ(m) → ϕ(m)

76 Luca Chiarabini

n>0

[f : ∀n.ϕ(n) → ϕ(n + 1)] m (f m) : ϕ(m) → ϕ(m + 1) [y : ϕ(m)]
(f m y) : ϕ(m + 1)

[p : ∀m.ϕ(m) → ϕ(n + m)] (m + 1) (p (m + 1)) : ϕ(m + 1) → ϕ(n + m + 1)
(p (m + 1) (f m y)) : ϕ(n + m + 1) (λ y. p (m + 1) (f m y)) : ϕ(m) → ϕ(n + m + 1) (λ m, y. p (m + 1) (f m y)) : ∀m.ϕ(m) → ϕ(n + m + 1) (λ p, m, y. p (m + 1) (f m y)) : (∀m.ϕ(m) → ϕ(n + m)) → (λ p, m, y. p (m + 1) (f m y)) : aaaaaaaaaaaa(∀m.ϕ(m) → ϕ(n + m + 1)) (λ n, p, m, y. p (m + 1) (f m y)) : ∀n.(∀m.ϕ(m) → ϕ(n + m)) → (λ n, p, m, y. p (m + 1) (f m y)) : ∀n.aaaaaaaa(∀m.ϕ(m) → ϕ(n + m + 1))

Phase Shifts of LFSM as Pseudorandom Number Generators for BIST for VLSI
Sung-Jin Cho1, Un-Sook Choi2, Han-Doo Kim3, Yoon-Hee Hwang4, and Jin-Gyoung Kim5
1 Division of Mathematical Sciences, Pukyong National University Busan 608-737, Korea, sjcho@pknu.ac.kr
2 Department of Multimedia Engineering, Tongmyong University Busan 626-847, Korea, choies@tu.ac.kr
3 Institute of Mathematical Sciences and School of Computer Aided Science Inje University, Gimhae 621-749, Korea, mathkhd@inje.ac.kr 4 Department of Information Security, Graduate School Pukyong National University Busan 608-737, Korea, yhhwang@pknu.ac.kr 5 Department of Applied Mathematics, Graduate School Pukyong National University Busan 608-737, Korea, 5892587@hanmail.net
Abstract. Large phase shifts are generally desirable as they result in less correlation and, therefore, higher fault coverage in the testing of VLSI. In this paper, we investigate the phase shifts of the sequences generated by companion matrices of primitive polynomials. Also we propose an algorithm for ﬁnding phase shifts of the sequences.
1 Introduction
The bit sequences generated by successive cells of built-in test pattern generators suﬀer in general from correlations and/or linear dependencies. This is problematic for pseudorandom and/or pseudoexhaustive generation of test patterns, structures commonly employed in a variety of test applications ([1] ∼ [3]). A linear feedback shift register (LFSR) is usually used as an on-chip test pattern generator, which drives ﬂip-ﬂop chains in parallel. In the context of digital circuit testing, LFSRs have been used as very popular signature analyzers. As the number of gates or digital circuits that can be integrated into one chip of silicon becomes hundreds of million, the testing of VLSI has become a very important engineering problem. Since the number of states that a VLSI can assume is astronomically large, it is impossible to check all the states of each chip of VLSI before shipping. So we check it by some mechanism of random sampling. It was customary up to several years ago to use an LFSR as a test pattern generator for this purpose. A linear ﬁnite state machine (LFSM) is an external XOR LFSRs.
This work was supported by grant No. (R01-2006-000-10260-0) from the Basic Research Program of the Korea Science and Engineering Foundation.

78 Sung-Jin Cho et al.

Linear dependencies among bit positions in the patterns generated by an LFSM are to be avoided because they limit the number of diﬀerent bit combinations seen by the circuit under test when that LFSM is used for pseudorandom testing of the circuit. The issue of linear dependencies has been studied by various researchers ([3] ∼ [6]). In general, the bit sequences produced by an LFSM are shifted versions of each other. The diﬀerence in the number of shift positions that each bit sequence exhibits with respect to a reference sequence is referred to as the phase shift of that sequence. Large phase shifts are generally desirable as they result in less correlation and, therefore, higher fault coverage in the testing of VLSI. For many years, many researchers have been working on how to ﬁnd phase shifts of maximum-length sequences. The phase shift analysis of 90/150 CA whose characteristic polynomials are primitive, has been investigated by Bardell [7], Das et al. [8], Sarkar ([9], [10]) and Cho et al. ([11], [12]). Also the phase shift analysis of LFSMs whose characteristic polynomials are primitive has been investigated by Bardell et al. [2] and Kagaris et al. ([4], [13] ∼ [17]).
In this paper, we investigate the phase shifts of the sequences generated by companion matrices of primitive polynomials. Also we propose an algorithm for ﬁnding phase shifts of the sequences.

2 Preliminaries

Let GF (2) be the Galois ﬁeld with cardinality 2. In this section, we investigate some properties of sequences generated by companion matrices of primitive polynomials.
Deﬁnition 2.1. Let f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1, where ci ∈ GF (2). Then the following n × n matrix T is said to be the companion matrix of f (x).

 cn−1

 cn−2

 T =


...

 

c1

1

1 0 ··· 0



0 1 ··· 0

...

...

...

...

  

=

 

C(n−1)×1

0

0

·

·

·

1

 



1

0 0 ··· 0


In−1   
O1×(n−1)

, where In−1 is an (n − 1) × (n − 1) identity matrix and C = (cn−1, cn−2, · · · , c1)t. Since |T | = |In−1| = 1, T is nonsingular. The inverse T −1 of T is

0 0 ··· 0 0

1 0 ··· 0 0

T

−1

=

  

...

...

...

...

...

 

0

0

·

·

·

1

0

0 0 ··· 0 1

1 cn−1 

 O1×(n−1)

1

...





=

 





c2

 

In−1



C(n−1)×1

 

c1

Deﬁnition 2.2. ([18], [19]) Let f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1 be an n-degree primitive polynomial, where c1, · · · , cn−1 ∈ GF (2). Then

Phase Shifts of LFSM as Pseudorandom Number Generators

79

f (x) generates a periodic sequence whose period is 2n −1. This sequence is called a pseudo-noise(PN) sequence.

PN sequences are of fundamental importance in computer science, cryptology
and engineering. The following are some basic properties of PN sequences of
degree n [18]. (P1) The period of a PN sequence of degree n is 2n − 1. (P2) In a PN sequence of period 2n − 1, the 0-run of length n − 1 occurs exactly
once.

3 Analysis of sequences generated by companion matrices of primitive polynomials
In this section, the method for ﬁnding phase shifts of the sequences generated by companion matrices of primitive polynomials is presented. The following theorems give various methods to compute phase shifts.
Let S be the (2n−1)×n matrix that has as rows x0, T x0, · · · , T 2n−2x0, where
n−1
x0 = (0, · · · , 0, 1)t. Then S is the (2n − 1) × n matrix consisting of n independent PN sequences as its columns. Denote T ix0 by xi for each i(1 ≤ i ≤ 2n − 2).

 x0t   0 0 0 · · · 0 0 1 

 x1t 

 

...

 

  

0 ...

0 ...

0 ··· 0 1 ... . . . ... ...

 

S

=

(sij )(2n−1)×n

=

 

xn−1t

 

=

 

1

0

0 ··· 0 0

 

xnt

 

 

...

 

   

cn−1 ...

cn−2 ...

cn−3 ...

··· ...

c2 ...

c1 ...

0 ...

  



0



1

...

  

x2n−2t

1 cn−1 cn−2 · · · c3 c2 c1

Let ps(i) be the phase shift of the ith column with respect to the 1st column of S. From the deﬁnition of the phase shift we obtain the following theorems.

Theorem 3.1. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. Then ps(1) = 0 and ps(n) = 1.
Theorem 3.2. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1 and let cn−1 = cn−2 = · · · = cn−m = 0 and cn−m−1 = 1. Then ps(i) = 2n − i (2 ≤ i ≤ m + 1).
Example 3.3. We can ﬁnd the phase shifts of the primitive polynomial x6 + x + 1 by Theorem 3.1 and Theorem 3.2. ps(1) = 0, ps(6) = 1 by Theorem

80 Sung-Jin Cho et al.
3.1. ps(2) = 26 − 2 = 62, ps(3) = 26 − 3 = 61, ps(4) = 26 − 4 = 60 and ps(5) = 26 − 5 = 59 by Theorem 3.2.
Lemma 3.4. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let T k+i + T k+(i+m) = T i(k ∈ N, 0 ≤ i ≤ n − 2 and 1 ≤ m ≤ n − 1), where N is the set of all positive integers. Then xk+i + xk+(i+m) = xi.
Lemma 3.5. Let f (x) be the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let the (j + 1)th row vector of S be (x1, x2, · · · , xn). Then the (j + 2)th row vector xj+1t of S is (x2, x3, · · · , xn, 0) + x1(cn−1, · · · , c1, 1).
Theorem 3.6. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let cn−1 = 1 and T k + T k+1 = I for some k ∈ N. Then ps(2) = k.
Proof. Since T k + T k+1 = I, T k+i + T k+(i+1) = T i(0 ≤ i ≤ n − 1). By Lemma 3.4, xk+i + xk+i+1 = xi(0 ≤ i ≤ n − 1). That is,
sk+1,n + sk+2,n = 1,
sk+2,n−1 + sk+3,n−1 = 1, ...
sk+n,1 + sk+n+1,1 = 1.
Hence the 1st (resp. 2nd) elements of the (n − 1) vectors xkt, · · · , xk+n−2t are the same. Suppose that the 1st and the 2nd elements of the (n − 1) vectors xkt, · · · , xk+n−2t are of the form 0 ∗ . In the 1st column of S there are two vectors (s11, · · · , sn−1,1)t = (0, · · · , 0)t and (sk+1,1, · · · , sk+n−1,1)t = (0, · · · , 0)t. Since the 0-run of length n − 1 occurs exactly once in a PN sequence of period 2n − 1 by (P2), this is a contradiction. This means that the 1st and the 2nd elements of the (n − 1) vectors xkt, · · · , xk+n−2t must be of the form 1 ∗ . Suppose that the 1st and the 2nd elements of the (n − 1) vectors xkt, · · · , xk+n−2t are of the form 11 . Since xk = (1, 1, ∗, · · · , ∗)t, xk+1 = (1, ∗, · · · , ∗, 0)t + (cn−1, cn−2, · · · , c1, 1)t = (0, ∗, · · · , ∗, 1)t by Lemma 3.5. But xk + xk+1 = (0, · · · , 0, 1)t. This is a contradiction. Thus the 1st and the 2nd elements of the (n − 1) vectors xkt, · · · , xk+n−2t must be all 10 . Since (n − 1) consecutive 0 s occur from the (k + 1)th element to the (k + n − 1)th element in the 2nd column of S, ps(2) = k.
Example 3.7. We can ﬁnd the phase shift ps(2) of the primitive polynomial x7 + x6 + x5 + x4 + 1 by Theorem 3.6. Since T 86 + T 87 = I, ps(2) = 86.
Theorem 3.8. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let ci+1 = 1 and ci = ci−1 = · · · = c1 = 0 for some i(1 ≤ i ≤ n − 2). Then ps(n − j) = j + 1(1 ≤ j ≤ i).

Phase Shifts of LFSM as Pseudorandom Number Generators

81

Proof. By Lemma 3.5, the (n + 2)th row vector xn+1t of S is of the following form:

xn+1t

=

cn−1(cn−1,

·

·

·

,

1,

n−i
0,

·

·

·

,

0,

0,

1)

+

(cn−2,

·

·

·

,

n−i−1
0,

0,

·

·

·

,

0,

1,

0)

n−i
= (cn−1 + cn−2, · · · , cn−1, 0 , · · · , 0, 1, cn−1).

By the similar method, S is of the following form:



 x0t 

0

n−i  0 ··· 0 0 0 ··· 0 0 1

  

x1t ...

  



 

xit

 

 

xi+1t

 

 

...

 



 

0

  

...

 

0

 

0

  

...

0 ··· 0 ... . . . ... 0 ··· 0 0 ··· 1 ... . . . ...

0

0

···

0

1

0

 

...

...

...

...

...

...

  

1

0

···

0

0

0

 

0

0

···

0

0

0

 

...

...

...

...

...

...

  

       

xn−1t xnt
xn+1t xn+2t

 =

1

0 ··· 0

 

 

cn−1

cn−2

···

1

 

 

∗

∗ · · · cn−1

 

 

∗

∗ ··· ∗

0

0

···

0

0

0

 













  

...

  

  

...

... . . . ...



B

 

 

xn+i−1t

 

 

∗

∗ ··· ∗

 

 

xn+it

 

 

∗

∗ ··· ∗

 

...  ... ... . . . ... ... ... . . . ... ... ... 

, where B is the following (i + 1) × (i + 1) submatrix of S:

 sn+1,n−i

B= 

...

sn+1,n−i+1 · · · sn+1,n 

...

...

...

 

sn+i+1,n−i sn+i+1,n−i+1 · · · sn+i+1,n

0 0 ··· 0 0 1

0 0 ··· 0 1 ∗

=

   

0 ...

0 ...

··· ...

1 ...

∗ ...

∗ ...

   

 0 1 ··· ∗ ∗ ∗

1 ∗ ··· ∗ ∗ ∗

For 1 ≤ j ≤ i, since sj+1,n−j = 1, sj+2,n−j = sj+3,n−j = · · · = sj+n,n−j = 0 and sj+n+1,n−j = 1, the number of consecutive 0 s in the (n − j)th column of S is n − 1. Thus ps(n − j) = j + 1.

Example 3.9. We can ﬁnd the phase shifts ps(3) = 3 and ps(4) = 2 of the primitive polynomial x5 + x3 + 1 by Theorem 3.8.
Corollary 3.10. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let ci+1 = 1 and ci = ci−1 = · · · = c1 = 0. Then ps(k) = n − k + 1(n − i ≤ k ≤ n − 1).

82 Sung-Jin Cho et al.

Theorem 3.11. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let ci+1 = 0. Then ps(n − i − 1) = ps(n − i) + 1(0 ≤ i ≤ n − 2).
Proof. Let ps(n − i) = p. Then sp+1,n−i = sp+2,n−i = · · · = sp+(n−1),n−i = 0 and sp+n,n−i = 1 by (P2). Since sk+1,n−i−1 = sk,n−i + ci+1sk,1(1 ≤ k ≤ 2n − 2) by Lemma 3.5 and ci+1 = 0,
sp+2,n−i−1 = sp+1,n−i = 0,
sp+3,n−i−1 = sp+2,n−i = 0, ...
sp+n,n−i−1 = sp+n−1,n−i = 0,
sp+n+1,n−i−1 = sp+n,n−i = 1.
Thus ps(n − i − 1) = ps(n − i) + 1.
Corollary 3.12. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let ci+k = ci+(k−1) = · · · = ci+1 = 0 and ps(n − i) = p. Then ps(n − i − j) = ps(n − i) + j = p + j(1 ≤ j ≤ k).
Theorem 3.13. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let cn−1 = · · · = cn−m+1 = 0 and cn−m = 1. If T k + T k+m = I for some k ∈ N, then ps(m + 1) = k.
Proof. Since T k + T k+m = I, T k+i + T k+m+i = T i(0 ≤ i ≤ n − 1). By Lemma 3.4, xk+i + xk+m+i = xi(0 ≤ i ≤ n − 1). Since xk + xk+m = x0, sk+1,m+1 = · · · = sk+m+2,m+1 = 0. From xk+i + xk+m+i = xi(1 ≤ i ≤ n − 1), we obtain sk+m+3,m+1 = · · · = sk+(n−1),m+1 = 0. Thus sk+1,m+1 = sk+2,m+1 = · · · = sk+(n−1),m+1 = 0 and sk+n,m+1 = 1. Hence by (P2) ps(m + 1) = k.

Example 3.14. We can ﬁnd the phase shift ps(3) = 14 of the primitive polynomial x6 + x4 + x3 + x + 1 by Theorem 3.13.

Theorem 3.15. Let T be the companion matrix of the n-degree primitive
polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let ci = 1 and ci−1 = · · · = c1 = 0. If T k + T k−i = I for some k ∈ N, then ps(n − i) = k + 1.

Proof. The matrix obtained by (i + 1) row vectors xk−it, xk−i+1t, · · · , xkt of S

is of the following form:

 xk−it 

xk−i+1 t

 

xk−i+2

t

 

  

...

=  

 xk−1t 

xk t

Phase Shifts of LFSM as Pseudorandom Number Generators

83

 sk−i+1,1

···

sk−in+−1i,n−i

sk−in+−1i,+n−1 i+1 · · · sk−i+1,n

 sk−i+2,1 · · · sk−i+1,n−i+1 + ci · sk−i+1,1

∗

· · · sk−i+1,1 

 

sk−i+3,1

···

sk−i+1,n−i+2 + ci · sk−i+2,1

∗

···

sk−i+2,1

 

  

...

  

 sk,1 · · · sk−i+1,n−1 + ci · sk−1,1

sk−i+1,n

sk−i+1,1

··· 

sk−i+1,1 · · ·

sk−i+1,n−1

sk−i+1,1

· · · sk−i+1,n

Since ci−1 = ci−2 = · · · = c1 = 0, sk+1,n−i+1 = sk−i+1,1. Since T k−i + T k =

I, by Lemma 3.4, xk−i + xk = x0 and thus sk−i+1,n−i+1 + sk+1,n−i+1 = 0.

Since sk+1,n−i+1 = sk−i+1,1, sk−i+1,n−i+1 = sk−i+1,1. Therefore sk−i+1,n−i+1 + ci · sk−i+1,1 = 0 because ci = 1. By the similar method we can show that sk−i+3,n−i = sk−i+4,n−i = · · · = sk,n−i = 0. Also sk−i+1,n−i = sk−i+1,n + ci · sk,1 = sk−i+1,n + sk−i+1,n = 1. Thus sk+1,n−i = 1 and sk+2,n−i = · · · = sk+n,n−i = 0. Hence ps(n − i) = k + 1.

Theorem 3.16. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. Let c1 = c2 = · · · = cm = 1 and cm+1 = 0. And let T ki + T ki+1 + · · · + T ki+i = I(1 ≤ i ≤ m).
Then ps(n − i) = ki + i + 1.

Proof. For ki such that T ki + T ki+1 + · · · + T ki+i = I, the matrix obtained by (i + 1) vectors xki , xki+1, · · · , xki+i of S is of the following form:

n−i

 xki t   ∗

∗ ··· 0 ∗ ∗ ··· ∗ 

 xki+1t  ∗ ∗ · · · 0 ∗ ∗ · · · ∗

  

...



 

=

 

...

...

... ... ... ... ...

...

  

 

xki +i−2 t

 

 

0

 

 xki+i−1t 



1

xki +i t

ski+(i+1),1 ski+(i+1),2 · · · 1 ∗ · · · · · · ski+(i+1),n

Since c1 = c2 = · · · = ci = 1, ski+1,n−i = · · · = ski+(i−1),n−i = 0 and ski+i,n−i = ski+(i+1),n−i = 1, so that ski+(i+2),n−i = · · · = ski+(i+n),n−i = 0. Thus ps(n −
i) = ki + i + 1.

Example 3.17. For the primitive polynomial x8 + x6 + x5 + x4 + 1, ps(1) = 0 and ps(8) = 1 by Theorem 3.1. By Theorem 3.2, ps(2) = 254. By Theorem 3.11, ps(5) = 4, ps(6) = 3 and ps(7) = 2. Since T 48 + T 50 = I, ps(3) = 48 by Theorem 3.16. Since T 100 + T 96 = I, ps(4) = 101 by Theorem 3.15.
The following theorem is the main theorem in this section.
Theorem 3.18. Let T be the companion matrix of the n-degree primitive polynomial f (x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. Let cj = 1 for some j (1 < j < n). And let T k + cj+1T k+1 + · · · + cn−1T k+(n−j−1) + T k+(n−j) = I. Then ps(n − j + 1) = k.
Example 3.19. Phase shifts for the primitive polynomial f (x) = x12 + x10 + x9 + x8 + x6 + x2 + 1 are in Table 1.

84 Sung-Jin Cho et al.
Table 1. Phase shifts for f (x)
Phase shifts Theorem name ps(1) = 212 − 1 Theorem 3.1 ps(2) = 4094 Theorem 3.2 ps(3) = 1156 Theorem 3.18 ps(4) = 2511 Theorem 3.18 ps(5) = 935 Theorem 3.18 ps(6) = 934 Theorem 3.11 ps(7) = 1162 Theorem 3.11 ps(8) = 1161 Theorem 3.11 ps(9) = 1160 Theorem 3.11 ps(10) = 1159 Theorem 3.15 ps(11) = 2 Theorem 3.10 ps(12) = 1 Theorem 3.1
4 Algorithms to compute Phase shifts
Using the theorems outlined above, we give an algorithm to ﬁnd the phase shifts of the sequences generated by companion matrices of primitive polynomials.
Algorithm FindPhaseShiftsOfLFSM
Input: n, The 1st column (cn−1, cn−2, · · · , c1, 1) of the companion matrix T of an n-degree primitive polynomial f (x) = xn+cn−1xn−1+cn−2xn−2+· · ·+c1x+1.
Output: The phase shifts ps(i)(1 ≤ i ≤ n) of the ith column of S. BEGIN ps(1) ← 2n − 1 ps(n) ← 1 count ← 0
Step 1. Find m = min{j|cj = 1, 1 ≤ j < n}; Step 2. for h ← 1 to m − 1
do { ps(n − h) ← h + 1 count ← count + 1 } if count > n − 3 then Stop. Step 3. ps(n − m) ← k subject to T k + T k−m = I count ← count + 1 if count > n − 3 then Stop. Step 4. Find M = max{j|cj = 1, m ≤ j ≤ n − 1 }; Step 5. for l ← n − 1 downto M + 1 do { ps(n − l + 1) ← ps(n − l) − 1

Phase Shifts of LFSM as Pseudorandom Number Generators

Table 2. Phase shifts for f (x) (In this table, 86540 stands for the polynomial x8 + x6 + x5 + x4 + 1.)

n f (x)

phase shifts

8 86540

0,254,48,101,4,3,2,1

8 87530

0,242,241,69,68,3,2,1

8 8765420

0,121,179,108,246,245,2,1

9 94310

0,510,509,508,507,24,317,316,1

9 9643210

0,510,509,275,274,356,331,460,1

9 986543210

0,107,106,92,459,325,248,109,1

10 10,4,3,1,0

0,1022,1021,1020,1019,1018,966,533,532,1

10 10,6,5,3,2,1,0 0,1022,1021,1020,769,326,325,856,450,1

10 10,7,6,5,4,3,2,1,0 0,1022,1021,479,527,315,292,45,439,1

11 11,10,8,1,0

0,308,309,316,315,314,313,312,311,310,1

12 12,10,2,1,0 17 17,3,0

0,4094,639,640,641,642,643,644,645,646,2369,1 0, 217 − 2, 217 − 3, · · · , 217 − 14,3,2,1

85

count ← count + 1 } if count > n − 3 then Stop. Step 6. while count ≤ n − 3, do { if CM = 1, then {ps(n − M + 1) = k subject to T k +cM+1T k+1+· · ·+cn−1T k+(n−M−1)+T k+(n−M) = I count ← count + 1 M ←M −1 } else { ps(n − M + 1) ← ps(n − M ) − 1 M ←M −1 }} END
Phase shifts with respect to the 1st column of the given primitive polynomial are in Table 2.
5 Conclusion
In this paper, we investigated the phase shifts of the sequences generated by companion matrices of primitive polynomials. And we proposed an algorithm for ﬁnding phase shifts of the sequences.

86 Sung-Jin Cho et al.
References
1. P.H. Bardell, Calculating the eﬀects of linear dependencies on m-sequences used as test stimuli, IEEE Trans. CAD of Integrated Circuits and Systems, 11 (1992) 83-86
2. P.H. Bardell, W.H. McAnney and J. Savir, Built-In test for VLSI: pseudorandom techniques, John Wiley & Sons, 1987
3. C.L. Chen, Linear dependencies in linear feedback shift registers, IEEE Trans. Comput, 32 (1986) 1086-1088
4. J. Kakade and D. Kagaris, Phase shifts and linear dependencies, in Proc. IEEE Int. Symp. Circuits Syst., (2006) 1595-1598
5. A. Lempel, Analysis and synthesis of polynomials and sequences over GF (2), IEEE Trans. Inform. Theory, IT-17 (1971) 297-303
6. J. Rajski and J. Tyszer, On linear dependencies in subspaces of LFSR-generated sequences, IEEE Trans. Computers, 45 (1996) 1212-1221
7. P.H. Bardell, Analysis of cellular automata used as pseudorandom pattern generators, Proc. IEEE int. Test. Conf. (1990) 762-767
8. A.K. Das and P.P. Chaudhuri, Vector space theoretic analysis of additive cellular automata and its application for pseudo-exhaustive test pattern generation, IEEE Trans. Comput. 42 (1993) 340-352
9. P. Sarkar, Computing Shifts in 90/150 cellular automata sequences, Finite Fields Their Appl. 42 (2003) 340-352
10. P. Sarkar, The ﬁlter-combiner model for memoryless synchronous stream ciphers, in Proceedings of Crypto 2002, LNCS, 2442 (2002) 533-548
11. S.J. Cho, U.S. Choi, Y.H. Hwang, Y.S. Pyo, H.D. Kim and S.H. Heo, Computing phase shifts of maximum-length 90/150 cellular automata sequences, LNCS, 3305 (2004) 31-39
12. S.J. Cho, U.S. Choi, H.D. Kim, Y.H. Hwang, J.G. Kim and S.H. Heo, New synthesis of one-dimensional 90/150 linear hybrid group cellular automata, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., 26 (2007) 1720-1724
13. D. Kagaris, F. Makedon and S. Tragoudas, A method for pseudoexhaustive test pattern generation, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., 13 (1994) 1170-1178
14. D. Kagaris and S. Tragoudas, Avoiding linear dependencies in LFSR test pattern generators, J. Electron. Testing: Theory Applicat. 6 (1995) 229-241
15. D. Kagaris, Linear dependencies in extended LFSMs, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., 21 (2002) 852-859
16. D. Kagaris, A uniﬁed method for phase shifter computation, ACM Trans. Des. Autom. Electron. Syst., 10 (2005) 157-167
17. J. Kakade and D. Kagaris, Minimization of linear dependencies through the use of phase shifters, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., 26 (2007) 1877-1882
18. S.W. Golomb, Shift register sequences, Holden Day, 1967 19. R. Lidl and H. Niederreiter, Finite ﬁelds, Cambridge University Press, 1997

Introducing Service Schemes and Systems Organization in the Theory of Interactive
Computation
Antˆonio Carlos da Rocha Costa and Gra¸caliz Pereira Dimuro
Programa de P´os-Gradua¸c˜ao em Inform´atica, ESIN, Universidade Cat´olica de Pelotas, Rua Felix da Cunha 412, 96010-000 Pelotas, Brazil. e-mail: {rocha,liz}@ucpel.tche.br
Abstract. This paper introduces the notion of service scheme to allow for a formal approach to the study of the realizability of services by interactive systems. It shows how the organization-theoretic conceptual framework required by the notion of system service can be formally introduced, and shows examples of its application that allow the identiﬁcation of some basic service schemes, that seem to be common to every interactive system. Finally, it brieﬂy considers the conceptual framework of the Theory of Interactive Computation proposing that its central problem is the formal characterization of the realizability of service schemes, that is, the identiﬁcation of the class of all service schemes realizable by interactive computational means.
Key words: Interactive computation, service schemes, systems organization.
1 Introduction
This paper aims to contribute to the foundations of the Theory of Interactive Computation (TIC) [1, 2] through the formalization of the notion of service. Services are understood in their usual intuitive sense, but are introduced as an organizational notion distinct from other notions such as the notion of organizational role that a component may play in an interactive system (IS), or of the notion of a behavior that a component may perform in such system.
The paper adopts the common terminology of the area of Organization of Multiagent Systems (MAS) [3–7] to express the concepts necessary for the formalization of the notion of service, but it is not essentially dependent on the conceptual framework of that area: an agent, as considered here, is simply a system component, i.e., a computing element integrated in a computing system where organizational features (roles, services, behaviors, relationships, interactions, etc.) are present. No other implication on the concept of agent is assumed.
We have refrained from adopting any of the current formalisms for describing interactive systems, like the various versions of process algebras [8, 9], or equivalent formalisms. Instead, we have adopted an intuitive trace-based framework for the presentation of our concepts, a choice that has led us to conceive

88 Antˆonio Carlos da Rocha Costa and Gra¸caliz Pereira Dimuro
system processes (i.e., organized sets of actions performed by sets of agents) as sets of interleaved action streams (IAS). Wherever possible, we have remained as much informal as we could, in order not to overwhelm the presentation of the conceptual framework with technical details that are not mandatory at the current stage of our work. Of course, this informality determines that the results presented in this paper can not be seen as completely accomplished.
The two main results of the paper are: (1) a deﬁnition of the notion of service scheme, a formal construct that contributes to methodology by allowing for suitable formal organizational speciﬁcations of IS; (2) the suggestion (of a foundational character) that the true object of the TIC is the mathematical determination of the class of computable service schems, i.e., the class of services that can be eﬀectively realized by interactive computational means.
2 Services and the Theory of Interactive Computation
Interactive computation (IC) is computation where data is continually exchanged between the computing system and its environment. In an IC, the stream of input information does not need to be completely speciﬁed before the computation starts, and since the speciﬁcation of the content of the input information may happen along with the performance of the computation, such speciﬁcation may depend on the very stream of output information that it helps to generate.
Circular constructions like that, making the speciﬁcation of the system’s input depend on the system’s own output, traditionally called “feedback loop” in the context of Cybernetics, is neatly out of the scope of the classical models of algorithmic computation, such as (any non-extended variant of) the Turing machine model (see [10, 2], for further elaboration of this idea).
Peter Wegner and Dina Golding ([1] and other papers by them, cited there) have long claimed that interaction takes computing power beyond the level attained by algorithms (i.e., computing based on the classical Turing machine model). They have stressed the importance of interaction for the right conception of software structures, and emphasized that interactive computations are service-oriented, not question answering-oriented.
We have previously argued [2, 10, 11] that the conceptual framework needed for the explication of service-oriented computations should take as its foundation the general conceptual framework of the cybernetic-oriented theories of systems dynamics and organization. This requires taking into account both the concept of behavior, which Theoretical Computer Science has already formalized in its various theories of concurrent processes (e.g. [8, 9, 12]), and the concept of role played by a component within a system, which is currently being formalized following various alternative methodological approaches in the area of MAS (e.g., [4–7]). Besides that, however, that foundational proposal also requires the formalization of the concept of service (or, systemic function) performed by a system component to other system components and/or to the system as a whole, such formalization demanding that the concept of service be considered distinct from the other two concepts of role and behavior [10, 11].

Service Schemes and Systems Organization in Interactive Computation

89

In this paper we formalize the concept of service by introducing the formal construct of a service scheme, aiming to further the TIC in this respect.
On the other hand, and as a consequence of this preliminary formalization of the concept of service, the paper also addresses the foundational question: “What object is computed when an IC is carried on?” Such question, which got the deﬁnite answer of “a mathematical function”, in the Classical Theory of Computation, seems to have not yet been adequately settled in the case of the TIC. We oﬀer it the answer: “a service scheme”.

3 Interleaved Action Streams

In the following, let T = (t0, t1, . . .) be a discrete linear time structure and ∆ = [ti, tj] ⊆ T , with ti ≤ tj, be a time interval. We take a trace-based approach to computational behavior, and formally model system processes as (sets of)
interleaved action streams (IAS):

Deﬁnition 1. Let Acts be a set of actions that can be performed during ∆. An action stream performed during ∆ is a partial function as∆ : ∆ → Acts. For any t ∈ ∆, if as∆(t) is deﬁned, we write as∆(t)↓, and as∆(t)↑ if as∆(t) is undeﬁned.

Let Ag be a set of agents, and ag ∈ Ag. Let Actsag be the set of actions that

agent

ag

may

possibly

perform.

We

denote

by

as

∆ ag

any

action

stream

that

agent

ag may perform during the time interval ∆, employing actions of Actsag.

Deﬁnition 2.

Let

a1, a2

∈

Ag

be

two

agents.

Let

as

∆ a1

and

as

∆ a2

be

the

two

action streams performed by the agents a1 and a2 during the time interval ∆,

so

that

for

any

t

∈

∆,

as

∆ a1

(t)↓

implies

that

as

∆ a2

(t)↑,

and

as

∆ a2

(t)↓

implies

that

as

∆ a1

(t)↑.

The

interleaved

action

stream

of

agents

a1

and

a2

during

∆

is

deﬁned

as

the

function

ias

∆ a1

,a2

:

∆

→

T,

given

by

 as


∆ a1

(t)

ias

∆ a1

,a2

(t)

=

as

∆ a2

(t)

undeﬁned

if

as

∆ a1

(t)↓,

if

as

∆ a2

(t)↓,

otherwise.

In the following, we will often let implicit the reference to the interval ∆.
An alternating action stream established by two agents a1 and a2 is a kind of interleaved action stream iasa1,a2 where actions of a1 and a2 alternate:
Deﬁnition 3. An alternating action stream established by two agents a1 and a2 is an interleaved action stream ias∆a1,a2 (t) where, for any sub-interval [ti, tk] ⊆ ∆ such that iasa1,a2 (ti) ↓, iasa1,a2 (tk) ↓ and, for any tj with ti < tj < tk, iasa1,a2 (tj )↑:
− if iasa1,a2 (ti) = asa1 (ti) then iasa1,a2 (tk) = asa2 (tk); − if iasa1,a2 (ti) = asa2 (ti) then iasa1,a2 (tk) = asa1 (tk).

90 Antˆonio Carlos da Rocha Costa and Gra¸caliz Pereira Dimuro
The concept of interleaved action stream can be extended in the obvious way to sets of agents {a1, a2, . . . , an}, where n ≥ 2. The concept of alternating action stream can then be correspondingly extended in a natural way to the concept of cyclicly interleaved action stream. The various sequences of actions performed during the intervals [ti, tk] which structure the alternation of actions are informally called the cycles that characterize the action stream.
In an interleaved action stream iasa1,...,an performed by a set of agents {a1, . . . , an}, the action α performed at time t by agent ai is denoted by ai : α t , or by ai : α , when we are not interested in the time at which the action occurred. A segment (with length k) of an interleaved action stream iasa1,...,an can be denoted by [ ai1 : t1 α1 , . . . , aik : tk αk ], or simply by [ ai1 : α1, . . . , aik : αk].
4 Interactions Modelled with Interleaved Action Streams
Let’s model the behavior of a simple client-server system with the help of IAS. We assume that the system operates through cycles of requests and service performances, each cycle servicing a single request from a single client, the server serving possibly many diﬀerent clients in a ﬁrst-come ﬁrst-served basis. Also, every performance of the service is initiated by a request from a client.
To structure the notion of request-service cycle, a relation is needed between request actions and the corresponding response actions. We capture this with the relation ⊆ i Actscli × Actss, called an activation relation, relating each possible response of the server to the request by a client that may initiated it.
Example 1. Consider a server s and a set of clients Cl = {cl1, cl2, . . . , cln}. Let Actss be the possible actions of the server, and Actscli the possible actions of the client cli. Let iass,Cl be any interleaved action stream involving the server s and the clients in the set Cl during the time interval ∆. Then, the client-service cycle occurs repeatedly at some sub-intervals [ti, tk] ⊆ ∆ as the server performs services for its clients. It goes on according to the following rules:
− for any time interval [ti, tk] such that iass,Cl(ti)↓, iass,Cl(tk)↓ and, for any ti < tj < tk, iass,Cl(tj)↑, then: (1) if iass,Cl(ti) = ascl(ti), for some cl ∈ Cl , then iass,Cl(tk) = ass(tk); (2) if iass,Cl(ti) = ass(ti), then iass,Cl(tk) = ascl(tk), for some cl ∈ Cl ;
− for any time interval [ti, tk] in which it holds that iass,cl1,...,cln (ti) = ascl(ti) and iass,cl1,...,cln (tk) = ass(tk), with no deﬁned intervening action between ti and tk, it happens that ascl(ti) ass(tk).
Example 2. Let db be a database server and supplier and producer be two of its clients, and the set of all possible service performances be speciﬁed as follows (Actv is the activation relation):
− Actsdb = {conﬁrm, reply}; Actssup = {update}; Actsprod = {query}; − Actv db,sup,prod = {update conﬁrm, query reply}; − if at time t it holds that iasdb,sup,prod (t)↓ and iasdb,sup,prod (t) = assup (t) = update then at the next time t where iasdb,sup,prod (t )↓ it should also hold that iasdb,sup,prod (t ) = asdb (t ) = conﬁrm;

Service Schemes and Systems Organization in Interactive Computation

91

− if at time t it holds that iasdb,sup,prod (t)↓ and iasdb,sup,prod (t) = asprod (t) = query then at the next time t where iasdb,sup,prod (t )↓ it should also hold that iasdb,sup,prod (t ) = asdb (t ) = reply .
Then, for instance, the following are possible interactions between the agents: − [ sup : update , db : conﬁrm , prod : query , db : reply ], and − [ prod : query , db : reply , prod : query , db : reply , sup : update ,
db : conﬁrm , prod : query , db : reply ].
The activation relation between the actions of the server and the clients captures the interaction constraints imposed by the nature of the service being performed by the server. In fact, such activation relation can be taken as the basis of the formal notion of service scheme, as is done in the next section.

5 Interaction Constraints as Services Schemes

The concept of a service scheme as a constraint imposed on the interactions that may happen between a server and its clients is established here, in a tentative way, under the simplifying assumption that the server is able to serve just one client at a time. This allows us to abstract the set of clients of the service into one single generalized client, able to represent any client in such set, thus establishing the concept of generalized interleaved action stream.

Deﬁnition 4. Let a1 and a2 be two agents, the server and the (generalized)

client. Let Actsa1 and Actsa2 be their respective set of possible actions. The

service scheme performed by the agent a1 to the agent a2 during the time interval

∆

is

given

by

the

structure

Srv

∆ a1

,a2

=

(Ias

∆ a1

,a2

,

) where

⊆ Actsa2 × Actsa1

is

an

activation

relation

between

the

actions

of

a1

and

a2,

and

Ias

∆ a1

,a2

is

the

set

of all possible alternating action streams iasa1,a2 : ∆ → Actsa1 ∪ Actsa2 , so that

the

following

statements

hold,

for

any

ias a1,a2

∈

Ias

∆ a1

,a2

:

− if t is the least time at which iasa1,a2 (t)↓ then iasa1,a2 (t) = asa2 (t) (that

is, any alternating action stream initiates with a client request);

− in any alternating action stream iasa1,a2 ∈ Iasa1,a2 , if at t and t there are iasa1,a2 (t) = asa2 (t) and iasa1,a2 (t ) = asa1 (t ), with no intervening action between them, then the activation relation is respected, asa2 (t) asa1 (t ), i.e., the server’s response is coherent with the (generalized) client’s request.

Ias

∆ a1

,a2

is

said

to

be

the

generalized

alternating

action

stream

underlying

the

service

scheme

Srv

∆ a1

,a2

.

We

say

that

the

agent

a1

performs

the

service

scheme

Srv

∆ a1

,a2

for

a2.

Also,

we

say

that

the

interactive

system

composed

of

the

agents

a1

and

(all

agents

represented

by)

a2

realizes

the

service

scheme

Srv

∆ a1

,a2

.

Deﬁnition

5.

Given

the

service

scheme

Srv

∆ a1

,a2

=

(Ias

∆ a1

,a2

,

) performed by

agent

a1

to

agent

a2,

the

interaction

constraint

on

Ias

∆ a1

,a2

induced

by

the

activa-

tion relation

is the relation

⊆

Acts a2

× Actsa1 ,

such

that,

for

any

ias

∆ a1

,a2

∈

Ias

∆ a1

,a2

and t, t

∈

∆,

if

t

is

such

that

ias

∆ a1

,a2

(t)

=

as

∆ a2

(t)

=

α2

and t

is the

92 Antˆonio Carlos da Rocha Costa and Gra¸caliz Pereira Dimuro

next

time

after

t

at

which

ias

∆ a1

,a2

(t

)↓

such

that

ias

∆ a1

,a2

(t

)

=

as∆a1 (t

)

= α1

and

α2 α1, then a2 : α2 t a1 : α1 t . If times t and t can be understood from the

context, the interaction constraint on Iass,cl induced by the action α2 of a client

a2 and the action α1 of the server a1 can be denoted by a2 : α2 a1 : α1 .

Srv a1,a2 = (Iasa1,a2 , ) can then be denoted by Srv a1,a2 = (Iasa1,a2 , ).

Deﬁnition 6. Two interaction constraints 1 and 2 are said to be equivalent, denoted by 1 ≡ 2, if and only if the following condition holds: whenever a2 and a4 are clients and a1 and a3 are servers, it holds that a2 : α2 1 a1 : α1 and a4 : α2 2 a3 : α3 if and only if α1 = α3.

Deﬁnition 7. Service schemes Srva1,a2 = (Iasa1,a2 , a1,a2 ) and Srva3,a4 = (Iasa3,a4 , a3,a4 ) are said to be equivalent, denoted by Srva1,a2 ≡ Srva3,a4 , if and only if: (1) the repertoires of actions of their corresponding agents are equal:
rep(a1) = rep(a3) and rep(a2) = rep(a4); (2) their corresponding interaction constraints are equivalent: a1,a2 ≡ a3,a4 .

6 Composition of Service Schemes
Here, we introduce a notation to represent compositions of service schemes.
Deﬁnition 8. The sequential composition of two services Srv1 and Srv2 is the service scheme denoted by Srv1 Srv2. An interleaved action stream ias satisﬁes Srv1 Srv2 during an interval [ti, tk] if and only if there is a time tj, with ti < tj < tk, such that ias satisﬁes Srv1 in [ti, tj] and Srv2 in [tj, tk].
Deﬁnition 9. The n-fold repetition of a service Srv (for n ≥ 1) is the service scheme denoted by Srvn. An interleaved action stream ias satisﬁes Srvn during an interval [ti, tk] if and only if there is an n-fold partition of [ti, tk], in the form [tj0 , tj1 ], [tj1 , tj2 ] . . . , [tjn−1 , tjn ], where tj0 = ti and tjn = tk, such that ias satisﬁes S in each interval [tji , tji+1 ].
Of course, Srvn = Srv . . . Srv (n times). Also, we denote by Srv+ an undeﬁned number of repetitions of Srv, but including at least one repetition.
Deﬁnition 10. The non-deterministic choice between two service schemes Srv1 and Srv2 is denoted by Srv1 Srv2. An interleaved action scheme ias satisﬁes the service scheme Srv1 Srv2 during an interval [ti, tk] if and only if it satisﬁes service scheme Srv1 or it satisﬁes service scheme Srv2.

7 Basic Service Schemes
Example 3. Let prod be the producer agent and con represent any of its consumer agents. Consider that Actsprod = {produce, accept delivery request, deliver , deny delivery request, delivery denied } and Actscons = {delivery request, receive, consume}. Then, the service is characterized by the interaction constraint: [ cons : delivery request [ prod : deliver prod : delivery denied ]]+.

Service Schemes and Systems Organization in Interactive Computation

93

We note that characterizing a service is not the same as characterizing the behaviors of the server and its clients. It’s just characterizing how the server and the client interact. Thus, in Ex. 3, the service performed by the producer is the delivery of the product to the client, not the full working necessary for the production of the product (which includes actions like produce and accept delivery request). The latter is the behavior that an agent has to perform in order to fulﬁll the role of producer. A similar conclusion can be drawn with respect to the consumer.

Example 4. Now, we model both the service scheme performed by an intermediary agent int that delivers products to a generalized consumer agent cl (the delivery of the products that it will consume) and the service performed by the producer agent prod to the intermediary agent int (the delivery of the products that the latter will deliver to the consumers). Let Actscons = {delivery request, receive, consume}, Actsprod = {produce, accept delivery request, deliver , deny delivery request, delivery denied }, Actsint = {delivery request, receive, accept delivery request, deliver , deny delivery request, delivery denied}. The service schemes may be characterized by [ int : delivery request [ prod : deliver prod : delivery denied ]]+, and [ cons : delivery request [ int : deliver int : delivery denied ]]+.

The interaction constraint that characterizes the service performed by the producer to the consumer in Ex. 3 has the same structure as the interaction constraint which characterizes the service performed by the producer to the intermediary, and that of the intermediary to the consumer, in Ex. 4. Thus:

Deﬁnition 11. Two agents a1 (the deliverer) and a2 (the generalized receiver), realize the delivery service scheme if their generalized alternating action stream

Iasa1,a2 satisﬁes the interaction constraint: [ a2 : delivery request a1 : deliver

a1 : delivery denied ]+.

Let’s now examine the case of agents that receive objects delivered to them by other agents. Let rec be an agent that is able to receive objects sent to it by agent sen. Let Actsrec = {accept reception request, receive, deny reception request, reception denied } and Actssen = {reception request, deliver }. The service scheme realized by rec and sen can then be characterized by:
[ sen : reception request [ rec : receive rec : reception denied ]]+. This motivates the general deﬁnition:

Deﬁnition 12. Two agents, a1 (the receiver) and a2 (the generalized sender), realize the reception service scheme if their generalized alternating action stream

Iasa1,a2 satisﬁes the interaction constraint: [ a2 : reception request [ a1 : receive

a2 : reception denied ]]+.

We note that in the reception service scheme the initiative of realization of the service scheme is taken by the agent that delivers the object, while in the delivery service scheme the initiative is with the agent that receives the object. This is so because in any service scheme the initiative towards the realization of the scheme is always taken by a client of the service, not the server.

94 Antˆonio Carlos da Rocha Costa and Gra¸caliz Pereira Dimuro

Example 5. Consider the case of agents that store objects for other agents, so that objects can be sent for storage at some time and got back later, when requested. Let sto be an agent that is able to store objects for a generalized client cl. Let Actssto = {accept reception request, receive, accept delivery request, deliver , deny reception request, reception denied } and Actscl = {reception request, deliver , delivery request, receive}. Then, the service performed by sto to cl is characterized by (using exponents as repetition operators, with m ≥ n): [[ cl : reception request [ sto : receive sto : reception denied ]]m
[ cl : delivery request sto : deliver n]+ This motivates the general deﬁnition:

Deﬁnition 13. Two agents, a1 (the store) and a2 (the accessor), realize the storage service scheme if their generalized alternating action stream satisﬁes the service scheme, for m ≥ n: [[ a2 : reception request a1 : receive ]m [ a2 : delivery request a1 : deliver ]n]+

The following proposition illustrates the equivalence of service schemes:

Proposition 1. Let

1)Storagea1,a2 = (Iasa1,a2 ,

Sto a1 ,a2

),

where

Sto a1 ,a2

is given by (for m ≥ n):

[[ a2 : reception request a1 : receive ]m [ a2 : delivery request a1 : deliver ]n]+

2)Receptiona1,a2 = (Iasa1,a2 ,

Rec a1 ,a2

),

where

Rec a1 ,a2

is given by:

a2 : reception request a1 : receive

3)Delivery a1,a2 = (Iasa1,a2 ,

Del a1 ,a2

),

where

Del a1 ,a2

is given by:

a2 : delivery request a1 : deliver

Then, one notice that

Sto a1 ,a2

≡

[[

Rec a1 ,a2

]m

[

Del a1 ,a2

]n

]+

,

and

so:

Storagea1,a2 ≡ [[Receptiona1,a2 ]m [Deliverya1,a2 ]n]+.

Example 6. Lendinga1,a2 = [Deliverya1,a2 Receptiona1,a2 ]+, the lending service scheme, is a compound service scheme where agent a1 (the lender ) lends an

object to the agent a2 (the borrower ), which later returns the lent object.

With the help of the above examples, we claim that the reception and delivery service schemes are two of the basic service schemes that can be found in any interactive computing system.

8 Organizing Interactive Systems with Service Schemes
As stated in the Introduction, and stressed in Sect. 7, we think that interactive computations cannot be fully explained by simply adding the concept of interaction to the conceptual framework usually employed in the understanding of classical computations. As established in [10] and [11], and further elaborated in [2], it seems to us that the explication of interactive computing requires the framing of interactive systems in a more complex conceptual framework, where the concept of organization [2, 10, 13] should have a preeminent role.
Organization is a concept highly worked out in areas like Biology and Sociology. It involves many component concepts, such as:

Service Schemes and Systems Organization in Interactive Computation

95

1. organizational role that, as explained in Sect. 7, concerns the full behavior that an agent may have to perform when participating in an adequate way in the organization;
2. organizational function [11], which can be equated to the notion of service that we have been using here;
3. organizational process [3], which can be equated to the notion of generalized IAS that was formally introduced here;
4. organizational link, which concerns the way organizational roles interact [3], and which has a strong relationship to the notion of service scheme, also formally introduced here.
Disentangling the concept of service from the concepts of role and behavior should help the methodology of interactive systems design, as the diﬀerent concerns of those organizational concepts should allow for their diﬀerent methodological imports. It also allows the advancing of issues related to the structural dynamics of systems, and the consequent possibility of introducing into the dynamics of the system a notion of system development [2, 3, 10, 13, 14].

9 Related Work
A connection can be established between the work presented here and WSCL (Web Service Conversation Language), a language deﬁned by the W3C (World Wide Web Consortium) for the abstract speciﬁcation of service interfaces, whose deﬁnition is available at http://www.w3.org/TR/wscl10/.
In WSCL, a service interface is a conversation (i.e., an exchange of XML documents) that the server and its clients should realize to get the service performed. A conversation is essentially deﬁned as a set of two types of elements: interactions (i.e., operations of exchange of documents) and transactions (i.e., pairwise temporal orderings of interactions). A conversation speciﬁes all the possible sequences of document exchanges that allow for the successful performance of the service (as well as the sequences that lead to failure in such performance).
The way to connect WSCL and service schemes seems to be immediate, by making service schemes serve as statements of formal behavioral requirements for WSCL conversations: an interaction constraint of a service scheme states a behavioral requirement for a WSCL transaction, and a service composition of a service scheme states a behavioral requirement for a WSCL interaction.

10 Conclusion: On the Object of the TIC
With the formalization of the concept of service scheme, this paper also aims to contribute to a foundational issue: it proposes that the object computed by an interactive system is, simply, a service scheme. Computability, in the context of interactive computations, is not a property of mathematical functions or relations, as it was in the context of classical computations, but is a property of schemes of system services: computability in the context of interactive computations is synonymous of realizability of service schemes.

96 Antˆonio Carlos da Rocha Costa and Gra¸caliz Pereira Dimuro
The fundamental question of the TIC seems to be, thus: What is the formal characterization of the class of all service schemes that are eﬀectively realizable by interactive systems?
Acknowledgements. This work was supported by FAPERGS and CNPq. We are grateful to the referees for the comments that helped us to improve the paper.
References
1. Goldin, D., Smolka, S., Wegner, P., eds.: Interactive Computation: The New Paradigm. Springer-Verlag, New York (2006)
2. Costa, A.C.R., Dimuro, G.P.: Interactive computation: Stepping stone in the pathway from classical to developmental computation. ENTCS 141(5) (2005) 5–31
3. Demazeau, Y., Costa, A.C.R.: Populations and organizations in open multi-agent systems. In: Proc. 1st. Symp. Parallel and Distributed AI, Hyderabad, India (1996)
4. Zambonelli, F., Jennings, N.R., Wooldridge, M.: Developing multiagent systems: the Gaia methodology. ACM Transactions on Software Engineering and Methodology 12(3) (2003) 317–370
5. Hu¨bner, J.F., Sichman, J.S., Boissier, O.: A model for the structural, functional, and deontic speciﬁcation of organizations in multiagent systems. In Bittencourt, G., Ramalho, G.L., eds.: Advances in Artiﬁcial Intelligence: 16th Braz. Symp. on Artiﬁcial Intelligence. Volume 2507 of LNAI., Berlin, Springer (2002) 118–128
6. Dignum, V., V´azquez-Salceda, J., Dignum, F.: Omni: Introducing social structure, norms and ontologies into agent organizations. In Bordini, R.H., Dastani, M., Dix, J., Seghrouchni, A.E.F., eds.: Programming Multi-Agent Systems: 2nd. Intl. Work. ProMAS 2004. Volume 3346 of LNCS., New York, Springer (2004) 181–198
7. Ferber, J., Gutknecht, O., Michel, F.: From agents to organizations: an organizational view of multi-agent systems. In Giorgini, P., Mu¨ller, J.P., Odell, J., eds.: Agent-Oriented Software Engineering IV. Volume 2935 of LNCS., Berlin, Springer (2004) 214–230
8. Milner, R.: Communication and Concurrency. Prentice-Hall, New Jersey (1989) 9. Hoare, C.A.R.: Communicating Sequential Processes. Prentice-Hall, NJ (1985) 10. Costa, A.C.R.: Machine Intelligence: sketch of a constructive approach. PhD thesis,
CPGCC/UFRGS, Porto Alegre (1993) (in Portuguese). 11. Costa, A.C.R., Castilho, J.M.V., Claudio, D.M.: Toward a constructive notion of
functionality. Cybernetics and Systems 26(4) (1995) 443–480 12. Reisig, W.: Petri Nets, An Introduction. Springer-Verlag, Berlin (1985) 13. Costa, A.C.R., Dimuro, G.P.: Semantical concepts for a formal structural dynamics
of situated multiagent systems. In Sichman, J., Noriega, P., Padget, J., Ossowski, S., eds.: Coordination, Organizations, Institutions, and Norms in Agent Systems III. Number 4870 in LNAI, Berlin (2008) 139–154 14. Dimuro, G.P., Costa, A.C.R.: Toward a domain-theoretic model of developmental machines. In Cooper, S.B., Kent, T.F., L¨owe, B., Sorbi, A., eds.: CiE 2007: Computation and Logic in the Real World. Quaderni del Dipartimento di Scienze Matematiche e Informatiche Roberto Magari of the Univ. of Siena (2007) 114–122

Online-division with Periodic Rational Numbers
G. de Miguel Casado1 , J.M. Garc´ıa Chamizo2, and H. Mora Mora2
1 GISED, University of Zargoza, Spain {gmiguel}@unizar.es
2 I2RC-SPALab, University of Alicante, Spain {juanma,hmora}@dtic.ua.es
Abstract. This paper discusses diﬀerent approaches for exact rational arithmetic and proposes a novel representation for periodic rational numbers with double mantissa (ﬁxed and periodic) based on signed-digit arithmetic. The representation is proposed under the scope of Type-2 Theory of Eﬀectivity (TTE) and the extension of an online-arithmetic algorithm for division is analyzed.
Key words: computer arithmetic algorithms; Type-2 Theory of Eﬀectivity
1 Introduction
Data codiﬁcation formats in digital computers deﬁne the features of the numeric sets and the arithmetic operations which support higher abstraction level operations required by computer applications [19]. In this context, positional fractional codiﬁcations provide a direct way to express rational numbers by means of both integer and fractional parts. One of the most representative formats is the ﬂoating point speciﬁcation IEEE754, which has been adopted as standard in most of general purpose computer systems and is currently under revision. After some years, the lack of reliability of this standard for scientiﬁc computing applications [16, 20] has motivated the development of alternative methods for numerical data codiﬁcation and operation [2, 10]. Among the whole research in the topic, some researchers in computer arithmetics have focused their eﬀorts in symbolic computing with algebraic expressions [17]. This approach has led to proposals in which rational numbers are represented by means of fractions [13, 14]. However, its main drawback is the fact that there are no low complexity algorithms to obtain irreducible fractions (Euclid’s Greatest Common Divisor Algorithm) and therefore the codiﬁcation redundancy cannot be easily restrained [4]. In addition, some operators such as comparators, outline a high complexity when compared with those based on fractional positional notations.
The continuous fractions approach also oﬀers an exact representation method for rational numbers. In this case, the codiﬁcation of numbers is performed by successive fractions [5]. However, early hardware designs [17] outlined a high
The authors thank the Spanish Education Ministry for ﬁnancial support (Project TIN2005-08832-C03-02)

98 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora

complexity of the arithmetic operations involved [21]. As it happened in symbolic computing approaches, if a numeric result in fractional positional notation is required, some additional operations have to be performed and then, imprecisions in expression translations may appear. Another interesting proposal for error-free codiﬁcation of rational numbers is based on the explicit representation of the periodic development of fractional numbers [12]. Nevertheless, as this research was limited to the theoretical formulation and no suitable procedures and architectures where developed at that time, no interest was shown by the scientiﬁc community.

The online arithmetic approach, based on signed digit arithmetic, deals with the hardware implementation of digit-serial left-to-right (online or Most Signiﬁcant Bit First) arithmetic operators for signed digit numbers [9]. As the operation dynamics resembles that of the Turing Machine model, a conceptual convergence with Type-2 Theory of Eﬀectivity (TTE) can be realized [22, 6]. Under this scope, partial implementation of Turing Machines can be done (according to the limited memory resources) and therefore, hardware design support for scientiﬁc computing tasks can be developed.

Let us consider the set-builder notation of the set of rational numbers Q :=

a b

| a, b ∈ Z,

b=0

3. A rational number can be expressed as a fraction

a b

where

a and b are integers and b = 0. The set Q is countable, continuous and dense

in R. In Type-2 Theory of Eﬀectivity, Q stands for the basis for higher ab-

straction level representations: the Cauchy representation of real numbers, some

computable topological spaces, continuous functions in C [0; 1], Lp Spaces and Sobolev Spaces, to name a few [22, 15, 23]. There are, at least, two standard no-

tations for the rational numbers: νQ [22, Def. 3.1.2] for a rational representation derived from the set-builder notation and νsd [22, Def. 7.2.4] for a fractional positional notation for non periodic binary rational numbers. A normalized notation

for these numbers is also proposed in [7].

This paper is focused on the proposal of a novel codiﬁcation of rational numbers which aims for a feasible hardware implementation of operators based on online-arithmetic. We propose the extension of the fractional positional notation νsd for non periodic binary rational numbers so that to include the periodic ones. In addition, normalization criteria is proposed for the representation so that to match the digital computer design criteria standards, which aim for simplifying hardware designs as well as for improving operand memory packaging and storage. An extension of online algorithms for the basic rational number operations is proposed.

The paper is arranged in the following sections: after the introduction and
the preliminary section, Section 3 develops a fractional positional notation for the rational numbers with double mantissa νQdm, Section 4 deals with the modiﬁed online-division algorithm for these periodic rational numbers in fractional
positional notation and, ﬁnally, Section 5 summarizes the conclusions.

3 The symbol Z derives from the German word Zahl (number) [8] and Q derives from the German word Quotient (ratio) [3, p. 671].

Online-division with Periodic Rational Numbers

99

2 Preliminaries

A fractional positional representation provides a direct value number, which is a desirable feature for a numeric codiﬁcation format. The codiﬁcation of rational numbers can be eﬀectively performed with an exact positional representation, as it can be expressed by means of a ﬁnite concatenation of digits such as:

∀w ∈ Q ⇔ ∃ i, j, k|w = w0 . . . wi wi+1 . . . wjwj+1 . . . wkwj+1 . . . wk . . . , (1)
where i, j, k ∈ N, such that ∀wl ∈ w, wl ∈ Γn, Γn = {0, . . . n − 1} and n ≥ 2 is the number base.
This codiﬁcation is based on the concatenation of a sequence of digits around the fractional point (ﬁxed mantissa w0 . . . wi wi+1 . . . wj) and a periodic series of digits (periodic mantissa wj+1 . . . wk). The shortest periodic sequence which is repeated is called the period. Conventionally, this periodic part is represented only once, embraced by an upper circumference arch, which permits the exact codiﬁcation of a rational number with a ﬁnite number of symbols:

w = w0 . . . wi wi+1 . . . wj wj+1 . . . wk.

(2)

According to the features of the period, the following cases can be considered:

– All the periodic digits are zero (wj+1, . . . , wk = 0). Then, the ﬁxed mantissa codiﬁes the exact rational number: w = w0 . . . wi wi+1 . . . wj.
– All the periodic digits are equal to the highest digit of the number base n.
Then, the number is codiﬁed by adding one unit in the last position (ulp)
to the less signiﬁcative digit of the ﬁxed mantissa and removing the periodic
digit w = w0 . . . wi wi+1 . . . wj + ulp, with ulp = v0 . . . vi vi+1 . . . vj−1vj, v0, . . . , vi, vi+1, . . . vj−1 = 0 and vj = n − 1.

The length of the periodic mantissa p = k − (j + 1) is tightly related to

the denominator of the irreducible fraction with respect to the set-builder based

representation of a rational number. Particularly is less than the denominator

value:

∀w ∈ Q,

w

=

a b

,

a, b ∈ Z , b = 0,

gcd (a, b) = 1 ⇒

w = mf mp ∧ p = Ψ (b) < b,

(3)

where mf = w0 . . . wi wi+1 . . . wj, mp = wj+1 . . . wk, gcd is the greatest common divisor of the numerator a and the denominator b, and Ψ (b) is a function based on the congruence relationship (≡) [1, 11]:

Ψ (b) ≡

1 mod(b) if gcd(n, b) = 1, 1 mod(b ) if gcd(n, b) > 1,

(4)

where mod is the modular operator (integer division). Notice that when

gcd(n, b) > 1 follows that ∃b , m ∈ N | b = m · b ∧ gcd(n, b ) = 1 ∧ m =

pi=1zij, i > 0 ∈ N, z1 . . . zp ∈ N is the set of prime factors of the number base

n

and

j

∈

N

such

as

a b

=

1a mb

∧ gcd(n, b

)

=

1∧Ψ

(b)

≡

1 mod(b

).

100 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora

According

to

the

modular

arithmetic

rules,

as

1 m

is

composed

by

prime

factors

z1 . . . zp ∈ N of the base n it makes p = 0. There exist inﬁnitely many values

which fulﬁll the previous congruence relationship, as every multiple of p satisﬁes

the equation:

Ψ (b) ≡ 1 mod (b) =⇒ ∀j ∈ N, nj·p ≡ 1 mod (b)

(5)

This fact justiﬁes that every digit group which includes the period several times and whose length is a multiple of p is itself a period. In addition, all the rational numbers are periodic, despite the fact that the period is not codiﬁed when all the digits are 0. The amount of periodic numbers with p > 0 is related with the base of the rational number to be codiﬁed.
There exist rational numbers in decimal with p = 0 which in binary base produce a number with p > 0, whereas 2 is a prime factor of 10 and then all the binary number with p = 0 produces, at the same time, a decimal number with p = 0. This has a great impact in numerical codiﬁcation, because in the domain of the binary numbers there are fractional numbers with period 0 which can generate errors in the binary codiﬁcation because the impossibility of codifying inﬁnite fractional digits.

Deﬁnition 1. (Periodic number) A periodic number is a rational number whose periodic mantissa has length greater than 0 (p > 0) such that

w ∈ Q, ∃f, p ∈ N | w = mf mp,

(6)

where mf = w0 . . . wi wi+1 . . . wj, mp = wj+1 . . . wkwj+1 . . . wk . . ., i, j, k ∈ N, such that ∀wl ∈ w, wl ∈ Γn, Γn = {0, . . . n − 1}, n ≥ 2, and n is the number base.

Deﬁnition 2. (Non periodic number) A non periodic number is a rational number whose periodic mantissa has length 0 (p = 0) such that

w ∈ Q, ∃f, p ∈ N | w = mf ,

(7)

where mf = w0 . . . wi wi+1 . . . wj, mp = λ (empty string), i, j ∈ N, such that ∀wl ∈ w, wl ∈ Γn, Γn = {0, . . . n − 1}, n ≥ 2, and n is the number base.

Deﬁnition 3. (Pure periodic number) A pure periodic number is a rational number whose ﬁxed mantissa is 0 (f = 1) such that

w ∈ Q, ∃f, p ∈ N | w = mf mp,

(8)

where mf = 0 , mp = wj+1 . . . wkwj+1 . . . wk . . ., j, k ∈ N, such that ∀wl ∈ w, wl ∈ Γn, Γn = {0, . . . n − 1}, n ≥ 2, and n is the number base.

Now, consider the following notations and representations in TTE. Let Σ = {1, 0, 1} and let Σ∗ and Σω denote the sets of ﬁnite and inﬁnite
sequences over Σ, respectively. We always assume that there exists an element # ∈/ Σ and denote Σ# := Σ ∪ {#}. Let ιw : Σ∗ → Σ#∗ be a wrapping function

Online-division with Periodic Rational Numbers 101

that assigns to a word u = u0 . . . uk ∈ Σ∗ with u0, . . . , uk ∈ Σ and k ∈ N the word ιw(u) := ##u0#u1#...#uk## and let ιu : Σ#∗ → Σ∗ be the corresponding unwrapping function that obtains from a word v ∈ Σ#∗ the word ιu(v) := w with w = w0 . . . wk ∈ Σ∗, and k ∈ N.

Deﬁnition 4. (Some standard notations and representations)[22, Def. 3.1.2 (1-4)].

1. The identity idΣ∗ : Σ∗ −→ Σ∗ is a notation of Σ∗ and the identity idΣω : Σω −→ Σω is a representation of Σω.

2. Deﬁne the binary notation νN :⊆ Σ∗ −→ N of the natural numbers by:
k
dom (vN) := {0} 1 {0, 1}∗and νN (ak, ..., a0) = ai · 2i, (a0, ..., ak ∈ {0, 1}).

i=0

3.

Deﬁne −1 {0,

1a}n∗o, tνaZti(own)ν=Z

:⊆ νN

Σ∗ −→ Z of the integers (w) and νZ (−w) = −νN

by: dom (w) for

(νZ) w∈

:= {0} 1 {0, 1}∗∪ dom (νN) \ {0}.

4. Deﬁne a notation νQ :⊆ Σ∗ −→ Q by:

dom (νQ) := {u/v | u ∈ dom (νZ) , v ∈ dom (vN) , vN (v) = 0} and νQ (u/v) :=

νZ (u) /νN (v). Later we will abbreviate νQ (w) by w.

Deﬁnition 5. (Signed digit notation and representation) [22, Def. 7.2.4].
Let 1 ∈ Σ abbreviate -1. Deﬁne the signed digit notation νsd of the binary rational numbers and the signed digit representation ρsd :⊆ Σ∗ −→ R of the real
numbers as follows:

 all 

an...a0

a−1a−2... ∈ Σω such that n ≥ −1,

dom (ρsd) := ai ∈ 1, 0, 1 for i ≤ n,

 an = 0, if n ≥ 0 and anan−1 ∈/ 11, 11 if n ≥ 1,

dom (νsd) := {u v | u, v ∈ Σ∗, u v0w ∈ dom (ρsd)} ,

−∞

ρsd (an...a0 a−1a−2...) := ai · 2i,

i=n
νsd (u v) := ρsd (u v0w) .

For p := an...a0 a−1a−2... ∈ dom (ρsd) and k ∈ N deﬁne p [k] := an...a0 a−1...a−k ∈ Σ∗.

3 Fractional Positional Notation for Periodic Rational Numbers

Deﬁne the notation νZsd for codifying integer numbers in signed digit notation:

ν sd
Z

:⊂

Σ∗

−→ Z,  all 

an

. . . a0

∈

Σ∗

for

n

≥

0,

dom νsd :=
Z

ai ∈ Σ for i ≤ n,

 an = 0, if n ≥ 0 and anan−1 ∈/

ν sd
Z

(en

.

.

.

e0)

:=

ρsd

(en

.

.

.

e0 0w )

.

11, 11

, if n ≥ 1,

(9)

102 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora

Now, deﬁne a fractional positional notation νQnsd of signed digit rational numbers, such as for a ∈ Q then 0 ≤ a < 1.

νnsd :⊂ Σ∗ −→
Q

A = {a | 0 ≤ a < 1} ∩ Q

dom νQnsd := {u v | u = 0, v ∈ Σ∗, u v0w ∈ dom (ρsd)} ,

νQnsd (u v) := ρsd (u v0w) .

(10)

Finally, deﬁne the notation νQdm for normalized fractional positional periodic rational numbers with double mantissa (ﬁxed and periodic) as:

ν dm
Q

:⊂

Σ∗

−→

Q

dom νQdm := ιw (e) ιw (mf ) ιw (mp) | e, mp ∈ dom νZsd and

mf ∈ dom

ν nsd
Q

,

νQdm (uvw) :=

q

=

a b

|

a = νsd (2e) · νsd (mf mp − mf )

with

e = ιu (u) , mf = ιu (v) , mp = ιu (w) , and b = {1}p {0}f with

f = |mf | , p = |mp|} ,

(11)

for a signed digit notation in radix-2.

Theorem 1. The notation νQdm induces the same concept of computability as

the

standard

notation

νQ

(ν dm
Q

≡

νQ)

Proof. (νQ ≤ νQdm). The translation from the notation νQ into νQdm notation can be done straight away by applying a modiﬁed version of the conventional school

division algorithm in which, when the fractional part is being obtained, we check

the repetition of partial remainders. Notice that this key issue is used for the

online-division algorithm proposed in Section 4. Therefore, we wrap the partial

remainders pr ∈ dom (νZ) and then successively concatenate them with a given word r ∈ Σ#∗ ∪ {λ} , which we initialize to λ (r = rιw (pr)). On every iteration related with the calculation of the fractional part, we check if the wrapped partial

remainder ιw (pr) . In that case, we ﬁnd the position of the subword by means of a subword count function c : Σ#∗ ∪ {λ} × Σ#∗ → Z, which outputs the number of words counted i ∈ N when the partial remainder subword was found and then

the algorithm stops. Notice that, as usual, the algorithm can also stop when the

partial remainder is zero (pr = 0). If the partial remainder is not found (pr is

not a subword of r), then output −1.

Now, let q ∈ dom (νQ) be the quotient obtained from the division, mf ∈

dom (νsd) the ﬁxed mantissa and mp ∈ dom

ν sd
Z

the periodic mantissa. Then

mf and mp are the following preﬁx and suﬃx from q, which are now translated

into signed digit based notations:

mf = νsd (q0 . . . . . . qi) ,

mp

=

ν sd
Z

(qi+1

...

. . . qj) ,

where j = |q| − 1. Now, in order to normalize the rational number obtained:

(12)

Online-division with Periodic Rational Numbers 103

Deﬁne the count function c : dom (νsd) → dom (νN) , which outputs a natural number corresponding to the position of the dot ” ” in the input word. If the

dot ” ” is not found, then the function outputs the length of the word.

Deﬁne

the

function

a

:

Σ∗

×

dom (ν ) N

→

Σ∗

×

dom (νZ)

which

removes

the

0s at the beginning of an input string u ∈ Σ∗ and successively decrements the

input n ∈ dom (νN). This function outputs the remaining string m ∈ Σ∗ and the

decremented input n, which can be negative (z ∈ dom (νZ)). Finally, deﬁne the function i : Σ∗ × dom (νZ) → dom (νsd) as

 0 {0}z u

if z − 1 ≤ 0,



i (u, z) := u0 . . . uz−1 uz . . . uk if 0 < z − 1 < νZ (k) ,

 u0 . . . u {0}z−νZ(k)−1 if z − 1 ≥ νZ (k) ,

(13)

where u = u0 . . . uk ∈ Σ∗, z ∈ dom (νZ) and k ∈ N. Then, the exponent is obtained in the following way:

1. Count the number of positions from the beginning of the string until the dot character ” ” is found with the function c : dom (νsd) → dom (νN). This will provide the initial exponent.
2. Remove the dot character ” ” from the string and then remove the 0s at
the beginning of it. At the same time, while removing the 0s, successively decrement the exponent by applying the function a : Σ∗ × dom (ν ) → Σ∗ ×
N
dom (νZ). The mantissa and the exponent at the output are the remaining chunk of the string and the decremented exponent, respectively.
3. Apply the wrapping function in order to wrap the exponent the ﬁxed man-
tissa and the periodic mantissa:
ιw(e) := u = ##e0#e1#...#ek##, ιw(mf ) := v = ##mf,0#mf,1#...#mf,k##, ιw(mp) := w = ##mp,0#mp,1#...#mp,k##, and then concatenate u and v such as w = uv ∈ dom (νnsd) .

Proof. (νQdm ≤ νQ). The translation from νQdm into νQ can be directly achieved by using TTE-computable string manipulation functions.
Deﬁne the search function s : Σ#∗ × N → Σ∗ such as for a word w ∈ Σ#∗ and j ∈ N it obtains the corresponding unwrapped subword from w :

s (w, j) := ιu (u) | u is the subword j in w with u ∈ Σ#∗ and j ∈ N . (14)

1.

For w ∈ dom

ν dm
Q

, apply the search function s : Σ#∗ × N → Σ∗ to obtain

the exponent e = s (w, 0) , the ﬁxed mantissa mf = s (w, 1) and the periodic

mantissa s (w, 2).

2.

Deﬁne

q

=

νZ (a) νN (b)

where

a

=

νsd (2e) · νsd (mf mp − mf )

∈

dom (νsd) ,

and

b = {1}p {0}f with f = |mf | , p = |mp| .

Then,

as

νQ

is

reducible

to

ν dm
Q

(νQ

≤

ν dm )
Q

and

also

ν dm
Q

is

reducible

to

νQ

(νQdm ≤ νQ) it can be concluded that νQdm is equivalent to νQ (νQdm ≡ νQ).

104 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora
4 Online Division with Periodic Rational Numbers
The computability of the general algorithm for developing online-arithmetic operators is absolutely concerned with the possibility to reduce the residual recurrence equation to additions and shifts [9, Chap. 9]. Among all the basic algorithms (addition, subtraction, multiplication and division) the case of the division outlines a great variety of cases. The extended online division algorithm for normalized signed digit operands X, D (0 < X, D < 1) with 4 delay cycles for the initialization is the following:
Online Division Algorithm REGQ, REGD, v, w := 0 ’Initialize (start CP, modul) := M axOverlap(X, D) For it := 0 To precision
x := ObtainDigit(. . . .); d := ObtainDigit(. . .) REGD := AppendOnline(REGD, d) If it < kDELAY DIV SD Then ’Initialization stage
v := 2w + x · 2−kDELAY DIV SD w := v Else ’For the rest of iterations v := 2w + x · 2−kDELAY DIV SD−REGQ · d · 2−kDELAY DIV SD q := SELD(v) w := v − q · REGD exit := CheckP eriod(. . .) If exit = T rue Then
Exit For; Else REGQ := AppendOnline(REGQ, q) EndIf EndIf Next it The extension of the algorithm is based on two functions. The function M axOverlap(X, D) calculates the iteration (start CP ) at which the residual is stored for a later identiﬁcation of the periodic mantissa with a predictable iteration modulus (modul). The function CheckP eriod(. . .) evaluates every matching residue w at a given iteration so that to identify the repetition of a periodic mantissa. For the latter function, the following cases have to be considered:
1. X, D are non-periodic. After storing the residual value res := w at the iteration given (it = start CP ), the residual is checked iteration after iteration so that to ﬁnd either w = 0 (non-periodic result) or res = w (periodic result of with mantissa length lmp = it − start CP ).
2. X is periodic and D is non-periodic. After storing the residual value res := w at the iteration given (it = start CP ), the residual is checked at the iterations which fulﬁll the condition mod (it, modul) = 0 so that to ﬁnd res = w (periodic result of with mantissa lengthlmp = it − start CP ). The value of

Online-division with Periodic Rational Numbers 105
modul assigned in M axOverlap(X, D) is the length of the periodic mantissa of X. 3. X is non-periodic and D is periodic. After storing the residual value res := w at the iteration given (it = start CP ), the residual is checked at the iterations which fulﬁll the condition mod (it, modul) = 0 so that to ﬁnd res − w < ulpres (periodic result of with mantissa length lmp = it − start CP ). The value of modul assigned in M axOverlap(X, D) is the length of the periodic mantissa of D. 4. X, D are periodic. The case is similar to the previous one, except that the value of modul assigned in M axOverlap(X, D) is the maximum of length of the periodic mantissas X and D, and the result is periodic with mantissa length lmp = it − start CP.
The algorithm initializes the internal variables REGQ, REGD, v and w which hold the quotient, the divisor as well as the estimator and residual, respectively. In the main loop, the following leftmost input bits from the operands X and D are obtained (x and d, respectively). The divisor digit obtained is concatenated to the divisor register REGD. Then, two stages are considered: ﬁrst, initialization of the residual (it < kDELAY DIV SD, the ﬁrst 4 cycles) and second, period check and/or digit output estimation. For the second stage, the resulting digit at this iteration is obtained with the selection function SELD(.) applied to the estimation value v and the new value of the residual is calculated. Then the conditions for checking the period are analyzed in CheckP eriod(. . .). If the residual is not repeated, the resulting digit is appended to the output quotient register REGQ with the function AppendOnline(...) and the algorithm performs a new iteration, unless it reaches the maximum iterations ﬁxed.
5 Conclusions
A computable fractional positional notation for periodic rational numbers has being proposed. It aims to improve the comparison operations as well as to reduce memory storage by managing periodic mantissas. With this approach, the space of representation for a ﬁxed precision is increased, as it is possible to emulate the inﬁnite digits of a periodic rational number with a ﬁxed length periodic mantissa. The algorithm proposed for the division identiﬁes the periodic mantissa of the result before its ﬁrst repetition by exploiting the concept of the residual w as the inner state of the algorithm. Similar results can be obtained with the addition, subtraction and multiplication. The complexity of the operations introduced implies: obtaining the algorithm iteration at which the singular residual has to be stored, calculating the iteration modulus step at which the next residuals have to be checked and subtracting and comparing at the iteration check established.
The encouraging experimental results obtained outline the interest of formalizing the periodic mantissa identiﬁcation using the congruence relationship based on the Fermat’s Little Theorem (generalized by Euler) and also of extending the algorithms to high-radix number bases as well as to other arithmetic operations.

106 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora
References
1. Belski, A.A. and Kaluzhnin L.A.: Divisi´on inexacta, Lecciones populares de matem´aticas. Ed. Mir, Moscow (Spanish Translation), (1980)
2. Blanck, J.: Exact real arithmetic systems: Results of competition, Computability and Complexity in Analysis, LNCS, Vol. 2064, (2001) 390–394
3. Bourbaki, N.: E´l´ements de math´ematique: Alg`ebre. Reprinted as Elements of Mathematics: Algebra I. Berlin: Springer-Verlag, (1998)
4. Buchberger, B.: Groebner Bases in MATHEMATICA: Enthusiasm and Frustation, Programming Environments for High-level Scientiﬁc Problem Solving, (1991) 80– 91
5. Brezinski, C.: History of Continued Fractions and Pade Approximants. SpringerVerlag, (1980)
6. de Miguel Casado, G. and Garc´ıa Chamizo, J.M.: The Role of Algebraic Models and Type-2 Theory of Eﬀectivity in Special Purpose Processor Design, LNCS, Vol. 3988, (2006) 137–146
7. de Miguel Casado, G.; Garc´ıa Chamizo, J.M. and Signes Pont, M.T.: Algebraic Model of an Arithmetic Unit for TTE-Computable Normalized Rational Numbers, LNCS, Vol. 4497, (2007) 218–227
8. Dummit, D. S. and Foote, R. M., ”Abstract Algebra, 2nd ed.,” Englewood Cliﬀs, NJ: Prentice-Hall, 1998.
9. Ercegovac, M.D. and Lang, T.: Digital Arithmetic. M. Kaufmann, (2004) 10. Gowland, P. and Lester, D.: A Survey of Exact Arithmetic Implementations, LNCS,
Vol. 2064, (2001) 30–47 11. Guelfond, A.O.: Resoluci´on de ecuaciones en nu´meros enteros, Lecciones populares
de matem´aticas- Ed. Mir, Moscow (Spanish Translation), (1979) 12. Hehner, E.C.R. and Horspool, R.N.S.: A New Representation of the Rational Num-
bers for fast Easy Arithmetic, SIAM J. Computing, Vol. 8(2), (1979) 13. Matula, D. and Kornerup, P.: Finite Precision Rational Arithmetic: An Arithmetic
Unit, IEEE Transactions on Computers, Vol. C-32, (1983) 378–387 14. Kornerup, P. and Matula, D.: Algorithms for Arbitrary Precision Floating Point
Arithmetic, Proceedings of the 10th IEEE Symposium on Computer Arithmetic, (1991) 15. Kunkle, D.: Type-2 computability on spaces of integrable functions, Math. Log. Quart., Vol. 50, (2004) 417–430 16. Lynch, T. and Schulte, M.: A High Radix On-line Arithmetic for Credible and Accurate Computing, Journal of UCS, Vol. 1, (1995) 439–453 17. Mencer, O: Rational Arithmetic Units in Computer Systems, PhD Thesis, Stanford University, (2000) 18. Mora, H.: Procesadores Aritm´eticos Especializados. Computaci´on Racional Exacta, Ph.D Dissertation, University of Alicante, (2003) 19. Patterson, D.A. and Hennessy J.L.: Computer Architecture a quantitative approach. M. Kaufmann, (2002) 20. Schr¨oder, M.: Admissible Representations in Computable Analysis, LNC,Vol. 3998, (2006) 471–480 21. Vuillemin, J.E.: Exact Real Computer Arithmetic with Continued Fractions, IEEE Transactions on Computers, Vol. 39, (1990) 1087–1105 22. Weihrauch, K.: Computable Analysis. Springer-Verlag, (2000) 23. Zhong, N. and Weihrauch, K.: Computability Theory of Generalized Functions, Journal of the ACM, Vol. 50, 2003 (469–505)

Abstract Geometrical Computation with Accumulations: Beyond the Blum, Shub and
Smale Model
J´eroˆme Durand-Lose
Laboratoire d’Informatique Fondamentale d’Orl´eans, Universit´e d’Orl´eans, B.P. 6759, F-45067 ORLE´ANS Cedex 2.
http://www.univ-orleans.fr/lifo/Members/Jerome.Durand-Lose, Jerome.Durand-Lose@univ-orleans.fr
Abstract. Abstract geometrical computation (AGC) naturally arises as a continuous counterpart of cellular automata. It relies on signals (dimensionless points) traveling and colliding. It can carry out any Turing computation, but since it works with continuous time and space, some analog computing capability exists. In Abstract Geometrical Computation and the Linear BSS Model (CiE 2007, LNCS 4497, p. 238-247), it is shown that AGC without any accumulation has the same computing capability as the linear BSS model. An accumulation brings inﬁnitely many time steps in a ﬁnite duration. This has been used to implement the black-hole model of computation (Fundamenta Informaticae 74(4), p. 491-510). It also makes it possible to multiply two variables, thus simulating the full BSS. Nevertheless a BSS uncomputable function, the square root, can also be implemented, thus proving that the computing capability of AGC with isolated accumulations is strictly beyond the one of BSS.
Key words: Abstract geometrical computation, Accumulations, Analog computation, BSS model, Signal machine.
1 Introduction
There is no agreed continuous/analog/R counterpart of the Church-Turing thesis. Relating the numerous models is crucial to understand the diﬀerences between their computing capabilities. For example, Bournez et al. [BH04,BCGH06] as well as others [Kaw05] have related Moore’s recursion theory on R [Moo96], computable analysis [Wei00] and the general purpose analog computer [PE74]. The aim of the present paper is to relate two models. One, abstract geometrical computation (AGC) deals with regular and automatic drawing on the Euclidean plane, while the second, the Blum, Shub and Smale model [BCSS98] relies on algebraic computations over R. Bournez [Bou99] has already provided some relations between linear BSS and Piecewise constant derivative systems which also generates Euclidean drawings. In [DL07], AGC without accumulation was proved equivalent to the linear BSS model (i.e. BSS restricted to multiplication only by constants) with an unbounded number of variables. In the present paper, the full BSS is related to AGC with isolated accumulations.

108 J´erˆome Durand-Lose
Let us note that AGC relies on intersection of lines (to ﬁnd the collisions), another model considers also intersections with circles [Huc89,Huc91]; AGC considers all collisions while in Huckenbeck’s geometrical machines the programs chooses which geometric object to build and to consider.
Abstract geometrical computation (ACG) arises from the common use in cellular automata (CA) literature of Euclidean settings to explain an observed dynamics or to design a CA for a particular purpose. While CA operate in discrete time over discrete space, Euclidean geometry deals with both continuous time and space. This switch of context is justiﬁed by the scaling invariance of CA and relates to our preference and ability for thinking in continuous rather than discrete terms. Abstract geometrical computation works in a continuous setting: discrete signals/particles are dimensionless points; the local function of CA – computing the next state of a cell according to the states of neighbouring cells– is replaced by collision rules: which signals emerge from a collision of signals. Signals and rules deﬁne signal machines (SM).
This recent model, restricted to rational numbers, is able to carry out any (discrete) Turing-computation [DL05b]; even with the additional restriction of reversibility and conservativeness [DL06c]. With continuous time, comes Zeno eﬀect (inﬁnitely many discrete steps during a ﬁnite duration): not only are accumulations possible, but they can be used to decide recursively enumerable problems by using the black-hole scheme [DL05a,DL06a]. Although accumulations can easily be generated, they can hardly be foreseen [DL06b].
In the Blum, Shub and Smale model (BSS), machines compute over any ring. Roughly speaking, polynomial functions can be performed on variables as well as tests (according to some order) for branching. The dimension of the input is not bounded, a shift operator is provided in order to access any variable (ﬁnitely many variables are considered since only ﬁnitely many are accessed in ﬁnite time).
In [DL07], AGC (without accumulation) and linear (multiplying two variables is forbidden) BSS over R with an unbounded number of variables are proved equivalent. This is done through linear real number unlimited register machines (the arguments of [Nov95] for the equivalence of URM and BSS translate to the linear case).
With a reasonable handling of accumulations, restrictions on multiplication can be lifted thus achieving the full BSS computing capability. They are handled in the following way: there is no second (or higher) order accumulation; only ﬁnitely many signals leave any accumulation; and one signal appears where an accumulation takes place. Multiplication is embedded in the same context and the same encoding of real numbers as in [DL07], so that addition, multiplication by constants and test do not have to be implemented again. Only the basis is recalled: a real number is encoded as the distance between two signals.
Multiplication of two real numbers, x and y, is done by producing the binary extension of y and according to yn adding or not x2n. With integers, n is increasing from 0, with real numbers, n goes down to −∞ which explains the use of an accumulation. Each iteration divides x by 2, computes the next bit of

Abstract Geometrical Computation with Accumulations 109
y and updates the partial product accordingly. All the values are geometrically decreasing to zero and so are the time and space used by an iteration, so that there is an accumulation. The signal left by the accumulation is located exactly at the value of the product.
To prove that AGC with accumulation is, as expected, strictly more powerful than the BSS model, it is explained how to implement the square root. Each iteration only uses addition and multiplication by constants.
Since the reader might be more familiar with BSS than with ACG, more care and illustrations are given to ACG. Section 2 provides all the needed deﬁnitions. Section 3 recalls basic encoding and geometric constructions for BSS simulation. Section 4 provides the multiplication between variables. Section 5 explains how to build a signal machine able to compute the square root with an accumulation. Conclusion, remarks and perspectives are gathered in Section 6.
2 Deﬁnitions
Abstract geometrical computation. In this model, dimensionless objects are moving on the real axis. When a collision occurs they are replaced according to rules. This is deﬁned by the following machines:
Deﬁnition 1 A signal machine with accumulation is deﬁned by (M, S, R, µa) where M (meta-signals) is a ﬁnite set, S (speeds) a function from M to R, R (collision rules) a partial function from the subsets of M of cardinality at least two into subsets of M (all these sets are composed of signals of distinct speed) and µa is a meta-signal, the one that comes out of any accumulation.
Each instance of a meta-signal is a signal. The function S assigns speeds to meta-signals. They correspond to the inverse slopes of the segments in space-time diagrams. The collision rules, denoted ρ−→ρ+, deﬁne what emerge (ρ+) from the collision of two or more signals (ρ−). Since R is a function, signal machines are deterministic. The extended value set, V , is the union of M and R plus two symbols: one for void, ⊘, and one for accumulation i. A conﬁguration, c, is a total function from R to V such that the set { x ∈ R | c(x) = ⊘ } is ﬁnite.
A signal corresponding to a meta-signal µ at a position x, i.e. c(x) = µ, is moving uniformly with constant speed S(µ). A signal can only start in a collision, except for µa that can also be generated by an accumulation. A signal can only end in a collision or an accumulation. This corresponds to condition 2 in Def. 2. At a ρ−→ρ+ collision signals corresponding to the meta-signals in ρ− (resp. ρ+) must end (resp. start) and no other signal should be present (condition 3). Condition 4 deals with accumulations, the ﬁrst line implies that suﬃciently close to the accumulation, outside of the light cone, there is nothing but a µa signal leaving the accumulation. The second line expresses that there is indeed an accumulation (this is not formalized since it would be too ponderous).
Let Smin and Smax be the minimal and maximal speeds. The causal past, or backward light-cone, arriving at position x and time t, J−(x, t), is deﬁned by all the positions that might inﬂuence the information at (x, t) through signals, formally:
J −(x, t) = { (x′, t′) | x − Smax(t−t′) ≤ x′ ≤ x − Smin(t−t′) } .

110 J´erˆome Durand-Lose

Deﬁnition 2 The space-time diagram issued from an initial conﬁguration c0 and lasting for T , is a function c from [0, T ] to conﬁgurations (i.e. a function

from R × [0, T ] to V ) such that, ∀(x, t) ∈ R × [0, T ] :

1. ∀t∈[0, T ], { x ∈ R | ct(x) = ⊘ } is ﬁnite,

2. if ct(x)=µ then ∃ti, tf ∈[0, T ] with ti<t<tf or 0=ti=t<tf or ti<t=tf =T s.t.: – ∀t′ ∈ (ti, tf ), ct′ (x + S(µ)(t′ − t)) = µ , – ti=0 or ( cti (xi) = ρ−→ρ+ and µ ∈ ρ+ ) or ( cti (xi)=i and µ=µa )
where xi=x + S(µ)(ti − t) , – tf =T or ( ctf (xf ) = ρ−→ρ+ and µ ∈ ρ− ) or ctf (xf )=i
where xf =x + S(µ)(tf − t) ; 3. if ct(x)=ρ−→ρ+ then ∃ε, 0<ε, ∀t′∈[t−ε, t+ε] ∩ [0, T ], ∀x′∈[x − ε, x + ε],
– (x′, t′) = (x, t) ⇒ ct′ (x′) ∈ ρ−∪ρ+ ∪ {⊘},

– ∀µ∈M , ct′ (x′)=µ ⇔ or

µ ∈ ρ− and t′ < t and x′ = x + S(µ)(t′ − t) , µ ∈ ρ+ and t < t′ and x′ = x + S(µ)(t′ − t) .

4. if ct(x) = i then

– ∃ε > 0, ∀t′∈[t−ε, t+ε] ∩ [0, T ], ∀x′∈[x − ε, x + ε],

(x′, t′) ∈/ J −(x, t) ⇒ or

ct′ (x) = ⊘ and x′ = x + S(µa)(t′ − t) ct′ (x) = µa and x′ = x + S(µa)(t′ − t) ,

– ∀ε>0, there are inﬁnitely many collisions in J−(x, t) ∩ R × [t−ε, t].

On space-time diagrams, the traces of signals are line segments whose directions are deﬁned by (S(.), 1) (1 is the temporal coordinate). Collisions correspond to the extremities of these segments. This deﬁnition can easily be extended to T = ∞. In the space-time diagrams, time increases upwards. To simplify, the same name is used for a signal throughout the computation, in fact, there is a diﬀerent meta-signal for each speed. As a computing device, the input is the initial conﬁguration and the output is the ﬁnal conﬁguration.

Blum, Shub and Smale model. (The reader is expected to be more familiar with the BSS model than with AGC, so this part is not very detailed.) BSS machines operate on an unbounded array containing real numbers in exact precision. The input/output is the content of the array. Apart from start and stop, the available instructions are: compute a polynomial function (and store the result), branch according to a sign test and shift. The machine can only access a ﬁnite part of the array at any time, the shift operator allows it to move on the array (like the head of a Turing machine).

3 Basic construction
Real number encoding. A Real number is encoded as the distance from a ba signal1 to its pairing val signal. Since signal machines are scaleless, two sca signals whose distance amounts for a scale are provided as depicted on Fig. 1. All numbers use the same scale. For the value 0, the superposition of ba and val is encoded as a single signal nul. This value is never considered in the rest of the paper; the reader is invited to check that it can be easily covered. 1 xx signal always means that it is an instance of the meta-signal xx.

Abstract Geometrical Computation with Accumulations 111

1 −π −1.5 √2 e

sca sca

val val ba val val

or nul(0)

Fig. 1. Encoding: scale and positions of val for values −π, −1.5, 0, √2 and e.

Geometric constructions. The construction of the multiplication, like the

addition and the multiplication by a constant, relies on some basic geometric

constructions. In each picture, the slopes of the line segments are indicated. It

can easily be checked that it is scale invariant and provides the desired eﬀect.

Signals with equal speeds result in parallel segments, the key to many geometric

properties. Figure 2(a) shows how a distance can be halved. This is done by

starting two signals. The ﬁrst one is going three times slower than the second

one. The time the ﬁrst one crosses half the way, the second one goes one full way

and half way back, so that they meet exactly in the middle. Doubling is done

the other way round. Figures 2(b) and 2(c) show two ways to halve a distance

while shifting it. They also work with two signals emitted that change speed

or direction at some point and meet at the desired location. Generating simple

shifts

is

done

by

modifying

only

one

slope

(

2 3

by

1 2

for

Fig. 2(b)

and

5 3

by

1

for

Fig. 2(b)).

l 2
3 1
1 3
l

1 1

(a) Halving.

2 1
l

l 2

3 1
2

1 3

3

(b) Shifting and halving 1.

l 2

3 1
3 13
l1

5 3

(c) Shifting and halving 2.

Fig. 2. Basic geometric constructions.

4 Multiplication
4.1 Algorithm
The multiplication of two real numbers x and y uses the (possibly) inﬁnite binary extension of y. This is done in two steps: normalization and then an inﬁnite loop.
The ﬁrst step starts by considering signs and special cases (i.e. multiplication by zero). Then, while y is greater than 2, x is multiplied by 2 and y divided by 2 (so that the product remains constant).
The second step carries out the multiplication. The binary extension y = y0.y1y2y3 . . . (the initialization ensures that 0 < y < 2) is generated bit by bit.

112 J´erˆome Durand-Lose

The underlying formula is:

xy =

x yi 2i

0≤i

.

This is computed iteratively with the updating on Table 1. The two last cases

correspond to yn = 1 and yn = 0 respectively. It is started with: p0 = 0 (prod-

uct), b0 = 1 (bit test), x0 = x and 0 < y0 = y < 2b0 = 2. The following invariants

are satisﬁed:

– bn = 2−n,

– 0 ≤ yn < 2bn,

– xn = x2−n, and

–

xy

=

pn

+

xn yn bn

.

The last invariant is trivially true for n = 0 and preserved by the loop. Since

xn

yn bn

<

2xn

and

xn

=

x2−n,

from

the

last

invariant

comes

that

lim
n→∞

pn

=

xy.

Table 1. Potentially inﬁnite loop to compute the product.

pn+1 xn+1 yn+1 bn+1

if yn = 0

stop

else if bn < yn pn + xn xn/2 yn − bn bn/2

else pn xn/2 yn bn/2

4.2 Initialisation
With our encoding, detecting whether x or y is zero is trivial. Detecting the signs and computing the sign of the product is also very easy. The following only deals with multiplication of positive values, the other cases are generated using absolute values and symmetry if the product is negative.
The above algorithm should be started with 0 < y0 < 2b0 = 2. So that there is a loop that multiplies x by 2 and divides y by 2 until y is small enough. This is illustrated on Fig. 3 (the ﬁrst two space-time diagrams go one above the other).
The algorithm is sequential: signals for base (ba ), bit testers (b ), x (x ) and y (y ) are ﬁx unless set on movement by some bouncing initialization signals (ini). All distances are from ba and are measured according to the “oﬃcial” scale (not represented). The b signal stays at position 2 for testing the end of the loop (i.e. y < 2). The signal ini comes from the left. If it meets y before b, this means that the loop is ﬁnished (Fig. 3(c)). Otherwise it has to half y, to double x and to test again (ﬁgures 3(a) and 3(b)).
At the end of initialization (which is always achieved in ﬁnite time and ﬁnitely many collisions), the b at position 2 is set at position 1 (halved as usual). The signal p amounting for p is generated. Since p is 0 at start, it is set on ba (this corresponds to a diﬀerent meta-signal). And ﬁnally, ini turns to mul that handles the second step of the multiplication. Everything is depicted on Fig. 3(c).

Abstract Geometrical Computation with Accumulations 113

double x

halve y

mul p

ini ba x b

y ba ini b y x

ba ini y b

(a) First iteration.

(b) Another iteration.

(c) Loop end.

Fig. 3. Preparing the data for multiplication.

x

4.3 Main loop
The multiplication follows the (possibly) inﬁnite loop deﬁned in Table 1. Basically, the things to do are additions, subtractions, divisions by 2 and tests. These are easy implementable inside signal machines. But, since the loop is inﬁnite, a correct accumulation have to be generated. By correct, it is understood as at the right location and there is indeed an accumulation. The second point is not to be underestimated since, for example, if at each iteration some constant distance would have to be crossed, then there would be an inﬁnite duration process but no accumulation.
The algorithm is driven by a mul signal that bounces between p and other signals. First of all, mul has to test to know which case to consider. This is done easily: going away from ba, if b is encountered before y this means that bn < yn otherwise yn < bn (when they are met simultaneously, i.e. they are equal, this leads to the end of the loop at the next iteration).
The simpler case is when yn < bn. There is nothing to do but to halve bn and xn, that is halve the distance from b and x to ba. Signals p and y are left untouched. This is done as depicted on the lower part of Fig. 4(b).
The other case bn < yn is depicted on Fig. 4(a) and on the upper part of Fig. 4(b). The addition of xn to pn is done by moving p to the location of x, meanwhile x is moved on the right by half the distance it has from p. The signal b is also moved on the right, at a distance from the new p that is half the previous distance. The signal y is moved to the right, at a distance from the new p that is equal to its original distance from b.
To ensure accumulation, all lengths are geometrically scaled down and all the computations take place by p and are shifted according to the moves of p. This can be seen on Fig. 4. Each iteration leads to halving the distance between ba

114 J´erˆome Durand-Lose

shift and halve shift

p ba mul

byx (a) First iteration.

p mul y b (b) Two iterations.

Fig. 4. Multiplication.

x

and both x and b. Since the distance from ba to y is at most twice the distance to

b,

this

ensures

that

all

distances

are

bounded

by

some

Ms 2n

at

the

nth

iteration.

Clearly the duration of an iteration is bounded proportionally to the maximum

distance

between

the

signals.

Thus

it

is

also

bounded

by

some

Mt 2n

at

the

nth

iteration. There is no time lag between two consecutive iterations, so that there

is indeed an accumulation.

5 Square root

In this section, it is brieﬂy argued why it is possible to compute the square root

with an accumulation and how. Let a be any positive real number. The construc-

tion follows the digit by digit approximation algorithm constructing a sequence

bn such that:

b2n ≤ a <

bn

+

1 2n

2

.

At each stage it should be tested whether

bn

+

1 2n+1

2

≤

a.

If

it

is

the

case

then

bn+1

= bn +

1 2n+1

otherwise

bn+1

= bn.

Using

the

sequences:

dn

= a − b2n,

en =

bn 2n

and fn =

,1
4n+1

the

test

corresponds to computing the

sign of

a−

bn

+

1 2n+1

2

= a − b2n −

bn 2n

−

1 4n+1

= dn − en − fn

.

The updating of all these sequences is shown on Table 2 (plus another helpful

sequence

gn

=

1 2n+1

).

The

only

operations

used

are

additions,

multiplications

by

constants and tests. Again the size of the computing part is decreasing geomet-

rically and computation can be done by the signal encoding the value of bn and shifted with it.

The algorithm starts by a pretreatment that ﬁnds the initial value for n. This

value might be negative (e.g. n = −11 for a = 220 + 1).

Abstract Geometrical Computation with Accumulations

Table 2. Inﬁnite loop to compute the square root.

bn+1

dn+1

en+1 fn+1 gn+1

if dn − en − fn = 0

stop

else if 0 < dn − en − fn bn + gn dn − en − fn en/2 + fn fn/4 gn/2

else bn dn en/2 fn/4 gn/2

115

6 Conclusion
In the present paper, AGC with accumulation is proved to be strictly more powerful than basic BSS. This is not very surprising because it is already known to decide in ﬁnite time any recursively enumerable problem (in the classical discrete setting). It would be natural to extends the structure BSS works with (i.e. R as a ring) with square rooting but many functions are computab√le with an accumulation. It would be interesting to identify them. Considering 2, an accumulation point of a rational signal machine can be irrational.
If the computation is stopped before the accumulation happens, then an approximation is generated. Computable analysis relies on the idea of an inﬁnite approximating sequence both for representing real numbers and for computing (type-2 Turing machine needs an inﬁnite number of iterations to compute a function on real numbers). The next step would be to relate these two models (the spirit of [CH99]). One problem would be to miniaturize and to ensure the generation of an accumulation. Another one is that computable analysis only provides continuous functions, while in ACG, there is, for example, the sign function which is clearly not continuous. On the other side, Moore’s recursion theory allows non continuous functions (even the characteristic function of rational numbers).
There might be many accumulations to simulate BSS, but none is of order two. Another issue is to consider nth order accumulation and connect with inﬁnite Turing machines and ordinals [Ham07].

References
[BCGH06] O. Bournez, M. L. Campagnolo, D. S. Gra¸ca, and E. Hainry. The general purpose analog computer and computable analysis are two equivalent paradigms of analog computation. In J.-Y. Cai, S. B. Cooper, and A. Li, editors, Theory and Appliacations of Models of Computations (TAMC ’06), Beijing, China, number 3959 in LNCS, pages 631–643. Springer, 2006.
[BCSS98] L. Blum, F. Cucker, M. Shub, and S. Smale. Complexity and real computation. Springer, New York, 1998.
[BH04] O. Bournez and E. Hainry. An analog characterization of elementarily computable functions over the real numbers. In J. Diaz, J. Karhum¨aki, A. Lepisto, and D. T. Sannella, editors, 31st Int. Col. on Automata, Languages and Programming (ICALP ’04), Turku, Finland, number 3142 in LNCS, pages 269–280. Springer, Jul 2004.
[Bou99] O. Bournez. Some bounds on the computational power of piecewise constant derivative systems. Theory of Computing Systems, 32(1):35–67, 1999.

116 J´erˆome Durand-Lose
[CH99] T. Chadzelek and G. Hotz. Analytic machines. Theoret. Comp. Sci., 219(12):151–167, 1999.
[DL05a] J. Durand-Lose. Abstract geometrical computation for black hole computation (extended abstract). In M. Margenstern, editor, Machines, Computations, and Universality (MCU ’04), number 3354 in LNCS, pages 176–187. Springer, 2005.
[DL05b] J. Durand-Lose. Abstract geometrical computation: Turing-computing ability and undecidability. In B. S. Cooper, B. L¨owe, and L. Torenvliet, editors, New Computational Paradigms, 1st Conf. Computability in Europe (CiE ’05), number 3526 in LNCS, pages 106–116. Springer, 2005.
[DL06a] J. Durand-Lose. Abstract geometrical computation 1: embedding black hole computations with rational numbers. Fund. Inf., 74(4):491–510, 2006.
[DL06b] J. Durand-Lose. Forcasting black holes in abstract geometrical computation is highly unpredictable. In J.-Y. Cai, S. B. Cooper, and A. Li, editors, Theory and Appliacations of Models of Computations (TAMC ’06), number 3959 in LNCS, pages 644–653. Springer, 2006.
[DL06c] J. Durand-Lose. Reversible conservative rational abstract geometrical computation is turing-universal. In A. Beckmann and J. V. Tucker, editors, Logical Approaches to Computational Barriers, 2nd Conf. Computability in Europe (CiE ’06), number 3988 in LNCS, pages 163–172. Springer, 2006.
[DL07] J. Durand-Lose. Abstract geometrical computation and the linear Blum, Shub and Smale model. In S. Cooper, B. L¨owe, and A. Sorbi, editors, Computation and Logic in the Real World, 3rd Conf. Computability in Europe (CiE ’07), number 4497 in LNCS, pages 238–247. Springer, 2007.
[Ham07] J. D. Hamkins. A survey of inﬁnite time turing machines. In J. DurandLose and M. Margenstern, editors, Machines, Computations and Universality (MCA ’07), number 4664 in LNCS, pages 62–71. Springer, 2007.
[Huc89] U. Huckenbeck. Euclidian geometry in terms of automata theory. Theor. Comput. Sci., 68(1):71–87, 1989.
[Huc91] U. Huckenbeck. A result about the power of geometric oracle machines. Theor. Comput. Sci., 88(2):231–251, 1991.
[Kaw05] A. Kawamura. Type-2 computability and moore’s recursive functions. Electr. Notes Theor. Comput. Sci., 120:83–95, 2005.
[Moo96] C. Moore. Recursion theory on the reals and continuous-time computation. Theoret. Comp. Sci., 162(1):23–44, 1996.
[Nov95] E. Novak. The real number model in numerical analysis. J. Complex., 11(1):57–73, 1995.
[PE74] M. B. Pour-El. Abstract computability and its relation to the general purpose analog computer (some connections between logic, diﬀerential equations and analog computers). Trans. Amer. Math. Soc., 199:1–28, 1974.
[Wei00] K. Weihrauch. Introduction to computable analysis. Texts in Theoretical computer science. Springer, Berlin, 2000.

Notions of Bisimulation for Heyting-Valued Modal Languages
Pantelis E. Eleftheriou1, Costas D. Koutras2, and Christos Nomikos3
1 Institut de Matem`atica, Universitat de Barcelona, Gran Via de les Corts Catalanes 585, 08007 Barcelona, Spain
pelefthe@gmail.com 2 Department of Computer Science and Technology, University of Peloponnese,
end of Karaiskaki Street, 22100 Tripolis, Greece ckoutras@uop.gr
3 Department of Computer Science, University of Ioannina, 45110 Ioannina, Greece cnomikos@cs.uoi.gr
Abstract. We deﬁne notions of bisimulation for the family of Heytingvalued modal logics introduced by M. Fitting. In this family of logics, each modal language is built on an underlying space of truth values, a Heyting algebra H. All the truth values are directly represented in the language, which is interpreted on relational frames with an H-valued accessibility relation. We investigate the correct notion of bisimulation in this context: we deﬁne two variants of bisimulation relations and derive relative (to a truth value) modal equivalence results for bisimilar states. We further investigate game semantics for our bisimulation, HennessyMilner classes and other relevant properties. If the underlying algebra H is ﬁnite, Heyting-valued modal models can be equivalently reformulated to a form relevant to epistemic situations with many interrelated experts. Our deﬁnitions and results draw from this formulation, which is of independent interest to Knowledge Representation applications.
Key words: Modal Logic, Many-Valued Logic, Bisimulations.
1 Introduction
Bisimulation is a very rich concept which plays an important role in many areas of Computer Science, Logic and Set Theory. Its origins can be found in the analysis of Modal Logic but it was independently rediscovered by computer scientists in their eﬀorts to understand concurrency. In Modal Logic, bisimulations were introduced by Johan van Benthem, under the name of p-relations or zigzag relations, in the course of his work on the correspondence theory of Modal Logic [vB83,vB84]. In Computer Science, bisimulations were introduced by Park in [Par91] and Henessy and Milner in [HM85], in the course of investigating the notion of equivalence among processes (see [San07] for a tutorial on the history
Corresponding author

118 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos
of bisimulations). In this context, bisimulations represent a fundamental notion of identity between process states and every language designed to capture the essential properties of processes should be blind for bisimilar states.
On the other hand, from the Modal Logic viewpoint, bisimulation is the correct notion of similarity between two modal models: modal formulas are unable to distinguish bisimilar points of the two models. But its importance lies far beyond; the celebrated van Benthem characterization theorem, published in the mid-‘70s, states that invariance for bisimulation captures the essential property of the ‘modal fragment’ of ﬁrst-order logic: a ﬁrst-order formula is invariant for bisimulation iﬀ it is (equivalent to the) the syntactical translation of a modal formula (see [BdRV01] for a nice exposition of this result and its consequences). The characterization theorem has generated an important stream of research in the analysis of logical languages. In particular, the bisimulation-based analysis of modal languages has been extensively studied in the Amsterdam school of modal logic [dR93,Ger99,MV03], and it has even been suggested that this notion is as important for modal logic as the notion of partial isomorphism has been for the model theory of classical logic (see the PhD thesis of M. de Rijke [dR93]). It is worth mentioning that bisimulations have been used beyond the realm of classical modal logic: see for instance the variant used to analyze since-until temporal languages [KdR97], M. Otto’s work related to Finite Model Theory [Ott99] and J. Gerbrandy’s dissertation [Ger99]. Bisimulations have been also used as a fundamental tool in the area of non-well founded set theory ([Acz88], see [BdRV01] for a few details and further references).
In this note, we address the question of what constitutes a suitable notion of bisimulation for the family of many-valued modal languages introduced by M. Fitting in the early ’90s [Fit92,Fit91]. Each language of this family is built on an underlying space of truth values, a Heyting algebra H. There exist three features that give these logics their distinctive character. The ﬁrst one is syntactic: the elements of H are directly encoded in the language as special constants and this permits the formation of ‘weak’, uncertainty-oriented versions of the classical modal epistemic actions [Kou03,KNP02,KP02]. The second is semantic: the languages we discuss are interpreted on H-labelled directed graphs which provide us a form of many-valued accessibility relation. Finally, the third one concerns the potential applicability of these logics in epistemic situations with multiple intelligent agents. More speciﬁcally, assuming that H is a ﬁnite Heyting algebra, these logics can be formulated in a way that expresses the epistemic consensus of many experts, interrelated through a binary ‘dominance’ relation [Fit92]. It is worth mentioning that, model-theoretically, every complete Heyting algebra can serve as the space of truth values. However, apart from the equivalent multipleexpert formulation of the logics, the ﬁniteness assumption for H is essential for the elegant canonical model construction of [Fit92] which leads to a completeness theorem; note that this ﬁniteness restriction seems to be also necessary for obtaining a many-valued analog of the ultraﬁlter extension construction [EK05].
We provide below two notions of bisimulation and derive modal equivalence results. The ﬁrst one is a rather strong notion, that allows us to formulate sim-

Notions of Bisimulation for Many-Valued Modal Languages 119
ple, intuitive, Ehrenfeucht-Fraiss´e type bisimulation games through which one can easily deﬁne bounded bisimulations, as in the classical case. Also, an appropriate notion of unravelling is given, through which one gets a form of the celebrated tree-model property, considered to be critical for the ‘robust decidability’ of modal logics [Var97]. A second, rather involved notion of weak bisimulation is discussed which allows us to obtain an interesting notion of Hennessy-Milner class of Heyting-valued modal models. Both notions of bisimulations draw inspiration from the equivalent multiple-expert formulation of these logics, which is actually a mixture of Kripke modal and Kripke intuitionistic semantics. Due to space limitations, this semantics, along with the interpretation of our bisimulation relations in this context, is left for the full paper.

2 Many-Valued Modal Languages

In this section we provide the syntax and semantics of many-valued modal languages, as introduced in [Fit92], with only minor changes in the notation. To construct a modal language of this family, we ﬁrst ﬁx a Heyting algebra H which will serve as the space of truth values. Thus, we ﬁrst brieﬂy expose the basic deﬁnitions and properties of Heyting algebras, ﬁxing also notation and terminology. We assume that the reader already has some familiarity with the elements of lattice theory and universal algebra. For more details the reader is referred to the classical texts [RS70,BD74].

Heyting Algebras A lattice L is a pair L, ≤ consisting of a non-empty set L

equipped with a partial-order relation ≤, such that every two-element subset

{a, b} of L has a least upper bound or join, denoted by a ∨ b, and a greatest lower

bound or meet, denoted by a ∧ b. A lattice L is complete if a join and a meet

exist for every subset of L. A least (or bottom) element of a lattice is denoted

by ⊥ and a greatest (or top) one by . An element x ∈ L is join-irreducible if

x = ⊥ (in case L has a bottom element) and x = a ∨ b implies x = a or x = b.

We frequently use indexed sets and denote (possibly inﬁnite) meets and joins by

at and at. Some fairly obvious properties of inﬁnite joins and meets, such

t∈T

t∈T

as t ∈ T a ∧ at = a ∧ at ∈ T t will be used, generally without comment. A lattice H = H, ≤ with the additional property that, for every pair of

elements a, b , the set {x | a∧x ≤ b} has a greatest element, is called a relatively

pseudo-complemented lattice. This element is denoted by a ⇒ b and is called

the pseudo-complement of a relative to b. A relatively pseudo-complemented

lattice is always a topped ordered set [RS70]. It is not always the case that a

relatively pseudo-complemented lattice has a least element. A relatively pseudo-

complemented lattice H with a least element is called a Heyting algebra (HA)

or a pseudo-Boolean algebra. It is known that the class of HAs includes the

class of Boolean algebras and is included in the class of distributive lattices;

both inclusions are proper. For ﬁnite lattices, the second inclusion becomes an

equality: the class of ﬁnite HAs coincides with the class of ﬁnite distributive

lattices [RS70]. The following lemma gathers some useful properties of relatively

120 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos

pseudocomplemented lattices that will be used in Section 3; whenever a possibly inﬁnite join or meet is involved, it is assumed that it exists. The proof of its items can be found in [RS70,BD74]. Note also that the ﬁrst item of the lemma can be equivalently considered as a deﬁnition of relative pseudo-complementation.

Lemma 1. 1. x ≤ (a ⇒ b) iﬀ (x ∧ a) ≤ b

2. (a ⇒ b) = iﬀ a ≤ b

3. If a1 ≤ a2 then (a2 ⇒ b) ≤ (a1 ⇒ b)

4. c ∧ (a ⇒ b) = c ∧ (c ∧ a) ⇒ (c ∧ b)

5. (a ∧ bt) = a ∧ bt

t∈T

t∈T

6. (a ∨ b) ⇒ c = (a ⇒ c) ∧ (b ⇒ c)

Syntax of Many-Valued Modal Languages Having ﬁxed a complete Heyting al-

gebra H we proceed to deﬁne the syntax of the modal language. The elements

of H are directly represented in the language by special constants, called propo-

sitional constants, and we reserve lowercase letters (along with ⊥, ) to denote

them. To facilitate notation, we use the same letter for the element of H and the

constant which represents it in the language; context will clarify what is meant.

Assuming also a set Φ of propositional variables (also called propositional let-

ters)

we

deﬁne

the

many-valued

modal

language

LH 23

(Φ)

with

the

following

BNF

speciﬁcation, where t ranges over elements of H, P ranges over elements of Φ

and

A

is

a

formula

of

LH 23

(Φ).

A ::= t | P | A1 ∨ A2 | A1 ∧ A2 | A1 ⊃ A2 | 2A | 3A

As we will see below, the modal logics deﬁned are in general bimodal, thus

we need both modal operators. Note also that ∨ and ∧ serve both as logical

connectives, as well as lattice operation symbols but it should be clear by context

what is meant. In the rest of the paper, we shall often omit Φ when possible and

speak

of

the

language

LH 23

.

A

(non-classical)

negation

¬X

can

be

deﬁned

as

(X ⊃ ⊥).

Semantics

of

Many-Valued

Modal

Languages

LH 23

is

interpreted

on

an

interesting

variant of a relational frame, which possesses a kind of Heyting-valued accessi-

bility relation. Note that there have been other approaches in the literature for

deﬁning many-valued modal logics, but all of them have kept the essence of clas-

sical

relational

semantics

intact

(see

[Fit92]

for

references).

Given

LH 23

(Φ),

we

deﬁne H-modal frames and H-modal models as follows:

Deﬁnition 1.

An

H-modal

frame

for

LH 23

(Φ)

is

a

pair

F

=

S, g , where S is a

non-empty set of states and g : S × S → H is a total function mapping pairs of

states to elements of H.

An H-modal model M = S, g, v is built on F by providing a valuation v, that

is a function v : S × (H ∪ Φ) → H which assigns a H-truth value to atomic

formulae in each state, such that v(s, t) = t, for every s ∈ S and t ∈ H. In other

words, the propositional constants are always mapped to ‘themselves’.

Notions of Bisimulation for Many-Valued Modal Languages 121

In the sequel, we shall often omit the adjective ‘modal’ and talk simply of

H-frames and H-models.

The

valuation

v

extends

to

all

the

formulae

of

LH 23

(Φ)

in

a

standard

recursive

fashion:

Deﬁnition 2. Let M = S, g, v be an H-model and s a state of S. The exten-

sion

of

the

valuation

v

to

the

whole

language

LH 23

(Φ)

is

given

by

the

following

clauses;

– v(s, A ∧ B) = v(s, A) ∧ v(s, B) – v(s, A ∨ B) = v(s, A) ∨ v(s, B) – v(s, A ⊃ B) = v(s, A) ⇒ v(s, B)

– v(s, 2A) =

g(s, t) ⇒ v(t, A)

t∈S

– v(s, 3A) =

g(s, t) ∧ v(t, A)

t∈S

The operators of necessity (2) and possibility (3) are not each other’s dual,

unless H is a Boolean algebra [Fit92]. Note also that all the deﬁnitions above

collapse to the familiar ones from the classical case, in the case of the classical

language

L2 23

,

where

2

is

the

lattice

of

two-valued

classical

logic.

3 Bisimulations for Many-Valued Modal Languages

In this section, we deﬁne two suitable general notions of bisimulation for a lan-

guage

LH 23

of

the

family

deﬁned

in

the

previous

section.

Before

proceeding,

we

have to deﬁne a reﬁned notion of modal truth invariance which ﬁts our aims and

which also has an interesting interpretation in the multiple-expert context. Note

that the following notion is trivial for t = ⊥.

Deﬁnition 3 (t-invariance). Let M = S, g, v and M = S , g , v be H-

models

for

LH 23

(Φ),

s

∈

S

and

s

∈S

two states and t ∈ H a truth value (t = ⊥).

We say that modal truth is t-invariant for the transition between s and s if for

every

X

∈

LH 23

(Φ)

t ∧ v(s, X) = t ∧ v (s , X)

3.1 Strong Bisimulation for Many-Valued Modal Languages
The following deﬁnition captures the idea of moving ‘back and forth’ between two H-models by matching steps (‘modulo’ t) in both directions.
Deﬁnition 4 (t-bisimulations). Given two H-models M = S, g, v and M = S , g , v and a truth value t ∈ H (t = ⊥), a non-empty relation Z ⊆ S × S is a t-bisimulation between M and M if for any pair s, s ∈ Z

122 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos

(base) t ∧ v(s, P ) = t ∧ v (s , P ) for every P ∈ Φ

(forth) for every r ∈ S such that t ∧ g(s, r) = ⊥,

there exists an r ∈ S such that t ∧ g(s, r) = t ∧ g (s , r ) and rZr

(back ) for every r ∈ S such that t ∧ g (s , r ) = ⊥,

there exists an r ∈ S such that t ∧ g (s , r ) = t ∧ g(s, r) and rZr

Two states s and s

are called t-bisimilar

(notation

s

↔
t

s

or

M,

s

↔
t

M

,

s

)

if there is a t-bisimulation Z between M and M such that sZs .

We can now prove the basic theorem which states that t-bisimulation implies

t-invariance.

Theorem 1.

If

M, s

↔
t

M ,s ,

then

t

∧

v(s, X)

=

t ∧ v (s , X) for every

X

∈

LH 23

.

Proof. The proof is left for the full version of the paper.

EF-type games for t-bisimulation The t-bisimulation game is a simple variant of the Ehrenfeucht-Fraiss´e game played in First-Order Logic. For the purposes of the rest of this section call a state r a t-compatible successor state of s if t ∧ g(s, r) = ⊥. Two elements a, a of H are called t∧-equivalent if t ∧ a = t ∧ a . We call labels the H-truth values attached to the graph’s edges and to the propositional letters of the language in each possible world. The t-bisimulation game is played on two pointed H-models (models with a single distinguished state) M, s0 and M , s0. There exists one marked element in each H-model; initially, the marked elements are the distinguished nodes s0 and s0. In each round of the game
– Player I selects one of the H-models, chooses a t-compatible successor of the marked element and moves the marker along the edge (labelled by aI) to its target
– Player II responds with a move of the marker in the other H-model in a corresponding t-compatible transition (labelled by aII) such that aI and aII are t∧-equivalent and the labels of the propositional letters in the marked elements (states) of the models are also t∧-equivalent
The length of the game is the (ﬁnite or inﬁnite) number of rounds and Player II loses the match if at a certain round cannot respond with an appropriate move. It is obvious that Player I is trying to spoil a t-bisimulation while Player II is trying to reveal one. Player II has a winning strategy in a game of n rounds if she can win every n-round game played on M, s0 and M , s0. In a classical fashion, we can proceed to a ﬁner analysis of t-bisimulations using the inductively deﬁned notion of the modal depth of a formula (the maximum number of modal operators encountered in a subformula, [BdRV01,MV03]). The notion of a tbisimulation bounded by a positive integer n, or any ordinal actually, can easily be deﬁned (see [Ger99, Chapter 2.1] for the classical case), but we will not give further details here, since the whole construction is identical to the classical one. We only provide the following proposition which generalizes the known classical results from two-valued modal logic:

Notions of Bisimulation for Many-Valued Modal Languages 123

Proposition 1. 1. Player II has a winning strategy in the n-round game played

on M, s0 and M , s0 iﬀ modal truth is t-invariant in s0 and s0 for every formula up to modal depth n.

2. Player II has a winning strategy for the inﬁnite game played on M, s0 and

M , s0

iﬀ

s0

↔
t

s0.

Proof. The proof of the ﬁrst item runs by induction on n and is actually a restatement of the proof of Theorem 1. The second item follows by the deﬁnitions above.

The t-bisimulation games can be formulated in a simple way for the class of languages built on ﬁnite linear orders. Assuming further that truth values are colours, linearly ordered, and given that the meet operation is simple in ﬁnite chains (a∧b = min(a, b)) the game can be described in an easy way that provides also an element of fun.

t-unravellings and the tree-model property The idea of unravelling a model into a modally-equivalent tree model is known both from modal logic and the theory of processes. In the latter ﬁeld, the states of the unravelled model represent traces (histories) of processes, starting from a state s. The following deﬁnition provides the many-valued analog of this notion.

Deﬁnition 5. Given a pointed model M = S, g, v , s1, its t-unravelling is the

model

Mu s1

=

Su s1

,

gu s1

,

vu s1

,

where

1.

Su s1

consists

of

all

tuples

s1, · · · , sk

where si+1 is a t-compatible successor

of si

2.

gu s1

(

s1, · · · , sk

,

s1, · · · , sk+1

) = t ∧ g(sk, sk+1), and ⊥ for any other pair of

tuples

3.

vu s1

(

s1, · · ·

, sk

, P ) = t ∧ v(sk, P ),

(P

∈ Φ)

Obviously

Mu s1

is

a

tree

model

and

the

following

proposition

can

be

proved

by a careful inspection on the deﬁnition of a t-bisimulation.

Proposition

2.

The

graph

of

the

function

from

Su s1

to

S,

which

maps

every

tuple to its last component (and s1 to s1) is a t-bisimulation.

Thus, modal truth is t-invariant for the transition from s1 to the root s1 of the tree and this is a generalized version of the tree model property [Var97].

Satisﬁability in Many-Valued Modal Languages The general satisﬁability prob-

lem

in

this

context

can

be

phrased

as

follows:

given

X

∈

LH 23

and

t

∈

H,

is there a state s of an H-model M in which v(s, X) ≥ t? This is equiva-

lent (by Lemma 1(2)) to t ⇒ v(s, X) = which is equivalent (by Def. 2) to

v(s, t ⊃ X) = . Thus, the general satisﬁability problem is subsumed by the

question of ﬁnding a state in which a formula is mapped to the top element of

the lattice. By the previous paragraph, if such a state/model exists, then this

formula can be also satisﬁed at the root of a ( -unravelled) tree. Imitating the

124 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos

classical arguments ( [MV03]), it is easy to prove that, if H is ﬁnite, every formula can be satisﬁed in a ﬁnite tree whose size is bounded: its depth is bounded by the modal depth of X and its branching degree is bounded by the number of box and diamond subformulas of x. This leads to a simple way of proving the fact that the many-valued analog of the system K (which is determined by the class of all H-models [Fit92]) has a decidable general satisﬁability problem.

3.2 Weak Bisimulations for Many-Valued Modal Languages

We proceed now to deﬁne, a weaker, more ﬁne-grained notion of bisimulation that is directly inspired from (and can be better explained in the context of) the multiple-expert semantics of these languages. We ﬁrst ﬁx some notation.
Let IH denote the set of join-irreducible elements of H. For the rest of this section, we ﬁx a complete Heyting algebra H that has the following property:

Every t ∈ H − IH is equal to the join of a ﬁnite number of elements in IH (1)
Deﬁne the function DH from H−{⊥} to 2IH , such that DH(t) = {c ∈ IH | c ≤ t}. Using Property (1), we see that t = c∈DH(t) c. Intuitively, DH provides a decomposition of a value t ∈ H into join-irreducible values. In the next deﬁnition, a bisimulation relation is deﬁned for every truth value, but in a way that it is “upwards (with respect to the lattice of truth values) consistent”.

Deﬁnition 6 (Weak bisimulation). Given two H-models M = S, g, v and M = S , g , v , a function Z from H − {⊥} to 2S×S is a weak bisimulation
between M and M if it satisﬁes the following properties:

– for every t1, t2 ∈ H (consistency) Z(t1 ∨ t2) = Z(t1) ∩ Z(t2)
– for every join-irreducible value t ∈ IH and any pair s, s (base) t ∧ v(s, P ) = t ∧ v (s , P ) for every P ∈ Φ

∈ Z(t)

(forth) for every r ∈ S such that t ∧ g(s, r) = ⊥

and for every c ∈ DH(t ∧ g(s, r)), there exists an r ∈ S such that c ≤ g (s , r ) and r, r ∈ Z(c)

(back ) for every r ∈ S such that t ∧ g (s , r ) = ⊥

and for every c ∈ DH(t ∧ g (s , r )), there exists an r ∈ S such that c ≤ g(s, r) and r, r ∈ Z(c)

Two states s and s are called weakly t-bisimilar (notation s t s or M, s t M , s ) if there is a weak bisimulation Z between M and M such that s, s

belongs to Z(t).

The reader can check that we have indeed deﬁned a weaker notion than that

of

a

t-bisimulation:

M,

s

↔
t

M

,

s

implies M, s

t M ,s .

The basic theorem of the previous section is still valid under this new notion,

but the proof requires some more elaboration.

Theorem 2. If M, s t M , s , then t ∧ v(s, X) = t ∧ v (s , X) for every

X

∈

LH 23

.

Proof. The proof is left for the full version of the paper.

Notions of Bisimulation for Many-Valued Modal Languages 125

Image-ﬁnite H-models and weak t-bisimulations One of the fundamental questions in the bisimulation-based analysis of modal languages, concerns the identiﬁcation of cases in which the converse of Theorem 2 is true. Much obviously, it is not always true: the classical counterexample of two tree models, both with a ﬁnite branch for each natural number, one of which possesses an inﬁnite branch, suﬃces (cf. [BdRV01, Chapter 2.2]). The simplest example of Hennessy-Milner classes of modal models (classes in which modal equivalence is itself a bisimulation relation) is the class of image-ﬁnite models, in which each state has only a ﬁnite number of successors. It is natural to consider a straightforward manyvalued analog of this notion by considering H-models in which for each state s, the set of successors of s is always ﬁnite and check whether in this case tinvariance implies t-bisimilarity. Formally, the notion of image-ﬁnite H-models is deﬁned as follows.

Deﬁnition 7 (Image-ﬁnite H-models). An H-model M = S, g, v is called image-ﬁnite if for every s ∈ S, the set Ss = {r ∈ S | g(s, r) = ⊥} is ﬁnite.
The following theorem states that for image-ﬁnite H-models, t-invariance implies t-bisimilarity.

Theorem 3. Let M = S, g, v and M = S , g , v be image-ﬁnite H-models

for

LH 23

(Φ).

Deﬁne

the

function

Z

from

H − {⊥}

to

2S×S

so that for every

s ∈ S, every s ∈ S and every t ∈ H − {⊥}, s, s ∈ Z(t) iﬀ modal truth is

t-invariant for the transition between s and s . Then Z is a weak bisimulation

between M and M .

Proof. The proof is left for the full version of the paper.

4 Conclusions - Related Work
In this paper, we have contributed to the extensive literature on the importance and the fundamental nature of bisimulation. Our main aim has been to deﬁne a ﬁne-grained notion of bisimulation for Heyting-valued modal languages and establish its basic facts. Our results have an interesting meaning for Knowledge Representation situations, when interpreted in the multiple-expert context. It remains to investigate appropriate extension of smallest and largest bisimulations in this context and address possible applications for Knowledge Engineering in complex epistemic situations.

References
[Acz88] P. Aczel. Non-Well-Founded Sets. CSLI Publications, 1988. [BD74] R. Balbes and Ph. Dwinger. Distributive Lattices. University of Missouri
Press, 1974. [BdRV01] P. Blackburn, M. de Rijke, and Y. Venema. Modal Logic. Number 53 in
Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 2001.

126 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos
[DP90] B. A. Davey and H. A. Priestley. Introduction to Lattices and Order. Cambridge University Press, 1990.
[dR93] M. de Rijke. Extending Modal Logic. PhD thesis, Institute for Logic, Language and Computation, University of Amsterdam, 1993.
[EK05] P. Eleftheriou and C. D. Koutras. Frame constructions, truth invariance and validity preservation in many-valued modal logic. Journal of Applied Non-Classical Logics, 15(4):367–388, 2005.
[Fit91] M. C. Fitting. Many-valued modal logics. Fundamenta Informaticae, 15:235– 254, 1991.
[Fit92] M. C. Fitting. Many-valued modal logics II. Fundamenta Informaticae, 17:55–73, 1992.
[Ger99] J. D. Gerbrandy. Bisimulations on Planet Kripke. PhD thesis, Institute for Logic, Language and Computation, University of Amsterdam, 1999.
[GG84] D. M. Gabbay and F. Guenthner, editors. Handbook of Philosophical Logic. D. Reidel, Dordrecht, 1984.
[HM85] M. Hennessy and R. Milner. Algebraic laws for nondeterminism and concurrency. Journal of the ACM, 32:137–162, 1985.
[KdR97] N. Kurtonina and M. de Rijke. Bisimulations for temporal logic. Journal of Logic, Language and Information, 6:403–425, 1997.
[KNP02] C. D. Koutras, Ch. Nomikos, and P. Peppas. Canonicity and completeness results for many-valued modal logics. Journal of Applied Non-Classical Logics, 12(1):7–41, 2002.
[Kou03] C. D. Koutras. A catalog of weak many-valued modal axioms and their corresponding frame classes. Journal of Applied Non-Classical Logics, 13(1):47– 72, 2003.
[KP02] C. D. Koutras and P. Peppas. Weaker axioms, more ranges. Fundamenta Informaticae, 51(3):297–310, 2002.
[MV03] M. Marx and Y. Venema. Local variations on a loose theme: Modal logic and decidability. In M. Y. Vardi and Sc. Weinstein, editors, Finite Model Theory and its Applications. Springer Verlag, 2003. To appear.
[Ott99] M. Otto. Bisimulation-invariant Ptime and higher-dimensional mu-calculus. Theoretical Computer Science, 224:237–265, 1999.
[Par91] D. Park. Concurrency and automata on inﬁnite sequences. In Proceedings of the 5th GI-Conference on Theoretical Computer Science, volume 104 of Lecture Notes in Computer Science, pages 167–183. Springer-Verlag, 1981.
[RS70] H. Rasiowa and R. Sikorski. The Mathematics of Metamathematics. PWN Polish Scientiﬁc Publishers, Warsaw, third edition, 1970.
[San07] D. Sangiorgi. On the origins of Bisimulation, Coinduction, and Fixed Points. Technical Report UBLCS-2007-24, Department of Computer Science, University of Bologna, 2007 (available at: http://www.cs.unibo.it/~sangio/DOC_public/history_bis_coind.pdf).
[Var97] M. Y. Vardi. Why is modal logic so robustly decidable? volume 31 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 561–572. AMS, 1997.
[vB83] J. van Benthem. Modal Logic and Classical Logic. Bibliopolis, Naples, 1983. [vB84] J. van Benthem. Correspondence Theory, pages 167–247. Volume 2 of Gab-
bay and Guenthner [GG84], 1984.

Algorithmic Properties of Structures for Languages with Two Unary Functional Symbols
Ekaterina B. Fokina
Sobolev Institute of Mathematics Siberian Branch of the Russian Academy of Sciences
4 Acad. Koptyug avenue 630090 Novosibirsk Russia
e fokina@math.nsc.ru
Abstract. When studying algorithmic properties of structures with interesting algebraic and model–theoretic properties, one often uses known structural properties of the structures. However, it is often the case that results on particular kinds of structures can be transferred to structures from many other interesting classes. One of the ways of such generalization involves coding of the original structure into a structure from the given class in a way that is eﬀective enough to preserve interesting algorithmic properties. There are several constructions that allow us to reduce algorithmic questions for arbitrary structures to graphs. They also show that if we have a result for a graph, we also have it for a structure for any language containing at least one n–ary relational symbol, where n ≥ 2. We prove that it is possible to generalize this approach and get the same results for structures for a language with two unary functional symbols. Thus, we get the results for structures for so–called rich languages.
1 Introduction
When studying algorithmic properties of structures with interesting algebraic and model–theoretic properties, one often uses known structural properties of the structures. To ﬁnd and study such connections between computability–theoretic and algebraic properties of structures is one of the main questions of computable model theory. However, it is often the case that results on particular kinds of structures can be transferred to structures from many other interesting classes. One of the ways of such generalization involves coding of the original structure into a structure from the given class in a way that is eﬀective enough to preserve interesting algorithmic properties. This approach is very well illustrated in [14] and [21]. Both papers contain a very nice survey on the known results about
The author was partially supported by the Russian Foundation for Basic Research (Grant 08-01-00336), the State Maintenance Program for the Leading Scientiﬁc Schools of the Russian Federation (Grant NSh-335.2008.1.) and Russian Science Support Foundation.

128 Ekaterina B. Fokina
diﬀerent computability–theoretic properties of many classes of structures and present several ways to transfer them to classes for various languages.
We introduce some basic deﬁnitions. We ﬁx a computable G¨odel numbering of a language L. Let all structures have universes contained in ω, which we think of as computable sets of constants.
Deﬁnition 1. A structure A of the language L is computable if its domain is a computable subset of ω and all basic operations and predicates are uniformly computable. We identify formulas with their G¨odel numbers. Then computability of a structure is equivalent to the condition that the atomic diagram D(A) of A is computable, where D(A) is the set of all atomic sentences and negations of atomic sentences with parameters in |A| true in A. A structure B has a computable presentation if it is isomorphic to a computable structure A. In this case we also say that B is computably presentable. A structure A is decidable if its complete diagram Dc(A) is computable, where Dc(A) is the set of all sentences with parameters in |A| true in A.
Deﬁnition 2. A theory T is decidable if it is a computable set of sentences. A complete theory T is α-categorical if any two models of T of the power α are isomorphic. A structure is α-categorical if its theory is α-categorical.
There are diﬀerent approaches to study algorithmic properties of computable structures. Here we list only some results illustrating possible ways to study computability–theoretic properties. More detailed survey can be found in [14] and [21].
One of the approaches is to study computable presentations of the structure up to d–computable isomorphism.
Deﬁnition 3. Given a degree d, the d–computable dimension of a computably presentable structure A is the number of computable presentations of A up to d–computable isomorphism. If A has d–computable dimension 1 then it is d– computably categorical.
There are examples of all computable dimensions 1 ≤ n ≤ ω. Moreover, many of such examples were found in well–known classes of algebraic structures. See, for example, [10], [11], [12], [13], [15], [18], [24], [26].
A related question is to look at the computable dimension of a computably categorical structure when it is expanded by ﬁnitely many constants. It was shown that in this case the computable categoricity is not always preserved.
Theorem 1 (Cholak, Goncharov, Khoussainov, and Shore in [5]). For each k > 0 there exist a computably categorical structure A and an a ∈ |A| such that (A, a) has computable dimension k.
Theorem 2 (Hirschfeldt, Khoussainov, and Shore in [20]). There are a computably categorical structure A and an a ∈ |A| such that (A, a) has computable dimension ω.

Algorithmic Properties of Structures 129
Deﬁnition 4. Let d be a degree. A structure A with computable domain is d– computable if its atomic diagram is d–computable. The degree of A, denoted by deg(A), is the least degree d (which always exists) such that A is d–computable. An isomorphism from a structure B to a d–computable structure with computable domain is called a d–computable presentation of B. The degree of a presentation is the degree of the image of the presentation. If B has a d–computable presentation then it is d–computably presentable.
Deﬁnition 5. The degree spectrum of a countable structure A (denoted by DgSp(A)) is the set of degrees of presentations of A.
Theorem 3 (Slaman in [27]; Wehner in [28]). There is a structure A that has presentations of every degree except 0. (In other words, DgSp(A) = D −{0}, where D is the set of all degrees.)
Theorem 4 (Goncharov, Harizanov, Knight, McCoy, Miller, Solomon in [16]). For every successor ordinal α there exists a structure with presentations in exactly those degrees of sets X, that ∆0α(X) is not equal to ∆0α. In particular, for every ﬁnite n there exists a structure with presentations in exactly non–lown degrees.
The other approach to study the computable presentation of a structure is to compare the images of additional relations in these presentations. Here we can look at degree spectra of relations on structures.
Deﬁnition 6. The degree spectrum of a relation U on the domain of a computable structure A (denoted by DgSpA(U )), is the set of degrees of the images of U in all computable presentations of A.
There exist many interesting examples of computable structures and relations on their domains that have diﬀerent degree spectra (see [19], [22], etc.).
The eﬀective transformations from [14] and [21] show that many algorithmic properties of structures may be reduced to algorithmic properties of graphs. On the other hand, we can pass back from graphs to classes of all structures for languages containing at least one n–ary relational symbol, where n ≥ 2. In this paper we show that such a transition is also possible to structures for languages with only two unary functions. The language, which contains either at least binary relational symbol or two unary functional symbols is called rich. Thus, the results of the present paper completes the picture for structures for all rich languages.
The third approach to study connections between deﬁnability and computability of classes of structures involves the notion of the index set. The well– known result of Nurtazin [25] shows that there exists a universal computable numbering of all computable structures for a ﬁxed relational language. In other words, there exists a computable sequence {An}n∈ω of computable structures, such that for every computable B for this language we can eﬀectively ﬁnd An which is isomorphic to B. The index set of a structure B for this language is the set I(B) of all indices of computable (isomorphic) copies of B in this numbering.

130 Ekaterina B. Fokina
For a class K of structures, closed under isomorphism, the index set is the set I(K) of all indices for computable members of K. Index sets were widely studied by many authors (see, for example, [1], [2], [3], [4], [6], [7] [17], etc.)
Deﬁnition 7. Let Γ be a complexity class (e.g., Σ30). I(K) is m-complete Γ if I(K) is Γ and for any S ∈ Γ , there is a computable function f such that
n ∈ S iﬀ f (n) ∈ I(K).
This condition is equivalent to the condition that there is a uniformly computable sequence {Cn}n∈ω for which
n ∈ S iﬀ Cn ∈ K.
In [8] and [9] we studied the index sets of the following important classes structures: decidable structures, decidable structures with countably categorical theories and computable structures with decidable theories. We gave the complete characterization of degrees of complexity of the index sets for classes of structures for the language containing at least one n–ary relational symbol, where n ≥ 2. This paper completes the proof for all rich languages.
2 Transformation from Graphs to L0–Structures
Let L0 = f, g be a language containing only two unary functional symbols. In this section we give an eﬀective transformation of the class of graphs into the class of L0–structures. In the next two sections we’ll show that it preserves all algorithmic and model–theoretic properties we are interested in.
Consider a graph G = |G|, P . Using G, we deﬁne a new L0–structure A, in which G will be deﬁnable in a very nice way. We build A as follows.
The structure A has its universe consisting of 3 pairwise disjoint inﬁnite sets:
|A| = U ∪ V ∪ W,
where U = {ui | i ∈ |G|}, V = {vi | i ∈ |G|}, W = i Wi = {wji | i ∈ ω, j ∈ |G|}. Now we need to deﬁne functions f and g on |A|. We do it in the following
way. For all ui ∈ U we let f (ui) = ui. For all vi ∈ V we let f (vi) = ui. For all
wji ∈ Wi we let f (wji ) = ui. That is, the function f is identical on the set U , it maps V 1–to–1 onto U , and for every ui ∈ U there are inﬁnitely many elements wji ∈ W that corresponds to ui via f .
Now we deﬁne the function g. For all vi ∈ V we let g(vi) = vi. For all ui ∈ U we let g(ui) = vi. Values of g on elements from W depends on P (i, j). If P (i, j) is true in G, then we choose a unique index k such that in A we have g(wki ) = uj. If ¬P (i, j) is true in G, then we choose a unique index k such that g(wki ) = vj. For diﬀerent js we choose diﬀerent kjs corresponding to P (i, js) or ¬P (i, js), and every wki witnesses (via g(wki )) one of P (i, j) or ¬P (i, j) for some (unique) j. The function g deﬁned as above is identical on the set V and maps U 1–to–1 onto V . Moreover, it allows us to deﬁne P and ¬P in A via existential formulas. The precise proof is given in the next section.

Algorithmic Properties of Structures 131
3 Algorithmic Properties of the Class of L0–Structures
To get all the results on computable categoricity and computable dimensions of structures and degree spectra of structures and relations we use the result from [21]. In the paper some suﬃcient conditions were given for a transformation to preserve all the properties mentioned above. Here we remind deﬁnitions and the suﬃcient conditions from [21].
Deﬁnition 8. A countable structure A is trivial if for some ﬁnite set S of elements of |A|, every permutation of |A| that keeps the elements of S ﬁxed is an automorphism of A.
The interesting case is nontrivial structures.
Deﬁnition 9. A relation U on a structure A is invariant if for every automorphism f : A → A we have f (U ) = U .
Deﬁnition 10. A relation U on the universe of a structure A is relatively intrinsically computable if for every presentation f : A → A, the image f (U ) is computable in deg(A).
Now let G be a nontrivial, countable directed graph with edge relation E and let A be a countable structure. The general idea of the suﬃcient conditions given below is to say that the transformation from graphs to structures for diﬀerent language is rather eﬀective. That is, the graph is deﬁnable in the structure in a nice way which allows us to preserve all algorithmic properties.
We give a more precise formalization of this idea. Let G be a graph, and A be a structure. Assume that there exist relatively intrinsically computable, invariant relations D(x) and R(x, y) on the universe of A and a map G → AG from the set of presentations of G to the set of presentations of A with the following properties.
(P0) For every presentation G of G, the structure AG is deg(G)-computable. (P1) For every presentation G of G there exists a deg(G)–computable map gG :
D(AG) −1−→1 |G| such that RAG (x, y) ⇔ EG(gG(x), gG(y)) for every x, y ∈
onto
D(AG). (P2) If h : D(A) −1−→1 D(A) is such that R(x, y) ⇔ R(h(x), h(y)) then h can be
onto
extended to an automorphism of A. (P3) For every presentation G of G there exists a deg(G)–computable set of exis-
tential formulas ϕ0(a¯, ¯b0, x), ϕ1(a¯, ¯b1, x), . . . such that a¯ is a tuple of elements of |AG|, each ¯bi is a tuple of elements of D(AG), each x ∈ |AG| satisﬁes some ϕi, and no two elements of |AG| satisfy the same ϕi.
Proposition 1. Satisfaction of the conditions (P0)–(P3) guarantees that the following statements hold.
1. DgSp(A) = DgSp(G).

132 Ekaterina B. Fokina
2. If G is computably presentable then (a) for any degree d, A has the same d–computable dimension as G; (b) if x ∈ |G| then there exists an a ∈ D(A) such that (A, a) has the same computable dimension as (G, x); (c) if S ⊆ |G| then there exists U ⊆ |D(A)|, such that DgSpA(U ) = DgSpG(S) and if S is intrinsically c.e. then so is U .
The proof can be found in [21]. Therefore, what we need to do is to introduce invariant, relatively intrinsically c.e. relations D(x) and R(x, y) deﬁning the universe of the graph and the edge relation correspondingly and check the conditions (P0)–(P3) for the transformation from Sect. 2. First of all, it is easy to see that all the sets U, V and W are deﬁnable by quantiﬁer–free formulas. For example, U = {x|f (x) = x}. Thus, we may use these sets in other formulas as abbreviation for their deﬁning formulas. Under this convention, we set D(x) U (x). That is, D(A) = U A. We deﬁne R(x, y) and its negation via two existential formulas:
x, y ∈ P ⇔ U (x)&U (y)&∃z(W (z)&(f (z) = x)&(g(z) = y)),
x, y ∈/ P ⇔ U (x)&U (y)&∃z, t(W (z)&V (t)&(f (z) = x)&(g(z) = t)&(g(y) = t)).
It is easily seen from the deﬁnitions that D(x) and R(x, y) are invariant and relatively intrinsically c.e. Now we check the properties (P0)–(P3).
(P0) Follows directly from the construction of the transformation. (P1) Let G be a presentation of G. Existence of deg(G)–computable map follows
from the deﬁnition of f ,g and from deﬁning formulas for BAG and RAG . (P2) Let h be a 1–to-1 map of D(A) onto D(A) such that R(x, y) ⇔ R(h(x), h(y)).
We extend h in the following way. For v ∈ V we ﬁnd the corresponding u ∈ U such that g(u) = v. After that we let h(v) = g(h(u)). For w ∈ W we ﬁrst check if g(w) is in U or in V . If g(w) = u ∈ U then we ﬁnd the unique w ∈ W connected with h(u). Such w exists by properties of h. We let h(w) = w . The second case is similar. (P3) Let G be a presentation of G. Every element v from V is the unique element satisfying the formula (g(v) = v)&(g(u) = v)&(f (u) = u) for corresponding u ∈ D(AG)(= U AG ). For w ∈ W the formula depends on the value of g(w). If g(w) ∈ U then we write that there exists exactly one u ∈ U which is the value of g(w). Similar for g(w) ∈ V .
Thus, we have proved the following theorems.
Theorem 5. Let G be a computably presentable graph. There exists a computably presentable L0–structure A such that for any degree d, A has the same d–computable dimension as G.
Corollary 1. For every 1 ≤ n ≤ ω there exist an L0–structure A with computable dimension n.

Algorithmic Properties of Structures 133
Theorem 6. Let G be a computably presentable graph. There exists a computably presentable L0–structure A such that for every x ∈ |G| then there exists an a ∈ D(A) such that (A, a) has the same computable dimension as (G, x).
Corollary 2. For every 1 ≤ n ≤ ω there exists a computably categorical L0– structure A and an element a ∈ |A|, such that the computable dimention of (A, a) equals n.
Theorem 7. Let G be a graph. There exists a structure A for the language L0 with only 2 unary functions such that DgSp(A)=DgSp(G).
Corollary 3. There exists a computable L0–structure with presentations in all degrees except 0.
Theorem 8. Let G be a computably presentable graph. There exists a computably presentable L0–structure A such that if S ⊆ |G| then there exists a U ⊆ |D(A)| such that DgSpA(U ) = DgSpG(S) and if S is intrinsically c.e. then so is U .
Corollary 4. For every computable ordinal α there exists a computable L0– structure A and a relation R on |A|, such that R is intrinsically Σα0 but not relatively intrinsically Σα0 .

4 Results on Index Sets

Now we turn to the question about connections between computability-theoretic and structural properties of structures in classes of L0–structures. More precisely, instead of the language L0 = f, g we consider a new relational language L = Pf , Pg . Here the predicate symbols Pf and Pg are binary. We will think of them as representing graphs of f and g correspondingly.
According to the Nurtazin’s result, there exists a universal computable numbering {An}n∈ω of all computable structures for the language L consisting of 2 binary relational symbols. When talking about index sets of L0–structures with ﬁxed algorithmic and model–theoretic properties, in fact, we consider index sets of L–structures with the same properties and additional requirement on Pf , Pg. Namely, we require that these relations represent graphs of total functions in
our structures.
Under these convention, using the results from [8] and [9] we prove the fol-
lowing theorem.

Theorem 9. 1. The index set CK of the class of decidable L0–structures is

m–complete Σ30 set;

2. The index set CK0 of the class of decidable L0–structures with countably

categorical theories is m–complete Σ30 − Σ30 set (where Σ30 − Σ30 is the class

of diﬀerences of two Σ30 sets);

3. The index set DT of the class of L0–structures with decidable theories is

m–complete

Σ 0,∅(ω)
2

set.

134 Ekaterina B. Fokina

Proof. First of all, we need to proof that in any of these cases the index sets belong to the corresponding classes of complexity.

Lemma 1. 1. CK ∈ Σ30.

2. CK0 ∈ Σ30 − Σ30 (where Σ30 − Σ30 is the class of diﬀerences of two Σ30 sets).

3.

DT

∈

Σ 0,∅(ω)
2

.

Proof of the lemma can be found in [8] and [9]. The only one additional condition for Pf and Pg is to represent graphs of total functions. This condition does not change the proofs.
To prove m–completeness of the index sets, we show that the transformation from Sect. 2 preserves decidability of structures as well as decidability and countable categoricity of their theories. Then from the results about index sets of graphs from [8] and [9] we get the proof of the theorem.

Proposition 2. Let G be a graph and let A be an L0–structure obtained from G by the transformation from Sect. 2. Then

1. A is computable ⇔ G is computable; 2. A is decidable ⇔ G is decidable; 3. T h(A) is countably categorical ⇔ T h(G) is countably categorical; 4. T h(A) is decidable ⇔ T h(G) is decidable.

Proof. From the construction of the transformation and observations in Sect. 3
we get the ﬁrst point.
Let G be decidable. We show that in this case A is also decidable. For this we prove that the complete diagram Dc(A) is computably axiomatizable and
complete, hence, decidable. We consider the following set of axioms.

1. Pf and Pg are graphs of total functions. 2. For every x, y, if Pf (x, y) then Pf (y, y). 3. For every x, y, if Pg(x, y) then exactly one of the following holds: either
¬Pf (x, x)&∃zPg(y, z)&Pg(z, z) or Pf (x, x)&Pg(y, y). 4. For every x, if Pf (x, x) then there exists exactly one z, such that ¬Pf (z, z)
and ¬Pg(z, z) and exactly one of 2 conditions holds:
(a) Pg(z, x) or (b) ∃y(Pg(x, y)&Pg(z, y)).
5. For every z, if ¬Pf (z, z)&¬Pg(z, z) then there exists a unique x, such that Pf (x, x)&Pg(z, x) or Pg(x, x)&Pg(z, x).
6. For every sentence ψ from Dc(G) we put into the set of axioms a sentence
ψ , which we deﬁne in the following way. We consider ψ to be in the prenex
normal form. If ψ is an open formula, let ψ be constructed from ψ by
the substitution of positive and negative occurrences of P (x, y) for their deﬁnitions by existential formulas using Pf , Pg. If ψ = ∃xψ1, then we let ψ = ∃x(Pf (x, x)&ψ1). If ψ = ∀xψ1, then we let ψ = ∀x(Pf (x, x) → ψ1).

Algorithmic Properties of Structures 135
To prove that the theory is complete, we consider any two of its saturated models A1, A2 of the cardinality ω1. We want to show that they are isomorphic. We deﬁne the predicate P on the set {x|Pf (x, x)} using its deﬁnition by existential formulas using Pf and Pg. The resulting graphs G1, G2 are saturated models of Dc(G), hence, they are isomorphic. Let ϕ : G1 → G2 give this isomorphism. In a way, similar to the one from Sect. 3 we can extend the isomorphism to an isomorphism between A1 and A2. Thus, the theory Dc(A) is axiomatizable by the decidable set of axioms and complete, hence, it is decidable.
Similar reasoning gives the other two points.
This completes the proof of the theorem.
References
1. Calvert W.: The isomorphism problem for classes of computable ﬁelds. Archive for Mathematical Logic. 75 (2004) 327–336.
2. Calvert W.: The isomorphism problem for computable Abelian p-groups of bounded length. Journal of Symbolic Logic 70 (2005) 331–345.
3. Calvert W., Cummins D., Knight J. F., Miller S.: Comparing classes of ﬁnite structures. Algebra and Logic 43 (2004) 374–392.
4. Calvert W., Harizanov V., Knight J. F., Miller S.: Index sets of computable structures. Algebra and Logic 45 (2006) 306–325.
5. Cholak P., Goncharov S. S., Khoussainov B., Shore R. A.: Computably categorical structures and expansions by constants. J. Symbolic Logic 64 (1999) 13–37.
6. Csima B. F., Montalb´an A., Shore R. A.: Boolean algebras, Tarski invariants, and index sets/ Notre Dame J. Formal Logic 47 (2006) 1–23.
7. Dobritsa V. P.: Complexity of the index set of a constructive model. Algebra and Logic 22 (1983) 269–276.
8. Fokina E. B.: Index Sets of Decidable Models. Siberian Math. J. 48 (2007) 939–948 (English translation).
9. Fokina E. B.: Index Sets of Computable Structures with Decidable Theories. Computation and Logic in the Real World - Third Conference of Computability in Europe, CiE 2007, Siena, Italy, June 2007, Proceedings, Lecture Notes in Computer Science, 4497 (2007) 290–296.
10. Goncharov S. S.: Problem of the number of non-self-equivalent constructivizations. Algebra and Logic 19 (1980) 401–414.
11. Goncharov S. S.: Groups with a ﬁnite number of constructivizations. Soviet Math. Dokl. 23 (1981) 58–61.
12. Goncharov S. S.: Limit equivalent constructivizations. Mathematical Logic and the Theory of Algorithms, vol. 2 of Trudy Inst. Mat., Nauka, Sibirsk. Otdel., Novosibirsk, (1982) 4–12, in Russian.
13. Goncharov S. S.: Countable Boolean Algebras and Decidability. Siberian School of Algebra and Logic, Consultants Bureau, New York (1997).
14. Goncharov S. S.: Computability and Computable Models. Mathematical problems from applied logic. II. Logics for the XXIst century. Edited by Dov M. Gabbay, Sergey S. Goncharov and Michael Zakharyaschev. International Mathematical Series (New York), Springer, New York (2007) 99–216.
15. Goncharov S. S., Dzgoev V. D.: Autostability of models. Algebra and Logic 19 (1980) 28–37.

136 Ekaterina B. Fokina
16. Goncharov, S., Harizanov V., Knight J.F., McCoy C., Miller R., Solomon R.: Enumerations in computable structure theory. Annals of Pure and Applied Logic 136 (2005), 219-246.
17. Goncharov S. S., Knight J. F.: Computable structure and non-structure theorems. Algebra and Logic 41 (2002) 351–373 (English translation).
18. Goncharov S. S., Molokov A. V., Romanovskii N. S.: Nilpotent groups of ﬁnite algorithmic dimension. Siberian Math. J. 30 (1989) 63–68.
19. Harizanov V.: The possible Turing degree of the nonzero member in a two element degree spectrum. Ann. Pure Appl. Logic 60 (1993) 1–30.
20. Hirschfeldt D. R., Khoussainov B., Shore R. A.: A computably categorical structure whose expansion by a constant has inﬁnite computable dimension. J. Symbolic Logic 68 (2003) 1199–1241.
21. Hirschfeldt D. R., Khoussainov B., Shore R. A., Slinko A. M.: Degree Spectra and Computable Dimensions in Algebraic Structures Annals of Pure and Applied Logic 115 (2002) 71–113.
22. Khoussainov B., Shore R. A.: Computable isomorphisms, degree spectra of relations, and Scott families. Ann. Pure Appl. Logic 93 (1998) 153–193.
23. Khoussainov B., Shore R. A.: Solution of the Goncharov-Ash problem and the spectrum problem in the theory of computable models. Doklady Math. 61 (2000) 178–179, research announcement, Russian version in Dokl. Akad. Nauk 371 (2000) 30–31.
24. Lempp S., McCoy C., Miller R., Solomon R.: Computable categoricity of trees of ﬁnite height. J. Symbolic Logic 70 (2005) 151-215.
25. Nurtazin A. T.: Computable classes and algebraic criteria for autostability. Ph.D. Thesis, Institute of Mathematics and Mechanics, Alma-Ata (1974).
26. Remmel J. B.: Recursively categorical linear orderings. Proc. Amer. Math. Soc. 83 (1981) 387–391.
27. Slaman T. A.: Relative to any nonrecursive set. Proc. Amer. Math. Soc. 126 (1998) 2117–2122.
28. Wehner S.: Enumerations, countable structures, and Turing degrees. Proc. Amer. Math. Soc. 126 (1998) 2131–2139.

Veriﬁcation of Newman’s and Yokouchi’s Lemmas in PVS
Andr´e Luiz Galdino1,2 and Mauricio Ayala-Rinc´on1
1 Instituto de Ciˆencias Exatas, Universidade de Bras´ılia, Brazil 2 Departamento de Matem´atica, Universidade Federal de Goi´as
Campus de Catal˜ao, Brazil {galdino, ayala}@unb.br
Abstract. This paper shows how a formalization for Abstract Reduction Systems (ARSs) in which noetherianity was deﬁned by the notion of well-foundness over binary relations is used in order to prove results such as well-known Newman’s and Yokouchi’s Lemmas. The former is known as the diamond lemma and the latter states a property of commutation between ARSs. The theory ars was speciﬁed in the Prototype Veriﬁcation System (PVS) for which to the best of our knowledge there are no available theory for dealing with rewriting techniques in general. In addition to proof techniques available in PVS, the veriﬁcation of these lemmas implies an elaborated use of natural and noetherian induction.
Key words: Abstract Reduction Systems, Rewriting Systems, PVS.
1 Introduction
The Prototype Veriﬁcation System (PVS), developed at the SRI and widely used by industrial and academic parties, consists of a speciﬁcation language built on higher-order logic, which supports modularity by means of parameterized theories, with a rich type-system and a prover which uses the sequent-style. A PVS theory, ars, built over the PVS prelude libraries for sets and binary relations that is useful for the treatment of Abstract Reduction Systems (ARS) was reported in [4]. In the theory ars basic ARS notions such as reduction, derivation, normal form, conﬂuence, local conﬂuence, joinability, noetherianity, etc., were adequately speciﬁed in such a way that non elementary proof techniques such as noetherian induction are possible. In this paper we describe the usefulness of ars by describing proofs of Newman’s and Yokouchi’s lemmas. The former proof is a well-known classical application of noetherian induction and the latter is of interest because it is based on several applications of natural and noetherian induction. The inductive proof of the Newman’s Lemma given by Huet in [6] is a classical example of proofs in higher-order logic.
The novelty of this work in not to present mechanical proofs of ARS theorems in PVS that were done previously in other proof assistants. In fact, well-known
Both authors are partially supported by the Brazilian Research Council, CNPq.

138 Andr´e Luiz Galdino and Mauricio Ayala-Rinc´on
formalizations of Newman’s Lemma have been speciﬁed in several proof assistants, e.g., ACL2 [15], Coq [7], Isabelle [14], Boyer-Moore [16], Otter [3], among others. ars is presented as the basis for the formalization of an elaborated and robust PVS theory for full Term Rewriting Systems (TRSs) [5]. The motivation for doing this work is the fact that rewriting has been applied to the speciﬁcation and synthesis of reconﬁgurable hardware [9] and that the correction of these speciﬁcations can be carried out by translating these rewriting speciﬁcations into the language of the PVS proof assistant as logic theories ([1] introduces a proved correct translation from ELAN rewriting speciﬁcations into PVS theories). And robust proof rewriting based methods are necessary to deal eﬃciently with the correctness of these theories that come from rewriting based speciﬁcations.
Section 2 presents analytical proofs of both lemmas. Sections 3 and 4 describe respectively the speciﬁcation and veriﬁcation of both lemmas in PVS. The theory ars together with proofs of Newman’s, Yokouchi’s, among other interesting lemmas is available at www.mat.unb.br/∼ayala/publications.html.
2 Background: analytical proofs
We suppose the reader is familiar with rewriting concepts and standard notations as presented in [2] or [17].
An abstract reduction relation is a binary relation R over a set T , denoted also as R, T . The relation is identiﬁed as R, →R or simply →. R+ and R∗ respectively denote the transitive and the reﬂexive transitive closure of R, denoted in arrow notation as →+ and →∗, respectively. In the elegant arrow notation, the inverse relation R−1, its transitive and its reﬂexive transitive closures are respectively denoted as ←, + ← and ∗ ←. The operator of composition is denoted as usual as ◦. An abstract reduction relation → over T is said to be: conﬂuent whenever (∗ ← ◦ →∗) ⊆ (→∗ ◦∗ ←) and locally conﬂuent whenever (← ◦ →) ⊆ (→∗ ◦∗ ←). → satisﬁes the diamond property whenever (← ◦ →) ⊆ (→ ◦ ←). Two elements of T , say x, y, are said to be joinable whenever ∃u.x →∗ u ∗← y. → is said to be noetherian whenever there is no inﬁnite sequence of the form x1 → x2 → · · · .
Lemma 1 (Newman’s Lemma [11]). Let R be a noetherian relation deﬁned on the set T . Then R is conﬂuent if, and only if it is locally conﬂuent.
proof (Sketch). The ⇒-direction follows immediately by deﬁnition. ⇐-direction is proved by noetherian induction using the predicate
P (x) = ∀y, z. y ∗← x →∗ z =⇒ y and z joinable
Obviously R is conﬂuent if P (x) holds for all x. Noetherian induction require us to show P (x) under the assumption P (t) for all t such that x →+ t. To prove P (x), we analyze the divergence y ∗← x →∗ z. If x = y or x = z, y and z are joinable immediately. Otherwise we have x → y1 →∗ y and x → z1 →∗ z as shown in the Figure 1(a), where as usual dashed arrows stand for existence. The existence of u follows by local conﬂuence (LC) of R, the existence of v and w follows by induction hypothesis (Ind).

Veriﬁcation of Newman’s and Yokouchi’s Lemmas in PVS 139

x / z1 ∗ / z

 y1

_

LC __

∗_∗/ u

I nd

 

∗ y

_

I nd __

∗_∗/

 v

_

_

_

∗_∗/ w

(a)

x R / z 

S D R∗◦S◦R∗

 y

_

R_∗

_/

 u

(b)

x R∗ / z 

S D

R∗ ◦S ◦R∗

 y

_

R_∗

_/

 u

(c)

Fig. 1. Proof of Newman’s Lemma, Diagram D and Generalization of D as D
Lemma 2 (Yokouchi’s Lemma [18]). Let R and S be two relations deﬁned on the same set T , R being conﬂuent and noetherian, and S having the diamond property. Suppose moreover that the diagram D as shown in the Figure 1(b) holds: Then the relation R∗ ◦ S ◦ R∗ has the diamond property.

proof (Sketch). The proof starts by generalizing the diagram D of the lemma as the diagram D in the Figure 1(c). This generalization is proved by noetherian induction using the predicate
P (x) := ∀y, z. xR∗z ∧ xSy ⇒ ∃u.(yR∗u ∧ zR∗ ◦ S ◦ R∗u)
Then, to prove that R∗ ◦ S ◦ R∗ has the diamond property, one also proceeds by noetherian induction but this time using the predicate
P (x) := ∀y, z. xR∗◦S◦R∗y ∧ xR∗◦S◦R∗z ⇒ ∃u.(yR∗◦S◦R∗u ∧ zR∗◦S◦R∗u)
One concludes, by induction in the length of the derivation of the ﬁrst R∗ in xR∗ ◦ S ◦ R∗y. In other words, we distinguish between the cases xR ◦ R∗ ◦ S ◦ R∗y and xS ◦ R∗y as is shown in the Figure 2, where C and DP stand for use of conﬂuence of R and diamond property of S hypotheses, respectively.

x R∗ / z1

S / z2 R∗ / z

R
 y1
R∗ ◦S ◦R∗
 y

_ _

C _
R∗
_

_/ _

u1R_R∗ ∗_◦DS_◦R_∗ / I nd
_____

u 2R_∗
R∗
__

◦RC_S_∗◦_R_/∗/ uu3R∗

R∗ ◦S ◦R∗

x S / z1 R∗ / z

S DP

 y1

_

_

_

_/

S

R∗ D

 y

_

_

_

_/

R∗ ◦S ◦R∗

 u1S_ u 3R_∗

D _
R∗
C __

_/ _/

 

R∗

u2

 

R∗

u

◦S

◦R∗

R∗

Fig. 2. Cases xR ◦ R∗ ◦ S ◦ R∗y and xS ◦ R∗y

3 Speciﬁcation
The PVS theory newman yokouchi presented in Table 1 speciﬁes Newman’s and Yokouchi’s lemmas. This theory is parameterized as newman yokouchi[T], where (within the newman yokouchi theory) T is treated as a ﬁxed uninterpreted type.

140 Andr´e Luiz Galdino and Mauricio Ayala-Rinc´on
Table 1. newman yokouchi PVS theory
newman_yokouchi[T : TYPE] : THEORY BEGIN IMPORTING results_confluence[T], noetherian[T] R, S: VAR PRED[[T,T]]
Newman_lemma: THEOREM noetherian?(R) => (confluent?(R) <=> local_confluent?(R))
Yokouchi_lemma_ax1: LEMMA (noetherian?(R) & confluent?(R) & (FORALL x,y,z: (S(x,y) & R(x,z)) => (EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u)))) => (FORALL x,y,z: (S(x,y) & RTC(R)(x,z)) => (EXISTS (w:T): RTC(R)(y,w) & (RTC(R) o S o RTC(R))(z,w)))
Yokouchi_lemma: THEOREM (noetherian?(R) & confluent?(R) & diamond_property?(S) & (FORALL x,y,z: (S(x,y) & R(x,z)) => (EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u)))) => diamond_property?(RTC(R) o S o RTC(R))
END newman_yokouchi
R and S denote reduction relations over T and x, y, z, w and u elements of T. =>, <=> and & are abbreviations for IMPLIES, IFF and AND, respectively.
Newman’s Lemma speciﬁcation is straightforward and based on predicates over reduction relations. In the speciﬁcation of the Yokouchi’s Lemma RTC(R) denotes the reﬂexive transitive closure of the relation R, i.e. R∗. The composition of relations is denoted as o. The second lemma, Yokouchi lemma ax1, corresponds to the generalization D of the diagram D presented in Figure 1.
The theories results confluence[T] and noetherian[T], also components of the whole ars theory, are imported by the newman yokouchi theory. The former contains results about conﬂuence and the latter the deﬁnition of noetherianity formulated in terms of the notion of well-foundness as it will be explained.
Speciﬁcations of rewriting properties are exempliﬁed by the following ones.
joinable?(R)(x,y): bool = EXISTS z:RTC(R)(x,z) & RTC(R)(y,z) local_confluent?(R): bool = FORALL x,y,z: R(x,y) & R(x,z) =>
joinable?(R)(y,z) confluent?(R): bool = FORALL x,y,z: RTC(R)(x,y) & RTC(R)(x,z) =>
joinable?(R)(y,z) diamond_property?(R): bool = FORALL x,y,z: R(x,y) & R(x,z) =>
EXISTS r: R(y,r) & R(z,r)
In order to make easy the use of natural induction, closures of relations are built as unions of iterations of their compositions. For instance, the reﬂexive transitive closure operator RTC of a relation R is speciﬁed as the union of the iterations iterate(R,i), for all i ≥ 0, where iterate(R,i) speciﬁes the relation

Veriﬁcation of Newman’s and Yokouchi’s Lemmas in PVS 141
R◦R···◦R available in the PVS orders library. The operators iterate and
i times IUnion, available in the PVS prelude theory are speciﬁed as
iterate(R,i): RECURSIVE PRED[[T,T]] = IF i = 0 THEN =[T] ELSE iterate(R,i - 1) o R ENDIF MEASURE i
IUnion(A): set[T] = {x | EXISTS i: A(i)(x)}
and using them, RTC can be speciﬁed as
RTC(R): reflexive_transitive = IUnion(LAMBDA n: iterate(R,n))
Notice that the type of RTC is reflexive transitive. This places in the basis of the system of types the intrinsic characteristics of the RTC construction creating the necessary proofs obligations to be proved (PVS Type Correctness Conditions - TCCs). This type is speciﬁed as follows.
reflexive_transitive?(R): bool = reflexive?(R) & transitive?(R) reflexive_transitive: TYPE = (reflexive_transitive?)
Noetherianity is formalized in terms of well foundness, and noetherian induction is then veriﬁed using the lemma wf induction, which expresses the principle of well-founded induction. The notion of well-founded relations and this principle are available in the prelude theory [12].
noetherian?(R): bool = well_founded?(converse(R)) noetherian: TYPE = (noetherian?)
wf_induction: LEMMA (FORALL (p: pred[T]): (FORALL x: (FORALL y: y < x => p(y)) => p(x)) => (FORALL (x:T): p(x)))
The lemma noetherian induction presented below corresponds to the principle of noetherian induction.
noetherian_induction: LEMMA (FORALL (R: noetherian, P: PRED[T]): (FORALL x, y: (TC(R)(x,y) => P(y)) => P(x)) => (FORALL x: P(x)))
This lemma uses the transitive closure operator TC, that is speciﬁed similarly to RTC. Its application depends on an adequate instantiation of the predicate P.
4 Veriﬁcation
The veriﬁcation of Newman’s and Yokouchi’s lemmas consists of 1857 lines (168247 bytes) of proofs. The formalizations of Newman’s and Yokouchi’s lemmas use 114 and 225 proof steps, respectively. Here the relevant fragment of the proof trees, focusing on the application of noetherian induction, are presented.

142 Andr´e Luiz Galdino and Mauricio Ayala-Rinc´on
4.1 Veriﬁcation of Newman’s Lemma
When the PVS prover is invoked the proof tree starts oﬀ with a root node (sequent) having no antecedent and as succedent the theorem to be proved.
Newman_lemma : |-------
{1} FORALL (R: PRED[[T,T]]): noetherian?(R) => (confluent?(R) <=> local_confluent?(R))
The reduction relation R is correctly universally quantiﬁed, since it was declared as a variable in the theory newman yokouchi (see Table 1). After skolemization by applying the proof command (skeep), the conjunctive splitting command (split) is applied to the goal obtaining two subgoals. The ﬁrst subgoal, Newman lemma.1, is to demonstrate that conﬂuence implies local conﬂuence, which is easily formalized. The second subgoal, Newman lemma.2, that is to demonstrate that local conﬂuence implies conﬂuence (under noetherianity hypothesis), is the truly interesting one.
For proving this subgoal, after disjuntive simpliﬁcation with (flatten), one introduces the noetherian induction scheme noetherian induction and instantiates its predicate P as:
(LAMBDA (a:T): (FORALL (b,c:T): RTC(R)(a,b) & RTC(R)(a,c) => joinable?(R)(b,c)))
Then, the subgoals Newman lemma.2.1 and 2.2 presented below are obtained by applying the command (split), that splits the implication of the instantiated noetherian induction scheme. The ﬁrst subgoal is easily veriﬁed by expanding the deﬁnition of the predicate confluent?, skolemization and adequate instantiation of the variables of the antecedent {-1}.
Newman_lemma.2.1 : {-1} FORALL (x:T): FORALL (b,c:T):
RTC(R)(x,b) & RTC(R)(x,c) => joinable?(R)(b,c) [-2] local_confluent?(R) [-3] noetherian?(R)
|------[1] confluent?(R)
Newman_lemma.2.2 : [-1] local_confluent?(R) [-2] noetherian?(R)
|------{1} FORALL (x:T): (FORALL (y:T): TC(R)(x,y) => (FORALL (b,c: T):
RTC(R)(y,b) & RTC(R)(y,c) => joinable?(R)(b,c))) => (FORALL (b,c:T): RTC(R)(x,b) & RTC(R)(x,c)=> joinable?(R)(b,c))
To prove the latter subgoal, one needs to show P(x) under the assumption P(y) for all y such that x →+ y. After skolemization, expansion of the deﬁnition of RTC and hidding unnecessary formulas one obtains the following sequent.

Veriﬁcation of Newman’s and Yokouchi’s Lemmas in PVS 143
Newman_lemma.2.2 : [-1] FORALL (y:T): TC(R)(x,y) => (FORALL (b,c:T):
RTC(R)(y,b) & RTC(R)(y,c) => joinable?(R)(b,c)) {-2} iterate(R,i)(x,b) {-3} iterate(R,j)(x,c) [-4] local_confluent?(R) [-5] noetherian?(R)
|------[1] joinable?(R)(b, c)
To prove this goal, one analyzes the cases x = b or x = c or b = x = c. To contemplate these cases one uses the command (case-replace "i = 0") which replaces i by 0 in the current subgoal and generates a second subgoal for the case x = b. Similarly, the case x = c is proved. The case x = b and x = c, i.e., x → x1 →∗ b and x → x2 →∗ c corresponds to the following sequent obtained after some simpliﬁcations. Compare with the diagram of Figure 1(a) (replacing some variable symbols: u, v and w).
Newman_lemma.2.2.2.2.1.1 :
[-1] RTC(R)(x1,b) [-2] RTC(R)(x2,c) [-3] FORALL (y:T): TC(R)(x,y) => (FORALL (b1,c1:T):
RTC(R)(y,b1) & RTC(R)(y,c1) => joinable?(R)(b1,c1)) [-4] R(x,x1) [-5] R(x,x2) [-6] RTC(R)(x1,u) [-7] RTC(R)(x2,u) [-8] noetherian?(R)
|------[1] j = 0 [2] i = 0 [3] joinable?(R)(b,c)
Firstly, make a copy of the formula -3 by using (copy -3). The existence of u follows by expanding local confluent? (instantiated with variables x, x1 and x2), joinable?, by introducing skolem constants (u) and by applying disjunctive simpliﬁcation flatten. Then one applies the lemma R subset TC, which states that a relation is contained in its transitive closure and one proves that x →+ x1 and x →+ x2. Thus, the existence of v and w follows by induction hypothesis, that is by instantiating [-3] conveniently, and the lemma follows.
4.2 Veriﬁcation of Yokouchi’s Lemma
The veriﬁcation starts with the sequent.
Yokouchi_lemma : |-------
{1} FORALL (R, S: PRED[[T,T]]): (noetherian?(R) & confluent?(R) & diamond_property?(S) & (FORALL x,y,z: (S(x,y) & R(x,z)) => (EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u)))) => diamond_property?(RTC(R) o S o RTC(R))
After skolemization and propositional ﬂattening, one introduces the auxiliary lemma Yokouchi lemma ax1 corresponding to the generalization D in Figure 1. Then one obtains the new goal:

144 Andr´e Luiz Galdino and Mauricio Ayala-Rinc´on
Yokouchi_lemma : {-1} FORALL (R, S: PRED[[T,T]]): (noetherian?(R) & confluent?(R) &
(FORALL x,y,z: (S(x,y) & R(x,z)) => (EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))))
=> (FORALL x,y,z: (S(x,y) & RTC(R)(x,z)) => (EXISTS (w:T): RTC(R)(y,w) & (RTC(R) o S o RTC(R))(z,w)))
[-2] noetherian?(R) [-3] confluent?(R) [-4] diamond_property?(S) [-5] FORALL x,y,z: (S(x,y) & R(x,z))
=> (EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u)) |------[1] diamond_property?(RTC(R) o S o RTC(R))
Notice that the antecedents [-2], [-3] and [-5] correspond to the hypotheses of {-1}. Then, after a suitable instantiation of {-1}, propositional simpliﬁcation and expansion of the deﬁnition diamond property?, one introduces the noetherian induction scheme instantiating its predicate P as

LAMBDA (a:T):(FORALL (b,c:T): (RTC(R) o S o RTC(R))(a,c) & (RTC(R) o S o RTC(R))(a,b)
=> (EXISTS (d:T): (RTC(R) o S o RTC(R))(b,d) AND (RTC(R) o S o RTC(R))(c,d)))

Then, after splitting conjunctions, one obtains the following two subgoals:

Yokouchi_lemma.1 : {-1} FORALL (x:T): FORALL (b,c:T):
(RTC(R) o S o RTC(R))(x,c) & (RTC(R) o S o RTC(R))(x,b) => (EXISTS (d:T):
(RTC(R) o S o RTC(R))(b,d) & (RTC(R) o S o RTC(R))(c,d)) ... [-7] (RTC(R) o S o RTC(R))(x,y) [-8] (RTC(R) o S o RTC(R))(x,z)
|------[1] EXISTS (r:T):
(RTC(R) o S o RTC(R))(y,r) & (RTC(R) o S o RTC(R))(z,r)
and

Yokouchi_lemma.2 :

[-1] FORALL x,y,z: (S(x,y) & RTC(R)(x,z))=>

(EXISTS (w:T): RTC(R)(y,w) & (RTC(R) o S o RTC(R))(z,w))

[-2] noetherian?(R)

[-3] confluent?(R)

[-4] FORALL(x:T), (y:T), (z:T):

S(x,y) & S(x,z) => (EXISTS (r:T): S(y,r) & S(z,r))

[-5] FORALL x,y,z: (S(x,y) & R(x,z)) =>

(EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))

[-6] (RTC(R) o S o RTC(R))(x,y) [-7] (RTC(R) o S o RTC(R))(x,z)

|-------

Veriﬁcation of Newman’s and Yokouchi’s Lemmas in PVS 145
{1} FORALL (x:T): (FORALL (y:T): TC(R)(x,y) => (FORALL (b,c:T): (RTC(R) o S o RTC(R))(y,c) & (RTC(R) o S o RTC(R))(y,b) => (EXISTS (d:T): (RTC(R) o S o RTC(R))(b,d) & (RTC(R) o S o RTC(R))(c,d))))
=> (FORALL (b,c:T): (RTC(R) o S o RTC(R))(x,c) & (RTC(R) o S o RTC(R))(x,b)
=> (EXISTS (d:T): (RTC(R) o S o RTC(R))(b,d) & (RTC(R) o S o RTC(R))(c,d)))
The subgoal Yokouchi lemma.1 is easily veriﬁed instantiating adequately the antecedent [-1] and asserting. The subgoal Yokouchi lemma.2 is proved following the scheme in Figure 2 as detailed below.
1. First step: introduce Skolem constants and consider the cases x = z1 and/or x = y1.
2. Second step: invoke the lemma iterate RTC which states that for all n, iterate(R,n) ⊆ RTC(R); expand the deﬁnitions of composition of relations, confluent? and joinable?; hide irrelevant formulas; and then, apply disjunctive simpliﬁcation.
3. Third step: conclude applying the conﬂuence of R, the auxiliary lemma Yokouchi lemma ax1 and induction hypothesis.
5 Conclusions and Future Work
This work illustrates that the ars theory, previously presented in [4], is in fact adequate for formalizing (well-known) non elementary results of ARSs. The formalizations of Newman’s and Yokouchi’s lemmas were described focusing on the key proof steps related with the applications of noetherian induction. Also it should be stressed here, that although this work does not advance the state of the art in the formalization of mathematics, since speciﬁcations of ARSs and even of TRSs are available since the development of the Rewriting Rule Laboratory (RRL) in the 1980’s [8], it is of practical interest doubtless. In fact, the availability of rewriting proving technologies are essential in any modern proof assistant and to the best of our knowledge before the development of the ars theory neither rewriting theories nor rewriting libraries were available in PVS.
As current work we are developing trs, a more elaborated PVS theory for dealing with TRSs [5], that is of interest to verify the correction of concrete rewriting based speciﬁcations of computational objects such as reconﬁgurable hardware as mentioned in the introduction. By this extension rewriting strategies and new tactic-based proving techniques will be available in PVS in a natural manner. For this purpose, the type term built over a signature of function symbols is speciﬁed as an abstract data type [13] with the type of function symbols and the type of variables as its parameters. In the trs theory the type term is the actual parameter of the theory ars[T]. From this point, term positions are given as usual by ﬁnite sequences of naturals, and useful operations on terms

146 Andr´e Luiz Galdino and Mauricio Ayala-Rinc´on
such as subterm at a given position and replacement of a subterm at a given position by using recursive declarations; substitutions are functions from variables into term. All ars deﬁnitions and results hold for the reduction relation induced over term by an speciﬁc TRS which is speciﬁed as a binary relation over term. The induced reduction relation is given by closing the rewriting one under substitutions and structure of terms (signature operations) as it is formalized in the standard rewriting literature [2, 17].
References
1. M. Ayala-Rinc´on and T. M. Sant’Ana. SAEPTUM: Veriﬁcation of ELAN Hardware Speciﬁcations using the Proof Assistant PVS. In 19th Symp. on Integrated Circuits and System Design - SBCCI06, pages 125–130. ACM Press, 2006.
2. F. Baader and T. Nipkow. Term Rewriting and All That. CUP, 1998. 3. M. Bezem and T. Coquand. Neman’s Lemma - a Case Study in proof automation
and geometric logic. Bull. of the EATCS, 79(86-100), 2003. 4. A.L. Galdino and M. Ayala-Rinc´on. A Theory for Abstract Rewriting Systems
in PVS. In 33th Latin American Conference on Informatics - CLEI07. Available: www.mat.unb.br/∼ayala/publications.html. 5. A.L. Galdino and M. Ayala-Rinc´on. A PVS Theory for Term Rewriting Systems. 2008. Available: www.mat.unb.br/∼ayala/publications.html. 6. G. Huet. Conﬂuent Reductions: Abstract Properties and Applications to Term Rewriting Systems. J.ACM, 27(4):797–821, 1980. 7. G. Huet. Residual Theory in λ-calculus: A Formal development. Jornal of Functional Programming, 4(3):371–394, 1994. 8. D. Kapur and H. Zhang. An overview of Rewrite Rule Laboratory (RRL). In N. Dershowitz, editor, Proc. 3rd Int. Conf. on Rewriting techniques and Applications, LNCS, 355, 1989. 9. C. Morra, J. Becker, M. Ayala-Rinc´on, and R. W. Hartenstein. FELIX: Using Rewriting-Logic for Generating Functionally Equivalent Implementations. In 15th Int. Conference on Field Programmable Logic and Applications - FPL 2005, pages 25–30. IEEE CS, 2005. 10. C. Mun˜oz and M. Mayero. Real automation in the ﬁeld. ICASE Interim Report 39 NASA/CR-2001-211271, NASA Langley Research Center, 2001. 11. M. H. A. Newman. On theories with a combinatorial deﬁnition of ”equivalence”. Ann. of Math., 43(2):223–243, 1942. 12. S. Owre and N. Shankar. The PVS Prelude Library. Technical report, SRI-CSL03-01, Computer Science Laboratory, SRI International, Menlo Park, CA, 2003. 13. S. Owre and N. Shankar. Abstract datatypes in PVS. Technical Report SRICSL-93-9R, Computer Science Laboratory, SRI International, Menlo Park, CA, December 1993. Extensively revised June 1997. 14. O. Rasmussen. The church-rosser theorem in isabelle: A proof porting experiment. Technical Report 364, University of Cambridge, Computer Lab., 1995. 15. J. L. Ruiz-Reina, J. A. Alonso, M. J. Hidalgo, and F.J. Mart´ın-Mateos. Formalizing Rewriting in the ACL2 Theorem Prover. LNCS, 1930, 2001. 16. N. Shankar. A Mechanical Proof of the Church-Rosser theorem. J.ACM, 35:475– 522, 1988. 17. C. J. van Rijsbergen, editor. Term Rewriting Systems. CUP, 2003. 18. H. Yokouchi. Church-Rosser Theorem for a Rewriting System on Categorical Combinators. Theoretical Computer Science, 65(3):271–290, 1989.

Computation over Groups
Christine Gaßner
Institut fu¨r Mathematik und Informatik, Ernst-Moritz-Arndt-Universit¨at, F.-L.-Jahn-Straße 15 a, 17487 Greifswald, Germany gassnerc@uni-greifswald.de
Abstract. We discuss the uniform model of computation over groups, in particular the P =? NP problems. We consider relativizations of the P =? NP question for groups, and we discuss in which setting the known method to extend and to expand structures in order to get P = NP for new structures over strings cannot be used.
1 Introduction
In [4] L. Blum, M. Shub, and S. Smale (BSS) introduced the uniform model of computation over the reals where the constants, the operations, and the relations in (IR; IR; ·, +, −; ≥) can be used in the instructions of programs. Ideas and methods of classical complexity theory (cf. [1, 2], for instance) were transferred to the BSS model (cf. [3, 4, 5]). The uniform model of computation over arbitrary algebraic structures considered in [6–10] was developed in analogy to the BSS model in order to investigate the computation over structures like groups. Here, we deﬁne this model for groups only. We want to discuss some special features of the computation over groups resulting from the existence of only one constant, which is neutral with respect to computations, and from properties of groups which are countable, which contain an element of inﬁnite order or all elements of which have ﬁnite order. We deﬁne NP-complete problems, in particular those which are similar to the classical setting. We construct oracles O in a uniform way for some classes of groups such that there holds POS = NPOS or POS = NPOS for several kinds of S-machines over groups. Moreover, we explain why it is not possible to construct new relations over extensions of any group G in the known way in order to get PS = NPS for new structures S over strings if not permitting additional parameters.
For groups (G; e; ◦; =) we consider the computation over (G; e; ◦; =) without parameters and over (G; G; ◦; =) with parameters. The computation without additional parameters means that only the unit element e can be a machine constant. In the other case, we allow any element to be a machine constant. Any deterministic (BSS) machine restricted to a group (G; e; ◦; =), (for instance, to (IR; 0; +; =)) cannot compute new values in G \ {e} from an input of the form (e, . . . , e). For this reason, neither the old form of machines in [4] nor the new form of machines in [3] is suitable for computation over groups. Therefore, in the
I thank R. Bialowons, M. Kl¨are, R. Schimming, and the referees for helpful hints.

148 Christine Gaßner
deﬁnition of our model some features of the BSS model have to be modiﬁed so that the model becomes suitable for groups. Additionally to an inﬁnite number of registers for the elements of the structure, each machine is provided with a ﬁnite number of index registers. The index registers can be used in order to simulate the usual while and loop instructions in processing all input values x1, . . . , xn for any n. In the model over the reals introduced in [4], two additional index registers are suﬃcient since auxiliary calculation, necessary to compute new indices, can be executed by means of the elements of the ring. In [3] the BSS machines assume a new form which is similar to the form of special Turing machines whose cells can contain real numbers. A blank symbol and index registers are not used. The length of the inputs (x1, . . . , xn) is determined by storing the unary code of n in additional registers on the left side of the point.
. . . , 0, 0, 1, . . . , 1 . x1, x2, . . . , xn, 0, 0, . . .
n×
Hence, for this model, at least two constants are necessary.

2 The Model of Computation over Groups

For each group G = (G; e; ◦; =) where |G| ≥ 2, let G¯ = (G; G; ◦; =). For the structures S ∈ {G, G¯}, every S-machine M is provided with registers Z1, Z2, . . .

for the indices

elements in IN+ =

of IN

G and with a ﬁxed number of \ {0}. For an input (x1, . . . , xn)

registers I1, ∈ G∞ =df

I2, .
∞ i=1

. . , IkM Gi, the

for se-

quence x1, . . . , xn−1, xn, xn, xn, . . . is assigned to the registers Z1, Z2, . . . and the

index registers get the content n. The labelled computation, copy, and branching

instructions have the form Zj := Zj1 ◦ Zj2 ;, Zj := e; (resp. Zj := g; for any machine constant g ∈ G), ZIj := ZIk ;, and if Zj = Zk then goto l1 else goto l2;. The index registers are used in the copy instructions. For copying, we also allow

Ij := 1;, Ij := Ij + 1;, and if Ij = Ik then goto l1 else goto l2;. If the output

instruction is reached, then (Z1, . . . , ZI1 ) is the output and the machine halts. Note that the machines over another structure S can use all operations and

all relations of S in the computation and branching instructions, respectively.

The non-deterministic machines are able to guess an arbitrary number of

arbitrary elements y1, . . . , ym ∈ G in one step after the input and to assign the

guesses to ZI1+1, . . . , ZI1+m. Note that we make no restriction of the domain for m. To simplify matters, m is independent of n. However, a machine can use

at most t guesses within t steps. In any case, the size of an input (x1, . . . , xn)

is, by deﬁnition, its arity n. The digital (or binary) non-deterministic machines

cannot guess any elements. They only can execute additional instructions of the

form goto l1 or goto l2;. Semi-decidability can be deﬁned as usual. If e is the only constant, then we say that the decidability of a problem A ⊆ G∞ results
from the semi-decidability of A and of G∞ \ A. Moreover, oracle machines can
execute if (Z1, . . . , ZI1 ) ∈ O then goto l1 else goto l2; for some oracle O ⊆ G∞. Let MS (O) and M[SD]N(O) be the sets of deterministic and [digital] non-
deterministic S-machines, respectively, which can use the oracle O.

Computation over Groups 149
Further, let POS , DNPOS , and NPOS denote the usual complexity classes of decision problems A ⊆ G∞ decided or non-deterministically recognized by an oracle machine in MS (O), in MDS N(O), or in MNS (O) in polynomial time (where an input is in A iﬀ this machine halts on this input: without guesses, for some guessed sequence of instructions, or for some guesses), and let PS = P∅S , DNPS = DNP∅S , and NPS = NP∅S . Note, that, for groups G containing at least two elements, we have POG¯ ⊆ DNPOG¯ ⊆ NPOG¯ and POG ⊆ DNPOG ⊆ NPOG . For groups with a ﬁnite universe G = {g1, . . . , gk} where k ≥ 2 we get DNPOG¯ = NPOG¯ since a DNPOG¯ -machine can select one of the instructions Zj := g1; . . . ; Zj := gk; nondeterministically.
3 A Finite Group with PG = NPG
For all inﬁnite abelian groups G, there holds PG = DNPG and PG¯ = DNPG¯ (cf. [13, 6, 14, 12]). Moreover, it is known that there are inﬁnite abelian groups G satisfying DNPG¯ = NPG¯ [11]. On the other hand there are inﬁnite abelian groups G satisfying DNPG = NPG and DNPG¯ = NPG¯ . For instance, there holds {x ∈ ZZ | (∃y ∈ ZZ)(x = y + y)} ∈ NP(ZZ;0;+;=) \ DNP(ZZ;ZZ;+;=).
In order to demonstrate the diﬀerence between the computation with and without parameters let us consider the dihedral group D2,4 = (D; e; ·; =) of order 8, the elements of which are the isometric transformations of a square onto itself. The group can be described by two generating elements r (a rotation of a square) and s (a reﬂection of a square) where r4 = e , s2 = e, and sr = r3s hold. We have D = {e, r, r2, r3, s, rs, r2s, r3s}. The center of the group which is, by deﬁnition, the set C = {x ∈ D | (∀y ∈ D)(x · y = y · x)} consists of the elements e and r2. The elements e, r2, s, rs, r2s, r3s are of order 2, the elements r and r3 are of order 4.
Proposition 1. For D2,4 = (D; e; ·; =), there holds PD2,4 = NPD2,4 .
Proof. For D2,4 = (D; e; ·; =), we have D \ C ∈ NPD2,4 since for any x ∈ D \ C, an element y can be guessed for which x · y = y · x holds.
On the other hand, if M is any deterministic D2,4-machine, then all inputs x ∈ D (which have the arity 1) of order 2 have the same computation path. Thus, M accepts all inputs x ∈ D of order 2 or, in the other case, it rejects all inputs x ∈ D of order 2. Consequently, no deterministic D2,4-machine can decide the problem D \ C, since r2 ∈ C and s ∈ D \ C are of the same order. Therefore, D \ C is not in PD2,4 .
For D¯2,4 = (D; D; ·; =), the simulation of any D¯2,4-machine by a Turing machine, and vice versa, is possible. Since any element of D can be encoded by a tuple over {0, 1}, a Turing machine can simulate any computation over D, especially, it can simulate the instruction Zj := d; by using the code of d. On the other hand, any machine can simulate a Turing machine if it possesses two constants. If, in contrast to the D2,4-machines, any element of D is permitted to be a machine constant, we get compatible results to the classical setting.

150 Christine Gaßner
Proposition 2. PD¯2,4 = NPD¯2,4 holds if and only if P = NP holds.
4 NP-Complete Problems
For any group G = (G; e; ◦; =), we can deﬁne a universal non-deterministic G¯machine which is able to simulate each step of any non-deterministic G¯-machine M on an input x in polynomial time if it gets x and a suitable code of M as input.
Let G ∪ {a, b} be an alphabet where a = b and {a, b} ⊆ G or a = 0 and b = 1. In order to deﬁne such a universal problem, we ﬁrst encode the programs of G¯-machines by strings in (G ∪ {a, b})∗ and then we transform these strings into suitable tuples. We distinguish between strings and tuples since the strings of the length k have to be transformed in k-tuples in order to store it in k registers.
Deﬁnition 1. For every non-empty string s = c1 · · · ck ∈ (G∗)(=k) ⊂ G∗, let s be the representation of s in the form of a tuple (c1, . . . , ck) ∈ Gk ⊂ G∞, that means that c1 · · · ck = (c1, . . . , ck).
Deﬁnition 2. Let Code∗a,b be an injective mapping of the set of all deterministic and non-deterministic G¯-machines into (G ∪ {a, b})∗ such that, with the exception of the machine constants in G \ {e}, every character of the program is unambiguously translated into a string in {a, b}∗ by this mapping and such that the codes contain the sub-string b2 as preﬁx only and the last character of the codes is a. The machine constants are encoded by themselves. For any G¯-machine M and a, b ∈ G, let Codea,b(M) = Code∗a,b(M) .
Note that (x, c1 · · · ck ) stands for (x1, . . . , xn, c1, . . . , ck), and the like. The following problems can be recognized by a universal machine.
Deﬁnition 3. For G = (G; e; ◦; =) with a, b ∈ G and a = b and for any O ⊆ G∞, let the Universal NPOG¯ -Problem UNIG¯(O) be the set
{(x, Codea,b(M), bt ) | x ∈ G∞ & M ∈ MNG¯ (O) & M(x) ↓t} where M(x) ↓t means that M accepts x within t steps.
Proposition 3. For G-machines where |G| ≥ 2, UNIG¯(∅) is NPG¯-complete. Moreover, UNIG¯(O) is NPOG¯ -complete. Deﬁnition 4. For G = (G; e; ◦; =) and any O ⊆ G∞, let the Universal NPOG Problem UNIG(O) be the set
a,b∈G {(x, Codea,b(M), bt ) | x ∈ G∞ & M ∈ MNG (O) & M(x) ↓t} a=b
∪ {(e, e, e, . . . , e) | M ∈ MNG (O) & M( en ) ↓t}
c(n,C (M),t)×
where c(n1, n2, n3) = Cantor(Cantor(n1, n2), n3) is the number of (n1, n2, n3) given by the Cantor diagonal method and C(M) is the number whose binary representation is Code∗0,1(M).

Computation over Groups 151
If an input x contains a component xi = e, a reduction machine can compute (x, Codexi,e(M), et ). In the other case a reduction to the latter subset is possible.
Proposition 4. For G-machines, UNIG(∅) is NPG-complete. UNIG(O) is NPOG complete.

5 Some Oracles O with POS = NPOS

We transfer the ideas given in [1] and [5] and modify the deﬁnitions, appropriately. The tuples which can occur in the oracles O1(G) and O2(G) have the same form as the elements of a universal problem. Whereas the deﬁnition in [5] is
recursive on the number of steps occurring in these tuples, we deﬁne the oracle
recursively on the length (size) of tuples as in [1].

Deﬁnition 5. Let O1(= O1(G)) and O2(= O2(G)) (For a given G, we omit the index G.) be universal oracles deﬁned by

O1 = k∈IN Vk, V0 = ∅, O2 = k∈IN Wk, W0 = ∅,

Vk = Gk ∩ UNIG¯( j<k Vj ), Wk = Gk ∩ UNIG ( j<k Wj ).

UNIG¯(O1) ∩ Gk ⊆ Vk and UNIG(O2) ∩ Gk ⊆ Wk hold since we assume that the codes of the oracle machines do not depend on the used oracle. This implies UNIG¯(O1) = O1 and UNIG(O2) = O2, respectively.
Proposition 5. For any group G, there holds POG 2 = NPOG 2 . If |G| ≥ 2, then there holds POG¯ 1 = NPOG¯ 1 .

6 Some oracles Q with PQS = NPQS
In order to get the inequality between the corresponding relativized complexity classes with respect to some oracles for groups G, we want to deﬁne oracles Qi(= Q(iG)) recursively. The ideas particularly go back to [1] and [5]. As T. Baker, J. Gill, and R. Solovay and T. Emerson we use diagonalization techniques in order to construct these oracles and several problems Li(= L(iG)) such that L1 ∈ NPQG 1 \ DNPQG 1 , L2 ∈ (NPQG 2 ∩ DNPQG¯ 2 ) \ PQG¯ 2 , and Li ∈ NPQG¯ i \ DNPQG¯ i for i ∈ {3, 4} for several kinds of machines.

6.1 Constructions for Countable Sets of Machine Constants
The ﬁrst deﬁnition works for all inﬁnite groups. Since groups are structures of ﬁnite signature, we can take the positive integers in order to encode all programs of digital non-deterministic oracle (G; e; ◦; =)-machines using one given oracle. Consequently, the set of all couples of all polynomials and of these programs

152 Christine Gaßner
whose form (including the oracle queries) is independent of the used oracle can be enumerated. Let i ∈ IN+ be the code of the ith pair (pi, Pi) which determines a class of digital non-deterministic oracle machines {NiB | B ⊆ G∞} by the following.
(a) For any input in Gn, the machine NiB executes pi(n) steps of the program Pi.
(b) If NiB queries an oracle, then NiB uses the oracle B. (c) NiB simultaneously counts the number of steps which are determined by Pi
by means of an additional index register. (d) For any input in Gn and for any guessed sequence of instructions, the ma-
chine NiB makes an output after at most pi(n) steps of the execution of Pi if the output of Pi is reached in this time. If the output instruction of Pi is not reached in this time, then NiB does not halt for the guessed sequence of instructions and it does not compute new values after the ﬁrst pi(n) steps.
Then, for any digital non-deterministic G-machine M, which recognizes some problem in DNPBG , there is some i ∈ IN+ such that M and NiB recognize the same problem.

The Construction of Q1. Let V0 = ∅ and m0 = 0. We construct the set Q1 in stages. For i ≥ 1, let ni some integer such that ni > mi−1 and pi(ni) + ni < 2ni .
Moreover, let

Wi = j<i Vj ,

Vi = {x ∈ Gni | NiWi does not accept (e, . . . , e) ∈ Gni non-deterministically & x is not queried by NiWi on input (e, . . . , e) ∈ Gni },
mi = 2ni .

Finally, let Q1 = i∈IN+ Wi and L1 = {y | (∃i ∈ IN+)(y ∈ Gni & Vi = ∅)}. Since the set L1 belongs to NPQG 1 , we get the following analogously to [1].

Lemma 1. For any inﬁnite group G, there holds L1 ∈ NPQG 1 \ DNPQG 1 .

Proof. The property, that G is inﬁnite, implies that Vi = ∅ if NiWi does not

accept (e, . . . , e) ∈ Gni non-deterministically. The computation of NiWi and

NiQ1 is the same on input (e, . . . , e) ∈ Gni since the length of the tuples in the

oracle

queries

are

bounded

by

pi(ni) + ni

and

the

queries

of

NiWi

and

N Wi+1
i

,

with respect to Vi, are the same. Since, for any i ≥ 1, the machine NiWi accepts

(e, . . . , e) ∈ Gni iﬀ L1 ∩ Gni = ∅ and it does not accept (e, . . . , e) ∈ Gni iﬀ

L1 ∩ Gni = Gni , there holds L1 ∈ DNPQG 1 .

Proposition 6. For any inﬁnite group G, there is an oracle Q such that DNPQG = NPQG .

Corollary 1. For any inﬁnite group G, there is an oracle Q such that PQG = NPQG .

Computation over Groups 153
For any group G deﬁned on a countable (ﬁnite or inﬁnite) universe G where |G| ≥ 2, the construction can be transferred to deterministic G¯-machines. Let us assume that the machines N˜iB are the deterministic oracle machines over G¯ and that there is some g ∈ G \ {e}. Then, the sets V˜i, W˜ i, Q2, and L2 can be deﬁned in the same manner and L2 is in (NPQG 2 ∩DNPQG¯ 2 )\PQG¯ 2 since there are machines in MNG (Q2) and in MDG¯N(Q2) which can guess or select every combination of e’s and g’s non-deterministically in order to use it in a query. For a group of two elements, the construction is the same as in [1]. Here, pi(ni) < 2ni implies that V˜i = ∅ if N˜iW˜ i does not accept (e, . . . , e) ∈ Gni .
Lemma 2. For any group G containing at least two elements, if G is countable, there holds L2 ∈ DNPQG¯ 2 \ PQG¯ 2 and L2 ∈ NPQG 2 .
Proposition 7. If a group G contains at least two elements and the universe is countable, then there is an oracle Q such that PQG¯ = DNPQG¯ and PQG = NPQG .
Corollary 2. If a group G contains at least two elements and the universe is countable, then there is an oracle Q such that PQG¯ = NPQG¯ .
6.2 Further Constructions for Inﬁnite Sets of Machine Constants
Now we consider structures of the form G¯ = (G; G; ◦; =) where the universe is inﬁnite and does not need to be countable. We know that any oracle machine over G¯ can be encoded by elements of G∞ where, for any oracle, any query is encoded by the same code. Let U ⊆ G∞ be the set of codes for all couples of all polynomials and of all programs of digital non-deterministic oracle machines over G¯ (independent of the used oracle). Then, any u ∈ U is the code for a pair (pu, Pu) which determines a class of digital non-deterministic oracle machines {NuB | B ⊆ G∞} by properties analogously to (a), (b), (c), and (d).
The Construction of Q3. Let us assume that G contains an element of inﬁnite order, a. Let V0 = ∅. We construct the set Q3 in stages. For i ≥ 1, let
Ki = {u ∈ U | (∀j > i)(∀B ⊆ G∞) (NuB does not compute or use the value aj on input u)},
Wi = k<i Vk,
Vi = {(ai+1, u) | u ∈ Ki & NuWi does not accept u non-deterministically}.
Finally, let Q3 = i∈IN+ Wi and L3 = {y | (∃n ≥ 2)((an, y) ∈ Q3)}.
Lemma 3. For groups G containing an element a for which ai = aj iﬀ i = j, L3 ∈ NPQG¯ 3 \ DNPGQ¯ 3 .
Proof. Let us assume that there is some machine NuQ3 with property (A).
(A) NuQ3 recognizes L3.

154 Christine Gaßner
In any case we have u ∈ L3 or u ∈ L3. If u ∈ L3, then there is some i such that (ai+1, u) ∈ Vi. By the deﬁnition of Ki the machines NuQ3 and NuWi do not compute or use any value aj on input u if j > i. Hence, NuQ3 works like NuWi on u and, consequently, it does not accept u non-deterministically such that u ∈ L3 because of (A). If u ∈ L3, then because of (A), NuQ3 does not accept u. However, by deﬁnition of K1, K2, . . . there is some i0 such that u ∈ Ki for all i ≥ i0. Thus, (ai0+1, u) ∈ Q3 and hence u ∈ L3.
Proposition 8. For groups G containing an element of inﬁnite order, there is an oracle Q such that DNPGQ¯ = NPQG¯ . Corollary 3. For groups G containing an element of inﬁnite order, there is an oracle Q such that PQG¯ = NPQG¯ .
Similar constructions are possible for any inﬁnite group. For any inﬁnite abelian group which does not contain an element of inﬁnite order, there is an inﬁnite sequence of elements such that any new element is independent of its predecessors. For these groups we can choose the following oracle.
The Construction of Q4. Let (Gi)i∈IN+ be an inﬁnite sequence of subgroups such that Gi ⊂ Gi+1. Let V0 = ∅. We construct the set Q4 in stages. For i ≥ 1, let
Ki = {u ∈ U | (∀j > i)(∀B ⊆ G∞) (NuB does not compute or use values in Gj \ Gi on input u)},
Wi = k<i Vk,
Vi = {(g, u) | g ∈ Gi+1 \ Gi & u ∈ Ki & NuWi does not accept u}.
Let Q4 = i∈IN+ Wi and L4 = {y | (∃n ≥ 2)(∃g ∈ Gn)((g, y) ∈ Q4)}. Lemma 4. For any inﬁnite abelian group G which contains an inﬁnite sequence of subgroups Gi ⊂ Gi+1, L4 ∈ NPQG¯ 4 \ DNPQG¯ 4 . Proposition 9. For any inﬁnite abelian group G which does not contain an element of inﬁnite order, there exists an oracle Q such that DNPQG¯ = NPQG¯ . Corollary 4. For any inﬁnite abelian group G which does not contain an element of inﬁnite order, there exists an oracle Q such that PQG¯ = NPQG¯ .
6.3 Open Questions
In order to get PQS = NPQS for some structures S in {G, G¯}, we showed one of the inequalities PQS = DNPQS and DNPQS = NPQS by uniform constructions for some classes of groups. Of course, for special structures we even know P∅S = DNP∅S and DNP∅S = NP∅S . If the set of constants of any structure S is not countable, then no uniform construction of oracles Q is known which imply the inequalities PQS = DNPQS . Moreover we do not know any uniform construction of oracles Q which imply the inequalities PQS = DNPQS for inﬁnite structures containing only one constant or which imply the inequalities PQS = DNPQS for ﬁnite structures containing only one constant.

Computation over Groups 155

7 Expansions of Extensions with PS = NPS?

Let us explain the meaning of padding in the known constructions of structures S over strings with PS = NPS and its bounds for groups. As in [9], for every group G = (G; e; ◦; =), where G contains two elements a and b, we can embed the structure G¯ into a new structure of the form G¯∗ = (G∗; G ∪ {ε}; ◦, add, subl, subr; =) and afterward into a new structure of the form
G¯R∗ 1 = (G∗; G ∪ {ε}; ◦, add, subl, subr; R1, =)
such that we get PG¯R∗ 1 = NPG¯R∗ 1 . Here, ε is the empty string, add is a binary operation for adding a character to a string, and subr and subl are unary operations for computing the last character and the remainder of a string, respectively. For the strings s ∈ G∗, r ∈ G∗ \ G, and c ∈ G, the operations are deﬁned by add(s, c) = sc, subl(sc) = s, subr(sc) = c, add(s, r) = ε, subl(ε) = ε, subr(ε) = ε, s ◦ r = ε, and r ◦ s = ε. For G¯∗, the relation R1 can be derived from some universal oracle O1 = UNIG¯∗ (O1) by using, for example, a ∈ G for padding such that, for any r = x Code∗a,b(M)bt (where, for x ∈ (G∗)∞, x is a suitable code for x in G∗), the string ra|r| satisﬁes R1 if and only if

(x, Codea,b(M), bt ) ∈ O1

holds. Then, any problem in NPG¯R∗ 1 can be reduced to the universal oracle O1 and any oracle query can be replaced by a branching instruction deﬁned by R1 by transforming each tuple of the form (x, Codea,b(M), bt ) used in oracle
queries into a single string. The deﬁnition of the additional relation R1 on padded codes of the elements of O1 guarantees that any input, any computed values,
and any guesses can be replaced by short strings without changing the path of
computation, and that the short strings x1, x2, . . . in a query can be compressed
into one single string in polynomial time. Since R1 has the properties (1) and (2), the strings of the form sw for which the test conditions R1(sai) are satisﬁed for some i, can be replaced by strings s w for which s ai satisfying R1.

(∀s ∈ G∗)(∀i ∈ IN)(∀j ∈ IN)(R1(sai) & R1(saj) → i = j), (∀s ∈ G∗)(∃r ∈ G∗)(R1(s) → s = rba|r|+1).

(1) (2)

If G contains the neutral element e and a second element g, then we can also transform G into a structure of the form (G∗; e, g, ε; ◦, add, subl, subr; R, =) in a
similar way.
If we do not allow the additional parameter g = e as constant, the same deﬁnition of R for the construction of some GR∗ = (G∗; e, ε; ◦, add, subl, subr; R, =) with PGR∗ = NPGR∗ is not possible. The reason is the following. Let us assume that the deﬁnition of an additional relation R2 is done on padded codes of the elements of some oracle O2 and that the tuples of the form

(x, Codea,b(M), bt ) ec(n,C(M),t)

(3) (4)

156 Christine Gaßner
used in oracle queries could be transformed into single strings in polynomial time in order to check it by R2. Then, any problem in NPGR∗ 2 could be reduced to O2 containing tuples of the form (3) and (4) and any oracle query could be replaced by a branching instruction deﬁned by R2. In this case the padding of the codes of the tuples of the form (3) and (4) by adding an element a are necessary since the properties (1) and (2) are the base for shortening. But, if the components in an input are only e, then only e can be used for padding since any other element in G cannot be computed. That means that the elements of the form (4) could be padded by e only. Then, for any string s = ek there are several i implying that R2(sei) is true and we do not know all i which satisfy this property. Consequently, the condition (1) cannot be true.
Remark 1. A natural extension of groups yielding structures S with PS = NPS can be realized by embedding groups in structures over trees. The ideas are given in [10].
References
1. Baker, T., J. Gill, and R. Solovay (1975). Relativizations of the P =? NP question. SIAM J. Comput. 4, 431–442.
2. J. Balca´zar, J. D´ıaz, and J. Gabarro´ (1988/90). Structural Complexity I and Structural Complexity II. Springer-Verlag.
3. Blum, L., F. Cucker, M. Shub, and S. Smale (1998). Complexity and Real Computation. Springer-Verlag.
4. Blum, L., M. Shub, and S. Smale (1989). On a theory of computation and complexity over the real numbers: NP-completeness, recursive functions and universal machines. Bulletin of the Amer. Math. Soc. 21, 1–46.
5. Emerson, T. (1994). Relativizations of the P =? NP question over the reals (and other ordered rings). Theoretical Computer Science 133, 15–22.
6. Gassner, C. (2001). The P-DNP problem for inﬁnite abelian groups. Journal of Complexity 17, 574–583.
7. Gassner, C. (2006). A structure with P = NP. CiE 2006. Computer Science Report Series of the University of Wales Swansea CSR 7, 85–94.
8. Gassner, C. (2006). Expansions of structures with P = NP. CiE 2006. Computer Science Report Series of the University of Wales Swansea CSR 7, 95–104.
9. Gassner, C. (2007). P = NP for Expansions Derived from Some Oracles. CiE 2007. Technical report no. 487 June 2007.
10. Gassner, C. (2004). NP ⊂ DEC und P = NP fu¨r Expansionen von Erweiterungen von Strukturen endlicher Signatur mit Identit¨atsrelation. Preprint-Reihe Mathematik, E.-M.-Arndt-Universit¨at Greifswald, Preprint 13/2004.
11. Koiran, P. (1994). Computing over the reals with addition and order. Theoretical Computer Science 133, 35–47.
12. Meer, K. (1992). A note on a P = NP result for a restricted class of real machines. Journal of Complexity 8, 451–453.
13. Poizat, B. (1995), Les Petits Cailloux. Al´eas. 14. Prunescu, M. (2002) A model-theoretical proof for P = NP over all inﬁnite abelian
groups. J. Symb. Logic 67, 235-238.

Singularities of Holomorphic Functions in Subsystems of Second Order Arithmetic
Yoshihiro Horihata1 and Keita Yokoyama2
1 Mathematical Institute, Tohoku University, Sendai 980-8578, Japan. sa6m31@math.tohoku.ac.jp or higurashi3873@yahoo.co.jp 2 Department of Mathematics, Tokyo Institute of Technology, 2-12-1 Oh-okayama, Meguro-ku, Tokyo 152-8551, Japan.
yokoyama@math.titech.ac.jp or k yoko tautology@infoseek.jp
Abstract. We study complex analysis in the context of weak subsystems of second order arithmetic. We are mainly concerned with integrability and singularities of holomorphic functions. Then, we develop a part of complex analysis concerned with Picard’s little theorem. We show that Picard’s little theorem is provable from WKL0 plus a version of the Riemann mapping theorem. Since a full version of the Riemann mapping theorem is provable in ACA0, we can prove Picard’s little theorem in ACA0. Key words: Reverse Mathematics, Picard’s theorem, Riemann mapping theorem, second order arithmetic
1 Introduction
This paper is a contribution to the program of Reverse Mathematics. Reverse Mathematics was pioneered by Harvey Friedman and Stephan Simpson in the 1970s. Its goal is to develop large parts of ordinary mathematics in second order arithmetic and determine which axioms are exactly required to prove theorems. In this paper, we aim to develop some parts of complex analysis especially related to singularities.
We mainly consider subsystems RCA0, WWKL0, WKL0 and ACA0 in this paper. RCA0 is a system of recursive comprehension axioms. This is the weakest system we consider and we can prove basic theorems for analysis such as the intermediate value theorem, the mean value theorem and Taylor’s theorem for holomorphic functions within RCA0. Within WKL0, we can prove the Heine/Borel theorem, and then, we can integrate a continuous function on a closed interval. If a continuous function is bounded, then, we can integrate it within a weaker system WWKL0, in which we can show a weak version of the Heine/Borel compactness (see [7]). Within ACA0, we can prove the Riemann mapping theorem. Actually, the Riemann mapping theorem is equivalent to ACA0 over WKL0 (see [6]).
Analysis in second order arithmetic is carried forward by many people (see, e.g. [3]). Complex analysis in second order arithmetic is carried forward in [5, 6]. We aim to expand these into studies for singularities or studies for Riemann surfaces.

158 Yoshihiro Horihata and Keita Yokoyama

2 Preliminaries

In RCA0, we can deﬁne the real number system R, the Euclidean space Rn and continuous functions on Rn in the usual way. We ﬁrst deﬁne the complex number
system and holomorphic functions.

Deﬁnition 1 (the complex number system). The following deﬁnitions are
made in RCA0. We identify a complex number, an element of C, with an element of R2, and we deﬁne +C, ·C and | · |C by:

(x1, y1) +C (x2, y2) = (x1 + x2, y1 + y2); (x1, y1) ·C (x2, y2) = (x1x2 − y1y2, x1y2 + x2y1); |(x, y)|C = (x, y) R2 = x2 + y2.
We write (0, 1) = i and (x, y) = x + iy = z, where x, y ∈ R and z ∈ C. We usually leave out the subscript C. A continuous (partial) function from C to C is a continuous (partial) function from R2 to R2.

Deﬁnition 2 (holomorphic functions). The following deﬁnition is made in RCA0. Let D be an open subset of C, and let f , f be continuous functions from D to C. Then a pair (f, f ) is said to be holomorphic if

∀z ∈ D

f (w) − f (z) lim = f (z).

w→z w − z

Informally, we write f for a holomorphic function (f, f ).

Within RCA0, we can show Taylor’s theorem, i.e., a holomorphic function is an analytic function. Thus, we can prove basic properties for holomorphic functions
in RCA0. Next, we deﬁne line integrals. Let a, b be elements of C and let r be a positive
real number. Then we deﬁne

[a, b] := {a + (b − a)x ∈ C | 0 ≤ x ≤ 1}, ∂B(a; r) := {z ∈ C | |z − a| = r}.
Deﬁnition 3 (line integral). Let D be an open or closed subset of C, and let f be a continuous function from D to C. Then the following deﬁnitions are made in RCA0.
1. Let γ be a continuous function from [0, 1] to D. Then, we deﬁne γ f (z) dz, the line integral of f along γ, as

γ

f

(z)

dz

=

lim
|∆|→0

Sγ∆(f )

if this limit exists. Here, ∆ is a partition of [0, 1], i.e. ∆ = {0 = x0 ≤ ξ1 ≤

x1 ≤ · · · ≤ ξn ≤ xn = 1}, Sγ∆(f ) =

n k=1

f

(γ(ξk

))(γ

(xk

)

−

γ(xk−1))

and

|∆| = max{xk − xk−1 | 1 ≤ k ≤ n}.

Singularities of Holomorphic Functions 159

2. If [a, b] ⊆ D, we deﬁne γ(t) = a + (b − a)t and deﬁne [a,b] f (z) dz as

f (z) dz = f (z) dz.

[a,b]

γ

3. If ∂B(a; r) ⊆ D, we deﬁne γ(t) = a + r exp(2πit) and deﬁne ∂B(a;r) f (z) dz as

f (z) dz = f (z) dz.

∂B(a;r)

γ

Let f be a continuous function from D ⊆ C to C, and let [a, b] ⊆ D. A modulus of integrability along [a, b] for f is a function h[a,b] from N to N such that for all n ∈ N and for all partitions ∆1, ∆2 of [0, 1] ⊆ R, if |∆1|, |∆2| < 2−h[a,b](n) then |S[∆a,1b](f ) − S[∆a,2b](f )| < 2−n+1. We say that f is eﬀectively integrable on D when for every [a, b] ⊆ D, we can ﬁnd a modulus of integrability along [a, b].
Theorem 1. Let D ⊂ C. Then, the following assertions are equivalent over RCA0.
1. WKL0. 2. Every continuous function on D is eﬀectively integrable.
For the proof, see [3, Theorem IV.2.7]. Next, we deﬁne subsystem WWKL0 which is introduced in [7].
Deﬁnition 4. WWKL0 is the system which consists of RCA0 plus WWKL, where the WWKL is weak-weak K¨onig’s lemma;
if an inﬁnite tree T ⊆ 2<N has no path, then limn→∞ |{σ ∈ T | lh(σ) = n}|/2n = 0.
The Heine/Borel theorem is not provable in WWKL0, but a weak version of the Heine/Borel theorem is provable.
Theorem 2. The following assertions are equivalent over RCA0.
1. WWKL0. 2. Weak Heine/Borel covering theorem; if n∈N B(an; rn) is a covering of [0, 1],
then
∃ (bij , cij )j≤li | i ∈ N, li ∈ N s.t. [0, 1] ⊆ n<i B(an; rn) ∪ j≤li (bij , cij ) ∧ limi→∞ j<li |cij − bij | = 0.
For the proof, see [7]. By the above therem, we only need WWKL0 to integrate a bounded function.
Theorem 3. Let D ⊂ C. Then, the following assertions are equivalent over RCA0.
1. WWKL0. 2. Every bounded continuous function on D is eﬀectively integrable.
For the proof, see [4, Theorem 3].

160 Yoshihiro Horihata and Keita Yokoyama

3 Complex analysis in weak second order arithmetic

We review some recent studies on complex analysis in weak second order arithmetic.
We ﬁrst state two basic theorems for holomorphic functions contained in [5].
Deﬁnition 5 (analytic function). The follwoing deﬁnition is made in RCA0. Let D be an open subset of C. An analytic function on D is deﬁned to be a triple (f, {an, rn}n∈N, {αnk}n∈N,k∈N), where f is a continuous function from D to C, an, αnk ∈ C and rn ∈ R+, satisfying the following conditions:
1. n∈N B(an; rn) = D; 2. ∀n ∈ N ∀z ∈ B(an; rn) f (z) = k∈N αnkzk.
Theorem 4 (Taylor’s theorem). The following is provable in RCA0. Holomorphic functions are analytic, i.e. given a holomorphic function (f, f ) on an open subset D ⊆ C, we can eﬀectively ﬁnd an analytic function (f, {an, rn}n∈N, {αnk}n∈N,k∈N) on D. In particular, the derivative of a holomorphic function is holomorphic.
Theorem 5 (Cauchy’s integral theorem). The following assertions are equivalent over RCA0.
1. WKL0. 2. Cauchy’s integral theorem for triangles: if f is a holomorphic function on an
open subset D ⊆ C, then for all abc ⊆ D, ∂ abc f (z) dz exists and

f (z) dz = 0.
∂ abc

To prove Taylor’s theorem, we need to use Cauchy’s integral theorem ‘locally’, i.e., we need to ﬁnd a good neighborhood for Cauchy’s theorem at each point. Note that a holomorphic function is locally eﬀectively integrable in RCA0. As we stated, we can show many theorems for holomorphic functions such as maximal value principle or mean value principle in RCA0. By Theorem 4, we can apply Cauchy’s integral theorem to a holomorphic function f if f is eﬀectively integrable within RCA0. However, Cauchy’s integral theorem as itself is not provable in RCA0.
Based on the line integrability in WKL0 or WWKL0 (Theorems 1 and 3) and the previous two theorems, we develop some more complex analysis in second order arithmetic. Proofs of the following theorems can be found in [1]. We ﬁrst study Laurent expansion.

Theorem 6 (Laurent expansion). The following is provable in RCA0. Let f be an eﬀectively integrable holomorphic function on D = {z | 0 ≤ R1 < |z − a| < R2}. Then, for all z ∈ D,

f (z) =

∞

a−n (z − a)n

+

∞

an(z − a)n

n=1

n=0

Singularities of Holomorphic Functions 161

where R1 < r < R2 and

1 a−n = 2πi

f (ζ)(ζ − a)n−1 dζ,
∂B(a;r)

1 f (ζ) an = 2πi ∂B(a;r) (ζ − a)n+1 dζ.

Using eﬀective integrability, we can show this theorem as usual. However, as for Cauchy’s integral theorem, we cannot omit the assumption that f is eﬀectively integrable. Actually, the following theorem holds.

Theorem 7. The following assertions are equivalent over RCA0.
1. WKL0. 2. If f is a holomorphic function on D = {z | 0 ≤ R1 < |z − a| < R2}, then,
there exists {an}n∈Z such that f (z) = n∈Z an(z − a)n for all z ∈ D.
We next study isolated singularities.

Deﬁnition 6 (isolated essential singularity). The follwing deﬁnition is made in RCA0. Let f be a holomorphic function on D = {z | 0 < |z − a| < R}. Then a is said to be an isolated essential singularity if there exists {an}n∈Z such that f (z) = n∈Z an(z − a)n for all z ∈ D and ∀m ∈ N ∃k ≥ m a−k = 0.
We can prove the following two theorems within WWKL0.
Theorem 8 (Riemann’s theorem on removable singularities). The following is provable in WWKL0. Let f be a holomorphic function on D = {z | 0 < |z − a| < r}. If there exists r > 0 such that r < r and f is bounded on {z | 0 < |z − a| < r }, then there exists a holomorphic function f˜ on D ∪ {a} such that f˜(z) = f (z) for all z ∈ D.

The following theorem is a special case of Picard’s theorem.

Theorem 9 (Casorati/Weierstraß theorem). The following is provable in WWKL0. Let f be a holomorphic function on D = {z | 0 < |z − a| < r} and a be an isolated essential singularity. Then, f (D) is dense in C.
To prove these theorems, we need to apply Cauchy’s theorem for bounded holomorphic functions. This can be done in WWKL0 by Theorem 3. Thus, we can imitate usual proofs within WWKL0. We can also prove the following two theorems and many other theorems in WWKL0 by the same reasoning. These theorems play key roles in the proof of Picard’s theorem.

Theorem 10 (Liouville theorem). The following is provable in WWKL0. If f is a bounded entire function, then it is a constant function.

Theorem 11 (Schwarz reﬂection principle). The following is provable in WWKL0. Let D ⊆ C+ = {x + iy | y > 0} be an open set and let L = (a, b) ⊆ R be an open interval such that L = ∂D ∩ R. Let f be a continuous function on D ∪ L such that f is holomorphic on D and f (z) ∈ R for all z ∈ L. Then there exists a holomorphic function f˜ on D˜ = D ∪ {z ∈ C | z¯ ∈ D ∪ L} such that f˜(z) = f (z) for all z ∈ D ∪ L.

162 Yoshihiro Horihata and Keita Yokoyama
Finally, we recall the Riemann mapping theorem contained in [6].
Theorem 12 (Riemann mapping theorem). The following assertions are equivalent over RCA0. 1. ACA0. 2. If D ⊆ C is a simply connected open set D = C, then there exists a conformal
map f : D → B(0; 1).

4 Picard’s little theorem

To study singularities, we need covering spaces.
Deﬁnition 7 (covering space). Let X, D ⊆ C be open sets, and let π be a continuous surjective function from X to D. Let {Uij}i∈I,j∈J and {Vi}i∈I be sequences of open sets, and let πij be homeomorphic functions from Uij to Vi. Then, π is said to be a covering map and a hextuple (X, D, π, Uij, Vi, πij) is said to be a covering space of D if they satisfy the following:

each of Uij and Vi is simply connected;
D = Vi;
i∈I
∀i ∈ I π−1(Vi) = Uij,
j∈J
∀i ∈ I ∀j ∈ J π|Uij = πij .
The following lemma plays a key role of the proof of Picard’s little theorem.
Lemma 1 (lifting). The following assertions are equivalent over RCA0.
1. WKL0. 2. Let D0, D ⊆ C be open sets, and let (X, D, π, Uij, Vi, πij) be a covering space
of D. If D0 is simply connected and f is a continuous function from D0 to D, then, there exists a continuous function fˆ from D0 to X such that π ◦ fˆ = f .

Proof. We ﬁrst show 1 → 2 in case D0 = [0, 1]. We reason within WKL0. Let

D0 = [0, 1] and aij = j/2i+1 for all j ≤ 2i+1. By WKL0, we can deﬁne Wi as

follows:

Wi =

B(aij ; 2−i).

∃kf (B(aij ;2−i))⊆Vk

Then Wi ⊆ Wi+1 for all i ∈ N and [0, 1] ⊆ i∈N Wi. By the Heine/Borel theorem, there exists N ∈ N such that [0, 1] ⊆ WN . Take a sequence kj | j ≤ 2N+1 such
that f (aNj) ∈ Vkj for all j ≤ 2N+1. Then, by the deﬁnition of the covering space, we can take a sequence lm | m ≤ 2N+1 such that

πk−m1,lm (f (aN,m+1)) ∈ Ukm+1,lm+1

for all m < 2N+1. Thus we can deﬁne a lifting fˆ as follows:

Singularities of Holomorphic Functions 163
fˆ(x) = πk−j1,lj ◦ f (x) if x ∈ [aN,j , aN,j+1].
Similarly, we can construct a lifting in case D0 = [0, 1] × [0, 1]. The general case can be proved easily by the previous two cases.
Next, we show ¬1 → ¬2. We assume ¬WKL0. Let D0 = {(x, y) ⊆ R2 | −1 ≤ x ≤ 1, −1 ≤ y ≤ 1}, D = ∂D0, and X = R. Then we can take π : X → D as a covering map by winding X on D. By ¬WKL0, there exists a continuous function f : D0 → D such that f |D = idD (by Shioji and Tanaka[2]). However, there is no lifting of f .
The complete proof of the previous lemma is in [1]. Now we study Picard’s little theorem. For this, we consider a version of the
Riemann mapping theorem:
(∗) Let D be an interior of a simple closed curve on the Riemann sphere. Then, D is conformally equivalent to the unit open ball B(0; 1) and a conformal map h : D → B(0; 1) can be expanded into a homeomorphism h¯ : D → B(0, 1).
We prove Picard’s little theorem, which asserts that an entire function is a constant if it omits two points, within WKL0 + (∗).
We reason within WKL0. Let f be an entire function which omits two points. Without loss of generality, we assume that f : C → C \ {0, 1}. By (∗) and the Schwarz reﬂection principle, we can construct a holomorphic covering map π : B(0; 1) → C \ {0, 1} in the usual way. Then, by Lemma 1, there exists a holomorphic function fˆ : C → B(0; 1) such that π ◦ fˆ = f . By the Liouville theorem, fˆ is constant. Thus, f is also constant and this completes the proof of Picard’s little theorem.
Since a full version of Riemann mapping theorem is provable in ACA0, we can prove Picard’s theorem within ACA0.
Theorem 13 (Picard’s theorem). The following is provable in ACA0. Let f be a holomorphic function from C to C. If the range of f omits two points, then, f is a constant function.
We conjecture that (∗) is provable within WKL0, which implies that WKL0 proves Picard’s theorem. We have not managed the reversal of Picard’s little theorem.
References
1. Y. Horihata. Reverse mathematics of complex analysis and general topology (in Japanese), Feb. 2008. Master’s Thesis, Tohoku University, February 2008.
2. N. Shioji and K. Tanaka. Fixed point theory in weak second-order arithmetic. Annals of Pure and Applied Logic, 47:167–188, 1990.
3. S. G. Simpson. Subsystems of Second Order Arithmetic. Springer-Verlag, 1999. 4. K. Yokoyama. Reverse mathematics for Fourier expantion. In the local proceedings
of the fourth conference on computability in Europe 2008. 5. K. Yokoyama. Complex analysis in subsystems of second order arithmetic. Arch.
Math. Logic, 46:15–35, 2007.

164 Yoshihiro Horihata and Keita Yokoyama
6. K. Yokoyama. Non-standard analysis in ACA0 and Riemann mapping theorem. Math. Logic Quart., 53(2):132–146, 2007.
7. X. Yu and S. G. Simpson. Measure theory and weak K¨onig’s lemma. Arch. Math. Logic, 30:171–180, 1990.

Modelling Linear Cellular Automata with the Minimum Stage Corresponding to CCSG based
on LFSR
Yoon-Hee Hwang1, Sung-Jin Cho2, Un-Sook Choi3, and Han-Doo Kim4
1 Department of Information Security, Graduate School Pukyong National University
Busan 608-737, Korea, yhhwang@pknu.ac.kr 2 Division of Mathematical Sciences, Pukyong National University
Busan 608-737, Korea, sjcho@pknu.ac.kr 3 Department of Multimedia Engineering, Tongmyong University
Busan 626-847, Korea, choies@tu.ac.kr 4 Institute of Mathematical Sciences and School of Computer Aided Science
Inje University, Gimhae 621-749, Korea, mathkhd@inje.ac.kr
Abstract. Cellular Automata(CA) with simple, regular, modular and cascadable structures, which the VLSI design community prefer, was made an alternative proposal of LFSRs. Pseudorandom sequences were produced by generators which accompany several LFSRs joined by nonlinear functions or irregular clocking techniques. Clock-Controlled Shrinking Generators(CCSGs) are a class of clock-controlled sequence generators. Clock-controlled LFSRs have become important building blocks for keystream generators in stream cipher applications, because they are known to produce sequences of long period and high linear complexity. In this paper, we propose a method of modelling linear CA with the minimum stage corresponding to CCSGs based on LFSR using the Cho et al.’s synthesis algorithm.
1 Introduction
The VLSI era has ushered in a new phase of activities into the research of linear machines, and specially the local neighborhood CA structures. The VLSI design community prefer simple, regular, modular and cascadable structures with local interconnections. The CA provides a wonderful solution in all these respects [1]. CA have the characters of simplicity of basic components, locality of CA interactions, massive parallelism of information processing, and exhibit complex global properties. These ensure that CA have higher speed and more potential applications than LFSR. The locality of signal path of CA contributes more higher speed than LFSR. So in the form of VLSI implementation, CA have more speed advantages than LFSR [2].
This work was supported by grant No. (R01-2006-000-10260-0) from the Basic Research Program of the Korea Science and Engineering Foundation.

166 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
Pseudorandom sequences were produced by generators which accompany several LFSRs joined by nonlinear functions or irregular clocking techniques. The theory for CA based pseudorandom number generator is well developed [1] and n-stage linear CA can be designed to generate sequences with desirable properties: maximum period 2n − 1, uniform distribution of n-tuples and balanced distribution of 1 and 0 ([3],[4]).
Pseudorandom sequence generators intend to be used in a stream cipher. Especially, the Shrinking Generator(SG) proposed by Coppersmith et al. [5] is a popular form of pseudorandom sequence generators that employ the irregular clocking. It has one or more LFSRs whose clocking is controlled by the output sequence of one. Such a sequence is called a clock-controlled sequence [6]. The SG generally uses two sources of pseudorandom sequences to create the third source of pseudorandom sequence, having better cryptographic quality(long period, high linear complexity, good statistical properties, etc.) than the original sources.
Clock-controlled LFSRs have become important building blocks for keystream generators in stream cipher applications, because they are known to produce sequences of long period and high linear complexity ([7], [8]).
In [9], they showed that CCSGs can be described in terms of linear CA conﬁgurations by using mirror image and the Cattell and Muzio synthesis algorithm [10]. Since the CA obtained by the Sabater et al.’s method has the maximum stage, the method has a waste of space. Also the sequence obtained by CA is not secure because the rule of this CA is symmetrical.
In this paper, we propose a new method of modelling linear CA with the minimum stage corresponding to CCSGs based on LFSR using the Cho et al.’s synthesis algorithm to overcome these weak points [11].
2 90/150 CA Preliminaries
CA are considered to be a good model of complex systems in which an inﬁnite one-dimensional array of ﬁnite state machines(cells) updates itself in a synchronous manner according to a uniform local rule. In the long history of the study of CA, generally speaking, the number of internal states of each cell is ﬁnite and the local state transition rule is deﬁned in a such way that the state of each cell depends on the previous states of itself and its neighboring cells [12]. CA consists of a number of cells. In a 3-neighborhood dependency, the next state qi(t + 1) of a cell is assumed to be dependent only on itself and on its two neighbors (left and right), and is denoted as
qi(t + 1) = f (qi−1(t), qi(t), qi+1(t))
where qi(t) represents the state of the i-th cell at the t-th instant of time and f is the next-state function. The cells evolve in discrete time steps according to some deterministic rule that depends only on logical neighborhood. The CA structure

Modelling Linear Cellular Automata 167
investigated by Wolfram [13] can be viewed as a discrete lattice of sites (cells), where each cell can assume either the value 0 or 1. In eﬀect, each cell consists of a storage element (D ﬂip-ﬂop) and a combinatorial logic implementing the next-state function.
The next-state function of a cell is called its rule. If the rule of a CA cell involves only XOR logic, then it is called a linear rule. A CA with all the cells having linear rules is called a linear CA. If all the CA cells obey the same rule, then the CA is said to be a uniform CA; otherwise, it is a hybrid CA. A CA is said to be a Null Boundary CA(NBCA) if the left(right) neighbor of the leftmost(rightmost) terminal cell is connected to logic 0-state.
If the next-state function of a cell is expressed in the form of a truth table, then the decimal equivalent of the output is conventionally called the rule number for the cell [13].

Table 1. Rule 90 and rule 150

Neighborhood state 111 110 101 100 011 010 001 000

Next state

0 1 0 1 1 0 1 0 rule 90

Next state

1 0 0 1 0 1 1 0 rule 150

In Table 1, the top row gives all eight possible states of the three neighboring cells (the left neighbor of the i-th cell, the i-th cell itself, and its right neighbor) at the time instant t. The second and third rows give the corresponding states of the i-th cell at the time instant t + 1 for two CA rules. The corresponding combinatorial logic for the above rules can be speciﬁed as

rule 90 : rule 150 :

qit+1 = qit−1 ⊕ qit+1 qit+1 = qit−1 ⊕ qit ⊕ qit+1

In this paper, CA are NBCA with rule 90 and 150. A natural form for the speciﬁcation is an n-tuple < d1, d2, · · · , dn >, called the rule vector, where

di =

0, 1,

if cell i uses rule 90 if cell i uses rule 150

Now we deﬁne the state qt of a CA at time t to be the n-tuple

qt = (q1t , q2t , · · · , qnt )Tn where AT is the transpose of the matrix A. Hence

qt+1 = f (qt)(:= Tnqt)

168 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim

where the product is a matrix-vector multiplication over GF (2). We call this

matrix Tn the CA state-transition matrix. If C is a CA whose rule vector is < d1, d2, · · · , dn >, then the state-transition matrix Tn of C is a tridiagonal matrix

 d1 1 0 0 0 · · · 0 0 0 

1 

d2

1

0 0 ··· 0

0

0 

0

Tn

=

 



...

1 d3 1 0 · · · 0 ... ... ... ... . . . ...



0 ...

0



...

 



 0 0 0 0 0 · · · 1 dn−1 1 

0 0 0 0 0 · · · 0 1 dn

The characteristic polynomial ∆n of C is deﬁned by

∆n = |Tn − xI|
where x is an indeterminate, I is the n × n identity matrix and Tn is the statetransition matrix of C.
For any n-stage 90/150 CA whose state-transition matrix is Tn, the minimal polynomial for Tn is the same as the characteristic polynomial for Tn [15].
A polynomial is said to be a CA-polynomial if it is the characteristic polynomial of some CA [10]. All irreducible polynomials are CA-polynomials([10], [11]).
In [10], authors proposed a method for the synthesis of one-dimensional 90/150 Linear Hybrid Group Cellular Automata(LHGCA) for irreducible CApolynomial.
In [11], Cho et al. proposed a new method for the synthesis of one-dimensional 90/150 LHGCA for any CA-polynomial. In this case CA-polynomial need not irreducible. This algorithm is eﬃcient and suitable for all practical applications. Table 2 shows an algorithm for ﬁnding the 90/150 CA for the given CA-polynomial. In this paper we propose a new method of modelling linear CA with the minimum stage corresponding to CCSGs based on LFSR using this algorithm.

3 Clock-Controlled Shrinking Generator

Two LFSRs are used, both clocked regularly. If the output of the ﬁrst LFSR is 1, the output of the second LFSR becomes the output of the generator. If the output of the ﬁrst LFSR is 0, however, the output of the second is discarded. This mechanism suﬀers from timing attacks on the second generator, since the speed of the output is variable in a manner that depends on the second generator’s state. This can be alleviated by buﬀering the output.
CCSGs are a class of clock-controlled sequence generators [14]. They have applications to cryptography, error correcting codes and digital signature. A CCSG consists of two LFSRs A(control register) and B(generating register). The A is clocked normally, but the B is clocked by one plus the integer value

Modelling Linear Cellular Automata 169
Table 2. Cho et al.’s Synthesis Algorithm
Algorithm Cho et al.’s Synthesis Algorithm
Input : CA-polynomial f (x) Output : 90/150 group/nongroup CA Step 1 : Make the matrix B which is the n × n matrix
obtained by reducing the n polynomials xi−1 + x2i−1 + x2i (mod f (x)) (i = 1, 2, · · · , n). Step 2 : Solve the equation Bv = (0, · · · , 0, 1)T . Step 3 : Construct a Krylov matrix H = K(CT , v) by the seed vector v which is a solution of the equation in Step 2. Step 4 : Compute the LU factorization H = LU . Step 5 : Compute CA for f (x) by the matrix U .

represented in selected w ﬁxed stages of the A. The output bits of the system are produced by shrinking the output of B under the control of A as the following. At any time t the output of B is taken if the current output of A is 1, otherwise it is discarded. Suppose as the following Table 3.

Table 3. LFSRs A and B

LFSR stage characteristic polynomial

Am

R(x)

Bn

S(x)

initial state A0 B0

F is a function that acts on the state of A at a given time t to determine the number of times which B is clocked such that
F (At) = 1 + 20Ai0 (t) + 21Ai1 (t) + · · · + 2w−1Aiw−1 (t)
for w < m, and distinct integers i0, i1, · · · , iw−1 ∈ {0, 1, · · · , m − 1}, At is the state at the time instant t. If no stages are selected (i.e. w = 0), deﬁne F (At) = 1.
In this way, the output sequence of a CCSG is obtained from a double decimation. First, the sequence {bi} of B is decimated by F (At) giving rise to the sequence {bi}. Next, if the output of A is 1, bi becomes the output of the generator, otherwise bi is discarded.
Example 3.1 Let A be the 4-stage LFSR with the characteristic polynomial R(x) = x4 + x + 1 and the initial state (0, 0, 0, 1). The sequence {ai} generated by A is
{ai} = {0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, · · ·}
with period 24 − 1 = 15. And let B be the 5-stage LFSR with the characteristic polynomial S(x) = x5 + x2 + 1 and the initial state (0, 0, 0, 0, 1). The sequence

170 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
{bi} generated by B is
{bi} = {0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, · · ·}
with period 25 − 1 = 31. If w = 1, then
F (At) = 1 + 20Ai0 (t)
Thus {Xti } is produced by F (At) as the following:
{Xti } = {1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, · · ·}
In [14], they deﬁned the cumulative function GA of A to be GA(Xti ) = 2m−1(2w + 1) − 1
Then GA(Xti ) = 24−1(21 + 1) − 1 = 23. That is, 1 + 1 + 1 + 2 + 1 + 1 + 2 + 2 + 1 + 2 + 1 + 2 + 2 + 2 + 2 = 23. In brief, after clocking A 24 − 1(= 15) times, B is clocked 23 times.
According to the following,

b0 := b0

bi+1 := bj ,

j=

i k=0

Xti

the underlined bits 0 or 1 of {bi} are outputted in order to produce the sequence {bi}.

{bi} 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, · · · {Xti } 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, · · ·
{bi} 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, · · ·
Then the output sequence {zi} of the CCSG is given by shrinking {bi} with {ai}
{ai} 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, · · · {bi} 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, · · · {zi} 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, · · ·
The underlined bits 0 or 1 of bi are outputted.

Modelling Linear Cellular Automata
4 90/150 CA-based CCSG

171

In this section, we analyze the period of sequences generated by CCSG based on LFSR.
Deﬁnition 4.1 Let {ai} be the sequence obtained by concatenations of {Ci}’s.

C0 := 1

 1,



Ci :=

k

 (0, · · · , 0, 1),

Xti = 1, Xti = k, (k ≥ 2).

Example 4.2 {bi} in Example 3.1 can be obtained by shrinking {bi} with {ai}. That is, {Xti } in Example 3.1 can be represented by {ai}.
{ai} = {1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, · · ·}
If ai is 1, bi becomes the output of the generator, otherwise the output of bi is discarded. This is just bi. The decimated sequence {bi} is given by
{ai} 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, · · · {bi} 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, · · · {bi} 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, · · ·

Theorem 4.3 Let A (resp. B) be an m (resp. n)-stage LFSR whose charac-
teristic polynomial is primitive. And let {ai} be the sequence obtained by concatenations of {Ci}’s in Deﬁnition 4.1. The period of {ai} is 2m−1(2w + 1) − 1 for a given w.
Proof. Because GA(Xti ) = 2m−1(2w + 1) − 1 for a given w, the period of {ai} is 2m−1(2w + 1) − 1 by Deﬁnition 4.1.

Theorem 4.4 Let A (resp. B) be an m (resp. n)-stage LFSR whose charac-
teristic polynomial is primitive. And let {bi} be a sequence given by shrinking {bi} with {ai}. The period of {bi} is

(2m − 1)lcm(GA(Xti ), 2n − 1) GA(Xti )

Proof. The period of the output sequence {bi} of B is 2n − 1. {ai} repeats

GA(Xti )

period

sequences

lcm(GA(Xti ),2n−1) GA(Xti )

times

and

(2m − 1)

1’s

occurs

in

a

full period of {ai}. Thus the period of {bi} is

(2m − 1)lcm(GA(Xti ), 2n − 1) GA(Xti )

172 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim

Theorem 4.5 Let A (resp. B) be an m (resp. n)-stage LFSR whose charac-

teristic polynomial is primitive. And let {zi} be a sequence given by shrinking {bi} with {ai}. The period of {zi} is

2m−1lcm(GA(Xti ), 2n − 1) GA(Xti )

Proof.

The period of the output sequence {bi} is (2m−1)

lcm(GA(Xti ),2n GA(Xti )

−1)

(:=

h).

{ai}

repeats

lcm(GA(Xti ),2n−1) GA(Xti )

period

sequences

lcm(2m −1,h) 2m −1

times

and

(2m−1)

1’s occurs in a full period of {ai}. Thus the period of {zi} is

2m−1

lcm(2m

− 1,

)lcm(GA(Xti ),2n−1)
GA(Xti )

2m − 1

= 2m−1lcm(GA(Xti ), 2n − 1)/GA(Xti )

Remark If 2n − 1 and GA(Xti ) are relatively prime, lcm(GA(Xti ), 2n − 1)/GA(Xti ) = 2n − 1. Therefore in this case, the period of the output sequence by CCSG is 2m−1(2n − 1). Thus the characteristic polynomial of such output
sequence is of the form F (x) = (n stage primitive polynomial)N , 2m−2 < N ≤
2m−1.

In [9], they proposed the algorithm that converts a given CCSG into a CA-
based linear model using mirror image and the Cattell and Muzio synthesis algorithm. Therefore N = 2m−1 is the maximum stage. Also this CA-based
linear model is symmetrical and has a waste of space.

5 Modelling Linear Cellular Automata with the minimum stage
In this section, we propose a method that converts a given CCSG into a CAbased linear model by using Cho et al.’s Synthesis Algorithm [11].
According to the previous results, the following algorithm that converts a given CCSG into a CA-based linear model is introduced in Table 5.
The following example shows modelling of linear CA with the minimum stage corresponding to CCSG based on LFSR using the above algorithm.
Example 5.1 Let A be the 4-stage LFSR with a primitive polynomial of degree 3, and B be the 5-stage LFSR with a primitive polynomial of degree 5. Let w = 1. Then CCSG with A and B has the characteristic polynomial F (x) = (5stage primitive polynomial)(24−2−1) = (x5 +x2 +1)3. By the proposed algorithm, we can compute a CA with T15 =< 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1 >. If the algorithm in [9] is used, they must compute a CA with T20 =< 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1 > corresponding to F (x) = (x5 + x2 + 1)4.

Modelling Linear Cellular Automata Table 5. Algorithm for modelling 90/150 CA

173

Algorithm ModellingOf90/150CA Input : A CCSG characterized by: The stages m of LFSR(A) and n of LFSR(B), w
(2n − 1 and GA(Xti ) = 2m−1(2w + 1) − 1 are relatively prime.) Output : Linear CA with the minimum stage corresponding to CCSG based on LFSR
Step 1 : Compute the characteristic polynomial F (x) for the given CCSG, where F (x) = (n stage primitive polynomial)N , N = 2m−2 + 1.
Step 2 : Compute CA by Algorithm “Cho et al.’s Synthesis Algorithm”.

6 Conclusion
In this paper we proposed a new method of modelling linear CA with the minimum stage corresponding to CCSGs based on LFSR to overcome the weak points of the Sabater et al.’s method.
References
1. P.P. Chaudhuri, D.R. Chowdhury, S. Nandy and S. Chattopadhyay, Additive Cellular Automata Theory and Applications 1, IEEE Computer Society Press, California, (1997)
2. Z. Chauanwu and L. Libin, “VLSI characteristic of cellular automata as LFSR,” Communications and Information Technology, 2005. ISCIT 2005, IEEE International Symposium. 2 (2005) 1031-1034
3. A.J. Menezes, P.C. van Oorschot and S.A. Vanstone, Handbook of Applied Cryptography, CRC Press, (1997)
4. P.H. Bardell, W.H. McAnney and J. Savir, Built-In Test for VLSI: Pseudorandom Techniques, A WILEY-INTERSCIENCE PUBLICATION, (1987)
5. D. Coppersmith, H. Krawczyk and Y. Mansour, “The shrinking generator,” Lecture Notes in Computer Science , 773 (1994) 22-39
6. G. Gong, “Theory and applications of q-ary interleaved sequences,”IEEE Transaction on Information Theory, 41-2 (1995) 400-411
7. J.D. Golic, “Toward fast correlation attacks on irregularly clocked shift registers,” Advances in Cryptology - EUROCRYPT’95, Lecture Notes in Computer Science, 921 (1995) 248-262
8. D. Gollmann and W.G. Chambers, “Clock controlled shift registers: a review,” IEEE J. Sel. Ar. Commun., 7-4 (1989) 525-533
9. A.F. Sabater and D.G. Martinez, “Modelling nonlinear sequence genterators in terms of linear cellular automata,” Applied Mathematical Modelling, 31 (2007) 226-235

174 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
10. K. M. Cattell and Jon C. Muzio, “Synthesis of One-Dimensional Linear Hybrid Cellular Automata,” IEEE Trans. Comput-Aided Des. Integr. Circuits Syst., 15-3 (1996) 325-335
11. S.J. Cho, U.S. Choi, H.D. Kim, Y.H. Hwang, J.G. Kim and S.H. Heo, “New synthesis of one-dimensional 90/150 linear hybrid group cellular automata,” IEEE Trans. Comput-Aided Des. Integr. Circuits Syst., 26-9 (2007) 1720-1724
12. H. Umeo, T. Yanagihara and M. Kanazawa, “State-Eﬃcient Firing Squad Synchronization Protocols for Communication-Restricted Cellular Automata,” Lecture Notes in Computer Science , 4173 (2006) 169-181
13. S. Wolfram, “Statistical mechanics of cellular automata,” Rev. Mod. Phys. 55 (1983) 601-644
14. A. Kanso, “Clock-controlled shrinking generators,” Lecture Notes in Computer Science , 2727 (2003) 443-451
15. M. Serra, T. Slater. J.C. Muzio and D.M. Miller, “The analysis of one dimensional linear cellular automata and their aliasing properties,” IEEE Trans. Comput-Aided Des. Integr. Circuits Syst., 9 (1990) 767-778

Multitape OrdinaRleMcuarcshioinnes and Primitive
Bernhard Irrgang and Benjamin Seyerth
University of Bonn, Mathematical Institute Beringstraÿe 1, D-53115 Bonn, Germany
irrgang@math.uni-bonn.de, benjamin.seyfferth@gmx.de
Abstract. We introduce a multitape version of the ordinal Turing machines which are dened in [4] as Turing machines computing on tapes of transnite length in transnite time. These machines are used to compute the primitive recursive ordinal functions which have a classical theory developed by Jensen and Karp [3]. Making use of that theory we are able to 1. identify the ∆1(Lα)-denable subsets of α as computable (if α is closed with respect to primitive recursive functions) and 2. characterize admissible ordinals by the means of transnite computations. Similar results linking α-recursion theory and ordinal computability are contained in [6].
Key words: Ordinal computability, multitape Turing machine, primitive recursive ordinal function, primitive recursive set function, admissible ordinal.
1 Introduction
Ordinal computability studies machine models generalized to perform computations on ordinals. Such machines have been used to compute Gödel's hierarchy of constructible sets L ([4], [5]) and can provide a computational approach to α-recursion theory ([6]). The α-Turing machine is a standard Turing program computing on tapes of length a limit ordinal α using a lim inf-rule to determine the machine conguration at limit times. The machine is said to terminate if it runs for less than α many steps, otherwise it diverges.
In [3] Jensen and Karp established a connection between the primitive recursive ordinal functions (PrimO, a generalization of the usual primitive recursive functions on natural numbers) and the primitive recursive set functions (PrimS) that are used in the theory of Gödel's constructible universe ([2]). The multitape α-Turing machines introduced in this paper are a straightforward multitape version of the α-Turing machine and are well suited to handle the calculus of PrimO functions, i.e. every PrimO function is also multitape αcomputable (Theorem 1).
Using a result from [3] that the PrimO functions are exactly the PrimS functions that map ordinals to ordinals we prove Theorem 2: If α is an ordinal closed under PrimO functions then the ∆1(Lα[B]) denable subsets of α are exactly

176 Bernhard Irrgang and Benjamin Seyerth

the ones that are multitape α-computable in B. Furthermore we are able to give a characterization of admissible ordinals by the means of multitape α-Turing machines (Theorem 3): A limit ordinal is admissible i there is no multitape αcomputable function mapping some β < α conally into α. Admissible ordinals, which play an important role in generalizing recursion theory, are classically dened by the means of denability over a constructible level Lα or axiomatized through the axioms of Kripke-Platek set theory. We hence provide an alternative approach to admissibility from a computational perspective.
Similar theorems are already contained in [6] but obtained in a dierent way: A (single-tape) α-computable truth function for Lα is employed to transfer denability over Lα to the computational context. The computability of this truth function however requires α to be suciently closed with respect to ordinal arithmetic. Seeing this in a talk the second author gave in a seminar on ordinal computability in Bonn in November 2007, the rst author suggested to make use of the well developed theory of primitive recursive ordinal and set functions and applied them in the proofs of Theorems 2 and 3. We kindly thank Peter Koepke for his suggestions and support of this work.

Turing2 Multitape α-

Machines

A program P for a standard Turing-machine with k tapes (each with an independent read-write head) can be seen as a nite subset of {0, 1}k × ω × {0, 1}k × ω × {−1, +1}k. An element (a, s, a , s , d) ∈ P codes the following instruction: If the k-many read write heads read the symbols corresponding to the entries of the vector a ∈ {0, 1}k and the machine is in state s ∈ ω then have the heads write the entries of a ∈ {0, 1}k to the respective tapes, change the machine state to s ∈ ω and move the read-write heads according to the vector d ∈ {−1, +1}k. Similarly to previous studies ([4, 6]) this can be used as a basis for the following notion of a transnite computation according to P :
At successor times the program P is used as in the standard Turing machine case, with the single exception that when dealing with tapes of transnite length a convention has to be found what should happen when a read-write-head is being moved left from a cell indexed by a limit ordinal. In this situation we want the head to be reset to the beginning of the tape (cell number `0').
For limit times the Turing-program cannot determine the tape content, head positions and program state so we have to dene them in a sensible way. Following the lines of [4] we use inferior limits: We want each single cell of every tape to contain the lim inf of its previous values, the machine state to be the lim inf of the previous machine states and every tape's read/write-head to be located on the cell indexed by the lim inf over the positions it previously assumed in the limit machine state, i.e. the least cell that was read conally often while the machine was in the same state as at the limit time.
More formally:
Denition 1. Let α be a limit ordinal or α = Ord. Let P ⊆ {0, 1}k × ω × {0, 1}k × ω × {−1, 1}k be nite and let T0 = (T0,0, T1,0, . . . , T(k−1),0) ∈ (α{0, 1})k

Multitape Ordinal Machines and Primitive Recursion 177

be the initial tape content of the k-many tapes of length α. A triple

(Tθ, Hθ, Sθ)θ≤Θ

is a k-tape α-Turing computation by P on input T0 if the following conditions hold:

− Θ ≤ α; − Sθ ∈ ω for θ ≤ Θ ;
− Hθ = (H0,θ, H1,θ, . . . , H(k−1),θ)
where Hi,θ ∈ α for 0 ≤ i < k and for θ ≤ Θ ;
− Tθ = (T0,θ, T1,θ, . . . , T(k−1),θ)
where Ti,θ : α −→ {0, 1} for 0 ≤ i < k and for θ ≤ Θ ; − (Tθ, Hθ, Sθ)θ≤Θ is dened recursively in P and the initial tape contents Ti,0
in the following way: Termination: Let θ ≤ Θ < α and let (Tθ , Hθ , Sθ )θ ≤θ be already dened. If there is no (a, s, a , s , d) ∈ P where (Ti,θ(Hi,θ))i<k = a and Sθ = s then the computation terminates, i.e. θ = Θ. Successor step: Let θ ≤ Θ, let (Tθ , Hθ , Sθ )θ ≤θ be already dened and let there be a c = (a, s, a , s , d) ∈ P where (Ti,θ(Hi,θ))i<k = a and Sθ = s. Choose c minimally with respect to some xed well-order on P . As usual we want the conguration (Tθ+1, Hθ+1, Sθ+1) to be derived from (Tθ, Hθ, Sθ) according to the instruction c. Let a = (a0, a1, . . . , ak−1). For all i < k we require:

Ti,θ+1(ξ) =

ai Ti,θ (ξ )

, if ξ = Hi,θ , else

 Hi,θ + 1

, if d = +1



Hi,θ+1 = Hi,θ − 1 , if d = −1 and Hi,θ is a successor ordinal

0 , if d = −1 and Hi,θ is a limit ordinal

Si,θ+1 = s .

Limit step: Now let θ ≤ Θ be a limit ordinal and let (Tθ , Hθ , Sθ )θ <θ be already dened. For i < k and ξ < α set

Sθ = lim inf Sθ
θ <θ
Hi,θ = lim inf Hi,θ
θ <θ,Sθ =Sθ
Ti,θ(ξ) = lim inf Ti,θ (ξ).
θ <θ
Note that the machine conguration at limit times is always dened whenever the congurations at all previous stages are dened. If θ = Θ = α we say that the computation diverges.

The primitive recursive ordinal functions we want to compute are n-ary functions mapping ordinals < α to α. So we dene:

178 Bernhard Irrgang and Benjamin Seyerth

Denition 2. Let B ⊆ α and n ∈ ω. A function f : αn −→ α is called k-αcomputable in B if there is a k-tape Turing program P and a nite sequence of ordinal parameters π = (π1, π2, . . . , πm) ∈ αm s.t. k ≥ n + m + 2 and for all ξ = (ξ1, ξ2, . . . , ξn) ∈ αn the k-tape α-Turing computation by P with input T0 (Tθ, Hθ, Sθ)θ≤Θξ is of the form

− Θξ < α, i.e. the computation terminates;

− − −

for every for every there is a

ijre<<adnm-otnhthleyererteaipsiesaacorerneatadad-io-noninnlylgytχataBppe;eccoonntataininininggχχ{{ξiπ}j;}

;

− all other tapes are initially empty;

− there is a tape that at time Θξ contains .χ{f(ξ)}

If B = ∅ we call f k-α-computable. A function g : αn −→ α is called multitape α-computable if there is a k such that g is k-α-computable.

The question arises which closure properties an ordinal α must have so that the notions of α-computable in B dened in [6] and multitape α-computable in B coincide. Clearly this is true for admissible ordinals, but much weaker closure properties will certainly suce.

Lemma 1. Let f : αn −→ α be k-α-computable by the program P in parameters π. Let (Tθ, Hθ, Sθ)θ≤Θξ be the k-α-computation by P for f(ξ). Then the function Timef : αn −→ α which maps ξ → Θξ is (k + 1)-α-computable.

Proof. We extend P to a (k + 1)-tape Turing program which still computes f but where every instruction in P additionally moves the (k + 1)-st tape's head to the right. The computation terminates after Θξ many steps, i.e. there is no command for the nal conguration. Add a new instruction for this conguration that writes a `1' at the current head position of the (k+1)-st tape. Since the k+1st tape's head now points to a `1' instead of a `0' the computation terminates with this new instruction. It follows that Timef is (k + 1)-α-computable.

3 Primitive Recursive Functions
The familiar primitive recursive functions on natural numbers are those recursive functions generated by a weak recursion scheme that allows recursive denitions using the supremum over the previous function values. A generalization of this concept to functions operating on the universe of sets has for instance been used in the study of the constructible hierarchy ([2]). In [3] Jensen and Karp dened the notion of primitive recursiveness for functions mapping ordinals to ordinals and developed their theory. Following [3] we dene:
Denition 3. The class PrimO(B) of primitive recursive ordinal functions in B ⊆ Ord is dened as the minimal set containing all the functions of type (1) to (5) and closed under the schemes for substitution (a) and (b) and recursion (R).

Multitape Ordinal Machines and Primitive Recursion 179

(1) f (ξ) = χB(ξ) (2) prn,i(ξ) = ξi, for all n ∈ ω, ξ = (ξ1, . . . , ξn) and 1 ≤ i ≤ n. (3) f(ξ) = 0 (4) f (ξ) = ξ + 1

(5) c(ξ, ζ, γ, δ) =

ξ, if γ < δ ζ, else

(a) f (ξ, ζ) = g(ξ, h(ξ), ζ) (b) f (ξ, ζ) = g(h(ξ), ζ) (R) f (ζ, ξ) = g(sup{f (η, ξ) | η < ζ}, ζ, ξ)

We write PrimO instead of PrimO(∅). If B ⊆ α and α is an ordinal that is closed under PrimO(B) functions then we call a function f : αn −→ α PrimO(B) i it is the restriction of a PrimO(B) function.

Denition 3 is a special case of the following denition of primitive recursive set functions also taken from [3]:

Denition 4. Let X be a one-place set function. The class PrimS(X) of primitive recursive set functions in X is dened as the minimal set containing all the functions of type (1) to (5) and closed under the schemes for substitution (a) and (b) and recursion (R).

(1) F (x) = X(x) (2) P rn,i(x) = xi, for all n ∈ ω, x = (x1, . . . , xn) ∈ αn and 1 ≤ i ≤ n. (3) F (x) = 0 (4) F (x, y) = x ∪ {y}

(5) C(x, y, u, v) =

x, if u ∈ v y, else

(a) F (x, y) = G(x, H(x), y)

(b) F (x, y) = G(H(x), y)

(R) F (y, x) = G(sup{F (z, x) | z < y}, y, x)

We write PrimS instead of PrimS(∅). If B is a set and X is the unary function X(x) = x ∩ B then we may write PrimS(B) for PrimS(X). If B ⊆ α and α is an ordinal that is closed under PrimO(B) functions then we call a function f : αn −→ α PrimS(B) i it is the restriction of a PrimS(B) function. An n-ary relation R ⊆ V n is PrimS(X) if there is a PrimS(X) function FR : V n −→ V s.t. FR(x) = 0 i x ∈ R. Theorem 1. If α is closed under PrimO functions, then every PrimO(B) function f : αn −→ α is multitape α-computable in B. Furthermore there is a PrimO function Mf that majorizes Timef , i.e. ∀ξ ∈ αn M(ξ) > Timef (ξ).

180 Bernhard Irrgang and Benjamin Seyerth
Proof. Functions (1) to (4) are easily seen to be multitape α-computable with majorizing functions the maximum input ordinal plus 1.
(5) We dene a program to compute C. Move the heads of the tapes containing γ and δ to the right to decide whether γ < δ. According to the outcome move the output tape's head together with the head of the tape containing ξ or ζ to the right until a `1' is read. Copy the `1' to the output tape and stop. This program runs at most MC(γ, δ, ξ, ζ) = max{γ, δ} + max{ξ, ζ} + 1-many steps. Since α is closed under PrimO functions and ordinal addition is PrimO the program terminates and C is multitape α-computable.
(a) Let g be k-α-computable by a program Pg with parameters πg and majorizing function Mg, h l-α-computable by Ph with parameters πh and majorized by function Mh. We dene a (k + l)-tape Turing program that computes f using πgˆπh as parameters. The program rst runs Ph to compute h(ξ). Note that any program computing f in parameters πgˆπh uses tapes containing the components of ξ, ζ, πh, πg and one tape containing the characteristic function of B; these can be used as input tapes for Ph and later on for Pg respectively. After resetting all heads to position zero the program continues with running Pg with the output tape of Ph, now containing h(ξ), as additional input tape. Resetting of heads takes only nitely many machine steps thanks to the well-foundedness of ordinals (to recognize head position `0' we may assume additional parameter tapes containing χ{0}). So the new Program will run in less than α many steps since addition of ordinals is PrimO and α is closed under PrimO functions. We can set Mf = (Mg · 2) + (Mh · 2).
(b) Similarly to (a). (R) Let g be k-α-computable by Pg in parameters πg and majorized by Mg. We describe a (k + 2)-tape Turing program that computes f with parameters πg. The algorithm runs through ζ-many stages. In stage η < ζ the new (k + 1)st tape contains the current value of sup{f(η , ξ) | η < η}. Pg is called once to compute f(η, ξ) = g(sup{f(η , ξ) | η < η}, η, ξ). If necessary the value of sup{f(η , ξ) | η ≤ η} has to be updated. The stage concludes by erasing all the work tapes used by Pg and resetting all heads to zero. We have to ensure that for all η < ζ the following conditions hold:
(i) At the time of the call of Pg to compute f(η, ξ) the (k + 1)-st tape contains in fact sup{f (η , ξ) | η ≤ η}.
(ii) The number of machine steps the program uses before entering stage η is less than α.
For (i) use the (k + 2)-nd tape to save the value of f(η, ξ) = γ not as χ{γ} but as χγ+1. At the beginning of every stage use tape (k +2) to write a `1' to the sup{f(η , ξ) | η ≤ η}-th cell of tape (k + 1) and use this as input to Pg. (i) holds inductively at successor stages. So let η be a limit ordinal. By the lim inf-rule tape (k + 2) contains in fact χsup{f(η ,ξ)|η ≤η}. So (i) holds.
Stage η consists of the following operations:
− Find the rst 0 on tape (k+2) and write a 1 to the respective cell of tape (k+ 1) (note that the the rst `0' occurs in cell number sup{f(η , ξ) | η < η}+1):

Multitape Ordinal Machines and Primitive Recursion 181
As seen above the number of machine steps βnd = sup{f(η , ξ) | η < η} + 2 is less than α. − Reset tape (k + 1)'s head: This needs at most βreset(k+1) = sup{f (η , ξ) | η < η many steps. − Run Pg to compute f (η, ξ) = g(sup{f (η , ξ) | η < η}, η, ξ), the number of steps βg = Timeg(sup{f (η , ξ) | η < η}, η, ξ) is (k + 1)-α-computable therefore < α. − Reset the head of Pg's output tape and update tape (k +2) if necessary: This can be decided and done in at most βupdate = f(η, ξ) · 2 < α many steps. − Erase the work tapes used by Pg. WLOG Pg maintains a `timer' tape as in Lemma 1 which we can use to determine up to which cell the tapes have to be erased. Since only cells up to index βg may have been used by Pg (all heads were at position 0 when Pg was called) this takes again at most βg · 2 many steps (·2 since we have to reset the heads before erasing the tapes). − Reset all heads. This takes at most max{βfind, βg} many steps.
We dene a majorizing function Mf for Timef by PrimO recursion:
Mf (η) = sup Mf (η )
η <η
+ (sup{f (η , ξ) | η ≤ η} + 2) · 2
+ Mg(sup{f (η , ξ) | η ≤ η}, η) + f (η, ξ) · 2
+ Mg(sup{f (η , ξ) | η ≤ η}, η) · 2 + max{sup{f (η , ξ) | η ≤ η}, Mg(sup{f (η , ξ) | η ≤ η}, η, ξ)}
It follows from inductive analysis of the algorithm above that Timef < Mf . Note that as supremum of the rst η many values of a PrimO function supη <η Mf (η ) is PrimO and therefore < α. So (ii) holds.
4 Applications
The following theorems are similar to Theorem 7 and 9 in [6] which provide a connection between α-recursion theory and ordinal computability. The proofs found in [6] explicitely give a truth function for bounded formulas in Lα which is α-computable if α is suciently closed with respect to ordinal arithmetic. Instead we use facts from the classical theory of PrimS functions to obtain these results.
Theorem 2. If α is an ordinal closed under PrimO functions then A ⊆ α is ∆1(Lα[B]) i A is multitape α-computable in B.
Proof. `⇐' follows from the recursion theorem as in [6]. `⇒' Let A ⊆ α be ∆1(Lα[B]) by
γ ∈ A ↔ Lα[B] |= ∃x φ[x, γ, p] γ ∈/ A ↔ Lα[B] |= ∃x ψ[x, γ, q].

182 Bernhard Irrgang and Benjamin Seyerth

where φ, ψ are Σ0B formulas. So we have:
γ ∈ A ↔ Lα[B] |= ∃x φ[x, γ, p] ↔ ∃x ∈ Lα[B] φ[x, γ, p]
In [3], Lemma 3.2, it is shown that there is a one-one PrimS function N mapping the ordinals onto the constructible sets s.t. N α : α −b−i→j Lα. It is easily seen that there is also a one-one PrimS(B) function N mapping the ordinals onto L[B] s.t. F = N α : α −b−i→j Lα[B]. So we can write:

↔ ∃ξ ∈ α φ[F (ξ), γ, F (π)]

Like in [2], Lemma I.2.4, we see that every Σ0B relation is PrimS(B). Hence there is a PrimS(B) function G s.t. G(z) = 0 i φ[z]:

↔ ∃ξ ∈ α G(F (ξ), γ, F (π)) = 0 ↔ ∃ξ ∈ α g(ξ, γ, π) = 0.

Where

0 , if G(F (ξ), γ, F (π)) = 0 g(ξ, γ, π) = 1 , else.

Since G is PrimS(B) so is g by (a) and (5). g mapping ordinals to ordinals, B ⊂ α, and we see like in Theorem 3.5 in [3] that g is PrimO(B). Similarly we obtain a function h for ψ. Now we can describe the following algorithm which computes the characteristic function of A:

WHILE (g(ξ, γ, π) = 1 AND h(ξ, γ, η) = 1) DO ξ + +; IF g(ξ, γ, π) = 0 THEN STOP = 0; IF h(ξ, γ, π) = 0 THEN STOP = 1;

We analyse the algorithm into its stages ξ < α, each consisting mainly of one computation for g and one for h plus some erasing of work tapes and resetting of heads. We have to make sure that this algorithm behaves nicely at limit stages, i.e. actually reaches every limit stage δ < α. Again it will be neccessary to store the counter ξ as χξ+1 and to decode this into χξ at the beginning of every stage. Similar to the proof of Theorem 1 we see that the algorithm up to stage ξ uses a number of steps majorized by a PrimO function M(ξ). Again also supζ<δ M(ζ) for δ < α is PrimO so the algorithm reaches every stage.
We can now give a characterization of admissible ordinals solely based on ordinal computations.

Theorem 3. A limit ordinal α is admissible i there is no multitape α-computable function mapping some β < α conally into α.

Multitape Ordinal Machines and Primitive Recursion 183

Proof. α is admissible i there is no Σ1(Lα)-denable total function that maps some β < α conally into α (cf. [1], Lemma II.7.2).
If α is already closed under PrimO functions then any multitape α-computable function f is ∆1(Lα) (and therefore Σ1(Lα)) by the recursion theorem as in [6] (cf. Theorem 2). Conversely let α be not admissible and let f : β −c−o→f α be a Σ1(Lα)-denable total function on β. So we have:
f (γ) = δ ↔ Lα |= ∃x φ[x, γ, δ, p]
↔ ∃x ∈ Lα φ[x, γ, δ, p]

Where φ is a Σ0 function. With F : α −b−i→j Lα PrimS from Lemma 3.2 in [3]:

↔ ∃ξ ∈ α φ[F (ξ), γ, δF (π)]

Since φ is Σ0 it is PrimS ([2], Lemma I.2.4):

↔ ∃ξ ∈ α G(F (ξ), γ, δ, F (π)) = 0 ↔ ∃ξ ∈ α g(ξ, γ, δ, π) = 0.

Where

0 , if G(F (ξ), γ, δ, F (π)) = 0 g(ξ, γ, δ, π) = 1 , else.

Since G is PrimS so is g by (a) and (5). g is mapping ordinals to ordinals and is therefore PrimO by Theorem 3.5 in [3]. The desired algorithm goes through less than α many stages to compute f(γ) given γ as input. In every stage η < α the algorithm computes the values of g(ξ, γ, δ, π) for all ξ, δ < η. g is PrimO so Timeg is multitape α-computable majorized by Mg which in turn is PrimO. So supξ,δ<η M(ξ, γ, δ, π) is less than α and every stage η is reached by the algorithm. At some stage one computation for g will return 0 and the value δ = f(γ) is found. So f is multitape α-computable as required.
In the case that α is not closed under PrimO functions we will dene β < α
and a multitape α-computable total function f : β −c−o→f α. Any limit ordinal is closed under (1)-(4) so assume in the following closure
under (1)-(4). If α is not closed under (5) then also for one instance of (5) f(ζ0, ζ1, ζ2, ζ3) the canonical algorithm will not terminate in less than α many steps. Analysing the algorithm in the proof of Theorem 1 we can extract two ordinals γ, δ < α with γ + δ ≥ α (set γ = max{ζ2, ζ3} and δ = max{ζ0, ζ1}). So there is a β ≤ δ such that the multitape α-computable function f : β −→ α, ξ → γ + ξ is conal in α.
If α is closed under two functions f and g so it is also closed under f(ξ, ζ) = g(ξ, h(ξ), ζ) and under f (ξ, ζ) = g(h(ξ), ζ). So if α is closed under (1)-(5) but not closed under PrimO functions it has to be not closed under (R).
Assume α closed under (1)-(5),(a),(b) but not closed under (R). Since the PrimO functions are dened by recursion, there are PrimO functions f, g s.t.

184 Bernhard Irrgang and Benjamin Seyerth
f(ζ, ξ) = g(sup{f(η, ξ) | η < ζ}, ζ, ξ) is an instance of (R) and α closed under g but not closed under f. Choose β minimally s.t. f(β, ξ) ∈/ α. f(β, ξ) = g(supγ<β f (γ), ξ), β, ξ) and since α is closed under g we have that supγ<β f (γ, ξ) = α. Now f β : β −→ α is conal in α.
References
1. Keith Devlin. Constructibilit y. Perspectives in Mathematical Logic. Springer-Verlag, Berlin, 1984.
2. Keith Devlin. Aspects of Constructibilit y. Perspectives in Mathematical Logic. Springer-Verlag, Berlin, Heidelberg, 1973.
3. Ronald B. Jensen and Carol Karp. Primitive recursive set functions. In: Axiomatic Set Theory, Proceedings of Symposia in Pure Mathematics, Volume XIII, Part I. American Mathematical Society, Providence, Rhode Island, 1971.
4. Peter Koepke. Turing computations on ordinals. The Bulletin of Symbolic Logic 11 (2005), 377397.
5. Peter Koepke and Martin Koerwien. Ordinal computations. Mathematical Structures in Computer Science (2006), 867884.
6. Peter Koepke and Benjamin Seyfferth. Ordinal Machines and Admissible Recursion Theory. Submitted to: Annals of Pure and Applied Logic, CiE 2007 special volume, to be published 2008.

Prescribed Learning of Indexed Families
Sanjay Jain1, Frank Stephan2, and Nan Ye3
1 Department of Computer Science, National University of Singapore, Singapore 117590, Republic of Singapore.
Email: sanjay@comp.nus.edu.sg . 2 Department of Computer Science and Department of Mathematics, National University of Singapore, Singapore 117543, Republic of Singapore.
Email: fstephan@comp.nus.edu.sg . 3 Department of Computer Science and Department of Mathematics, National University of Singapore, Singapore 117543, Republic of Singapore.
Email: u0407028@nus.edu.sg .
Abstract. This work extends studies of Angluin, Lange and Zeugmann on how learnability of a language class depends on the hypothesis space used by the learner. While previous studies mainly focused on the case where the learner chooses a particular hypothesis space, the goal of this work is to investigate the case where the learner has to cope with all possible hypothesis spaces. In that sense, the present work combines the approach of Angluin, Lange and Zeugmann with the question of how a learner can be synthesized. The investigation for the case of uniformly r.e. classes has been done by Jain, Stephan and Ye [6]. This paper investigates the case for indexed families and gives a special attention to the notions of conservative and non U-shaped learning.
1 Introduction
The goal of inductive inference [1, 2, 4] is to model the process of learning rigorously. Following many real-world scenarios, the learner observes more and more data which in the limit uniquely determines the concept to be learnt. The learner is supposed to determine the target concept from the data it observes. Following the model of linguistics, the concept to be learnt is always considered to be an (often inﬁnite) set of ﬁnite items which can be coded as natural numbers. The language to be learnt is chosen from a concept class {L0, L1, L2, . . .} and the learner is using an explicit hypothesis space {H0, H1, H2, . . .}. This hypothesis space may be either the same as {L0, L1, L2, . . .} (exact learning [1]) or chosen by the learner (class-preserving and class-comprising learning [8, 13, 14]) or imposed on the learner (prescribed and uniform learning [6]). Angluin [1] considered the important case that the concept class and hypothesis class are both given by an indexed family, that is, the class is uniformly recursive. She
The work for this paper is supported in part by NUS grants number R252-000-212112 and R252-000-308-112. A full version is available as Technical Report TRB9/07, School of Computing, National University of Singapore, 2007.

186 Sanjay Jain, Frank Stephan, and Nan Ye
has given a characterization when such a class is explanatorily learnable and introduced also important variants like consistent and conservative learning.
The goal of the present work is to study prescribed and uniform learning and to contrast the results obtained for them to the well-studied cases of exact, class-preserving and class-comprising learning. The idea that the learner has to accept a given choice of the hypothesis class is not completely new; besides the case of exact learning (for which the results would be equivalent to the (not considered case of) class-preserving prescribed learning), it has also been considered under the framework of synthesis of learners. But the models like those considered by Zilles [15, 16] diﬀer from the scenario in the present work. Jain, Stephan and Ye [6] have studied the more general case of uniformly r.e. concept and hypothesis spaces in a separate paper. The main diﬀerence to the setting in the r.e. case is that there it is more reasonable to consider class-preserving-uniformly and class-preserving-prescribed learning instead of uniform and prescribed learning. Furthermore, the relation between non U-shaped learning and conservative learning depends crucially on the indexed family nature of the hypothesis space.
The study of prescribed and uniform learning is done for the criteria of ﬁnite learning (Section 2), conservative learning (Section 3), non U-shaped learning (Section 4) and the various notions of monotonic learning (Section 5).
In the following, we will provide more details, but have to introduce some formal notations ﬁrst. Let N be the set of natural numbers and ·, · is a ﬁxed pairing function: a recursive bijective mapping from N2 to N. Furthermore, |S| denotes the cardinality of set S. ϕ0, ϕ1, ϕ2, . . . denotes a ﬁxed acceptable numbering of the partial recursive functions from N to N. In some cases, we use ϕi as a function of two arguments. In such cases one implicitly assumes a pairing function being used to code the inputs: thus, ϕi(x, y) means ϕi( x, y ). The set We is the domain of ϕe. The set K = {e : e ∈ We} is the diagonal halting problem which is used as a standard example of an r.e. but nonrecursive set. Let Kt denote the set of elements enumerated into K within t steps, via some standard enumeration procedure. We assume without loss of generality that K0 = ∅.
Deﬁnition 1. A learner is a mapping from (N ∪ {#})∗ to N ∪ {?}. Let M be a given learner, {L0, L1, L2, . . .} be a language class and {H0, H1, H2, . . .} be a hypothesis space. In this paper, M is a partial-recursive function and {L0, L1, L2, . . .}, {H0, H1, H2, . . .} are indexed families of subsets of the natural numbers, that is, the mappings e, x → Le(x) and e, x → He(x) are recursive functions from N × N to {0, 1}. Let σ, τ, ρ range over (N ∪ {#})∗. Furthermore, let σ ⊆ τ denote that τ is an extension of σ as a string. T is a text if T is a total function which maps N to N ∪ {#} and T is a text for La iﬀ the numbers occurring in T are exactly those in La.
A learner converges [4] on T to b iﬀ there is an n with M (T [m]) = b for all m ≥ n; here T [m] is the ﬁnite string consisting of the ﬁrst m members of T .
The learner M is total if M (σ) is deﬁned for all ﬁnite strings σ in (N ∪ {#})∗. For the learning criteria in this paper, a given learner can always be eﬀectively converted to a total leaner which learns the same classes. Hence learners are assumed to be total here onwards.

Prescribed Learning of Indexed Families 187
The learner M is ﬁnite [4] if for every text T there is one index e such that for all n, either M (T [n]) = ? or M (T [n]) = e.
The learner M is conﬁdent [10] if M is total and converges on every text T to a hypothesis.
The learner M is conservative [1] if for all σ, τ with HM(σ) = HM(στ) there is an x occurring in στ such that x ∈/ HM(σ). M is non U-shaped [3] if there are no a and σ, τ, ρ ∈ (La ∪ {#})∗ such that HM(σ) = HM(στρ) = La and HM(στ) = La. In other words, M never changes from a correct to an incorrect and then back to a correct hypothesis. M is decisive [3] if there are no σ, τ, ρ such that HM(στρ) = HM(σ) and HM(στ) = HM(σ). In other words, M never returns to a once abandoned hypothesis (even semantically).
The learner M is monotonic [5] if for every La and for every σ, τ ∈ (La∪{#})∗ the inclusion La ∩ HM(σ) ⊆ La ∩ HM(στ) holds. M is strong-monotonic [5] if for all σ, τ ∈ (N ∪ {#})∗ the inclusion HM(σ) ⊆ HM(στ) holds.
Here note that ? is not considered as a conjecture and thus the constraints in conditions like conservative, monotonic, strong-monotonic, non U-shaped and decisive refer only to inputs where M makes a conjecture and does not output ?: so, more formally, a learner would be strong-monotonic iﬀ for all σ, τ , M (σ) = ? and M (στ ) = ? implies HM(σ) ⊆ HM(στ). Similarly for the other criteria.
Finite learning is quite restrictive since the learner has to make up its mind without having viewed all of the available inﬁnite information. Learning in the limit (or just “learning”) is more powerful since the learner can revise its hypothesis a ﬁnite but arbitrary number of times.
In this paper we will only be concerned about learning indexed families and using hypothesis spaces which are also indexed families. Angluin, Kapur, Lange and Zeugmann [1, 7–9, 12–14] studied how learnability of the family to be learned depends on the hypothesis space {H0, H1, H2, . . .} used by the learner. To formalize this, they introduced the notions of exact, class-preserving and classcomprising learning. In addition to this we consider notions like uniform and prescribed learning [6]. Here I ranges over properties of the learner as deﬁned in Deﬁnition 1, so I stands for “conservative”, “ﬁnite”, “monotonic” and so on.
Deﬁnition 2. In the following, let {L0, L1, L2, . . .} and {H0, H1, H2, . . .} be indexed families.
{L0, L1, L2, . . .} is explanatorily learnable [4] with hypothesis space {H0, H1, H2, . . .} iﬀ there is a learner M which converges on every text of a language La to a hypothesis b such that Hb = La.
For a property I from Deﬁnition 1, {L0, L1, L2, . . .} is I learnable with hypothesis space {H0, H1, H2, . . .} iﬀ there is a learner M which explanatorily learns {L0, L1, L2, . . .} using hypothesis space {H0, H1, H2, . . .} and furthermore satisﬁes the requirement I.
{L0, L1, L2, . . .} is class-comprisingly I learnable iﬀ it is I learnable with some hypothesis space {H0, H1, H2, . . .}; note that learnability automatically implies {L0, L1, L2, . . .} ⊆ {H0, H1, H2, . . .}.

188 Sanjay Jain, Frank Stephan, and Nan Ye
{L0, L1, L2, . . .} is class-preservingly I learnable iﬀ it is I learnable with some hypothesis space {H0, H1, H2, . . .} satisfying {H0, H1, H2, . . .} = {L0, L1, L2, . . .}.
{L0, L1, L2, . . .} is exactly I learnable iﬀ it is I learnable with {L0, L1, L2, . . .} itself taken as the hypothesis space.
{L0, L1, L2, . . .} is prescribed I learnable iﬀ it is I learnable with respect to every hypothesis space {H0, H1, H2, . . .} such that {L0, L1, L2, . . .} ⊆ {H0, H1, H2, . . .}.
{L0, L1, L2, . . .} is uniformly I learnable iﬀ there is a recursive enumeration of partial-recursive functions M0, M1, M2, . . . such that whenever ϕe is a decision-procedure b, x → Hb(x) for an indexed family {H0, H1, H2, . . .} ⊇ {L0, L1, L2, . . .} then Me is an I learner for {L0, L1, L2, . . .} using this hypothesis space {H0, H1, H2, . . .}.
{L0, L1, L2, . . .} is class-preserving-uniformly I learnable iﬀ there is a recursive enumeration of partial-recursive functions M0, M1, M2, . . . such that whenever ϕe is a decision-procedure b, x → Hb(x) for an indexed family {H0, H1, H2, . . .} = {L0, L1, L2, . . .} then Me is an I learner for {L0, L1, L2, . . .} using this hypothesis space {H0, H1, H2, . . .}.
Remark 3. For explanatory learning, class comprising learning and exact learning are the same as shown in [13]. This can be easily extended to the equivalence between uniform learning and exact learning, by noting that given a language class {L0, L1, L2, . . .}, then for any hypothesis space {H0, H1, H2, . . .} ⊇ {L0, L1, L2, . . .}, one can ﬁnd in the limit a b such that Ha = La for each a.
Exact ﬁnite learning and class comprising ﬁnite learning are the same [13]. For strong-monotonic, monotonic and conservative learning, there is a proper hierarchy for learning from exact, class preserving and class comprising hypothesis spaces [13]. For every criterion I, the following implications hold:
– Every uniformly I learnable family is also class-preserving-uniformly I learnable and prescribed I learnable.
– Every class-preserving-uniformly I learnable family and every prescribed I learnable family is also exactly I learnable.
– Every exactly I learnable family is also class-preservingly I learnable. – Every class-preservingly I learnable family is also class-comprisingly I learn-
able.
It depends on the actual choice of I what other implications hold (besides the transitive ones).
For example, for conﬁdent learning, the class containing all {x} where |Wx| < ∞ and {x, y} where x = y is not class-preservingly but class-comprisingly conﬁdently learnable. Although conﬁdent learnability becomes more general in the class-comprising case, one can show that it coincides for all other criteria from Deﬁnition 2. Suppose that N is an exact conﬁdent learner for the class {L0, L1, L2, . . .} and e is given such that ϕe is total and the hypothesis space {H0, H1, H2, . . .} satisﬁes Hd = {x : ϕe( d, x ) = 1} for all d and {L0, L1, L2, . . .} ⊆ {H0,

Prescribed Learning of Indexed Families 189
H1, H2, . . .}. Then Me simulates the learner N as follows: if N on text T converges to a then Me on text T converges to the least b such that Hb = La.
From the deﬁnition of uniform learning, we can easily obtain the following useful lemma.
Lemma 4. Let L be a uniformly I learnable indexed family. If H0, H1, H2, . . . is a recursive enumeration of indexed family hypothesis spaces for L, then there exists a recursive enumeration of learners M0, M1, M2, . . . such that Mn I learns L with respect to Hn.
We will often make use of the following simple set in our proofs.
Deﬁnition 5. Deﬁne S = ∪n=0,1,2,...Jn, where Jn contains for each e < n the ﬁrst element, if any, of We enumerated from In = {2n−1, 2n, 2n+1, . . . , 2n+1−2}. Then: S is recursively enumerable; S intersects with every inﬁnite recursively enumerable set; for every n there is an m in In which is not in S. In other words, S is a simple set [11]. Let St be the set of elements enumerated into S within t steps via some standard procedure. Here we take S0 = ∅.
In the following sections, without loss of generality we assume for i, j < |{L0, L1, L2, . . .}| that Li = Lj implies i = j.
2 Finite Learning
Finite learning or one-shot learning requires the learner to make a correct guess using only ﬁnite amount of information. So it is not a surprise that this criterion turns out to be very restrictive for prescribed and uniform learning, as shown below. The following theorem gives some characterization results, and separates various notions of ﬁnite learning. It can be shown that class comprising ﬁnitely learnable classes are also exact ﬁnitely learnable [13].
Theorem 6. Let {L0, L1, L2, . . .} be exactly ﬁnitely learnable. (a) {L0, L1, L2, . . .} is not uniformly ﬁnitely learnable. (b) {L0, L1, L2, . . .} is class-preserving-uniformly ﬁnitely learnable. (c) {L0, L1, L2, . . .} is prescribed ﬁnitely learnable iﬀ the class is ﬁnite and for all i, j < |{L0, L1, L2, . . .}|, either i = j or Li ⊂ Lj.
Hence, the class {{0}, {1}, {2}, . . .} is exactly ﬁnitely learnable but not prescribed ﬁnitely learnable; furthermore, a class is prescribed ﬁnitely learnable iﬀ it is ﬁnitely learnable and ﬁnite.
3 Conservative Learning
Conservative learning is non-trivial: there is an inﬁnite indexed family which is uniformly conservatively learnable. This is shown in the next example.
Example 7. Let La = N−{a} for all a ∈ N. Then {L0, L1, L2, . . .} is uniformly conservatively learnable.

190 Sanjay Jain, Frank Stephan, and Nan Ye

The class used in above example consists of co-ﬁnite sets only. The next result shows that this is necessary for uniform conservative learning.

Theorem 8. If {L0, L1, L2, . . .} is uniformly conservatively learnable then every set La is coﬁnite. Moreover, there is a recursive function r bounding the nonelements of La for all a < |{L0, L1, L2, . . .}|.

Proof. Let S be as in Deﬁnition 5. Furthermore, let Ga = La if a < |{L0, L1,
L2, . . .}| and Ga = N otherwise.
We deﬁne a sequence of hypothesis spaces H0, H1, H2, . . ., where for each n ∈ N the space Hn = {H0n, H1n, H2n, . . .} is deﬁned as follows:

 

Gi,

if j ∈/ S and j > n;

H

n i,j

=

Gi ∪ {t + 1, t + 2, t + 3, . . .}, if j ∈ St+1 − St and j > n;

 N,

j ≤ n.

H0, H1, H2, . . . is a recursive enumeration of indexed families. Since {L0, L1, L2, . . .} is uniformly conservatively learnable, there exists a recursive enumeration of learners M0, M1, M2, . . . such that for all n, Mn conservatively learns {L0, L1, L2, . . .} with respect to Hn.
For all a < |{L0, L1, L2, . . .}| and n ∈ N, let e = v(a, n), w(a, n) be the ﬁrst number found (in some dovetailing search) such that Mn outputs e on the canonical text Ta of La and one of the following conditions hold:

(a)

w(a, n)

∈

S

and

La

⊆

H

n v(a,n),w(a,n)

(note that this can be veriﬁed by

ﬁnding

a

t

with

w(a, n)

∈

St

and

checking

La(x)

≤

H

n v(a,n),w(a,n)

(x)

for

all

x ≤ t);

(b) v(a, n) = a;

(c) w(a, n) ≤ n.

First note that for every a < |{L0, L1, L2, . . .}| there is an n such that either

w(a, n) ≤ n or w(a, n) ∈ S: Otherwise the set {w(a, n) : n ∈ N} would be

an inﬁnite r.e. set disjoint to S which does not exist as S is simple. Hence

there is a recursive function u which searches for this n; that is, w(a, u(a)) ≤

u(a) ∨ w(a, u(a)) ∈ S for all a < |{L0, L1, L2, . . .}|.

It is easy to see that there is a further recursive function r such that either

r(a) = 0 ∧ w(a, u(a)) ≤ u(a) or r(a) > 0 ∧ w(a, u(a)) ∈ Sr(a). Note that La ⊆

H

u(a) v(a,u(a)),w(a,u(a))

and

{r(a), r(a)+1, r(a)+2, . . .}

⊆

H

u(a) v(a,u(a)),w(a,u(a))

.

Since

each

Mn

is

conservative,

it

follows

that

La

=

H

u(a) v(a,u(a)),w(a,u(a))

and thus N−La

contains only elements below r(a) for all a < |{L0, L1, L2, . . .}|.

For prescribed conservative learning, we have a less stringent necessary condition as compared to uniform conservative learning.

Theorem 9. If {L0, L1, L2, . . .} is prescribed conservatively learnable then almost every set in {L0, L1, L2, . . .} is coﬁnite. Moreover, there is a recursive function r bounding the non-elements of the coﬁnite La.

Prescribed Learning of Indexed Families 191

Proof. Let S and In be as in Deﬁnition 5. Deﬁne a hypothesis space {H0, H1, H2, . . .} as follows. For m ∈ N, deﬁne Hm as follows: suppose n is such that m ∈ In; then let

Hm =

Ln, Ln ∪ {m + t, m + t + 1, . . .},

if m ∈/ S; if m ∈ St+1 − St for some t ∈ N.

{H0, H1, H2, . . .} is an indexed family which is a superclass of {L0, L1, L2, . . .}. Let M be a learner for {L0, L1, L2, . . .} with respect to {H0, H1, H2, . . .}. Let mn be the ﬁrst index output by M on the canonical text Tn for Ln such that mn ∈ In (mn may not always be deﬁned). Then the set {mn : n ∈ N and mn exists} is recursively enumerable. Let e be an index for this set. If there are inﬁnitely
many coinﬁnite sets in {L0, L1, L2, . . .}, then there is an a > e such that La is coinﬁnite. Since La is coinﬁnite, ma exists (as only indices in In can be indices for coinﬁnite Ln). Furthermore, ma ∈ S because ma is the only element in We ∩ Ia. But this implies Hma ⊃ La. Hence M cannot be conservative. The function r can be found by techniques similar to those in the proof of Theorem 8.

The above is not a characterization as the class of all coﬁnite sets is not learnable in the limit. The next example shows that there is also a learnable class of coﬁnite sets which is not prescribed conservatively learnable; this class is still class-comprising conservative learnable.

Example 10. Let {L0, L1, L2, . . .} contain all sets of the form N − {a} and all sets of the form N − {a, b} where a < b, a ∈ K − Kb. Then {L0, L1, L2, . . .} is class-preservingly conservatively learnable but not prescribed conservatively
learnable.

The following theorem shows that class-preserving-uniformly conservative learnability and prescribed conservative learnability are not comparable.

Theorem 11. (a) There exists {L0, L1, L2, . . .} which is class-preserving-uniformly conservatively learnable, but not prescribed conservatively learnable. (b) There exists {L0, L1, L2, . . .} which is prescribed conservatively learnable but not class-preserving-uniformly conservatively learnable.

4 Non U-Shaped Learning
Every conservative learner is clearly non U-shaped. Furthermore, one can modify a conservative learner to be decisive by only changing to a new hypothesis if it is consistent with the input.
The following theorem thus shows that non U-shaped learning is equivalent to conservative learning in the case of exact, class-preserving and class-preservinguniform learning.
Theorem 12. Assume that {L0, L1, L2, . . .} is class-preserving-uniformly non U-shaped learnable. Then {L0, L1, L2, . . .} is already class-preserving-uniformly conservatively learnable by the same learner. The same applies for exact and class-preserving learning.

192 Sanjay Jain, Frank Stephan, and Nan Ye
Proof. We show that if M non U-shaped learns {L0, L1, L2, . . .} with respect to a class-preserving hypothesis space {H0, H1, H2, . . .}, then M conservatively learns {L0, L1, L2, . . .} with respect to {H0, H1, H2, . . .}. Assume M is not conservative, then there exist τ, σ such that HM(τ) = HM(τσ), but content(τ σ) ⊆ HM(τ). Since {H0, H1, H2, . . .} is class-preserving, there exists an n such that Ln = HM(τ). Let T be a text for Ln, then τ σT is a text for Ln. However, M is not non U-shaped on τ σT , as it ﬁrst outputs a correct hypothesis HM(τ) = Ln and then abandons it. The same argument applies for an exact hypothesis space.
However, for class-comprising learning, non U-shaped learning is more powerful than conservative learning, as shown by the following theorem.
Theorem 13. Assume that {L0, L1, L2, . . .} contains all sets {x, x+1, x+2, . . .} and all ﬁnite sets D such that there is an s with min(D) ∈ Ks+1 − Ks and 0 < |D| < s. Then {L0, L1, L2, . . .} has a non U-shaped class-comprising learner but not a conservative class-comprising learner.
The following theorem gives a suﬃcient condition for uniform non U-shaped learnability. Furthermore, this condition helps us to separate uniform non Ushaped learnability from prescribed conservative learnability.
Theorem 14. If the class {L0, L1, L2, . . .} is exactly ﬁnitely learnable then {L0, L1, L2, . . .} is uniformly non U-shaped learnable. In particular, there are classes which are uniformly non U-shaped learnable but not prescribed conservatively learnable.
Proof. Let M be an exact ﬁnite learner for {L0, L1, L2, . . .}, we deﬁne a recursive enumeration of non U-shaped learners M0, M1, M2, . . . which uniformly learn {L0, L1, L2, . . .}. For each i ∈ N, Mi(T [t]) is deﬁned as follows: if M (T [t]) = ?, then output ?; if M (T [t]) = e, then for each j ≤ t, deﬁne rj = min{x : x > t or Le(x) = ϕi(j, x)}. Output the minimal j which maximizes rj. It can be easily veriﬁed that the above learners witness that {L0, L1, L2, . . .} is uniformly non U-shaped learnable.
Take any exactly ﬁnitely learnable language collection with inﬁnitely many coinﬁnite languages, then it is uniformly non U-shaped learnable but not prescribed conservatively learnable. An example is the language collection {L0, L1, L2, . . .} where Ln = {0, 2, 4, 6, . . .} ∪ {2n + 1}.
With the following result we can see that non U-shaped learning and decisive learning are equivalent for prescribed learning and uniform learning.
Theorem 15. If {L0, L1, L2, . . .} is prescribed non U-shaped learnable then {L0, L1, L2, . . .} is also prescribed decisively learnable. If {L0, L1, L2, . . .} is uniformly non U-shaped learnable then {L0, L1, L2, . . .} is also uniformly decisively learnable.

Prescribed Learning of Indexed Families 193
Proof. It suﬃces to show that if M non U-shaped learns {L0, L1, L2, . . .} with respect to a given hypothesis space {H0, H1, H2, . . .}, then we can eﬀectively build another learner M which decisively learns {L0, L1, L2, . . .} with respect to {H0, H1, H2, . . .}. The desired M can be deﬁned as follows. Given a text T , let M (T [0]) = M (T [0]). For t > 0, M (T [t]) = M (T [t]) if for all t < t, for some x ≤ t, HM(T [t])(x) = HM (T [t ])(x); and M (T [t]) = M (T [t − 1]) otherwise. It can be easily veriﬁed that M is decisively learning {L0, L1, L2, . . .} using {H0, H1, H2, . . .}.
As in the case of conservative learning, class-preserving-uniform non U-shaped learnability and prescribed non U-shaped learnability are not comparable as well.
Theorem 16. (a) There exists an {L0, L1, L2, . . .} which is class-preservinguniformly non U-shaped learnable but not prescribed non U-shaped learnable. (b) There exists an {L0, L1, L2, . . .} which is prescribed non U-shaped learnable but not class-preserving-uniformly non U-shaped learnable.
5 Monotonic Learning
The prescribed and uniform versions of strong-monotonic and monotonic learning are very restrictive.
Theorem 17. (a) {L0, L1, L2, . . .} is prescribed strong-monotonically learnable iﬀ {L0, L1, L2, . . .} is ﬁnite. (b) {L0, L1, L2, . . .} cannot be uniformly strong-monotonically learnable.
As in the case of uniform conservative learning, there also exists an inﬁnite class which is uniformly monotonically learnable.
Example 18. Let La = {a}. Then {L0, L1, L2, . . .} is an inﬁnite class which is uniformly monotonically learnable.
The following result shows that in fact it is necessary for a class to contain only ﬁnite sets in order to be uniformly monotonically learnable.
Theorem 19. If {L0, L1, L2, . . .} is uniformly monotonically learnable, then {L0, L1, L2, . . .} contains only ﬁnite sets.
For prescribed monotonic learning, ﬁnitely many sets in the language class can violate the above necessary condition for uniform monotonic learning.
Theorem 20. If {L0, L1, L2, . . .} is prescribed monotonically learnable, then {L0, L1, L2, . . .} contains only ﬁnitely many inﬁnite sets.
Theorem 21. (a) There exists a class {L0, L1, L2, . . .} which is class-preserving-uniformly strong-monotonically learnable but not prescribed monotonically learnable. (b) There exists a class {L0, L1, L2, . . .} which is prescribed monotonically learnable but not class-preserving-uniformly monotonically learnable.

194 Sanjay Jain, Frank Stephan, and Nan Ye
(c) Every prescribed strong-monotonically learnable class is also class-preservinguniformly strong-monotonically learnable.
Acknowledgements: We thank the anonymous referees for helpful comments.
References
1. Dana Angluin. Inductive inference of formal languages from positive data. Information and Control, 45:117–135, 1980.
2. Lenore Blum and Manuel Blum. Toward a mathematical theory of inductive inference. Information and Control, 28:125–155, 1975.
3. Ganesh Baliga, John Case, Wolfgang Merkle, Frank Stephan and Rolf Wiehagen. When unlearning helps. Information and Computation, to appear.
4. E. Mark Gold. Language identiﬁcation in the limit. Information and Control, 10:447–474, 1967.
5. Klaus-Peter Jantke. Monotonic and Non-monotonic Inductive Inference. New Generation Computing, 8:349–360, 1991.
6. Sanjay Jain, Frank Stephan and Nan Ye. Prescribed Learning of R.E. Classes. In M. Hutter, R. Servedio and E. Takimoto, editors, Algorithmic Learning Theory, 18th International Conference, ALT’ 07, Sendai, Japan, October 2007, pages 64– 78. Lecture Notes in Artiﬁcial Intelligence 4754. Springer Verlag, 2007.
7. Steﬀen Lange. Algorithmic Learning of Recursive Languages. Habilitationsschrift, der Fakult¨at fu¨r Mathematik und Informatik der Universit¨at Leipzig eingereichte, Mensch and Buch Verlag, Berlin, 2000.
8. Steﬀen Lange and Thomas Zeugmann. Language learning in dependence on the space of hypotheses. Proceedings of the Sixth Annual Conference on Computational Learning Theory, Santa Cruz, California, United States, pages 127–136, 1993.
9. Steﬀen Lange, Thomas Zeugmann and Shyam Kapur. Monotonic and dual monotonic language learning. Theoretical Computer Science, 155:365–410, 1996.
10. Daniel Osherson, Michael Stob and Scott Weinstein. Systems That Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. Bradford — The MIT Press, Cambridge, Massachusetts, 1986.
11. Emil Post. Recursively enumerable sets of positive integers and their decision problems, Bulletin of the American Mathematical Society, 50:284–316, 1944.
12. Thomas Zeugmann. Algorithmisches Lernen von Funktionen und Sprachen. Habilitationsschrift, Technische Hochschule Darmstadt, 1993.
13. Thomas Zeugmann and Steﬀen Lange. A guided tour across the boundaries of learning recursive languages. Algorithmic Learning for Knowledge-Based Systems, ﬁnal report on research project Gosler, edited by Klaus P. Jantke and Steﬀen Lange, Springer Lecture Notes in Artiﬁcial Intelligence 961:193–262, 1995.
14. Thomas Zeugmann, Steﬀen Lange and Shyam Kapur. Characterizations of monotonic and dual monotonic language learning, Information and Computation, 120:155–173, 1995.
15. Sandra Zilles. Separation of uniform learning classes. Theoretical Computer Science, 313:229–265, 2004.
16. Sandra Zilles. Increasing the power of uniform inductive learners. Journal of Computer and System Sciences, 70:510–538, 2005.

Lower Bounds for Syntactically Multilinear Algebraic Branching Programs
Maurice J. Jansen
Centre for Theory in Natural Science Department of Computer Science, University of Aarhus IT-Parken, Aabogade 34, DK-8200 Aarhus N, Denmark.
mjjansen@daimi.au.dk
Abstract. It is shown that any weakly-skew circuit can be converted into a skew circuit with constant factor overhead, while preserving either syntactic or semantic multilinearity. This leads to considering syntactically multilinear algebraic branching programs (ABPs), which are deﬁned by a natural read-once property. A 2n/4 size lower bound is proven for ordered syntactically multilinear ABPs computing an explicitly constructed multilinear polynomial in 2n variables. Without the ordering restriction a lower bound of level Ω(n3/2/ log n) is observed, by considering a generalization of a hypercube covering problem by Galvin [1].
Key words: Computational complexity, arithmetical circuits, lower bounds, multilinear polynomials, algebraic branching programs.
1 Introduction
It is not known whether polynomial size arithmetical circuits (VP) are computationally more powerful than polynomial size arithmetical formulas (VPe). For the former, we have a surprising construction by Valiant, Skyum, Berkowitz and Rackoﬀ, which shows that VP = VNC2 [2]. For the latter, we know by a result of Brent that VPe = VNC1 [3]. Recently, Raz made a breakthrough by showing that polynomial size multilinear circuits are strictly more powerful than polynomial size multilinear formulas. Raz proved that
Theorem 1 ([4]). s-mlin-VNC1 = s-mlin-VNC2.
Here “s-mlin-” denotes syntactic multilinearity. Technically, multilinearity comes in two ﬂavors: syntactic and semantic (See Section 2). For formulas these two notions are equivalent, but this is not known to be true for circuits.
Intermediate between circuits and formulas we have so-called skew and weakly-skew circuits (See [5]). Letting VPs and VPws stand for the classes of pfamilies of polynomials that have skew circuits or weakly-skew circuits of polynomial size, respectively, Malod an Portier prove that VPs = VPws. The situation can be summarized as follows:
Theorem 2 ([5, 2]). VNC1 ⊆ VPs = VPws ⊆ VNC2 = VP.

196 Maurice J. Jansen
A priori it is not clear whether the equality VPs = VPws holds up when passing to the multilinear variants of these classes, as the proofs in [5] appeal to the completeness of the determinant or trace of iterated matrix multiplication for the class VPws. For the determinant, currently no polynomial size multilinear circuits are known. Furthermore, multilinearity is not necessarily preserved under Valiant projections.
1.1 Results
It will be demonstrated one can convert any weakly-skew circuit into a skewcircuit with constant factor overhead, while maintaining either syntactic or semantic multilinearity. Further, it will be observed that the conversion without multilinearity restrictions can be done with constant factor overhead as well1. Note, that the conversions indicated in [5] suﬀers an at least cubic blow-up in size, as an appeal is made to either polynomial size skew circuits for the determinant [6], or for the trace of iterated matrix multiplication. As a consequence one obtains that Theorem 3.
s-mlin-VNC1 ⊆ s-mlin-VPs = s-mlin-VPws ⊆ s-mlin-VNC2 = s-mlin-VP.
In the above, the rightmost equality was proven in [7]. Looking at Theorem 3, the question is raised whether perhaps the techniques used to prove Theorem 1 can be strengthened to show that s-mlin-VPs = s-mlin-VNC2, or that we can prove s-mlin-VNC1 = s-mlin-VPs, by showing, in the terminology of [7, 8], some full rank polynomial has polynomial size skew circuits.
As a skew circuit can be transformed into an algebraic branching program (ABP, See [9]) with relatively little overhead, we turn to algebraic branching programs to investigate the above questions. If the initial skew circuit is syntactically multilinear, this results in an ABP B which is syntactically multilinear in the following natural sense: on any directed path in B, any variable can appear at most once. This can be thought of as the algebraic analog of a Boolean read-once branching program. In the latter model we know of tight exponential lower bounds [10]. Also exponential lower bounds are known for ABPs in the non-commutative case [9]. Bryant introduced so-called ordered binary decision diagrams (OBDDs), for which he proved exponential lower bounds [11]. These are read-once Boolean branching programs in which variables are restricted to appear in the same order on all directed paths. This restriction can naturally also be considered for ABPs, leading to the following result:
Theorem 4. Let X be a set of 2n variables. For any ﬁeld F , there exists an extension ﬁeld G of F and explicitly constructed multilinear polynomial f ∈ G[X], such that any ordered algebraic branching program over X and G computing f has at least 2n/4 nodes.
1 This observation has simultaneously been made by Kaltofen and Koiran in an unpublished paper, which was unknown to the author at the time of this research. They do not consider the preservation of multilinearity.

Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 197

Finally, the problem of proving lower bounds for unrestricted ABPs and

(unordered) multilinear ABPs is explored. For any ﬁxed constant 0√< α < 1, it will be shown that any unrestricted ABP requires size Ω(n3/2α 1 − α) to

compute the elementary symmetric polynomial of degree αn in n variables.

Next a relation between proving lower bounds for multilinear ABPs and the

generalization of a hypercube covering problem by Galvin will be established

[1]. By straightforward counting this yields a lower bound for multilinear ABPs

of

level

Ω

(

n3/2 log n

),

for

computing

any

full

rank

polynomial.

Potentially

however,

this technique yields up to quadratic lower bounds, provided linear lower bounds

can be proven for certain generalizations of Galvin’s Problem.

2 Preliminaries
For non-negative integer n, [n] denotes the set {1, 2, . . . , n}. Let F be a ﬁeld and X = {x1, x2, . . . , xn} be a set of variables. An arithmetical circuit Φ over F and X is a directed acyclic graph with nodes of in-degree zero or two. Nodes with indegree zero are called inputs and are labeled by variables or ﬁeld elements. Nodes with in-degree two are called gates and have labels ∈ {×, +}. For each gate g in Φ, one has associated the polynomial computed by g, denoted by Φg, which is deﬁned in the obvious manner. Also let Φg stand for the subcircuit rooted at gate g. It will be made clear from the context which meaning is intended. Denote by Xg the set of variables used by the subcircuit Φg. The size of Φ, denoted by |Φ|, is taken to be the number gates. If the underlying graph of Φ is a tree, Φ is called a formula. For a polynomial f , C(f ) and L(f ) denote the smallest size of a circuit or formula, respectively, computing f .
An arithmetical circuit Φ is called weakly-skew if at every multiplication gate g with inputs g1 and g2, one of Φg1 and Φg2 is disjoint from the rest of Φ. Φ is called skew if for each multiplication gate at least one g1 and g2 is an input gate (See [5]). For a polynomial f , Cws(f ) and Cs(f ) denote the smallest size of a weakly-skew or skew circuit computing f , respectively.
A polynomial f is called multilinear if for any monomial of f , every variable has degree at most one. A circuit Φ is semantically multilinear if every polynomial computed at any gate of Φ is multilinear. Φ is called syntactically multilinear if for each multiplication gate g with inputs g1 and g2, Xg1 and Xg2 are disjoint. For a polynomial f , Csyn(f ) and Csem(f ) denote syntactic and semantic multilinear circuit size, respectively. Similarly, Lsyn(f ) and Lsem(f ) denote syntactic and semantic multilinear formula size. These deﬁnitions will be combined in the obvious manner. For example, Csyn,ws(f ) denotes the smallest size of a syntactically multilinear weakly-skew circuit computing f . Standard notation for arithmetical circuit classes will be followed (See e.g. [12]). For any class C ∈ {VNC1, VNC2, VPs, VPws, VP}, let syn-mlin-C and mlin-C stand for the class of p-families of polynomials that have C-circuits which additionally are required to be syntactically or semantically multilinear, respectively.
Deﬁnition 1 (See [9]). An algebraic branching program (ABP) over a ﬁeld F and a set of variables X is a 4-tuple B = (G, w, s, t), where G = (V, E) is a

198 Maurice J. Jansen

directed acyclic graph for which V can be partitioned into levels L0, L1, . . . , Ld, where L0 = {s} and Ld = {t}. Vertices s and t are called the source and sink of
B, respectively. Edges may only go between consecutive levels Li and Li+1. The weight function w : E → F [X] assigns homogeneous linear forms to the edges of
G. For a path p in G, we extend the weight function by w(p) = e∈p w(e). For i, j ∈ V , let Pi,j be the collection of all path in G from i to j. The program B computes the polynomial p∈Ps,t w(p). The size of B is taken to be |V |.

For a linear form L(x) =

n i=1

cixi,

deﬁne

coef[L, xi]

=

ci.

An

ABP

B

is

called

syntactically multilinear if for any directed path p in B, for all i, there is at most

one edge e on p with coef[w(e), xi] = 0. B(f ) denotes the smallest size of an ABP

computing f , and Bsyn(f ) denotes such size with the additional restriction of

syntactic multilinearity.

3 Circuit Transformations
It is convenient to work with the following data structure: a skew schedule is a directed acyclic graph G with weights on the edges ∈ F ∪ X, where the outdegree of a vertex is either zero, one or two, and where for any vertex v with distinct edges e1 = (v, w) and e2 = (v, u), the labels of e1 and e2 equal 1. For a directed acyclic graph G with node s ∈ V [G], a path p in G is called a maximal path with starting point s, if the ﬁrst vertex of p is s and the last vertex of p has no outgoing edges.
Lemma 1. For any polynomial f , Csyn,s(f ) ≤ 5Csyn,ws(f ).
Proof. First process Φ so that any addition gate has its input coming in from diﬀerent gates by inserting dummy addition gates. This at most doubles the size. Let e be the new size. Let g1, g2, . . . , ge be a topological sort of the gates of Φ, where wlog. we assume Φge = f , and that ge is the only gate with out-degree zero. Let g−m+1, g−m+2, . . . , g0 be the set of inputs of Φ. Sequentially for stages k = 1, 2, . . . , e , we construct a skew schedule Gk from Gk−1. To initialize, let G0 consists of m distinct directed edges. For each input g of Φ, we select a unique edge among E[G0] and put the label of g on it. Let B be the set of vertices in G0 with out-degree zero. The set B will remain as a subset of vertices in each Gk. We will never change the in-degree of vertices in B. At the beginning of stage k, the skew schedule Gk−1 will satisfy:
1. Each node g = gk with k < k will correspond one-to-one with a vertex vg ∈ V [Gk−1]\B.
2. Let Gk−1 be the set of nodes gk with k < k, that are used by gates gk for some k ≥ k. For any g ∈ Gk−1, p∈P e∈p w(e) = Φg, where P is the collection of all maximal paths with starting point vg in Gk−1,
3. On any directed path in Gk−1 no variable appears more than once, 4. For any node g = gk with k < k, the set of nodes not in B reachable in
Gk−1 from vg, is precisely {vg : g ∈ Φg}.

Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 199
At the beginning of stage k = 1, we have that G0 is the set of all input gates. For each input gate g, vg is deﬁned to be the starting vertex of the unique edge we have selected for it. Properties (1)-(4) can now be veriﬁed to hold. At stage
k we do the following:
Case I: gk = +(gi, gj). We have that gi, gj ∈ Gk−1. We construct Gk from Gk−1 by adding one new vertex w with edges of weight 1 from w to vgi and vgj . No parallel edges are created since i = j. Let us verify the needed properties. Property (3) is clear. It is also clear that if we let P we the collection of all
maximal paths starting in w that p∈P e∈p w(e) = Φgk . If we are at the last iteration, i.e. k = e , then this is all we are required to verify. Otherwise, gk will be used later on, i.e. gk ∈ Gk. Observe that Gk = Gk−1 ∪ {gk} − S, where S ⊆ {gi, gj}. We deﬁne vgk = w. By our above observation for the vertex w, and the fact that we do not modify connectivity for the other vertices, Property (2)
holds. Property (4) is clear.
Case II: gk = ×(gi, gj). Wlog. assume Φgj is disjoint from the rest of Φ. We have that gi, gj ∈ Gk−1. For s ∈ {i, j}, let Ws be the set of vertices in Gk−1 reachable from vgs . Wi and Wj are disjoint. Namely, suppose w ∈ Wi ∩ Wj. If w ∈/ B then Property (4) implies there exists a shared node in Φgi and Φgj , which is a contradiction. In case w ∈ B, then since we do not add edges into w, we have a vertex w = vg for some input gate g with w ∈ Wi ∩ Wj. Hence we again have a contradiction.
Let E ⊆ Wj be the set of vertices reachable from vgj with out-degree zero. We deﬁne Gk to be the graph Gk−1 modiﬁed by adding an edge (v, vgi ) of weight 1 for each v ∈ E. We add2 a new vertex w and edge (w, vgj ) with weight 1, and let vgk = w. Since Wi ∩Wj = ∅, no vertex from E is reachable from vgi . Hence Gk is an acyclic graph. Observe Gk is a skew schedule. We will now verify Properties (1)-(4).
Let P be the set of maximal paths starting in vgk in Gk. Let Ps be the set of maximal paths in Gk−1 starting in vgs , for s ∈ {i, j}. For p ∈ Pi and q ∈ Pj, let q#p denote the path in Gk that is (vgk , vgj ), followed by q, followed by the edge with weight 1 into vgi , followed by p. We have that P = {q#p : q ∈ Pj, p ∈ Pi}. This means r∈P e∈r w(e) = p∈Pi e∈p w(e) · q∈Pj e∈q w(e) = Φgi · Φgj = Φgk . In case this was the last iteration, i.e. k = e , this is all we need together with Property (3) to be veriﬁed below. Otherwise, since gk will be used later again, gk ∈ Gk. Observe that Gk = Gk−1 − S ∪ {gk}, where S is the set of nodes in Φgj .
By what we observed above, Property (2) holds for gk. For g = gk in Gk, the only way Property (2) can be disturbed is if some vertex w ∈ E is reachable from vg in Gk−1. This means some w ∈/ B is reachable from both vg and vgj in Gk−1, but then Φg and Φgj share a vertex by ’previous’ Property (4). Since Φ is weakly-skew, g must be a node in Φgj , but that is a contradiction since g ∈ Gk.
To check Property (3), we note that the only edges with variables are of the form (v, b) with b ∈ B and v = vg, for some input g ∈ Φ. Property (3) can be violated only, if in Gk−1 we have for such (v, b) that some vertex in E can be
2 This is not strictly necessary, but we do so to simplify the proof.

200 Maurice J. Jansen
reached from b, and that in Gk−1 there exists a path starting in vgi going over an edge (v , b ) with b ∈ B and v = vg , for some input gate g , that has the same variable label as (v, b). This means that g ∈ Φgi . Similar as above, by Property (4), it must be that g ∈ Φvgj . By syntactic multilinearity, we conclude the labels of (v, b) and (v , b ) must be diﬀerent. Property (4) clearly holds.
This completes the description the graphs G1, G2, . . . , Ge . Ge is a skew schedule of size at most 2m + e ≤ 5e . We can evaluate it node by node in a bottom-up fashion. This yields a syntactically multilinear skew circuit computing f of size at most 5e ≤ 10e gates. To optimize the constant to be 5 instead of 10, we observe adding dummy addition gates is not required.
Lemma 2. For any polynomial f , Csem,s(f ) ≤ 5Csem,ws(f ).
Proof. Modify the multiplication case in the proof of Lemma 1 as follows. For each variable xi appearing in the polynomial Φgi , replace any edge weight xi in the induced subgraph Gk−1[Wj] by zero. This does not alter the polynomial represented at vertex vgj , since it cannot contain variable xi. Polynomials represented at other vertices in Gk[Wj] can have changed, but cannot be used at later stages. The substitution has forced all these polynomials to be multilinear.
Removing Property (3) in the proof of Lemma 1 immediately yields a proof that for any polynomial f , Cs(f ) ≤ 5Cws(f ). We put together the basic facts about the measures that are being considered.
Lemma 3. For a homogeneous polynomial of degree d,
1. C(f ) ≤ Cws(f ) ≤ L(f ) and Csyn(f ) ≤ Csyn,ws(f ) ≤ Lsyn(f ). 2. Cws(f ) ≤ Cs(f ) ≤ 5Cws(f ) and Csyn,ws(f ) ≤ Csyn,s(f ) ≤ 5Csyn,ws(f ). 3. B(f ) ≤ d · (4Cs(f ) + 2) and Bsyn(f ) ≤ d · (4Csyn,s(f ) + 2). 4. α Cs(f )/n ≤ B(f ) and α Csyn,s(f )/n ≤ Bsyn(f ), for some constant
α > 0.
4 Ordered ABPs
Let B = (G, w, s, t) be an ABP over a ﬁeld F and set of variables X = {x1, x2, . . . , xn}. Say a directed path p from s to t respects a permutation π : [n] → [n], if whenever an edge e1 appears before an edge e2 on p and coef[w(e1), xπ(i)] = 0 and coef[w(e2), xπ(j)] = 0, one has that i < j. B is called ordered, if there exists a permutation π that is respected by all directed s, t-paths. For a polynomial f , we denote ordered ABP size by Bord(f ). We ﬁrst observe that lower bounds for non-commutative ABPs of Nisan [9] transfer to the ordered model. This follows by evaluating the ordered ABP over non-commutative variables. For example, one obtains that
Theorem 5. Any ordered algebraic branching program B = (G, w, s, t) computing the permanent or determinant of an n × n matrix of variables has size at least 2n.

Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 201
√
The above bound for the permanent and determinant is of level 2Ω( N), where N is the number of variables. In [9] a bound of 2Ω(N) is proven for the non-commutative model, but this is for a polynomial that is not multilinear. In order to obtain a bound of level 2Ω(N), we now turn to the aforementioned full rank polynomials [4, 7, 8].
Let X = {x1, x2, . . . , x2n}, Y = {y1, y2, . . . , yn} and Z = {z1, z2, . . . , zn} be sets of variables. Following [4, 7, 8], for a multilinear polynomial g ∈ F [Y, Z], we deﬁne the 2n × 2n partial derivatives matrix Mg, by taking Mg(m1, m2) = coeﬃcient of monomial m1m2 in g, where m1 and m2 range over all multilinear monic monomials in Y and Z variables, respectively. A partition of X into Y and Z is any bijection A : X → Y ∪ Z. For f ∈ F [X] denote by f A the polynomial obtained from f by substitution of xi by A(xi), for all i ∈ [2n]. f is of full rank if for every partition A, rank MfA = 2n. For multilinear g ∈ F [Y, Z], let Yg and Zg be Y and Z variables appearing in g.
Proposition 1 ([8]). Let g, g1, g2 ∈ F [Y, Z] be multilinear polynomials. Then
1. rank Mg ≤ 2min(|Yg|,|Zg|), 2. rank Mg1+g2 ≤ rank Mg1 + rank Mg2 , and 3. rank Mg1·g2 = rank Mg1 ·rank Mg2 , provided Yg1 ∩Yg2 = ∅ and Zg1 ∩Zg2 = ∅.
We obtain the following lower bound:
Theorem 6. Let X be a set of 2n variables, and let F be a ﬁeld. For any full rank homogeneous polynomial f of degree n over X and F , Bord(f ) ≥ 2n/4.
A proof of above theorem will appear in the full version of the paper. To indicate the idea, suppose that B respects the permutation π : [2n] → [2n]. In this case consider any partition A that assign all n y-variables to {xπ(1), xπ(2), . . . , xπ(n)} and all n z-variables to {xπ(n+1), xπ(n+2), . . . , xπ(2n)}. Applying this partition to B, we obtain an ABP computing f A. Partition A has been selected to frustrate “progress” of the rank measure rank MgA , for gates g of B. Regardless of that, the ﬁnal output f A must still have rank MfA = 2n. Using Proposition 1, it is then possible to argue this implies the middle level Ln/2 must of size at least 2n/4.
We need to modify the construction of a full rank polynomial from [7] to yield a homogeneous full rank polynomial. This can be done, provided we work over a suitable extension ﬁeld of the underlying ﬁeld F .
Let W = {ωi,l,j}i,l,j∈[2n] be sets of variables. For each interval [i, j] ⊆ 2n of even length, we deﬁne a polynomial fi,j ∈ F [X, W] inductively as follows: if |[i, j]| = 0, then deﬁne fi,j = 1. If |[i, j]| > 0, deﬁne fi,j = (xi + xj)fi+1,j−1 +
l ωi,l,jfi,lfl+1,j, where we sum over all l such that |[i, l]| is even. Finally, f is deﬁned to be f1,2n. It follows inductively that fi,j is multilinear and homogeneous of degree |[i, j]|/2 in the X-variables. It can be veriﬁed that f is indeed a full rank polynomial when considered as a polynomial in G[X], where G = F (W) is the ﬁeld of rational functions over the ﬁeld F and variables W. Theorem 4 then follows from Theorem 6. More details will appear in the full version.

202 Maurice J. Jansen

5 Unrestricted and Multilinear ABPs

Consider the following observation by Kristoﬀer Hansen: if for an ABP B the
number of edges between any two consecutive levels Ld and Ld+1 is at most K < n, then the polynomial f (x1, x2, . . . , xn) computed by B vanishes on a linear space of dimension at least n − K.√Provided this is a contradiction for f , one concludes that max(|Ld|, |Ld+1|) ≥ K. For example, for the elementary symmetric polynomial of degree d in n variables deﬁned by S⊂[n],|S|=d i∈S xi, using Theorem 1.1 in [13], this yields the following theorem:

Theorem 7. Let α be a constant with 0 < α < 1, and assume αn is integer. Over ﬁelds of characteristic zero, for the elementary symmet√ric polynomial Snαn of degree αn is n variables, it holds that B(Snαn) = Ω(n3/2α 1 − α).

Above argument is limited to yield O(n3/2) lower bounds, for polynomials of

degree Θ(n). In order to overcome this, we turn to a hypercube covering problem.

Consider H2n = {−1, 1}2n and B2n = {0, 1}2n over the real numbers. Deﬁne

inner product x · y =

2n i=1

xiyi.

Let

1

denote

the

vector

12n.

Let

H2en

=

{x

∈

H2n : x · 1 = 0}. Of interest are minimal size coverings of H2en by hyperplanes,

where coeﬃcients for the deﬁning equations are taken from particular subsets of

B2n. More precisely, for W ⊂ [2n], deﬁne B2Wn = {b ∈ B2n : wt(b) ∈ W }, where wt(b) = b · 1. Deﬁne

Q(n, W, d) = min{|E| : E ⊆ B2Wn, (∀x ∈ H2en), (∃e ∈ E), |x · e| ≤ d}.

Finding the value of Q(n, W, d) becomes interesting only for certain weight sets W . For example, if 2n ∈ W , then E = {1} trivially covers all of H2en. The discrepancy parameter d should also be small w.r.t. min(W ), e.g. trivially
Q(n, W, min(W )) = 1. Also, in case W does not contain an even number and
d = 0, we have that Q(n, W, d) is ill-deﬁned. In all other cases Q(n, W, d) is welldeﬁned. Namely, say 2 ∈ W . By taking E to be all 2n cyclic shifts of 12 02n−2 , we have that for each x ∈ H2en, there must exist some e ∈ E, x · e = 0. Similarly, this set E works, in case W contains no even numbers, but d ≥ 1. The crucial
question is whether for cases that avoid trivialities, one can do signiﬁcantly better than |E| being linear in n.
The special case of ﬁnding m(k) := Q(2k, {2k}, 0) is a problem posed origi-
nally by Galvin (See [14]). For an upper bound, note one requires only half of all 4k cyclic shifts, i.e. m(k) ≤ 2k. For odd k, Frankl and R¨odl proved the linear
lower bound m(k) > k, for ﬁxed > 0 [1]. The proof relies on a strong result in
extremal set theory they proved, which resolved a $250 problem of Erd˝os. Later the bound was improved to m(k) ≥ 2k, for odd k [15].
Consider Q(n, [ 0n, (1 + 1)n], 2 log n), for ﬁxed 0 < 0 < 1 < 1. From
Theorem 8 below it will follow, that proving an Ω(n) lower bounds on this quantity would yield an Ω(n2) multilinear ABP lower bound. In light of the
result by Frankl and R¨odl such a linear lower bound appears plausible. Also
note the linear lower bound by Alon et al. for covering the entire hypercube, in

Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 203

case the deﬁning equations have coeﬃcients in {−1, 1} instead of in {0, 1} [14].
They deﬁne for n ≡ d (mod 2), K(n, d) = min{|E| : E ⊆ Hn, (∀x ∈ Hn), (∃e ∈ E), |x · e| ≤ d}, and prove K(n, d) = n/(d + 1) .

Theorem 8. Let X be a set of 2n variables, and let F be a ﬁeld. For any

full rank homogeneous polynomial f of degree n over X and F , Bsyn(f ) =

Ω

n−1 r=1

min(n,

Q(n,

[r,

n

+

r],

2

log

n))

.

Proof. Let B = (G, w, s, t) be a multilinear ABP computing f . Let

L0, L1, . . . , Ln be the levels of B. For v, w ∈ V [G], let fv,w denote the poly-

nomial computed by the subprogram of B with source v and sink w. Let Xv,w

denote the set of all variables appearing on directed paths from v to w. Consider

r such that 0 < r < n. Write f = fs,t = v∈Lr fs,vfv,t. By syntactic multilinearity, we have that |Xs,v| ≥ r and |Xv,t| ≥ n − r. The
latter implies |Xs,v| ≤ n + r, again by syntactic multilinearity. Let χ(Xs,v) ∈ B2[rn,n+r] denote the characteristic vector of Xs,v.
Suppose that |Lr| < Q(n, [r, n + r], 2 log n). Then there exists γ ∈ H2en such that for every b ∈ {χ(Xs,v) : v ∈ Lr}, |γ · b| > 2 log n. Let A : X → Y ∪ Z be any
partition that assigns a Y variable to xi, if γi = 1, and a Z variable otherwise, for all i ∈ [2n].

Let B be the branching program obtained from B by substituting according

to A. For nodes v and w, we let Yv,w and Zv,w denote the sets of y and z variables, respectively, appearing on paths from v to w in B . Then for any

v ∈ Lr, min(Ys,v, Zs,v) ≤ (Ys,v + Zs,v)/2 − log n ≤ |Xs,v|/2 − log n. Hence, by
Item 1 of Proposition 1, we have that rank MfsA,v ≤ 2|Xs,v|/2−log n. By syntactic multilinearity, none of the variables appearing on paths from s to v can appear

on paths from v to t. So |Xv,t| ≤ 2n − |Xs,v|. By Item 1 of Proposition 1 we

get rank MfvA,t ≤ 2n−|Xs,v|/2. Using multiplicativity (Proposition 1, Item 3), we

conclude rank MfsA,vfvA,t ≤ 2n−log n, and thus using subadditivity (Proposition 1,

Item 2) and since f = of full rank, rank MfA

=

v2∈nL. rWfse,vcfovn,tcltuhdaet

rank Mf that |Lr|

A
≥

≤ |Lr n.

|2n−log

n

.

Since

f

is

Counting yields that Q(n, [r, n + r], d) = Ω((d + 1)−1

r(n−r) 2n

).

Applying

Theorem 8, and summing for r ∈ [ 0n, 1n], for ﬁxed 0 < 0 < 1 < 1 yields

Theorem 9. Let X be a set of 2n variables. For any ﬁeld F , there exists an ex-

tension ﬁeld G of F and explicitly constructed multilinear polynomial f ∈ G[X],

such that a√ny multilinear algebraic branching program over X and G computing

f

has

Ω

(

n log

n n

)

nodes.

6 Conclusions
It should be noted the ABP-model can be quite powerful. The prime example being that, given two n × n matrices X and Y of variables, one ca√n compute f = i,j∈[n](XY )ij with a syntactically multilinear ABP with O( N ) many

204 Maurice J. Jansen

nodes, where N = 2n2 is the number of input variables. This is an example of

a polynomial, for which the multilinear ABP-model is at least “quadratically

more eﬃcient” than the syntactic multilinear circuit model. In the latter model,

the

currently

best

know

lower

bound

for

an

explicit

function

is

of

level

Ω

(

n4/3 log2 n

)

[8].

Theorem

8

supplies

a

lower

bound

strategy,

which

yielded

an

Ω

(

n3/2 log n

)

lower

bound for multilinear ABPs. By resolving a certain generalization of Galvin’s

Problem this method can yield an Ω˜(n2) lower bound for syntactically multilin-

ear ABPs.

Acknowledgments I thank Peter Bro Miltersen, Kristoﬀer Hansen and Oded Lachish for helpful discussions.

References
1. P. Frankl and V. R¨odl. Forbidden intersections. Trans. Amer. Math. Soc., 300(1):259–286, 1987.
2. L. Valiant, S. Skyum, S. Berkowitz, and C. Rackoﬀ. Fast parallel computation of polynomials using few processors. SIAM J. Comput., 12:641–644, 1983.
3. R. Brent. The parallel evaluation of general arithmetic expressions. J. Assn. Comp. Mach., 21:201–206, 1974.
4. R. Raz. Separation of multilinear circuit and formula size. Theory of Computing, 2(6):121–135, 2006.
5. Guillaume Malod and Natacha Portier. Characterizing Valiant’s algebraic complexity classes. In MFCS, pages 704–716, 2006.
6. S. Berkowitz. On computing the determinant in small parallel time using a small number of processors. Inf. Proc. Lett., 18:147–150, 1984.
7. R. Raz and A. Yehudayoﬀ. Balancing syntactically multilinear arithmetical circuits. Journal of Computational Complexity (to appear).
8. R. Raz, A. Shpilka, and A. Yehudayoﬀ. A lower bound for the size of syntactically multilinear arithmetic circuits. In Proc. 48th.Annual IEEE Symposium on Foundations of Computer Science, pages 438–448, 2007.
9. N. Nisan. Lower bounds for non-commutative computation: extended abstract. In Proc. 23rd Annual ACM Symposium on the Theory of Computing, pages 410–418, 1991.
10. A. Andreev, J. Baskov, E. Clementi, and R. Rolim. Small pseudo-random sets yield hard functions: new tight lower bounds for branching programs. In Proc. 26th Annual International Conference on Automata, Languages, and Programming, volume 1644 of Lect. Notes in Comp. Sci., pages 179–189, 1999.
11. R.E. Bryant. On the complexity of vlsi implementations and graph representations of boolean functions with application to integer multiplication. IEEE Trans. Computers, 40(2):205–213, 1991.
12. P. Bu¨rgisser, M. Claussen, and M.A. Shokrollahi. Algebraic Complexity Theory. Springer Verlag, 1997.
13. A. Shpilka. Aﬃne projections of symmetric polynomials. In Proc. 16th Annual IEEE Conference on Computational Complexity, pages 160–171, 2001.
14. N. Alon, E. Bergmann, D. Coppersmith, and A. Odlyzko. Balancing sets of vectors, 1988.
15. H. Enomoto, P. Frankl, N. Ito, and K. Nomura. Codes with given distances. Graphs and Combinatorics, 3:25–38, 1987.

TheDUesceisoiofnInTforeremLateiaornniAngannitdy EinvaPluoasstiiboinlistic
Ilyes Jenhani1, Salem Benferhat2, and Zied Elouedi1
1 LARODEC, Institut Supérieur de Gestion de Tunis, Tunisia. 2 CRIL, Université d'Artois, Lens, France.
ilyes.j@lycos.com, benferhat@cril.univ-artois.fr, zied.elouedi@gmx.fr
Abstract. This paper investigates the issue of building decision trees from data with imprecise class values where imprecision is encoded in the form of possibility distributions. The Information Anity similarity measure is introduced into the well-known gain ratio criterion in order to assess the homogeneity of a set of possibility distributions representing instances's classes belonging to a given training partition. For the experimental study, we proposed an information anity based performance criterion which we have used in order to show the performance of the approach on well-known benchmarks.
1 Introduction
Machine learning and data mining researches have rapidly emerged in the last decade. Especially, classication is considered as one of the most successful branches of Articial Intelligence and it is playing a more and more important role in real-world applications.
Classication tasks are ensured by several approaches such as: discriminant analysis, articial neural networks, k-nearest neighbors, Bayesian networks, decision trees. etc. The latter, namely, decision trees, is considered as one of the most popular classication techniques. They are able to represent knowledge in a exible and easy form which justies their use in decision support systems, intrusion detection systems, medical diagnosis, etc.
For many real-world problems and particulary for classication problems, imprecision is often inherent in modeling these applications and should be considered when building classiers. For example, for some instances, an expert or a sensor may be unable to give the exact class value: an expert in ballistics in the scientic police who is unable to provide the exact type of a gun used in a crime, a mechanic who is unable to provide the exact fault of an engine, a doctor who cannot specify the exact disease of a patient, etc.
Hence, in these dierent examples, the expert can provide imprecise classications expressed in the form of a ranking on the possible classes. Obviously, rejecting these pieces of information in a learning process is not a good practice. A suitable theory dealing with such situations is possibility theory which is a non-classical theory of uncertainty proposed by [12] and developed by [4].

206 Ilyes Jenhani et al.

In this paper, we propose a new decision tree approach that allows the induction of decision trees from imprecisely labeled instances, i.e., whose class labels are given in the form of possibility distributions. We introduced the concept of similarity into the attribute selection step of the proposed approach.
It is important to mention that existing possibilistic decision trees do not deal with uncertainty in classes, except, the work we have proposed in [7] using the concept of non-specicity in building possibilistic decision trees. A work proposed by Borgelt and al. [2] deals with crisp (standard) training sets: the authors encode the frequency distributions as possibility distributions (an interpretation which is based on the context model of possibility theory [2]) in order to dene a possibilistic attribute selection measure. The possibilistic decision tree approach proposed by Hüllermeier [5] uses a possibilistic branching within the lazy decision tree technique. Again, this work does not deal with any uncertainty in the classes of the training objects. A work by Ben Amor et al. [1] have dealt with the classication of objects having possibilistic uncertain attribute values within the decision tree technique.
This paper is organized as follows. Section 2 gives necessary background on possibility theory. Section 3 describes some basics of decision trees. Then, in Section 4, we present our approach, so-called A-PDT. Section 5 presents and analyzes experimental results carried out on modied versions of commonly used data sets from the U.C.I. repository [9]. Finally, Section 6 concludes the paper.

2 Possibility Theory
Possibility distribution
Given a universe of discourse Ω = {ω1, ω2, ..., ωn}, a fundamental concept of possibility theory is the possibility distribution denoted by π. π corresponds to a function which associates to each element ωi from the universe of discourse Ω a value from a possibilistic scale [0,1]. This value is called a possibility degree: it encodes our knowledge on the real world.
By convention, π(ωi) = 1 means that it is fully possible that ωi is the real
world, π(ωi) = 0 means that ωi cannot be the real world (is impossible). Flexi-
bility is modeled by allowing to give a possibility degree from ]0,1[. In possibility
theory, extreme cases of knowledge are given by: -Complete knowledge: ∃ωi, π(ωi) = 1 and ∀ ωj = ωi, π(ωj) = 0.
Nor-mToatlaizl aigtnioonrance: ∀ ωi ∈ Ω, π(ωi) = 1 (all values in Ω are possible).
A possibility distribution π is said to be normalized if there exists at least one state ωi ∈ Ω which is totally possible. In the case of sub-normalized π,

Inc(π) = 1 − max{π(ω)}
ω∈Ω

(1)

is called the inconsistency degree of π. It is clear that, for normalized π, maxω∈Ω{π(ω)} = 1, hence Inc(π)=0. The measure Inc is very useful in assessing the degree of conict between two distributions π1 and π2 which is given by Inc(π1 ∧ π2). We take the ∧ as the minimum operator. Obviously, when π1 ∧ π2

Information Anity in Possibilistic Decision Trees 207

gives a sub-normalized possibility distribution, it indicates that there is a conict
bInetfworemenaπt1ioanndAπ2n(iItnyc:(πa1p∧oπss2i)b∈il]i0s,t1i]c).similarity measure
After a deep study of existing possibilistic similarity measures, we have proposed in a recent work [6], a new similarity index satisfying interesting properties.
The information anity index, denoted by InfoAff takes into account a classical informative distance, namely, the Manhattan distance along with the well known inconsistency measure. InfoAff is applicable to any pair of normalized possibility distributions.
Denition 1 Let π1 and π2 be two possibility distributions on the same universe of discourse Ω. We dene a measure InfoA(π1, π2) as follows:

Inf oAf f (π1,

π2)

=

1

−

d(π1, π2)

+

I nc(π1 2

∧

π2)

(2)

where d(π1, π2) between π1 and

= π2

1
annd

n i=1

|π1(ωi)

−

Inc(π1 ∧ π2)

π2(ωi)|
tells us

represents the Manhattan distance about the degree of conict between

the two distributions (see Equation (1)).

Two possibility distributions π1 and π2 are said to have a strong anity (resp. weak anity) if Inf oAf f (π1, π2) = 1 (resp. Inf oAf f (π1, π2) = 0).
For sake of simplicity, in the rest of the paper, a possibility distribution π on a nite set Ω = {ω1, ω2, ..., ωn} will be denoted by π[π(ω1), π(ω2), ..., π(ωn)].

3 Decision trees

Decision trees, also called classication trees, are graphical models with a treelike structure: they are composed of three basic elements: decision nodes corresponding to attributes, edges or branches which correspond to the dierent possible attribute values. The third component consists of leaves including objects that typically belong to the same class or that are very similar.
Several algorithms for building decision trees have been developed, e.g., ID3 [10] and its successor C4.5 "the state-of-the-art" algorithm developed by Quin-
lan [11]. These algorithms have many components to be dened: a) Attribute selection measure generally based on information theory,
serves as a criterion in choosing among a list of candidate attributes at each decision node, the attribute that generates partitions where objects are distributed less randomly, with the aim of constructing the smallest tree among those con-
sistent with the data. The well-known measure used in the C4.5 algorithm of
Quinlan [11] is the gain ratio. Given an attribute Ak, the information gain relative to Ak is dened as follows:

where

Gain(T, Ak) = E(T ) − EAk (T )

E(T ) = −

n

n(Ci, T ) |T |

log2

n(Ci, T ) |T |

i=1

(3) (4)

208 Ilyes Jenhani et al.

and

EAk (T ) =

|TvAk | |T |

E(TvAk )

v∈D(Ak )

(5)

n(Ci,T ) denotes the number of objects in the training set T belonging to the

class Ci, D(Ak) denotes the nite domain of the attribute Ak and |TvAk| denotes the cardinality of the set of objects for which the attribute Ak has the value

v.

Note

that

n(Ci,T ) |T |

corresponds

to

the

probability

of

the

class

Ci

in

T.

Thus,

E(T ) corresponds to the Shannon entropy of the set T . The gain ratio is given

by:

Gr(T, Ak)

=

Gain(T, Ak) SplitInf o(T, Ak)

(6)

where SplitInfo(T,Ak) represents the potential information generated by dividing T into n subsets. It is given by:

SplitInf o(T, Ak) = −

|TvAk | |T |

log2

|TvAk | |T |

v∈D(Ak )

(7)

b) Partitioning strategy consisting in partitioning the training set according to all possible attribute values (for symbolic attributes) which leads to the generation of one partition for each possible value of the selected attribute. For continuous attributes, a discretization step is needed.
c) Stopping criteria stopping the partitioning process. Generally, we stop the partitioning if all the remaining objects belong to only one class, then the node is declared as a leaf labeled with this class value. We, also, stop growing the tree if there is no further attribute to test. In this case, we take the majority class as the leaf's label.

4 Anity based possibilistic decision trees

An anity based possibilistic decision tree (A-PDT) has the same representation of a standard decision tree, i.e., it is composed of decision nodes for testing attributes, branches specifying attribute values and leaves dealing with classes of the training set.

4.1 Imperfection in classication problems
As models of the real world, databases, or more specically, training sets are often permeated with forms of imperfections, including imprecision and uncertainty. The topic of imperfect databases is gaining more and more attention the last years [8] since commercial database management systems are not able to deal with such kind of information.
Examples of imperfect class values include the exact type of an attack in an intrusion detection system, the exact cancer class of a patient in cancer diagnosis

Information Anity in Possibilistic Decision Trees 209
applications, the exact location or type of a detected aerial engine in military applications, etc. These imperfections might result from using unreliable information sources, such as faulty reading instruments, or input forms that have been lled out incorrectly (intentionally or inadvertently).
In order to deal with such kind of imperfection, in this work, we used a convenient mathematical model, namely, possibility theory [4, 12]. More formally, a possibility degree will be assigned to each possible class value indicating the possibility that the instance belongs to a given class [3]. These possibility degrees can be obtained from direct expert's elicitation, i.e., each expert is asked to quantify by a real number between 0 and 1 the possibility that a training instance belongs to each one of the dierent classes of the problem.
4.2 Building procedure
In the possibilistic setting, instances classes in the training set will be represented by possibility distributions over the dierent classes of the problem instead of exact classes. Hence, one must nd a way to assess homogeneity of a given training partition. The idea consists in measuring the entropy of each partition weighted by the mean similarity degree of the possibility distributions in the corresponding partition. Let us dene the basic components for the A-PDT approach:
a) Attribute selection measure
Given a training set T (the initial partition) containing n instances and given the set of attributes, let us denote by πi the possibility distribution labeling the class of the instance i in T .
In standard decision trees, homogeneity of a partition is determined by the entropy of that partition. However, in our context, πi's are most of the time very dierent, so it has no sense to directly compute their frequencies in order to determine the entropy of T (the entropy will be equal to 1). Moreover, one cannot simply view each πi as a new class. First, because the number of classes will be exponential. Second, there are similar distributions that should be considered as globally expressing same or similar pieces of information. For instance, we cannot simply consider the distributions [1, 0.2] and [1, 0.21] as two dierent exclusive classes, but we will consider them as similar.
Hence, we need a nite set of Meta-Classes MCj=1..m. Each MCj corresponds to a meta-class which gathers together all possibility distributions similar to a predened wrapper possibility distribution (say W Dj). More precisely, wrapper possibility distributions are binary possibility distributions (i.e., ∀ω ∈ Ω, π(ω) ∈ {0, 1}) representing special cases of complete knowledge, partial ignorance and total ignorance representing the set of reference distributions.
After specifying the set of Meta-Classes MC, we will assign to each possibility distribution πi labeling an instance i a meta-class MCj such that: MCj = arg maxmj=1{Inf oAf f (πi, W Dj)} where m is the total number of meta-classes and InfoAff corresponds to the information Anity index (Equation (2)). Note that, as in standard decision trees, ties are broken arbitrarily.

210 Ilyes Jenhani et al.

After mapping the dierent πi's to their corresponding MCj's , it becomes possible to assess the discriminative power of each attribute in partitioning a set into homogeneous subsets by extending the well-known gain ratio criterion [11]. First, we dene the Anity-Entropy Gain (AGain) of an attribute Ak by:

AGain(T, Ak) = AE(T ) − AEAk (T )

(8)

where

AE(T ) = −

m

(AvgAf f (M Cj))

∗

( |M Cj| |T |

log2

|M Cj |T |

|

)

j=1

(9)

and

AEAk (T ) =

|TvAk | |T |

SE(TvAk )

v∈D(Ak )

(10)

where |MCj| in Equation (9) denotes the number of objects in the training set T belonging to the meta-class MCj. Obviously, to compensate for the information loss resulting from grouping resemblant πi's into their corresponding MCj's, we have introduced the AvgAff(MCj) factor which corresponds to the average similarity between the original possibility distributions πp=1..n assigned to MCj:

AvgAf f (M Cj) =

n−1 p=1

n q=p+1

I

nf

oAf

f

(πp

,

πq

)

n∗(n−1)

2

(11)

Proposition 1 When dealing with crisp training sets, i.e., with precise classes (MCj ≡ Cj), we will always have AvgAff(MCj) = 1 and |MCj| will correspond to number of instances labeled by the same class Cj, thus we recover the standard C4.5 approach.

Then, the Anity-gain ratio is expressed in the same way as the classical gain ratio using SplitInfo (Equation (7)):

AGr(T, Ak)

=

AGain(T, Ak) SplitInf o(T, Ak)

(12)

Obviously, the attribute maximizing AGr will be assigned to the decision node at hand.

Example 1 Let us use a modied version of the golf data set [9] to illustrate the

notion of wrapper distributions. Let T be the training set composed of fourteen in-

stances .i=1..14 A possibility distribution was given for each possible class of each

instance of T. The set of wrapper distributions relative to this example is W D =

{[1, 0], [0, 1], [1, 1]}. Consequently, M C = {M C1, M C2, M C3} such that M C1 =

{i4, i9, i10, i11, i12, i13}, M C2 = {i1, i2, i6, i8, i14} and M C3 = {i3, i5, i7, }. The

Anity-Entropy of the set T is computed (using Equation (9)) as follows :

AE(T

)

=

−0.956∗(

6 14

∗log2

6 14

)−0.95∗(

5 14

∗log2

5 14

)−0.966∗(

3 14

∗log2

3 14

)

=

1.464.

Information Anity in Possibilistic Decision Trees
Table 1. Imprecisely labeled Training Set
Outlook Temp Humidity Wind C1 C2
i1 sunny hot high weak 0.2 1 i2 sunny hot high strong 0.4 1 i3 overcast hot high weak 1 0.7 i4 rainy mild high weak 1 0 i5 rainy cool normal weak 1 0.8 i6 rainy cool normal strong 0.4 1 i7 overcast cool normal strong 1 0.9 i8 sunny mild high weak 0.3 1 i9 sunny cool normal weak 1 0.3 i10 rainy mild normal weak 1 0 i11 sunny mild normal strong 1 0.2 i12 overcast mild high strong 1 0 i13 overcast hot normal weak 1 0.3 i14 rainy mild high strong 0 1

211

b) Partitioning strategy

Since we only deal with nominal attributes, the partitioning strategy will be the

same as with standard decision trees.

c) Stopping criteria

We will stop growing the tree if:

1. There is no further attribute to test.

2.AGain ≤ 0, i.e., no information is gained.

3. |Tp|=0, i.e., the generated partition does not contain any instance.
d) Structure of leaves

Leaves of our induced A-PDT trees will be labeled by possibility distributions

on the dierent classes rather than crisp classes. In fact, when the above stopping

criterion 1 or 2 is satised for a training partition Tp containing n possibility distributions, we will declare a leaf labeled by the representative possibility dis-

tribution of that set (πRep), that is, the possibility distribution which corresponds to the closest distribution to all the remaining distributions in the set Tp:

πRep(Tp) = arg maxni=1{

j

=i

I

nf oAf (n−1)

f

(πi

,πj

)

}

Hence, when considering the special case of a leaf with only certain possibility

distributions, if we take the fully possible class of (πRep(Tp)) as a nal decision, we join the solution of majority class adopted by standard decision trees.

Finally, when stopping criterion 3 is satised, we declare an empty leaf labeled

by a randomly chosen wrapper possibility distribution from W D.

4.3 Classication procedure
Once the A-PDT is constructed, we can classify any new object given values of its attributes. We start with the root of the constructed tree and follow the path corresponding to the observed value of the attribute in the interior node of the tree. This process is continued until a leaf is encountered. As mentioned

212 Ilyes Jenhani et al.

above, each leaf of our decision tree will be labeled by a possibility distribution over the dierent class values. Hence, to make a decision about the class of a given object, the decision maker can take the fully possible class label (i.e. the class having a possibility degree equal to 1).

5 Experimental results

Our experimental studies are divided in two parts. First, we evaluate our A-

PDT approach. Second, we compare our results with those of the C4.5 algorithm

if we ignored uncertainty. Note that we do not intend to compare A-PDT with

C4.5 since this latter do not deal with uncertainty: the aim of the comparison is

to show whether ignoring uncertainty in training data is a good practice or not.

The experimental study is based on several data sets selected from the U.C.I

repository [9]: (1) Wisconsin Breast cancer (699 instances, 8 attributes, 2 classes),

(2) Voting (497, 16, 2), (3) Balance scale (625,4,3), (4) Solar Flare (1389, 10, 3),

(5) Nursery (12960, 8, 5). We have modied these data sets by transforming the

original crisp classes by possibility distributions over the dierent classes. We

have used levels of uncertainty (L%) when generating these possibilistic training

sets: for each training instance from the L% randomly chosen instances, we

have assigned a possibility degree equal to 1 to the original class and a random

possibility degree to the remainders in an uniform way. To each one of the

remaining (100 − L)% instances, we have assigned a completely sure possibility

distribution corresponding to the original crisp instance's class.

In order to determine the accuracy of the induced trees, we have used two

criteria, the rst is relative to the percentage of correct classication (P CC =

number of well total number of

classif ied classif ied

instances instances

× 100)

and

the

second

corresponds

to

a

similar-

ity based criterion (P CC_Aff) which we have proposed in [6] as a new criterion

that is more appropriate to the possibilistic context:

P CC_Af f

=

n j=1

Inf oAf f (πres, πj)

total_nbr_classif ied_inst

×

100

(13)

Let us recall that the output of a possibilistic decision tree is given in the form of a possibility distribution (πres). Thus, the standard P CC is computed by choosing for each instance to classify the class having the highest possibility degree (equal to 1). If more than one class is obtained, then one of them is chosen randomly. Finally, this class label is compared with the true class label.

Table 2. A-PDT (P CC_Aff and standard deviation)
L% 0% 30% 50%
W.B.cancer 93.37(1.3) 91.2(1.7) 88.71(2.1) Voting 96.76 (1.7) 95.35(1.9) 94.11(1.9) Solar Flare 87.26(2.2) 84.90(1.8) 83.77(1.6) Balance 83.54 (1.5) 73.83 (1.2) 72.88(1.2) Nursery 98.74 (0.8) 96.96(1.1) 95.95(1.4)

Information Anity in Possibilistic Decision Trees 213
Table 2 reports the dierent obtained results after varying the training sets' level of uncertainty L% for each database. P CC_Aff values of the induced A-PDT trees are complemented by standard deviations after the use of a 10fold cross validation testing process. Note that high values of the P CC_Aff criterion do not only imply that the induced trees are accurate but also imply that the possibility distributions provided by the induced A-PDT trees are of high quality and faithful to the original possibility distributions. From Table 2, we can see that P CC_Aff values decrease when L% increases. This can be explained by the fact that the higher the level of uncertainty (L%), the less informative the training set becomes (consequently, the harder the learning becomes), and therefore the less accurate the predictions are.
Now, let us see what happens when ignoring imprecisely labeled training instances when building decision trees. To respond to this question, we have conducted our experimentations as follows: for each training set and for each uncertainty level L%, we have induced an A-PDT tree. On the other hand, a C4.5 tree was induced from the corresponding training set, i.e., the standard training set from which we have discarded the L% instances to which we have assigned imprecise class labels since the C4.5 algorithm cannot deal with such instances. Then, both approaches are evaluated on the same testing sets: standard testing sets for C4.5 trees have been used and their corresponding testing sets (with completely sure possibility distributions on the original class labels) for A-PDT trees: this corresponds to one iteration of the 10-fold cross validation process used for the evaluation of the approach. Table 3 reports the dierent obtained results after varying the training sets' level of uncertainty L% for each database. MP CC denotes the mean P CC (complemented by standard deviation) of the induced decision trees for the 10-fold cross validation process.
Table 3. C4.5 and A-PDT (MPCC and standard deviation)
Database Method L = 0% L = 30% L = 50%
W.B.cancer C4.5 94.54(1.1) 91.05(2.5) 90.11(3.2) A-PDT 94.54(1.1) 91.63(2.3) 90.73(2.6)
Voting C4.5 94.56(3.2) 90.15(3.8) 87.27(4.6) A-PDT 94.56(3.2) 91.62(3.5) 88.52(4.0)
Solar are C4.5 81.96(3.3) 77.03(3.7) 74.37(3.9) A-PDT 81.96(3.3) 80.57(3.7) 78.48(3.9)
Balance C4.5 78.48(4.2) 74.78(5.3) 70.38(5.7) A-PDT 78.48(4.2) 77.06(4.9) 74.82(5.4)
Nursery C4.5 98.78(0.8) 94.45(1.6) 92.81(2.6) A-PDT 98.78(0.8) 97.11(1.2) 94.37(2.2)
Table 3 shows that the A-PDT approach gives interesting results when compared with the C4.5 algorithm. Again, we can see that classication accuracies of both approaches decrease when the level of uncertainty increases (for the same explanation provided above for Table 2). In spite of this decrease in accuracy, we can see that the classication rate of A-PDT is always (even slightly) greater

214 Ilyes Jenhani et al.
than the one of C4.5. Note that the aim of this comparison is not to directly compare the two approaches. In fact, the C4.5 is used only in certain environments: it is trained from reduced training sets (imprecisely labeled instances are omitted) while the A-PDT approach deals with both certain and uncertain environments: it is trained from complete training sets (including both precisely and imprecisely labeled instances). Besides, Table 3 conrms Proposition 1. In fact, our approach recovers the C4.5 one when dealing with crisp instances (with precise labels, i.e., L%=0).
From the results given in this table, we can conclude that, generally, rejecting training instances, classes of which are imprecisely dened, is not a good practice and reduces the accuracy of the induced classier. This issue can be avoided and well handled by the use of the proposed A-PDT approach which can exploit the information contained in imprecise labels.
6 Conclusion
This paper proposes a generalization of the C4.5 approach to the imprecise setting. The new approach has the advantage of allowing the induction of decision trees from training instances having possibilistic class labels. The proposed APDT approach blends information anity with entropy in order to asses the homogeneity of a given training partition. Experiments have shown that rejecting training instances, classes of which are imprecisely dened, is not a good practice and reduces the accuracy of the induced classier. We plan to add an automatic clustering phase for the specication of the wrapper distributions which could enhance the performance of the approach.
References
1. N. Ben Amor, S. Benferhat, Z. Elouedi: Qualitative classication and evaluation in possibilistic decision trees, ,(FUZZ-IEEE'04) Hungary, 2004, 653-657.
2. C. Borgelt, J. Gebhardt, R. Kruse: Concepts for Probabilistic and Possibilistic Induction of Decision Trees on Real World Data. ,(EUFIT'96) 1996, 1556-1560.
3. T. Denoeux and L. M. Zouhal. Handling possibilistic labels in pattern classication using evidential reasoning. Fuzzy Sets and ,Systems 122(3), 2001, 47-62.
4. D. Dubois and H. Prade: Possibility theory: An approach to computerized processing of uncertainty, Plenum ,Press New York, 1988.
5. E. Hüllermeier. Possibilistic Induction in decision tree learning. ,ECML'02 Helsinki, Finland, 2002, 173-184.
6. I. Jenhani, N. Ben Amor, Z. Elouedi, S. Benferhat and K. Mellouli: Information Anity: a new similarity measure for possibilistic uncertain information, EC,SQARU'07 Hammamet, Tunisia, 2007, 840-852.
7. I. Jenhani, N. Ben Amor, Z. Elouedi: Decision Trees as Possibilistic Classiers, International Journal of Approximate ,Reasoning Elsevier, 2007, to appear.
8. A. Motro: Sources of Uncertainty, Imprecision and Inconsistency in Information Systems. ,Uncertainty Management in Information Systems: From Needs to Solutions 1996, 9-34.
9. P. M. Murphy, D. W. Aha: UCI repository of machine learning databases, 1996. 10. J. R. Quinlan: Induction of decision trees, Machine ,Learning 1, 1986, 81-106. 11. J. R. Quinlan: C4.5: Programs for machine learning, Morgan Kaufmann, 1993. 12. L. A. Zadeh: Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets ans ,Systems
1, 1978, 3-28.

Ordering Finite Labeled Trees
Herman Ruge Jervell
Department of Computer Science, Pb 1080 University of Oslo
N-0317 Oslo, Norway herman@ifi.uio.no

Abstract. Previously — in CiE 2005 [2005] — we have given an ordering of ﬁnite trees and showed that it is a well ordering reaching up to the small Veblen ordinal. Here we extend this ordering to ﬁnite labeled trees — similar to Takeutis ordinal diagrams [1975] — and show that this extended ordering is also a well ordering.
Key words: ordinals, ordinal notation, ﬁnite labeled trees

1
We have given a wellordered set of labels Λ and consider ﬁnite trees with labels at the nodes. In the trees the branches are ordered — from left to right. Below there are four examples of ﬁnite labeled trees
0

000

021

0 12 2
Here the smallest label is 0, then 1, 2 . . . . The ﬁrst tree is the ordinal 0. The second tree — consisting of only one node with label 1 — is the smallest tree larger than all trees with just 0 as label. It corresponds to the small Veblen ordinal. The third tree corresponds to the Howard ordinal. The fourth tree is larger than the three others and is just an example of the more general type of labeled trees considered.

2

In CiE 2005 [2005] we gave an ordering of ﬁnite trees. The ordering was given by

where

A<B ⇔ A≤ B ∨ ( A <B ∧ A < B )

216 Herman Ruge Jervell
A ≤ B : There is an immediate subtree Bi of B such that either A < Bi or A = Bi
A < B : For all immediate subtrees Aj of A we have Aj < B A < B : The inverse lexicographical ordering of the immediate subtrees —
we ﬁrst check which sequence have smallest length, and if they have equal length we look at the rightmost immediate subtree where they diﬀer
We then prove by induction over subtrees
– The relation is transitive – The relation is total: A < B ∨ A = B ∨ B < A where A = B means the
ordinary equality between trees and the three cases are mutually exclusive – The relation is decidable
We used Π11 − CA to prove that the relation is a well order. There is a 11 correspondence between the ﬁnite trees and the ordinals less than the small Veblen ordinal.
3
In the generalization to ﬁnite labeled trees we shall
– Consider more orderings — an ordering <n for each label n and an ordering <∞
– Have a proof of the well ordering going beyond Π11 − CA – Consider a new notion — visibility in a labeled tree
First the new notion — visibility. For each label n we deﬁne n-visibility. Say that we are in a labeled tree S and at node ν. Then from node ν a node µ is n-visible if
– Node µ has label ≥ n – All nodes strictly between ν and µ have labels > n
So from node ν we can n-see all nodes which can be reached through nodes with labels ≥ n and up to the ﬁrst nodes with label n. A node with label ≤ n will block the n-visibility for all nodes above it. We use the visibility to deﬁne the n-subtrees of a tree S
S n = the sequence of all n-subtrees n-visible from the root of S
Now we are ready to deﬁne the orderings
S <j T ⇔ S ≤j T j ∨ ( S j <j T ∧ S <j+ T ) S <∞ T ⇔ lexicographical ordering
Here we use abbreviations like for the ﬁnite trees

Ordering Finite Labeled Trees 217
– ≤j means either ordinary = or <j – S ≤j T j : There exists a j-subtree T0 of T with S ≤j T – S j < T : For all j-subtrees S0 of S , S0 <j T – j+ is the smallest label in S and T larger than j if it exists, else it is ∞ – The lexicographical ordering is such that we compare in priority
• The labels at the root of S and the root of T • For the same label i: The lengths of S i and T i • The rightmost place where the two sequences diﬀer in the >i-ordering
As for the ﬁnite trees we get that the orderings are transitive, total and decidable. This is proved by induction over the subtrees and using the ﬁnite set of labels occurring in the trees. The following diﬀers from our theory of ﬁnite trees
– We consider many orderings – In the theory of ﬁnite trees to each ordinal there corresponded a unique ﬁnite
tree. This is not so any more. The two ﬁnite labeled trees below is equal in the ordering
0
00
11
Both trees have the same 1-immediate subtrees — they have none, and to get equality this is what we compare.
4
To make the proof of the well ordering more perspicuous we go through the following cases
– Only label 0 – Finite number of labels – A well ordered set of labels
Trees with only label 0 is the same as ﬁnite trees without labels. Our proof here is not the same as in [2005]. The proof is a variant of the usual minimal bad argument. We remind the reader
Bad tree: A tree is bad if there is an inﬁnitely descending sequence starting with the tree
Minimal tree: A tree is minimal if no immediate subtree is bad

218 Herman Ruge Jervell
Now consider the case where the only label is 0. We must prove that there are no bad trees. Assume we have a bad tree S0. If S0 is not minimal, then by going to an immediate subtree we get a smaller tree which is bad. And then we can continue this going to immediate subtrees until we get a bad tree where all its immediate subtrees are not bad. We call this minimal bad tree for
S00
Consider now the bad sequence starting with it. We shall construct a sequence of minimal bad trees. Say we have constructed the minimal bad trees
S00 >0 S10 >0 · · · >0 Sn0
The sequence can be continued with bad trees
S00 >0 S10 >0 · · · >0 Sn0 >0 Tn+1 >0 Tn+2 >0 · · ·
Observe now that for any immediate subtree Un+1 of Tn+1 we have Tn+1 >0 Un+1 and by transitivity Sn0 >0 Un+1. Hence by continuing taking immediate subtrees of Tn+1 we get a minimal bad tree Sn0+1 with
S00 >0 S10 >0 · · · >0 Sn0 >0 Sn0+1
and we can continue the construction to get an inﬁnite sequence of minimal bad trees
S00 >0 S10 >0 · · · >0 Sn0 >0 Sn0+1 >0 Sn0+2 >0 · · ·
We next observe that from the deﬁnition of the ordering
S <j T ⇔ S ≤j T j ∨ ( S j <j T ∧ S <j+ T )
we cannot use the ﬁrst condition. If Sm0 ≥0 Sm0 +1 then Sm0 would have an immediate subtree which is bad — contradicting the minimality of Sm0 . We must use the second condition and get
S00 >∞ S10 >∞ · · · >0 Sn0 >∞ Sn0+1 >∞ Sn0+2 >∞ · · ·
Now look at the lexicographical ordering. Here our trees have only labels 0. From some stage oﬀ all lengths of sequences of immediate subtrees must be the same, and we get an immediate subtree which starts an inﬁnitely >0-descending sequence and a bad immediate subtree — contradicting that we had a sequence of minimal trees.

Ordering Finite Labeled Trees 219

5

Now we come to the case where we have a ﬁnite number of labels. We must generalize the notions of bad and of minimal

n-bad: There is an inﬁnite >n-descending sequence n-minimal: No m-immediate subtree is m-bad for any m ≤ n

Now assume we have a 0-bad tree. As in the previous section we construct a 0-minimal 0-bad sequence

We get as above

S00 >0 S10 >0 S20 >0 · · ·

S00 >1 S10 >1 S20 >1 · · ·
This is a 1-bad sequence. We must construct a sequence which is also 1minimal. Observe that going to the 1-immediate subtrees does not destroy the 0-minimality. By going to the 1-immediate subtrees we may get rid of some 0immediate subtrees but we do not create any new 0-immediate subtrees. Therefore we get a 1-minimal 1-bad sequence

S01 >1 S11 >1 S21 >1 · · · And then we continue this line for line until we have the sequence

S0∞ >∞ S1∞ >∞ S2∞ >∞ · · ·
which is k-minimal for all k. Then from some stage in the sequence we have the same label n at the root and the same length of the n-immediate subtrees. But then we get an n-immediate subtree which is n-bad — contradicting the n-minimality of all trees in the sequence.

6
In the ﬁrst construction of minimal bad sequence we used Π11-CA. We had to decide whether an immediate subtree is bad or not. This is more problematic for the case where we have more labels. The construction of a 0-minimal 0-bad sequence of trees use Π11-CA as before. But we had to do more in the construction of a 1-minimal 1-bad sequence. So say we have the 0-minimal 0-bad sequence
S00 >0 S10 >0 S20 >0 · · · And we get from the 0-minimality
S00 >1 S10 >1 S20 >1 · · ·

220 Herman Ruge Jervell
Now we had to show that this sequence can also be made 1-minimal. But then we had to decide whether an immediate subtree is starting an inﬁnite sequence of >1 descending 0-minimal trees. This goes beyond Π11-CA. Further down in the construction we do the same — we had to decide whether an immediate subtree is starting an inﬁnite sequence of >n+1 descending n-minimal trees.

7

We now consider the general case where the labels are taken from a well ordered set. As before we assume we have a 0-bad tree, and then construct trees
Smn for each label n and each number m
Each row is a sequence of n-minimal n-bad trees

S0n >n S1n >n S2n >n · · ·
The construction goes row for row. We must say how the construction goes at limit rows. Remember that in the construction row n and n + 1 may be quite similar. The ﬁrst place m where they diﬀer Smn+1 is a subtree of Smn . We then prove that in the columns the trees are mostly as the tree above — in each column there are only a ﬁnite number of changes. For assume not. Call the leftmost column m with an inﬁnite number of changes for the critical column. Then from some row n there are no changes to the left of the critical column m. That means that from row n all changes in the critical column comes from going from a tree to a subtree. But this can only be done a ﬁnite number of times — and the critical column was not critical. In each column there are only a ﬁnite number of changes. And we have the obvious construction for limit rows — just take the limit along each column.
Now look at a limit row

Here we have

T0λ >λ T1λ >λ T2λ >λ · · ·

– The row gives a λ-bad sequence – All elements are n-minimal for each n < λ.

Now we can use the construction above to also get a λ-bad sequence which is λ-minimal. And then we continue as before. The contradiction comes at row ∞.

Theorem 1. If the labels Λ is well ordered, then <0 is a well order.

Ordering Finite Labeled Trees 221
8
We shall now prove that all the orderings <n are well orderings. Assume we have proved it for all labels < n. But then
– No tree is m-bad for m < n – All trees are m-minimal for m < n.
Assume now we have an n-bad tree. So we get an n-bad sequence
T0n >n T1n >n T2n >n · · · This sequence is m-minimal for all m < n since all trees are. But then we go through the construction as above and get an n-bad n-minimal sequence
S0n >n S1n >n S2n >n · · · and we can continue the construction as above to get the last row
S0∞ >∞ S1∞ >∞ S2∞ >∞ · · · which is ∞-bad and m-minimal for all labels m. And then we have a contradiction as above and can conclude
Theorem 2. All orderings <k and <∞ are well orderings.
9
We now consider trees with labels up to some ﬁnite number N . Then consider the treeclasses
– TN — trees with label N at the root – TN−1 — trees with label N − 1 or N at the root – TN−2 — trees with label N − 2, N − 1 or N at the root – ... – S0 — trees with only label 0 – S1 — trees with only label 0 or 1 – S2 — trees with only label 0, 1 or 2 – ...
We then have
Theorem 3. The following are isomorphic orderings
– TN and <N — S0 and <0 – TN−1 and <N−1 — S1 and <0 – TN−2 and <N−2 — S2 and <0 – ...

222 Herman Ruge Jervell
And we have
Theorem 4. The one node trees
– 1 is the supremum of S0 under <0 – 2 is the supremum of S1 under <0 – 3 is the supremum of S2 under <0 – ...
10
Let us now compare our ﬁnite labeled trees with the ordinal diagrams of Takeuti. It is easiest to compare them with the variant of ordinal diagrams of ﬁnite order made by Levitz [1970]. We use the following variant of the ordinal diagrams O(n)
– In O(n) we consider labeled ﬁnite trees which are unordered – We have given a pair of two numbers as the labels
• The ﬁrst number is from 0 to n and is called the order • The second number is a natural number called the degree – In our labeled trees we deﬁned sequences T k for each order k – In the ordinal diagrams O(n) we deﬁne similarly multisets [T ]k for each order k – The orderings <k are deﬁned similarly for the labeled ﬁnite trees and the ordinal diagrams using the gap condition – The orderings <∞ are deﬁned lexicographically but this comes out diﬀerently for our ordered labeled trees and the unordered ordinal diagrams – In the ordinal diagrams the elements of the multisets [T ]k all have the same order at the root but they may have diﬀerent degrees – Then for the multisets we ﬁrst compare the elements of largest degree and so on
Levitz have the connection between the ﬁnite ordinal diagrams and iterated inductive deﬁnitions
Theorem 5 (Levitz). The ordinal diagram O(n) gives the ordinals connected with IDn−1. In particular the ordinal diagrams with order 0 and 1 give the ordinal below the Howard ordinal.
On the other hand we give connections between the unordered ordinal diagrams and our ordered ﬁnite labeled trees. We can use the degrees to embed the ordered trees into the unordered trees and use the possibility of having large branchings to embed ordered trees with large degrees. We get
Theorem 6. The ﬁnite labeled trees Sn corresponds to the ordinal diagrams O(n). In particular the labeled tree with the single node 2 under <0 corresponds to the Howard ordinal.
Helmut Pfeiﬀer [1972] has generalized Levitz connections between ordinal diagrams and Schu¨ttes notation system to arbitrary wellordered set of orders. As above we also get connections to ﬁnite labeled trees where the labels come from a wellordered set.

Ordering Finite Labeled Trees 223
References
[1970] Levitz, Hilbert: On the relationship between Takeuti’s ordinal diagrams O(n) and Schu¨tte’s system of ordinal notations Σ(n). Intuitionism and Proof Theory. (Eds Myhill, Vesley, Kino). North-Holland
[1972] Pfeiﬀer, Helmut: Vergleich zweier Bezeichnungssysteme fu¨r Ordinalzahlen. Arch. math. Logik 15, 1972.
[1975] Takeuti, Gaisi: Proof theory. North-Holland [2005] Jervell, Herman Ruge: Finite trees as ordinals. CiE 2005, Springer Lecture
Notes.

Towards Reverse Proofs-as-Programs
Reinhard Kahle
CENTRIA and Departamento de Matem´atica, Universidade Nova de Lisboa, 2829-516 Caparica, Portugal kahle@mat.uc.pt
Abstract. In this programmatic paper we renew the well-known question “What is a proof?”. Challenged by computer based theorem provers, we argue that a (good) proof is not the formalized one. Looking for the notion of equality of proofs, we propose to investigate proofs in relation to Moschovakis’s theory of algorithms. This should be carried out by a “reverse engineering” of program extraction functions underlying the proofs-as-programs paradigm.
Key words: Proof; equality of proofs; algorithm; proofs-as-programs
1 What is a proof ?
The birth of science is often attributed to the Ancient Greeks since they introduced the concept of proof in mathematical arguments. While the success of mathematics since the ancient world is an everlasting story, it is surprising that the central notion of proof in its own only recently got in the focus of research. Although one might ﬁnd a lot of controversial “proofs” in the history of mathematics, the question of what is a proof had a direct impact on mathematics only in the last 150 years.
A ﬁrst example seems to be Hilbert’s proof of his ﬁnite basis theorem which caused a sensation due to its non-constructive character. In a ﬁrst reaction, Gordan labeled it as theology rather than mathematics. From a modern perspective, it was not the notion of proof which was changed by Hilbert, but rather the (traditional) meaning of a particular mathematical concept: the existential quantiﬁer. In fact, most of the controversies about certain proofs may actually be regarded as controversies about the concepts which are involved in these proofs. However, historically Hilbert’s non-constructive proof was important since it gave rise to a profound discussion of the foundations of mathematics, which eventually led to the introduction of proof theory by Hilbert himself as a new discipline within mathematical logic. This discussion was accompanied by the rigid formalization of mathematics in the work of Boole, Peano, Frege, Whitehead and Russell (to mention the most important ones). As a result we nowadays have at hand a quite clear deﬁned notion of formalized proof in mathematics. But these formalized proofs have a speciﬁc aim in the foundations of mathematics, and, as a matter of fact, a working mathematician nearly never formalizes a proof in such a rigid way.

Towards Reverse Proofs-as-Programs 225
Formalized proofs play, however, the central role in computer-aided theorem proving. But, at present, computer generated proofs do not meet the standards we have for (good) mathematical proofs. They also do not help (yet) to decide the question of equality of proofs. Based on the close relation of (constructive) proofs and algorithms, we propose to investigate (constructive) proofs as inverse images of algorithms under a program extraction function. Using an existing notion of equality for algorithms, as given, for instance, in Moschovakis’s theory of algorithms [26], one can expect to obtain a corresponding notion of equality for (constructive) proofs which should meet certain requirements we expect for such an equality.
2 The computer challenge
The use of computers in mathematical theorem proving is an issue which is becoming more and more important. In this ﬁeld one can distinguish two diﬀerent directions: Proof search and Proof check. Proof search suﬀers from the wellknown complexity problems and is not very promising for general mathematical problems.1 While it might be very hard to ﬁnd a proof, to check a proof for correctness is, in general, of lower complexity, although it can be rather technical and long. Thus, proof check seems to be a perfect application for computers.
In a recent book [36] (see also our review [18]), edited by Wied√ijk, seventeen theorem provers are presented how they prove the irrationality of 2. This book gives an interesting insight into the state of the art of theorem proving, showing that in particular the proof representation is still far from being satisfactory for a human reader. In the introduction the editor presents a six line proof of a theorem, taken from the textbook of Hardy and Wright [15, p. 39f]:
√ “The traditional proof ascribed to Pythagoras runs as follows. If 2 is rational, then the equation
a2 = 2b2
is soluble in integers a, b with (a, b) = 1. Hence a2 is even, and therefore a is even. If a = 2c, then 4c2 = 2b2, 2c2 = b2, and b is also even, contrary to the hypothesis that (a, b) = 1.”
This proof should be understandable for everybody with basic mathematical knowledge. The interesting thing is that the editor seems to see the overall objective of proofs in proof check when he writes after this proof [36, p. 3]: “Ideally, a computer should be able to take this text as input and check it for its correctness.”
We think that this perspective is misdirecting—at least with respect to mathematical proofs. Of course, the correctness of a proof is essential; but, in most cases, it is not the primary objective of the proof. Only if a proof is the very ﬁrst
1 But, of course, proof search has its merits for specialized problems which are properly modeled, in particular, with respect to applications.

226 Reinhard Kahle
of a theorem, its correctness is the ﬁrst question. Of course, we do not like to see faulty proofs later on, but when a theorem is accepted in the mathematical community as true, most of its proofs have the role to convince the reader of the correctness of the theorem. Here, the representation of a proof is essential. Over centuries, mathematicians developed a rather sophisticated, however implicit, standard to represent proofs which allow to convince other mathematicians. It is just a matter of fact, that theorem provers still do not have the ability to represent proofs in a way that they can compete with usual textbook proofs. To guarantee correctness, they have to take into account too many details, details which a mathematician does not like to see exposed in the proof. This was formulated by Scott [33, p. ixf] as follows: “[F]or veriﬁcation (. . . ) checkable proofs have to be generated and archived. Computers are so fast now that hundreds of pages of steps of simpliﬁcations can be recorded even for simple problems. Hence, we are faced with the questions, ‘What really is a proof?’ and ‘How much detail is needed?’ ”.
The two questions of Scott can be described as the computer challenge. Let us mention here one particular shortcoming of proofs given by computers. They seem to be inadequate for an exchange between diﬀerent proof systems. Thus, we would like to challenge the theorem prover community to provide interfaces which allow to transfer proofs performed in one theorem prover to another. Of course, many systems diﬀer signiﬁcantly in the underlying logic, the internal representation of datatypes, automatic components, user deﬁned e√xtensions, etc. However, a proof—in the sense we look for—of the irrationality of 2 should not depend on any of these particularities. Thus, if there is something like an abstract proof of this theorem, the theorem provers should be able to give such one, and they should be able to exchange it among each other. We conjecture that a very lot of the disturbing details, which most likely depend on the speciﬁc implementation of a theorem prover, would be ﬁltered out in the proof representation suitable for computer interaction.2 At present, we consider that computers are still not able to give us a satisfactory representation of proofs, as we would like to have for mathematical theorems.3 The critique can be even extended to formalized proofs in general.4 We will not discuss this issue at length here, but let us mention only two indications for it: First, mathematical textbooks as well as research papers still avoid formalized proofs as much as possible; they serve, as everybody knows, rarely for didactical reasons or to convince the reader of the correctness of an argument. Secondly, one should have a look how we examine proofs: We would hardly be
2 Avigad [3] proposes to use methods (or tactics) as available in theorem provers as Isabelle for such a purpose. This is deﬁnitely a step in the right direction, but we think, that the the methods have still not the suﬃcient level of abstraction. In particular, they depend too much on the particulars of the implementation.
3 We are aware, that this opinion is not shared by everybody, in particular, several advocates of computer-aided theorem provers. Let us clarify, that here we do not refer to the question of the correctness of proofs, but only to the question of (human understandable) representation of proofs.
4 Cf., for example, [3, p. 129].

Towards Reverse Proofs-as-Programs 227
satisﬁed if a student would give us the raw text of a formalized proof. But, our the principle argument against formalized proofs is that it fails to provide a satisfactory notion of equality of proofs, which goes beyond the literal equality.5
3 Equality of proofs
It seems to be a deﬁciency of proof theory to not be able, up to today, to provide a satisfying criterion for equality of proofs.
The only proposal which is generally discussed is Prawitz’s normalization conjecture, which proposes to identify two proofs with the same normal form, [30, 31]. A good discussion of the proposal can be found in Doˇsen [12]6. It also includes the “rather neglected” [12, p. 477] generalization proposal of Lambek, which is stated in terms of category theory. Both are of interest in logic and category theory; however, they failed to apply to mathematical proofs. This is admitted by Doˇsen when he writes [12, p. 500]: “Faced with two concrete proofs in mathematics—for example, two proofs of the Theorem of Pythagoras, or something more involved—it could seem pretty hopeless to try to decide whether they are identical just armed with the Normalization Conjecture or the Generality Conjecture.” Then, however, he expresses some hopes: “But this hopelessness might just be the hopelessness of formalization. We are overwhelmed not by complicated principles, but by sheer quantity.”
But, the normalization conjecture suﬀers from a conceptional problem: although it might be an interesting technical logical approach, it deﬁnitely is not the notion of equality of proof we can accept in mathematics. By the opposite, the introduction of a lemma is often considered as the speciﬁc achievement in a new proof of a theorem.7
About the general claims that category theory and/or proof nets are able to solve the problem of equality of proofs, cf., for instance, [11, 35], we like to remark that these approaches are mainly concerned with the logical structure of a proof, but not with mathematical arguments. This is often even admitted, or at least posed as a challange, as in the citation of Doˇsen given above, or by Straßburger [35, §4]: “Let me ﬁnish with mentioning some of the questions that are still waiting for an answer: [. . . ] Are these identiﬁcations useful from the point of view of mathematics (i.e., can we use them for identifying real mathematical proofs)?”
To sharpen the question of equality a fundamental distinction should be made ﬁrst: whether we consider proofs of one theorem, or proofs of diﬀerent theorems.
5 Of course, there are proposals around for other notions of equality of proofs, ﬁrst of all, Prawitz’s normalization conjection. However, we do not consider them as satisfactory; see the discussion below.
6 This article also contains comprehensive references to the normalization conjecture and to the generalization proposal which we don’t repeat here.
7 Moschovakis (personal communication) suggested that one should identify the normalform of a proof with its denotation rather with its meaning. For our question, however, it is the meaning we would like to capture.

228 Reinhard Kahle
For two diﬀerent theorems (most likely with some structural similarities) we might have proofs which are considered “essentially the same”, in the sense that they are substitutional variants of each other or that they follow the same pattern etc. This case is, for instance, investigated by use skeletons of (formal) proofs, i.e., proof trees in which the formulas at the leafs are replaced by variables, cf. e.g. [19, 5]. With certain uniﬁcation methods, Baaz [4] showed how diﬀerent proofs can be identiﬁed as instances of the same skeleton, and, in this respect they can be considered as equal. However, there are more informal notions of equality of proofs in diﬀerent areas. It is a standard experience that mathematicians, as soon as they realize any equality, tend to abstract from the concrete cases and to build a theory which encapsulates the common part of the diﬀerent instances. This is also the case for proofs: Baaz8 pointed to the example of universal algebra which grows out of the experience that the proof of the homomorphism theorem is “always the same” in the diﬀerent algebras. This might be a very good example to show that mathematicians indeed already have a tool to “deﬁne” equality of proofs, just by introducing an abstract theory which identiﬁes the “diﬀerent” instances.9
The question when two proofs of one theorem are equal seems to be more subtle. Very often we can easily decide that two proofs are diﬀerent, just by identifying some clear diﬀerences. But to give a theoretical account for criteria of equality is much more complicated. One might separate two levels here: the structure of a proof, and the demonstration of single steps within a large proof. The problem is that there is no clear way to separate these two aspects. One attempt could be to relate them to the diﬀerentiation made by theorem provers between tactics (methods) and the actual proof terms. The tactic should represent the general structure of a proof, while single steps can (often) be given to an automatic problem solver unit. As the separation might be, we now have even two questions: what does it mean that the structure of two proofs is equal? And, what does it mean that two demonstrations of a single proof step are equal?
In the following we will compare the question of equality of proofs with the question of equality for algorithms.
4 What is an algorithm?
It exists a ﬁeld very closely related to proofs with analogous questions: there is no commonly accepted abstract notion of an algorithm. In the next section we will discuss how the ﬁeld of algorithms can be directly linked to the ﬁeld of proofs via the proofs-as-programs paradigm. Before, we like to review shortly an
8 Personal communication. 9 It is an interesting observation that this form of unifying diﬀerent theories implies
that there are common patterns in the proofs, not only in the theorems: since the abstract theorem should have one proof in the abstract theory, it will be possible to give the “same” proof as instantiations of this abstract one for all instances, at least ex post.

Towards Reverse Proofs-as-Programs 229
answer to the question what an algorithm is given by Moschovakis, [21, 22, 24–26].
In [26, §3], entitled The Insuﬃciency of Machine Models, Moschovakis argues convincingly on the basis of the mergesort algorithm, given by a recursive speciﬁcation, that machine models are not good for deﬁning algorithms. Assuming that algorithms are machines, he emphasizes two problems [26, p. 924f]:
“Now the ﬁrst obvious problem is that there are many compilation procedures, and so we don’t get one, but many machines with competing claims ‘to be’ the mergesort algorithm. Moreover, there are essential diﬀerences among these machines, . . . . On an intuitive level, these machines are, of course, equivalent, but it is hard to see how to make this notion of equivalence precise, . . . ; and if we could make the relevant equivalence relation precise, then one could argue that the mergesort algorithm is the appropriate equivalence class (which is much wider than machine isomorphism), and not any particular member of it. “One might try to get out of this dilemma by choosing some one, ‘natural’, ‘most general’ machine which implements the mergesort, . . . . It is not clear how that could be done in a systematic way for all recursive algorithms, but in any case, it would not suﬃce: because we want to know that the conclusion of Prop. 3.1 [some formal properties of mergesort] holds for all ‘implementations of the mergesort’, not just the most general one, . . . . “The second problem is that the details of particular implementations are irrelevant for the elementary proof of Prop. 3.1, . . . .”
It is striking that these problems are the same we encounter for mathematical proofs, if we would like to identify them with formal proofs: there are many formal systems around, as Hilbert-style calculi, sequent calculi, natural deduction, dialogues, etc., and neither we can distinguish one as the ‘natural’ or ‘most general’ one nor we have any reasonable idea what could be the equivalence relation over diﬀerent calculi. The second problem relates to the fact that formal proofs—in particular the ones given by theorem provers—lavish us with irrelevant details.
In contrast to machines, Moschovakis proposes to deﬁne algorithms in terms of recursors which are associated with recursion equations. He stresses the non-syntactic character of recursors saying “. . . I have avoided the word ‘definition’ in their name since it suggests syntactical objects, which algorithms are not” and adding the footnote: “Recursors are related to systems of recursive equations in the same way that diﬀerential operators are related to diﬀerential equations.” [26, p. 925, footnote 11]. Even if he is quite cautious with claiming to have found a deﬁnite answer to the question what an algorithm is,10 to our knowledge, this is the ﬁrst and only proposal which provides a—non-trivial and non-useless—approach to equality of algorithms, [26, §8.2], (which, concededly, still needs further elaboration).
10 He writes: “I would not claim, however, that mine is the only approach, or the best approach — or, perhaps, even an adequate approach: my chief goal is to convince the reader that the problem of founding the theory of algorithm is important, and that it is ripe for solution.” [26, p. 920].

230 Reinhard Kahle
In view of the similarity of the situation with respect to proofs and algorithms, it seems to be promising to investigate whether, and if so, to which extend, Moschovakis’s abstract notion of algorithm can give rise to an abstract notion of proof. In the following section we discuss in which way such a relation could be established.
5 Reverse Proofs-as-programs
The proofs-as-programs paradigm can be taken directly from the Curry-Howardisomorphism. Writing (constructive) proofs as λ terms, they are in principle already functional programs.11
Based on this theoretic relation, the practical implementation of proofs-asprograms is already quite advanced. As one of the ﬁrst implementations, we just mention Bates and Constable’s system PRL, [6]. Nowadays, every good proof system implements a proofs-as-programs component.12 In this process, the main emphasize is put on the question how constructive content, needed for the programs, can be extracted from proofs using classical logic. For our purpose the particular logical framework is not important; all we need is that the programs are generated from a proof by use of a given program extraction function.
Our main suggestion is to attack the questions of what is a proof and how equality of proofs can be deﬁned by use of reverse engineering of the extraction function:
Given Moschovakis’s notion of algorithm, we have to study the inverse images of such algorithms under a program extraction function.
The hope is to ﬁnd (one-to-one) correspondences between concepts about algorithms and about proofs. From the “non-reverse” proofs-as-program perspective, some interesting examples concerning the principle of mathematical induction can already be found in [20]. A very concrete instance of a particular concept we are interested is, is given in [6, p. 115]: “Another possibility in problems of this sort is that some property of A(n, p) can be generalized to add an extra parameter, say A(m, n, p), and then we can use induction on m. This technique is called generalization in Polya [29]; Dijkstra [10], Gries [14], and Reynolds [32] call it weakening in the context of programming.”
If one focus on Moschovakis’s notions introduced for algorithms, at best, this should lead in a natural way to a concept which corresponds to recursors on the side of the proofs. Also, one can hope to get an analog of the equality relation given for recursors.
In its ﬁrst step, this analysis applies to proofs only which give rise to algorithms, ﬁrst of all, constructive proofs of Π20 statements. However, in any
11 There exists a lot of literature about the Curry-Howard-isomorphism which we will not include here. As a standard reference, we can refer to [34].
12 We restrict ourselves to refer to the book [27], which contains a lot of additional references.

Towards Reverse Proofs-as-Programs 231
reasonable account to equality of proof, however a notion of equality for such constructive proofs will look like, this notion has to be part of the notion of equality of proofs in general. Therefore, we do not see this approach as restrictive, but, on the contrary, as a step which provides, ﬁrstly a necessary condition for a general notion of equality, and secondly as a heuristic guideline which probably can give rise to an extension to general proofs.
In any case, there are already several questions which have to be checked: neither it is clear whether the inverse image of a given algorithm is unique— one might even wonder if there always is one—, nor whether a given extraction function is canonical or whether the same proof can give rise to diﬀerent algorithms.13
A negative answer to the ﬁrst of these questions would mean that we have two proofs which are considered as diﬀerent, but the extracted algorithms are equal. In such a case, it would be interesting to see on which parts the diﬀerence of these proofs is based.
A negative answer to the second question would mean that the extraction function itself has to be analyzed more carefully, because it would add computationally relevant components to a proof.
All in all, our proposal of reverse engineering on program extraction should not only provide insights in the notion of proof, but also contribute to a better understanding of program extraction, algorithms, and veriﬁcation proofs.
Note added in proof. A referee drew our attention to the paper Problems in the logic of provability of Beklemishev and Visser [7] of which we were not aware while writing this text. In fact, their consideration about an Informal concept of proof in section 2, concide to a large extend with our view. Beklemishev and Visser propose to relate the notion of proof with the notion of algorithm as given in the theory of Abstract State Machines by Gurevich with references to [8, 9]. Of course, our proposal should work, in principle, with any theory of algorithms which provide a notion of equality. We just proposed Moschovakis’s theory because we are more familiar with it. A comparison of both, Gurevich’s and Moschovakis’s, theories would be desirable in any case, but also with respect to its implications for a notion of equality of proofs. About the abstract theory of provability (or also the more speciﬁc logic of proofs of Arte¨mov, [2]) which is proposed as an account in this context by Beklemishev and Visser we are rather sceptical, since it seems to be formulated on a “too abstract” level. However, the fruitfulness of it is still subject to research.
6 The broader view: Sense and Denotation as Proof and Truth
It is an old question why the two equations a = a and a = b should have a diﬀerent epistemological status, if a and b refer to (denote) the same object.
13 Moschovakis raised the question of the relation between (constructive) proofs and algorithms already in [25, §3.4].

232 Reinhard Kahle
This question, still asked by Poincare´ [28], gave reason to Frege’s seminal invention of the distinction between sense and denotation, [13]. This distinction is discussed in length in philosophical logic, and there are several attempts to formalize sense within intensional logic, cf. e.g., [1]. Up to today, these approaches, however, did not succeed to provide a satisfactory analysis.
An alternative approach was suggested by Moschovakis who analyses sense and denotation as algorithm and value, [23]:
“In contemporary computing terms, we have deﬁned an algorithm which computes the truth value of χ. . . . This algorithm is the referential intension of just intension of χ,
int(χ) = the algorithm which computes the truth value of χ,
and we propose to model the sense of χ by its referential intension.”
Of course, for the underlying notion of algorithm, Moschovakis suggests to use the notion discussed above.
At other places we considered to relate intentional phenomena to proofs while the extensional counterpart is related to truth. Thus our reading of a sense of a formula φ would be a proof of φ; in this context, obviously, a formula can have diﬀerent senses if it has diﬀerent proofs. This approach depends on the underlying axiom system (or better: the underlying axiom system is implicitly part of the sense of formula). But√, with this reading the epistemological diﬀerence of, for instance, 2 = 2 and 2 = 4 is obviously on the level of senses: the ﬁrst one has a trivial proof—it is an axiom (in any reasonable axiom system)—; the second one requires some deduction steps.
Some ﬁrst applications of our proposal are given in [17] with respect to necessity, and in [16] with respect to belief revision. However, these approaches are still informal, and a better theory of proofs is needed, a theory which we are looking for in this paper.
References
1. C. Anthony Anderson. General intensional logic. In D. Gabbay and F. Guenthner, editors, Handbook of Philosophical Logic, volume II, pages 355–385. Kluwer, 1984.
2. S. Artemov. Logic of proofs. Annals of Pure and Applied Logic, 67:29–59, 1994. 3. J. Avigad. Mathematical method and proof. Synthese, 153:105–149, 2006. 4. M. Baaz. Note on the generalization of calculations. Theoretical Computer Science,
224(1–2):3–11, 1999. 5. M. Baaz and P. Pudl´ak. Kreisel’s conjecture for l∃1. In P. Clote and J. Kraj´ıˇcek,
editors, Arithmetic Proof Theory and Computational Complexity, pages 30–49. Oxford University Press, 1993. 6. J. L. Bates and R. L. Constable. Proofs as programs. ACM Transactions on Programming Languages and Systems, 7(1):113–136, 1985. 7. L. Beklemishev and A. Visser. Problems in the logic of provability. In D. M. Gabbay, S. S. Goncharov, and M. Zakharyaschev, editors, Mathematical Problems from Applied Logic, volume I, pages 77–136. Springer, 2006.

Towards Reverse Proofs-as-Programs 233
8. A. Blass and Y. Gurevich. Algorithms vs. machines. Bulletin of the European Association for Theoretical Computer Science, 77:96–118, 2002.
9. A. Blass and Y. Gurevich. Algorithms: A quest for absolute deﬁnitions. Bulletin of the European Association for Theoretical Computer Science, 81:195–225, 2003.
10. E. W. Dijkstra. A Discipline of Programming. Prentice-Hall, 1976. 11. K. Doˇsen and Z. Petri´c. Proof-Theoretical Coherence. KCL Publications, 2004. 12. K. Doˇsen. Identity of proofs based on normalization and generality. The Bulletin
of Symbolic Logic, 9(4):477–503, 2003. 13. G. Frege. U¨ ber Sinn und Bedeutung. Zeitschrift fu¨r Philosophie und philosophische
Kritik, (NF 100):25–50, 1892. 14. D. Gries. The Science of Programming. Springer, 1982. 15. G. H. Hardy and E. M. Wright. An Introduction to the Theory of Numbers. Oxford,
4th edition, 1960. 16. R. Kahle. Structured belief bases. Logic and Logical Philosophy, 10:49–62, 2002. 17. R. Kahle. A proof-theoretic view of necessity. Synthese, 148(3):659–673, 2006. 18. R. Kahle. Review of: Freek Wiedijk (editor), The Seventeen Provers of the World.
Springer, 2006, [36]. Studia Logica, 87:369–374, 2007. 19. J. Kraj´ıˇcek and P. Pudl´ak. The number of proof lines and the size of proofs in ﬁrst
order logic. Archive for Mathematical Logic, 27:69–84, 1988. 20. Z. Manna and R. J. Waldinger. Toward automatic program synthesis. Communi-
cations of the ACM, 14:151–165, 1971. 21. Y. Moschovakis. The formal language of recursion. The Journal of Symbolic Logic,
54:1216–1252, 1989. 22. Y. Moschovakis. A mathematical modeling of pure, recursive algorithms. In A. R.
Meyer and M. A. Taitslin, editors, Logic at Botik ’89, pages 208–229. Springer, 1989. 23. Y. Moschovakis. Sense and denotation as algorithm and value. In J. Oikkonen and J. V¨a¨an¨anen, editors, Logic Colloquium ’90, pages 210–249. Springer, 1994. 24. Y. Moschovakis. A game-theoretic, concurrent and fair model of the typed lambdacalculus, with full recursion. In M. Nielsen and W. Thomas, editors, CSL ’97, pages 341–359. Springer, 1998. 25. Y. Moschovakis. On founding the theory of algorithms. In H. D. Dales and G. Oliveri, editors, Truth in mathematics, pages 71–104. Clarendon Press, 1998. 26. Y. Moschovakis. What is an algorithm? In B. Engquist and W. Schmid, editors, Mathematics unlimited — 2001 and beyond, pages 919–936. Springer, 2001. 27. I. H. Poernomo, J. N. Crossley, and M. Wirsing. Adapting Proofs-as-Programs. Springer, 2005. 28. H. Poincar´e. la Science et l’Hypoth´ese. Flammarion, 1902. 29. G. Polya. How To Solve It. Princeton University Press, 1945. 30. D. Prawitz. Natural Deduction. Almquist and Wiksell, 1965. 31. D. Prawitz. Ideas and results in proof theory. In J. E. Fenstad, editor, Proceedings of the Second Scandinavian Logic Symposium, pages 235–307. North-Holland, 1971. 32. J. C. Reynolds. The Craft of Programming. Prentice-Hall, 1981. 33. D. Scott. Foreword. In F. Wiedijk, editor, The Seventeen Provers of the World, pages vii–xii. Springer, 2006. 34. M. H. S¨orensen and P. Urzyczyn. Lectures on the Curry-Howard isomorphism. Elsevier, 2006. 35. L. Straßburger. What is a logic, and what is a proof? In Jean-Yves Beziau, editor, Logica Universalis, pages 135–145. Birkh¨auser, 2005. 36. F. Wiedijk, editor. The Seventeen Provers of the World. Springer, 2006.

Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems
Basil A. Kar´adais
Mathematisches Institut, Ludwig-Maximilians-Universit¨at Theresienstraße 39, 80333 Mu¨nchen, Germany karadais@math.lmu.de
Abstract. By a Plotkin deﬁnability theorem we mean here a statement of the following sort: a PCF-like language can be extended to a language so that a functional will be deﬁnable by a term if and only if it is computable [1–4]. We prove this result directly for a class of Scott information systems [5] that feature “atomicity” and “coherence”.
Key words: Atomic-Coherent Information Systems, Partial Computable Functionals, PCF Deﬁnability.
1 Partial Continuous Functionals
In late seventies, Gordon Plotkin [1] proved PCF-deﬁnability of computable functions on ﬂat domains. What we prove in section 2 directly is an analogous result in the slightly but interestingly diﬀerent setting of “atomic-coherent information systems”. These were introduced in [6] to serve as a better yet interpretation of data types than general information systems: they induce domains which are nonﬂat, in a way that allows constructors to be injective and to have disjoint ranges. Moreover, and interestingly enough, it can be shown that these domains are exactly the “coherent” ones (see Appendix), a notion that also goes back to Plotkin and the late seventies [7].
In this section we list notions and facts that form the raw material for the main result, drawing mainly from [6, §2.2]; for further preliminary material see Appendix. In the second section we proceed to our main theorem, namely, a direct proof of Plotkin’s deﬁnability result for atomic-coherent information systems; contrary to what one might expect, we will see that the proof goes through in a substantially more intricate way than in the ﬂat case.
Basic notions and facts. The information systems we will use are simple in that all discussion of consistence and entailment of information can be conducted on the primary level of atomic pieces of information, just by using binary relations. An atomic-coherent information system—from now on acis1—is a triple α = (Tα, Qα, £α), where Tα is a nonempty countable set of atomic pieces of
1 We will be using the term as a proper english noun, hence allowing for the forms acises for the plural and acis’s for the possessive.

Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems 235
information or just atoms (or tokens), Qα is the consistency, a reﬂexive and symmetric binary relation on Tα and £α is the entailment, a reﬂexive and transitive binary relation on Tα, so that consistency propagates through entailment, that is,
∀ . a Qα b ∧ b £α c → a Qα c .
a,b,c∈Tα
We will drop the subscripts in places we can aﬀord to. For U, V ⊆ T adopt the following shorthands: U Q a := ∀b∈U b Q a, U Q V := ∀a∈V U Q a, a £ U := ∀b∈U a £ b, U £ a := ∃b∈U b £ a, U £ V := ∀a∈V U £ a. Call U ⊆f Tα a (formal ) neighborhood (or ﬁnite approximation or consistent set), and write U ∈ Conα, if a Qα b for all a, b ∈ U . Call u ⊆ Tα an ideal (set) (or element or point), and write u ∈ Ideα, if a Qα b for all a, b ∈ u, and, whenever a £α b for some a ∈ Tα, then b ∈ u. In the following we will try to keep the notation ﬁxed: we will write a, b, . . . for atoms, U, V, . . . for consistent sets, X, Y, . . . for arbitrary ﬁnite sets of atoms, and u, v, . . . for inﬁnite sets of atoms, mainly ideals.
Lemma 1. Let α = (T, Q, £) be an acis. For a, b ∈ T , U, V, W ∈ Con, u, v ∈ Ide, the following hold.
1. a £ b → a Q b, 2. a1 £ b1 ∧ a2 £ b2 ∧ a1 Q a2 → b1 Q b2, 3. U1 £ V1 ∧ U2 £ V2 ∧ U1 Q U2 → V1 Q V2, 4. U Q V ∧ V £ W → U Q W , 5. u £ v ↔ v ⊆ u.
Denote the empty ideal ∅ ∈ Ide by ⊥. For a ﬁnite set of atoms X ⊆f T , deﬁne its (deductive) closure by X := {a ∈ T | X £ a} and the cone (of ideals) above it by ∇X := {u ∈ Ide | X ⊆ u}. Denote by Con the class of all closures of formal neighborhoods and by Kgl the class of all cones in the acis.
Lemma 2. Let X, Y ⊆ T and U, V ∈ Con.
1. X ∈ Con ↔ X ∈ Ide, 2. X ∪ Y = X ∪ Y , 3. X Q Y ↔ X Q Y , 4. X £ Y ↔ X ⊇ Y , 5. U Q V → ∇U ∩ ∇V = ∇(U ∪ V ).
Let α and β be two acises. Deﬁne their function space α → β by Tα→β := Conα × Tβ, (U, a) Qα→β (V, b) when U Qα V → a Qβ b, and (U, a) £α→β (V, b) when V £α U ∧a £β b. Deﬁne their cartesian product α×β, by Tα×β := Tα Tβ, a Qα×β b when either a, b belong to diﬀerent acises or are consistent in the same one, and a £α×β b when a entails b in either α or β; in this way we are able to approximate any (u, v) ∈ Ideα×β separately in each component.
Both the function space and the cartesian product of two acises can be proven to be again acises. Moreover, one can show that the pairs (u, v) ∈ Ideα × Ideβ are in a bijective correspondence with the ideals u ∪ v ∈ Ideα×β, as well as that

236 Basil A. Kar´adais
the Scott continuous mappings from Ideα to Ideβ (see Appendix) are exactly the ideals of α → β [6].
In the following we will be largely concerned with “application of ideals”. In general, deﬁne (set) application · : P(Tα→β) × P(Tα) → P(Tβ), by
{(Xi, bi)}i∈I · Y :=β {bi | Y £α Xi} .
Lemma 3. For the application operation the following hold.
1. If U ∈ Conα→β and V ∈ Conα, then U · V ∈ Conβ. Furthermore, if U Qα→β U and V Qα V then U · V Qβ U · V .
2. For all V ∈ Conα, it is U £α→β U if and only if U · V £β U · V . 3. For all U ∈ Conα→β, if V £α V then U · V £β U · V . 4. It is U · V := U · V . 5. For X ⊆f Tα→β, Y ⊆f Tα and Z ⊆f Tβ, where α and β are ﬁxed, the
relation Z = X · Y is Σ10-deﬁnable.
Arithmetical and boolean acises. We deﬁne the acis of lazy natural numbers as follows. Let ∗ be a preatom, intuitively meaning least information. Natural numbers given as usual by the constructors 0 and S give rise to an acis N as follows: a ∈ TN if a = ∗, or a = 0, or a = Sa for some a ∈ TN; a QN b if a = ∗, or b = ∗, or a = b = 0, or a = Sa ∧ b = Sb for a QN b ; a £N b if b = ∗, or a = b = 0, or a = Sa ∧ b = Sb for a £N b . Similarly, the boolean numbers tt and ff give rise to an acis B as follows: b ∈ TB if b = ∗, or b = tt, or b = ff; b1 QB b2 if b1 = ∗, or b2 = ∗, or b1 = b2 = tt, or b1 = b2 = ff; b1 £B b2 if b2 = ∗, or b1 = b2 = tt, or b1 = b2 = ff. Note that the least info atom ∗ is not considered to be a proper atom and that we will count it out whenever we talk about atoms, unless otherwise stated; on the level of ideals, least info is expressed by ⊥.
Acises which are function spaces whose target is N will be called arithmetical acises. In particular, we think of ideals of N as partial numbers and ideals of an arithmetical function space as partial continuous functionals. Write ∞ for the ideal {. . . , Sn∗, . . . , S∗} and n for the ideal {Sn0}. Furthermore, we take the risk of overloading the symbols tt and ff to mean the corresponding ideals of B as well. We refer to function spaces consisting of both arithmetical and boolean acises as arithmetical-boolean acises.
Deﬁne the total ideals Gα ⊆ Ideα, in an arithmetical acis α, inductively as follows: u ∈ GN if u = 0, or u = Sv for some v ∈ GN; u ∈ Gα→β if u(v) ∈ Gβ for all v ∈ Gα. In what follows, the total numbers 0, 1, . . . will mainly serve as indices.
2 Computable Functionals
Call a partial continuous functional computable if it is Σ10-deﬁnable as a set of atoms. It is direct to check (cf. Appendix) that evaluation and currying functionals are computable, that composition, application and cartesian products of computable functionals are computable, that projections are computable, and, consequently, that the functional [[M ]] is computable for any term M .

Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems 237

Recursion on parallel conditionals and existentials. We pave the road for
the deﬁnability statement by introducing the standard ﬁxed point and parallel
functionals. The proofs of the corresponding statements found in [3] are easy to
adapt to our nonﬂat setting. Let α be an arbitrary acis and f : α → α a continuous mapping. An ideal set
u ∈ Ideα is said to be the least ﬁxed point of f if f (u) = u and f (v) = v → u ⊆ v for all v ∈ Ideα. One can prove that the mapping f has a least ﬁxed point given by the equation Y(f ) = n∈GN f n(⊥), and that the ﬁxed point functional Y : (α → α) → α is continuous and computable for any α.
From now on we restrict our attention to arithmetical-boolean acises. Deﬁne the parallel conditional functional pcond : B → N → N → N by

 u 
pcond(p, u, v) := v
u ∩ v

p = tt p = ff . p=⊥

One can prove that the parallel conditional functional is continuous and computable.
Deﬁne the existential functional exist : (N → B) → B by

 ff 
exist(u) = tt
⊥

u(⊥) = ff ∃n∈GN u(n) = tt . otherwise

For the ﬁrst clause of the deﬁnition, notice that whenever u(⊥) = ff, it should be v(⊥) = ff for all ideals v £N→B u. Again, one can prove that the existential functional is continuous and computable.
Call a partial continuous functional u ∈ Ideα1→···→αp→N recursive in pcond and exist if it can be deﬁned explicitly for all arguments v1, . . . , vp by an equation

u(v1, . . . , vp) = t(v1, . . . , vp)

where t is a simply-typed lambda term (cf. Appendix) built up from variables v1, . . . , vp, algebra constructors, ﬁxed point functionals, parallel conditional functionals and existential functionals.
A functional we will need in what follows is the disjunction functional or : B → B → B deﬁned by
or(p, q) := pcond(p, tt, q)

which unfolds to

 tt

p = tt ∨ q = tt



or(p, q) := ff p = q = ff .

⊥ otherwise

This is a continuous and computable functional, since it is deﬁned by pcond. Its p-ary generalization we will denote by ORpi=1.

238 Basil A. Kar´adais

Enumeration and inconsistency functionals. We begin with two lemmas.

Lemma 4. The following hold.

1. For all a, b ∈ TN, we have the following conditional dichotomy property: a QN b → (a £N b ∨ b £N a).
2. For an arithmetical acis α, if {(Ul, bl)}l≤n ∈ Conα→N and u ∈ Ideα, then
{(Ul, bl)}l≤n(u) ∈ ConN.
We introduce a notion of “relative height” between partial numbers. Let u, v ∈ IdeN and n ∈ GN. Say that u is above n, and write u n, if it contains information built from at least n constructors, that is, if either u £N Sn−10 or u £N Sn∗. Say further that u is above v, and write u v, if it is above any index below v, that is, if v n → u n for all n ∈ GN. Note that aboveness is not antisymmetric, since Sn0 Sn+1⊥ and Sn+1⊥ Sn0 for all n. It is also
obvious that aboveness between ideals is a total preorder with single maximum element ∞ and single minimum element ⊥, as well as that, for total ideals, it reduces to the standard ≥ relation.
This straightforward notion, which is merely based on the simple tree struc-
ture of the lazy natural numbers, is nevertheless the necessary and suﬃcient
step beyond the techniques used in the ﬂat case. The calculus it provides is
summarized in the following.

Lemma 5. Concerning aboveness ⊆ IdeN × IdeN the following hold.

1. If u £N v then u v. 2. If v w and w v then v = w for all v, w ⊆ u, u ∈ IdeN. 3. If u QN v and u v then u £N v. 4. If u v and u QN v then v ∈ GN.

For every arithmetical-boolean acis we ﬁx an enumeration of consistent sets
{Un}n∈GN so that U0 = ∅ holds, and the following are primitive recursive relations: Un Qα Um, Un £α Um, Unα→β · Umα = Ukβ and Un ∪ Um = Uk, with k = 0 if Un Qα Um. In the following we will lift the notational convention of the ﬁrst
section and write b for singleton neighborhoods, whenever we want to stress that

they behave as atoms. We will use inconsistency functionals incnsα : α → GN → B, deﬁned by

 tt 
incnsα(u, n) := ff
⊥

u Qα Un u £α Un . otherwise

having application in mind: let β be an arbitrary acis and (Un, bn) an atom of a partial continuous functional v of type α → β; the only case where the atom contributes information concerning the value of v(u) is for incnsα(u, n) to be ff, so bn will belong to the value.2
2 To make case distinctions in subsequent proofs less branching, we adopt the following convention. Let α and β be arbitrary arithmetical-boolean acises. An ideal mapping f : α → N → β that satisﬁes f (u, x, v) = ⊥ for x ∈ GN, will be informally typed by α → GN → β, and be treated only on its arguments where x is total.

Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems 239

In order to deﬁne the functionals incnsα, we will further need to deﬁne an appropriate enumeration functional enα : GN → N → α to be able to enumerate all ﬁnitely generated extensions of Um, for Um ∈ Conα.
Proposition 6. Let α be an acis over N and B.

1. There exists a functional enα : GN → N → α, with the following properties.
enα(m, x) = Um, when x ∈ GN , enα(m, n) = Un, when Un £α Um ,

which is recursive in pcond and exist. 2. The inconsistency functional incnsα is recursive in pcond and exist.

Proof. By induction on α. For the enumeration functionals. Let α = α1 → · · · → αp → N, Um ∈
Conα1→···→αp→N and j, k, and h be primitive recursive functions such that
Um = (Uj(m,1,l), . . . , Uj(m,p,l), bk(m,l)) l<h(m) ,

with

l > l → bk(m,l) bk(m,l ) .

(1)

In this representation we have Uj(m,i,l) ∈ Conαi and bk(m,l) ∈ ConN, for all l < h(m) and all 1 ≤ i ≤ p, while h(m) denotes the number of elements of
Um.3 Consider the collection {Um,l}l of progressive approximations to Um; in particular, for l ∈ GN, deﬁne

Ux,l := ∅ if x ∈ GN , Um,0 := ∅ , Um,l+1 := Um,l ∪ (Uj(m,1,l), . . . , Uj(m,p,l), bk(m,l))

.

Observe that Um,h(m) = Um. For u ∈ Ideα1→···→αp , let qu,m,l express whether the application Um,l+1(u)
does not contribute information to the value of Um,l(u), that is,



p

tt 

qu,m,l

:=

OR
i=1

incnsαi (ui,

j(m,

i,

l))

=

ff

⊥

∃pi=1 ui Qαi Uj(m,i,l) ∀pi=1 ui £αi Uj(m,i,l)
otherwise

3 Observe that, since j, k, and h are primitive recursive, the respective generated ideals are total, and so all values (m, i, l), k(m, l), and h(m) will be total for total arguments. Based on this observation, we write j(m, i, l) for (m, i, l), and so on, with the convention that m, i, and l, as well as n (for m, ı, l, and n respectively), will denote total ideals in N; for not necessarily total ones, together with u and v, we reserve x and y.

240 Basil A. Kar´adais

Observe that, in the last case, it is ∃pi=1 ui £αi Uj(m,i,l), but we still have ∀pi=1 ui Qαi Uj(m,i,l). Similarly, let

 tt 
qy,m,l := incnsN(y, k(m, l)) = ff
⊥

y QN bk(m,l) y £N bk(m,l) . otherwise

Observe again that in the last case, by the dichotomy property, we have bk(m,l) £N y. Deﬁne now an auxiliary functional Ψ : α1 → · · · → αp → N → N → GN → N by
Ψu,x(y, l) := y if x ∈ GN , Ψu,m(y, 0) := y ,
Ψu,m(y, l + 1) := pcond qu,m,l, Ψu,m(y, l),
bk(m,l) ∪ pcond qΨu,m(y,l),m,l, ⊥, Ψu,m(y, l) .
Using Lemma 5 and Lemma 4 respectively, one can prove the following.
Lemma 7. It is Ψu,x(⊥, l) = Ux,l(u).
Lemma 8. If ∀l<h(m) . qu,m,l = tt → qy,m,l = qu,m,l, then ∀l<h(m) Ψu,m(y, l) = y.
Let
Φ(u, x, y) := Ψu,x y, h(x) , en(m, x)(u) := Φ (u, m, Φ(u, x, ⊥)) .
One can now easily show how the desired properties of en hold. The ﬁrst one follows from Lemma 7. For the second one, suppose that Un £α1→···→αp→N Um, for n ∈ GN; we get en(m, n)(u) = Ψu,m(Un(u), h(m)) by Lemma 7, with Un(u) £N Um(u) by Lemma 3; let l < h(m); if qu,m,l = ff, then ui £αi Uj(m,i,l) for all i, so Um,l(u) = bk(m,l), which gives Um(u) £N bk(m,l); by hypothesis and transitivity, we have Un(u) £N bk(m,l), which yields qUn(u),m,l = ff; if, on the other hand, qu,m,l = ⊥, then, by hypothesis, it is ui Qαi Uj(m,i,l) for all i and
p
∀ Uj(m,i,l) £αi Uj(n,i,l ) ∧ bk(n,l ) £N bk(m,l)
i=1
for some l < h(n); by propagation of consistency through entailment and the deﬁnition of consistency in a function space, we get
Un(u) QN bk(n,l ) ∧ bk(n,l ) £N bk(m,l) ,
which, by propagation again, gives Un(u) QN bk(m,l), and so, qUn(u),m,l = ⊥; so Lemma 8 applies and we are done.
For the inconsistency functionals. Omitted.

Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems 241

Deﬁnability. We are now in the position to prove the main result.
Theorem 9. A partial continuous functional of type α → N over N and B is computable if and only if it is recursive in pcond and exist.

Proof. Since all λ-terms are computable and computable functionals are closed under application, it follows that any functional is computable if it is recursive in pcond and exist. For the converse, we consider here arrow types only. Let Ω : α1 → · · · → αp → N be a computable functional. It will be represented as a primitive recursively enumerable set of atoms, that is,

Ω = (Uf1(n), . . . , Ufp(n), bg(n)) n∈GN ,

where, for each i = 1, . . . , p, Ufi(n) follows an enumeration of Conαi , bg(n) follows an enumeration of ConN, and f1, . . . , fp, g are ﬁxed primitive recursive functions.
For arbitrary u ∈ Ideα1→···→αp , v ∈ IdeN, let



p

tt 

qu,n

:=

OR
i=1

incnsαi (ui,

fi(n))

=

ff

⊥

∃pi=1 ui Qαi Ufi(n) ∀pi=1 ui £αi Ufi(n)
otherwise

,

 tt 
qv,n := incnsN(v, g(n)) = ff
⊥

v QN bg(n) v £N bg(n) , otherwise

and deﬁne a functional ω : α1 → · · · → αp → (N → N) → GN → N by

ωu(ψ)(n) := pcond qu,n, ψ(n + 1), bg(n) ∪ pcond qψ(n+1),n, ⊥, ψ(n + 1) .

We show that Ω(u) = Y(ωu)(0). In particular, will show that both recursion on ωu at 0 and Ω(u) entail the very same information, that is,

∀ . Ω(u) £N bg(n) ↔ Y(ωu)(0) £N bg(n) .
n∈GN
For the right direction, suppose that there exists an index n ∈ GN such that Ω(u) £N bg(n). This means that

p

∃.
m∈GN

∀ ui £αi
i=1

Ufi(m) ∧ bg(m) £N bg(n)

.

(2)

We claim that

∀ ωuk+1(λx. ⊥)(m − k) £N bg(m)
k≤m

(3)

and we prove it by induction on k. For k = 0, by (2), it is ωu(λx. ⊥)(m) = bg(m) £N bg(m). For brevity, let v := ωuk+2(λx. ⊥)(m − k − 1) and v0 := ωuk+1(λx. ⊥)(m − k); the induction hypothesis is that v0 £N bg(m). For k + 1 we
have

v = pcond qu,m−k−1, v0, bg(m−k−1) ∪ pcond (qv0,m−k−1, ⊥, v0) .

242 Basil A. Kar´adais

We argue by cases on the outer boolean argument. If qu,m−k−1 = tt, then v :=
v0 £N bg(m) by induction hypothesis. If qu,m−k−1 = ff, then ui £αi Ufi(m−k−1) for all i, so Ω(u) £N bg(m−k−1).
By (2) and Lemma 1 we get bg(m) QN bg(m−k−1), and by Lemma 4 this yields

bg(m) £N bg(m−k−1) ∨ bg(m−k−1) £N bg(m)

(4)

while v = bg(m−k−1) ∪ pcond (qv0,m−k−1, ⊥, v0)). We distinguish cases on the inner boolean now. If qv0,m−k−1 = tt, then v0 QN bg(m−k−1); if it were bg(m) £N bg(m−k−1), by induction hypothesis, transitivity of entailment would yield v0 £N bg(m−k−1), which is absurd; the dichotomy property (4) gives bg(m−k−1) £N bg(m), and
v := bg(m−k−1) ∪ ⊥ = bg(m−k−1) £N bg(m) .
If qv0,m−k−1 = ff, then v0 £N bg(m−k−1), and

v := bg(m−k−1) ∪ v0 = v0 £N bg(m)
by induction hypothesis. If qv0,m−k−1 = ⊥, then v0 £N bg(m−k−1) ∧ v0 QN bg(m−k−1); if it were bg(m) £N bg(m−k−1), we would again have a contradiction due to transitivity of entailment and induction hypothesis, so dichotomy (4) gives bg(m−k−1) £N bg(m), and

v := bg(m−k−1) ∪ ⊥ = bg(m−k−1) £N bg(m) .
If now it is qu,m−k−1 = ⊥, then ui Qαi Ufi(m−k−1) for all i, so by (2) and propagation we get Ufi(m−k−1) Qαi Ufi(m). By deﬁnition of consistency in a function space, we get bg(m−k−1) QN bg(m), and this yields the dichotomy property (4) again, so we distinguish cases on the inner boolean and argue as before.
We proved that v £N bg(m) in all cases. Letting k = m in (3), we get
ωun+1(λx. ⊥)(0) £N bg(m) ⇒ Y(ωu)(0) £N bg(n)
by (2) and the deﬁnition of the ﬁxed point functional. So, Y(ωu)(0) entails all information of Ω(u).
Conversely, suppose that

Y(ωu)(0) £N bg(n) .

(5)

We claim that

∃ . Ω(u) £N bg(m) ∧ bg(m) £N bg(n) .
m∈GN

(6)

Suppose on the contrary that this is not the case. Then, for all m ∈ GN, if Ω(u) £N bg(m) then bg(m) £N bg(n). Now, the conditional clause gives qu,k = ff ∧ bg(k) £N bg(m) for some k ∈ GN, which a fortiori gives

ωu(λx. ⊥)(k) £N bg(k) ∧ bg(k) £N bg(m) .

Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems 243
By transitivity we get ωu(λx. ⊥)(k) £N bg(m), and so, all put together give
Y(ωu)(k) £N bg(m) → bg(m) £N bg(n)
for some k. But for m = n and k = 0 we have a contradiction to (5). By transitivity of entailment and (6) we get Ω(u) £N bg(n). So, Ω(u) entails all information of Y(ωu)(0).
Acknowledgements
Many thanks go to Helmut Schwichtenberg for his impressive knack of coming up with insightful hints in minimal time. Sincere thanks are also due to the referees. The author was supported by a Marie Curie Early Stage Training fellowship (MEST-CT-2004-504029).
References
1. Plotkin, G.D.: LCF considered as a programming language. Theoret. Comput. Sci. 5(3) (1977/78) 223–255
2. Escard´o, M.H.: PCF extended with real numbers: a domain-theoretic approach to higher-order exact real number computation. PhD thesis, University of London, Imperial College of Science, Technology and Medicine, Department of Computing (1997)
3. Schwichtenberg, H.: Classifying recursive functions. In Griﬀor, E., ed.: Handbook of computability theory. Volume 140 of Stud. Logic Found. Math. North-Holland, Amsterdam (1999) 533–586
4. Normann, D.: Computability over the partial continuous functionals. J. Symbolic Logic 65(3) (2000) 1133–1142
5. Scott, D.S.: Domains for denotational semantics. In: Automata, languages and programming (Aarhus, 1982). Volume 140 of Lecture Notes in Comput. Sci. Springer, Berlin (1982) 577–613
6. Schwichtenberg, H.: Recursion on the partial continuous functionals. In Dimitracopoulos, C., Newelski, L., Normann, D., Steel, J., eds.: Logic Colloquium ’05. Volume 28 of Lecture Notes in Logic., Association for Symbolic Logic (2006) 173– 201
7. Plotkin, G.D.: T ω as a universal domain. J. Comput. System Sci. 17(2) (1978) 209–236
8. Stoltenberg-Hansen, V., Lindstr¨om, I., Griﬀor, E.R.: Mathematical theory of domains. Volume 22 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press (1994)
9. Kar´adais, B.A.: Atomicity, coherence of information, and pointfree structures. Submitted (2008)
10. Winskel, G.: Event structures. In: Advances in Petri nets 1986, Part II (Bad Honnef, 1986). Volume 255 of Lecture Notes in Comput. Sci. Springer, Berlin (1987) 325–392
11. Zhang, G.Q.: dI-domains as prime information systems. Inform. and Comput. 100(2) (1992) 151–177
12. Santocanale, L.: A nice labelling for tree-like event structures of degree 3. In: CONCUR 2007 – Concurrency Theory. Volume 4703 of Lecture Notes in Computer Science. Springer, Berlin (2007) 151–165

244 Basil A. Kar´adais
Appendix
Scott topology, cartesian closure, syntax and semantics. We collect here material which is preliminary to the previous exposition and hints at the theoretical background of the present work. Again, one may further refer to [6] and [3] (corresponding results of the latter, which refers to the ﬂat case, are easy to adapt to our setting).
The set of ideals in an acis carries a natural topology based on the cones above its formal neighborhoods. In particular, for every acis α, Kglα forms the basis of a topology on Ideα, called Scott topology. A collection of ideals U ⊆ Ideα is an open set in the Scott topology if and only if u ⊆ v implies v ∈ U for all u ∈ U (Alexandrov condition), and for all u ∈ U there is a U ⊆f u so that U ∈ U (Scott condition), or equivalently, if U = U∈U ∇U . Let f : Ideα → Ideβ be a mapping between ideals, or an ideal mapping. It is monotone when it preserves inclusion, that is, u ⊆ v → f (u) ⊆ f (v). It is (Scott) continuous if and only if either of the following equivalent conditions holds: (a) the mapping is monotone and satisﬁes the principle of ﬁnite support, that is, if b ∈ f (u) then b ∈ f (U ) for some U ⊆f u; (b) the mapping is monotone and commutes with directed unions, that is, f u∈D u = u∈D f (u) for all directed sets D of ideals of α. Easy examples of continuous ideal mappings are identity and constant mappings, compositions of continuous mappings, application of ideals and the standard projections πα, πβ of cartesian products α × β.
Let r ∈ Ideα→β be an ideal and f : Ideα → Ideβ a continuous ideal mapping. Deﬁne a continuous mapping cm(r) : Ideα → Ideβ and an ideal set is(f ) ∈ Ideα→β respectively by
b ∈ cm(r)(u) :⇔ ∃ . U ⊆f u ∧ (U, b) ∈ r ,
U ∈Conα
(U, b) ∈ is(f ) :⇔ b ∈ f (U ).
Indeed, through these assignments we obtain that the continuous mappings from Ideα to Ideβ are exactly the ideals of α → β [6]. Since is (cm(r)) = r and cm (is(f )) = f , we identify r with cm(r) and f with is(f ), and rely on the context to prevent ambiguity. Moreover, we allow ourselves to write f : α → β meaning f : Ideα → Ideβ, or equivalently, f ∈ Ideα→β.
Let α, β and γ be acis with Tα∩Tβ = ∅. For every pair f : γ → α, g : γ → β of continuous mappings, there exists a unique continuous mapping h : γ → α × β such that f = πα ◦ h and g = πβ ◦ h, given by h(u) := (f (u), g(u)) for all u ∈ Ideγ (universal property of cartesian products). Deﬁne the cartesian product f × g : α × β → γ × δ of two continuous mappings f : α → γ and g : β → δ, where Tα ∩ Tβ = ∅, by f × g(u, v) := (f (u), g(v)). A mapping f : α × β → γ is continuous if and only if it is continuous in each component, that is, if and only if all sections fαv : α → γ, v ﬁxed and all sections fβu : β → γ, u ﬁxed, deﬁned by fαv(u) := f (u, v) and fβu(v) := f (u, v) are continuous.
Let α, β, and γ be acises. Deﬁne the evaluation mapping eval : (α → β)×α → β by eval(f, u) := f (u) and the currying mapping curry : (α × β → γ) → (α →

Plotkin Deﬁnability Theorem for Atomic-Coherent Information Systems 245
(β → γ)) by curry(f )(u, v) := f (u, v); it is direct to see that the currying mapping is bijective. The evaluation and currying mappings are continuous. All of the above facts together provide that the acises together with continuous mappings between their function spaces form a cartesian closed category, with the triple (∅, ∅, ∅) being the terminal information system.
The types we consider are N and B as base types, function types α → β and product types α × β. We build simply-typed λ-terms based on typed variables uα, vα, wα, . . . and typed constants cα by application (M α→βN α)β and λ-abstraction (λuα. M β)α→β. As expected, we associate each type to the corresponding acis. For every λ-term M α whose variables are among uα1 1 , . . . , uαnn there is an ideal [[M ]] : α1 × · · · × αn → α such that
[[ui]](u1, . . . , un) = ui ,
[[M N ]](u1, . . . , un) = [[M ]](u1, . . . , un) ([[N ]](u1, . . . , un)) ,
[[λu. M ]](u1, . . . , un)(v) = [[M ]](u1, . . . , un, v) .
Acises vs. other structures. Let α = (T, Con, ) be a Scott information system in the sense of, say, [8]. Call it atomic if, whenever U b, it is {a} b for some a ∈ U , and coherent if for all U ⊆f T , it is U ∈ Con if and only if {a, b} ∈ Con for all a, b ∈ U . It is an easy exercise to see that a Scott information system which is atomic and coherent is exactly an acis in our sense. Moreover, let D = (D, ≤, ⊥) be a domain (in the sense again of [8]). Call it coherent if lubi∈I ui ∈ Dc exactly when lub {ui, uj} ∈ Dc for all i, j ∈ I. One can show the following [9]. (a) Given an arbitrary information system α, one can deﬁne an atomic information system αa and a coherent information system αc, so that Ideα ∼= Ideαa and Ideα ⊆ Ideαc . So, to the extend that one is concerned with the ideals induced by an information system, one can choose to work with their more practical atomic version at no cost. On the other hand, coherent information systems are generally idealwise richer. (b) The domains induced by coherent information systems are exactly the coherent domains.
As pointed out by a referee, a structure that holds a strong resemblance to acises—more accurately, to Scott information systems in general—is an event structure in the sense of [10], a fact that has already been noticed elsewhere in the literature. Apart from the obvious diﬀerence in terms of axiomatization (for instance, in an event structure there is no ‘propagation of consistency through enabling’ required) the diﬀerence really lies in their quite disjoint perpective and intended meaning: information systems describe computation “logically” whereas event structures do it “temporally” [11]. Bringing the relation down to coherence, it is interesting that coherent event structures have also been considered in the literature (for example in [12]); these seem to correspond exactly to our acises. It is an interesting question whether these could interrelate in any fruitful way in the future.

Safety Properties Veriﬁcation for Pfaﬃan Dynamics
Margarita Korovina1 and Nicolai Vorobjov2
1 The University of Manchester, UK, and IIS SB RAS, Novosibirsk, Russia korovina@brics.dk,
http://www.brics.dk/~korovina 2 Department of Computer Science, University of Bath, UK
nnv@cs.bath.ac.uk, http://www.bath.ac.uk/~masnnv
Abstract. In this paper we propose an algorithm for veriﬁcation of safety properties of Pfaﬃan dynamical systems. We also show that this algorithm has an elementary (doubly-exponential) upper complexity bound.
1 Introduction
One of the important problems in the theory of dynamical systems is understanding of the behavior of a dynamical system with respect to safety properties. In other words it would be desirable for a given dynamical system to verify a safety property which states that ”something bad does never happen”, for examples, the power plant will never blow up, the reactor temperature will never exceed 100o C.
In mathematical settings this problem is formalised in the following way. We consider a continuous dynamical system γ : G1 × T → G2, where G1 ⊆ IRk1 is a set of control parameters, T is an interval of time and G2 ⊆ IRk2 is a state space. Let U be a set of control parameters. A safety property is formalised by an invariant which given by a condition Φ on the states and requires that Φ holds for all reachable states under the control U , i.e. ∀x ∈ U ∀t ∈ T Φ(γx(t)). In this case we say the subset U ⊆ G1 satisﬁes the invariant Φ and the dynamic γ is safety under the control U. We assume that dynamical systems, invariants and sets of control parameters are deﬁned by Pfaﬃan functions, either implicitly (via triangular systems of ordinary diﬀerential equations) or explicitly (by means of equations and inequalities involving Pfaﬃan functions). Such functions naturally arise in applications as real analytic solutions of triangular ﬁrst order partial diﬀerential equations with polynomial coeﬃcients, and include polynomials, algebraic functions, exponentials, and trigonometric functions in appropriate domains. Pfaﬃan functions form the largest natural class of real analytic functions which have a uniform description and an explicit characterisation of complexity of their representations in terms of formats.
Our goal is to characterise the subsets of control parameter space which satisfy a given invariant. In order to achieve our goal we use encoding trajectories

Safety Properties Veriﬁcation for Pfaﬃan Dynamics 247
of a Pfaﬃan dynamical system by ﬁnite words [3, 7] and cylindrical cell decomposition for semi-Pfaﬃan sets [10, 4]. Based on this technique we construct an algorithm for safety properties veriﬁcation for Pfaﬃan dynamical systems with an elementary exponential upper bound.

2 Basic Deﬁnitions and Notions

2.1 Pfaﬃan functions and related sets

In this section we overview the theory of Pfaﬃan functions and sets deﬁnable with Pfaﬃan functions. The detailed exposition can be found in the survey [4].

Deﬁnition 1. A Pfaﬃan chain of the order r ≥ 0 and degree α ≥ 1 in an open domain G ⊂ IRn is a sequence of real analytic functions f1, . . . , fr in G satisfying
diﬀerential equations

∂fj ∂xi

=

gij(x, f1(x), . . . , fj(x))

(1)

for 1 ≤ j ≤ r, 1 ≤ i ≤ n. Here gij(x, y1, . . . , yj) are polynomials in x = (x1, . . . , xn, y1, . . . , yj) of degrees not exceeding α.
A function
f (x) = P (x, f1(x), . . . , fr(x)),

where P (x, y1, . . . , yr) is a polynomial of a degree not exceeding β ≥ 1, the sequence f1, . . . , fr is a Pfaﬃan chain of order r and degree α, is called a Pfaﬃan function of order r and degree (α, β).

It is worth noting, apart from polynomials, the class of Pfaﬃan functions includes real algebraic functions, exponentials, logarithms, trigonometric functions, their compositions, and other major transcendental functions in appropriate domains (see [4, 5]). Now we introduce classes of sets deﬁnable with Pfaﬃan functions. In the case of polynomials they reduce to semialgebraic sets whose quantitative and algorithmic theory is treated in [1].
Deﬁnition 2. A set X ⊂ IRn is called semi-Pfaﬃan in an open domain G ⊂ IRn if it consists of the points in G satisfying a Boolean combination of some atomic equations and inequalities f = 0, g > 0, where f, g are Pfaﬃan functions having a common Pfaﬃan chain deﬁned in G. A semi-Pfaﬃan set X is restricted in G if its topological closure lies in G.

Deﬁnition 3. A set X ⊂ IRn is called sub-Pfaﬃan in an open domain G ⊂ IRn if it is the image of a semi-Pfaﬃan set under a projection into a subspace.

In the sequel we will be dealing with the following subclass of sub-Pfaﬃan sets.

248 Margarita Korovina and Nicolai Vorobjov
Deﬁnition 4. Suppose I¯ ⊂ IR is a closed interval. Consider the closed cube I¯m+n in an open domain G ⊂ IRm+n and the projection map π : IRm+n → IRn. A subset Y ⊂ I¯n is called restricted sub-Pfaﬃan if Y = π(X) for a restricted semi-Pfaﬃan set X ⊂ I¯m+n.

Note that a restricted sub-Pfaﬃan set need not be semi-Pfaﬃan.

Deﬁnition 5. Consider a semi-Pfaﬃan set

X :=

{x ∈ IRn| fi1 = 0, . . . , fili = 0, gi1 > 0, . . . , giji > 0} ⊂ G,

1≤i≤M

(2)

where fis, gis are Pfaﬃan functions with a common Pfaﬃan chain of order r and
degree (α, β), deﬁned in an open domain G. Its format is the tuple (r, N, α, β, n), where N ≥ 1≤i≤M (li + ji). For n = m + k and a sub-Pfaﬃan set Y ⊂ IRk such that Y = π(X), its format is the format of X.

Remark 1. In this paper we are concerned with complexities of computations, as functions of the format. In the case of Pfaﬃan dynamical systems these sizes and complexities also depend on the domain G. So far our deﬁnitions imposed no restrictions on an open set G, thus allowing it to be arbitrarily complex and to induce this complexity on the corresponding semi- and sub-Pfaﬃan sets. To avoid this we will always assume in the context of Pfaﬃan dynamical systems that G is “simple”, like IRn, or In for open I ⊆ IR.

Remark 2. In this paper we construct and examine complexities of algorithms for veriﬁcation of safety properties. In order to estimate the “eﬃciency” of a computation we need to specify more precisely a model of computation. As such we use a real number machine which is an analogy of a classical Turing machine but allows the exact arithmetic and comparisons on the real numbers. Since we are interested only in upper complexity bounds for algorithms, there is no need for a formal deﬁnition of this model of computation (it can be found in [2]). In some of our computational problems we will need to modify the standard real number machine by equipping it with an oracle for deciding feasibility of any system of Pfaﬃan equations and inequalities. An oracle is a subroutine which can be used by a given algorithm any time the latter needs to check feasibility. We assume that this procedure always gives a correct answer (“true” or “false”) though we do not specify how it actually works. An elementary step of a real number machine is either an arithmetic operation, or a comparison (branching) operation, or an oracle call. The complexity of a real number machine is the number of elementary steps it makes in the worst case until termination, as a function of the format of the input.
In the special case of semialgebraic sets, the oracle can be replaced by a proper real number machine, so the algorithm for checking of satisﬁability of an invariant can be realized as a standard real number machine.

Safety Properties Veriﬁcation for Pfaﬃan Dynamics 249
2.2 Cylindrical Cell Decompositions
Now we deﬁne cylindrical decompositions of semi- and sub-Pfaﬃan sets in a cube I¯n, where I¯ is a closed interval.
Deﬁnition 6. A cylindrical cell in I¯n is deﬁned by induction as follows.
1. A cylindrical 0-cell in I¯n is an isolated point. 2. A cylindrical 1-cell in I¯ is an open interval (a, b) ⊂ I¯. 3. For n ≥ 2 and 0 ≤ k < n a cylindrical (k + 1)-cell in I¯n is either a graph of a
continuous bounded function f : C → IR, where C is a cylindrical (k +1)-cell in I¯n−1 and k < n − 1, or else a set of the form
{(x1, . . . , xn) ∈ I¯n| (x1, . . . , xn−1) ∈ C and
f (x1, . . . , xn−1) < xn < g(x1, . . . , xn−1)}, where C is a cylindrical k-cell in I¯n−1, and f, g : C → I¯ are continuous bounded functions such that f (x1, . . . , xn−1) < g(x1, . . . , xn−1) for all points (x1, . . . , xn−1) ∈ C.
Deﬁnition 7. A cylindrical cell decomposition D of a subset A ⊂ I¯n with respect to the variables x1, . . . , xn is deﬁned by induction as follows.
1. If n = 1, then D is a ﬁnite family of pair-wise disjoint cylindrical cells (i.e., isolated points and intervals) whose union is A.
2. If n ≥ 2, then D is a ﬁnite family of pair-wise disjoint cylindrical cells in I¯n whose union is A and there is a cylindrical cell decomposition of π(A) such that π(C) is its cell for each C ∈ D, where π : IRn → IRn−1 is the projection map onto the coordinate subspace of x1, . . . , xn−1.
Deﬁnition 8. Let B ⊂ A ⊂ I¯n and D be a cylindrical cell decomposition of A. Then D is compatible with B if for any C ∈ D we have either C ⊂ B or C ∩ B = ∅ (i.e., some subset D ⊂ D is a cylindrical cell decomposition of B).
Deﬁnition 9. For a given ﬁnite family f1, . . . , fN of Pfaﬃan functions in an open domain G we deﬁne its consistent sign assignment as a non-empty semiPfaﬃan set in G of the kind
{x ∈ G | fi1 = 0, . . . , fiN1 = 0, fiN1 +1 > 0 . . . , fiN2 > 0, fiN2 +1 < 0, . . . , fiN < 0},
where i1, . . . , iN1 , . . . , iN2 , . . . , iN is a permutation of 1, . . . , N .
Theorem 1. [5, 9] Let f1, . . . , fN be a family of Pfaﬃan functions in an open domain G ⊂ IRn, G ⊃ I¯n having a common Pfaﬃan chain of order r, and degrees (α, β). Then there is an algorithm (with the oracle) producing a cylindrical cell decomposition of I¯n which is compatible with each consistent sign assignment of f1, . . . , fN . Each cell is a sub-Pfaﬃan set represented as a projection of a semiPfaﬃan set in DNF. The number of cells, the components of their formats and the complexity of the algorithm are less than
N (r+n)O(n) (α + β)(r+n)O(n3) .

250 Margarita Korovina and Nicolai Vorobjov
We summarize main properties of Pfaﬃan functions in the following propositions.
• Pfaﬃan functions can be considered as generalisation of algebraic functions. • Pfaﬃan functions have the uniform description and the explicit characteri-
zation of complexity of their representations. • The class of Pfaﬃan functions includes exp, trigonometrical functions deﬁned
in appropriate domains, and more generally solutions of a large class of diﬀerential equations. • The structure IR = IR, +, ∗, 0, 1, <, {f1, . . . , fn} , where f1, . . . , fn are Pfafﬁan, is o-minimal, i.e. deﬁnable sets have only a ﬁnite number of connected deﬁnable components, in the other words, it has ﬁniteness property.
3 Pfaﬃan Dynamical Systems
3.1 Pfaﬃan Dynamics and Related Sets
We now recall deﬁnitions concerning Pfaﬃan dynamical systems. Deﬁnition 10. Let G1 ⊂ IRk1 and G2 ⊂ IRk2 be open domains. A Pfaﬃan dynamical system is a map
γ : G1 × (−T, T ) → G2
with a semi-Pfaﬃan graph, where G1 is a set of control parameters, (−T, T ) is an interval of time, and G2 is a state space.
For a given x ∈ G1 the set
Γx = {y|∃t ∈ (−T, T ) (γ(x, t) = y)} ⊂ G2
is called the trajectory (or evolution) determined by x, and the graph
Γx = {(t, y)| γ(x, t) = y} ⊂ (−T, T ) × G2
is called the integral curve determined by x.
Deﬁnition 11. Let U ⊆ G1. A set Inv ⊆ G2 is called invariant under the dynamical system γ and the control U if for all x ∈ U and for all t ∈ T , γx(t) ∈ Inv.
In the next sections we investigate the behavior of a Pfaﬃan dynamical system with respect to a given semi-Pfaﬃan invariant.
3.2 Encoding Trajectories by Words
We now introduce, following [3, 7], a technique of encoding trajectories of dynamical systems by words. Consider a Pfaﬃan dynamical system γ : G1 × (−T, T ) → G2, where G1 ⊂ IRk1 and G2 ⊂ IRk2 are open domains, and a partition P := {P1, . . . , Ps} of G2 into s semi-Pfaﬃan sets Pj. Let the graph of γ and each set

Safety Properties Veriﬁcation for Pfaﬃan Dynamics 251

Pj have a format (r, N, α, β, n), where n ≥ k1 + k2 + 1, and all Pfaﬃan functions involved have a common Pfaﬃan chain. Fix x ∈ G1. Deﬁne the set of points and open intervals in IR:

Fx := {J| J is a point or an interval in (−T, T) maximal w.r.t. inclusion for the

property ∃i ∈ {1, . . . , s}∀t ∈ J (γ(x, t) ∈ Pi)}.
Let the cardinality |Fx| = r and y1 < · · · < yr be the set of representatives of Fx such that γ(x, yj) ∈ Pij . Then deﬁne the word ω := Pi1 · · · Pir in the alphabet P. Informally, ω is the list of names of elements of the partition in the order they are visited by the trajectory Γx. In our setting ω is called the type of trajectory Γx. Introduce the set of words Ω := {ω| x ∈ G1}.
Theorem 2. [3, 7] The set Ω is ﬁnite and the number of diﬀerent trajectory types of γ with respect to the partition P is less than

(sN )(r+n)O(n) (α + β)(r+n)O(n3)

(3)

Theorem 3. There is a cell decomposition of the control parameter space G1 such that if x1 and x2 belong to the same cell then Γx1 and Γx2 are labelled by the same word.

Proof. Consider the family F = {f1, . . . , fk} of Pfaﬃan functions in the domain G1 × (−T, T ) × G2 consisting of all functions in variables x, t, y involved in the deﬁning formulas for the graph of the map γ : (x, t) → y, and for all
sets Pj. According to Theorem 1, there is a cylindrical decomposition D of G1 × (−T, T ) × G2 with respect to the variables x, t, y having the following properties.

1) D is compatible with each consistent sigh assignment of f1, . . . , fk. 2) There are at most (3) cylindrical cells. 3) Each of these cells is sub-Pfaﬃan. 4) D induces a cylindrical decomposition on G1 which we denote by E.
We claim that for any cell C ∈ E and any two points x1, x2 ∈ C the trajectories Γx1 , Γx2 ∈ G2 are intersecting sets P1, . . . , Ps in the same order (i.e., are encoded by the same word from Ω). Indeed, let π : G1 × (−T, T ) × G2 → G1 be the projection on G1. The decomposition D induces cylindrical decompositions D1 and D2 on π−1(x1) and π−1(x2) respectively. In particular, each of the integral curves Γx1 and Γx2 is decomposed into a sequence of alternating points and open intervals. Due to basic properties of cylindrical decomposition, there is a natural bijection ψ : D1 → D2 such that
(i) the restriction of ψ to the set of all cells in Γx1 is a bijection onto the set of all cells in Γx2 ;
(ii) for each 1 ≤ j ≤ s the restriction of ψ to the set of all cells in (−T, T ) × Pj ∩ π−1(x1) is a bijection onto the set of all cells in (−T, T ) × Pj ∩ π−1(x2).

252 Margarita Korovina and Nicolai Vorobjov
(iii) the bijection ψ preserves the order in which cells appear in the trajectories.
It follows that if a cell B ∈ D1 is a subset of Γx1 ∩((−T, T )×Pj) for some 1 ≤ j ≤ s, then ψ(B) ⊂ Γx2 ∩((−T, T )×Pj). Moreover, if for cells B1, B2 ∈ D1 there exist t1, t2 ∈ (−T, T ) such that t1 < t2 and γ(x1, t1) ∈ B1 ∧ γ(x1, t2) ∈ B2 then there exist t1, t2 ∈ (−T, T ) such that t1 < t2 and γ(x2, t1) ∈ ψ(B1)∧γ(x2, t2) ∈ ψ(B2). The claim is proved.
It follows that the cardinality of Ω does not exceed the cardinality of E which does not exceed the cardinality of D which in turn is at most (3).

4 An Algorithm for Safety Properties Veriﬁcation

Consider a Pfaﬃan dynamical system γ : G1 × (−T, T ) → G2, a semi-Pfaﬃan subset of control parameters U ⊆ G1 and a semi-Pfaﬃan invariant Inv ⊆ G2. Let the graph of γ and the sets U, Inv have a format (r, N, α, β, n), and all
Pfaﬃan functions involved have a common Pfaﬃan chain.

Theorem 4. There is an algorithm which checks whether the control U satisﬁes the invariant Inv. The complexity of this algorithm does not exceed

(sN )(r+n)O(n) (α + β)(r+n)O(n3)

(4)

Proof. First the algorithm produces the set of words Ω corresponding to the Pfafﬁan dynamical system γ : G1 × (−T, T ) → G2 and the partition P = {P1, P2}, where P1 = Inv and P2 = G2 \ Inv. Consider the family of Pfaﬃan functions in the domain G1 × (−T, T ) × G2 consisting of all functions in variables x, t, y involved in the deﬁning formulas for the graph of the map γ : (x, t) → y, for the set U , and for the partition P. According to Theorem 1, there is a cylindrical decomposition D with respect to (x, t, y) which is compatible with this family
and consists of at most (4) cylindrical cells. This cell decomposition D induces the cell decomposition E (see the proof of
Theorem 3). Using the oracle, which decides feasibility of any system of Pfaﬃan equations and inequalities, the algorithm selects the cells from D which are subsets of {(x, t, y)|y = γ(x, t)}. Denote the set of the selected cells by B. Observe that for any ﬁxed x ∈ G1 the set B∈B B ∩ {(x, t, y)| x = x } coincides with the
integral curve Γx . Then the algorithm determines the order in which the cells B ∈ B intersected with {(x, t, y)| x = x } appear in the trajectory Γx .
More precisely, for each pair of distinct cells B1, B2 ∈ B the algorithm decides, using the oracle, whether

∃x∃t1∃t2∃y1∃y2 ((x, t1, y1) ∈ B1 ∧ (x, t2, y2) ∈ B2 ∧ (t1 < t2)).
For a given C ∈ E, after all pairs of cells are processed we get the ordered set of cells B1, . . . , Bk in D such that for any 1 ≤ i ≤ k and any x ∈ C the sequence of points and intervals

B1 ∩ {(x, t, y)| x = x }, . . . , Bk ∩ {(x, t, y)| x = x }

Safety Properties Veriﬁcation for Pfaﬃan Dynamics 253
forms the integral curve Γx . By the deﬁnition of cylindrical decomposition, for any pair Bi, Pj either Bi ⊂ (C × (−T, T ) × Pj) or Bi ∩ (C × (−T, T ) × Pj) = ∅. The algorithm uses the oracle to decide for every pair which of these two cases takes place. As the result, the sequence B1, . . . , Bk becomes partitioned into subsequences of the kind
(B1, . . . , Bk1 ), (Bk1+1, . . . , Bk2 ), . . . , (Bk −1+1, . . . , Bk),
where for any i, 0 ≤ i ≤ − 1, the cells Bki+1, . . . , Bki+1 lie in C × (−T, T ) × Pji for some ji, while Bki ∩C ×(−T, T )×Pji = ∅ and Bki+1+1 ∩C ×(−T, T )×Pji = ∅. Then the word ω := Pj0 · · · Pj −1 corresponds to the cell C. Considering all cells in E the algorithm ﬁnds Ω.
Then the algorithm collects all cells from E such that their union is U . If all of these cells corresponds to the word P1, then γ is safety under the control U . This completes the description of the algorithm.
A straightforward analysis shows that the complexity of the algorithm does not exceed (4), taking into account the bounds from Theorem 1.
References
1. S. Basu, R. Pollack and M.-F. Roy, Algorithms in Real Algebraic Geometry, Springer, Berlin-Heidelberg, 2003.
2. L. Blum, , F. Cucker, M. Shub, and S. Smale, Complexity and Real Computation, Springer, New York, 1997.
3. T. Brihaye, C. Michaux, C. Riviere, C. Troestler, On o-minimal hybrid systems, in: Hybrid Systems: Computation and Control, R. Alur, G. J. Pappas, (Eds.), LNCS, 2993, Springer, Heidelberg, 2004, 219–233.
4. A. Gabrielov, N. Vorobjov, Complexity of computations with Pfaﬃan and Noetherian functions, in: Normal Forms, Bifurcations and Finiteness Problems in Differential Equations, Yu. Ilyashenko et al., (Eds.), NATO Science Series II, 137, Kluwer, 2004, 211–250.
5. A. Gabrielov, N. Vorobjov, Complexity of cylindrical decompositions of subPfaﬃan sets, J. Pure and Appl. Algebra, 164, 1–2, 2001, 179–197.
6. A. Khovanskii, Fewnomials, Number 88 in Translations of Mathematical Monographs. American Mathematical Society, Providence, RI, 1991.
7. M. Korovina and N. Vorobjov, Pfaﬃan hybrid systems. In Springer Lecture Notes in Comp. Sci., volume 3210 of Computer Science Logic’04, 2004, 430–441.
8. M. Korovina and N. Vorobjov, Upper and lower Bounds on Sizes of Finite Bisimulations of Pfaﬃan Hybrid Systems. In Proceedings of CiE’06, invited talk, LNCS 3988, 2006, 235–241.
9. S. Pericleous, N. Vorobjov, New complexity bounds for cylindrical decompositions of sub-Pfaﬃan sets, in: Discrete and Computational Geometry. Goodman-Pollack Festschrift, B. Aronov et al. (Eds.), Springer, 2003, 673–694.
10. L. van den Dries, Tame Topology and O-minimal Structures, Number 248 in London Mathematical Society Lecture Notes Series. Cambridge University Press, Cambridge, 1998.

On Extending Wand’s Type Reconstruction Algorithm to Handle Polymorphic Let
Sunil Kothari and James L. Caldwell
Department of Computer Science University of Wyoming
Laramie, WY 82071-3315, USA {skothari,jlc}@cs.uwyo.edu
Abstract. We have extended Wand’s type reconstruction algorithm to polymorphic let by extending the constraint language and by using a multi-phase uniﬁcation algorithm in the constraint solving phase. We show the correctness of our approach by extending the Wand’s soundness and completeness results. We have validated our approach against other popular type reconstruction algorithms by implementing OCaml prototypes and running them on non-trivial examples.
1 Introduction
The general type reconstruction problem can be formulated as:
Given a well-formed term M without any types, does there exist a type τ and a type environment1 Γ such that a judgment Γ M : τ is valid ?
Type reconstruction is a popular feature in modern functional programming languages. Underlying any type reconstruction algorithm is a set of rules encoding a type system. One of the most widely used type systems is the HindleyMilner (HM) type system, ﬁrst mentioned in [Mil78] by Milner, but discovered independently by Hindley [Hin69]. Various type reconstruction algorithms [Mil78,DM82,LY98] have been proposed to implement the HM type system. Many of these algorithms are characterized by intermittent constraint generation and constraint solving. But over the years, focus has shifted to algorithms having a clear separation of constraint generation and constraint solving phases [Hee05,PR05,Wan87]. This separation leads to better error messages [Hee05] when the constraint set is unsatisﬁable (since a larger set of constraints is available to reason about the error). Moreover, the separation provides a clean ab-
This material is based upon work supported by the National Science Foundation under Grant No. NSF CNS-0613919. 1 We assume the initial type environment is empty since we are dealing with closed terms.

On Extending Wand’s Type Reconstruction Algorithm 255
straction of the various substitution-based algorithms2 since most well known algorithms are speciﬁc instances of various constraint solving strategies.
The type inference involving polymorphic let construct is a non-trivial problem. In fact, in the worst case, it is a DEXPTIME3-complete and PSPACEhard problem [PJ89,Mai89] in the level of nested lets. Moreover, the literature on constraint-based type reconstruction is sparse and uneven. For example, Pierce’s book [Pie02] has no references on how to handle ML-Let construct in constraint-based algorithms, HM(X) [SOW97,PR05] requires specialized knowledge, whereas Aiken and Wimmers [AW93] use subtyping constraints. Furthermore, none of the literature, in our view, describes it as a direct extension to well known Wand’s algorithm [Wan87].
The Helium compiler [HLI03] is known for giving good quality error messages and a very simple constraint representation is used for handling the let construct4. This paper describes an approach where Helium’s constraint representation is used to handle let polymorphism. Our approach builds upon Wand’s algorithm, and our proofs rely on the soundness and completeness of Wand’s algorithm. We have validated our approach with some of the known type reconstruction algorithms [Kot07]. In summary, our contributions are:
1. A new algorithm extending Wand’s algorithm [Wan87] to include polymorphic let.
2. New soundness and completeness proofs for Wand’s system and the extended system using a novel desugaring of polymorphic lets.
The rest of this paper is organized as follows: Section 2 reviews the previous methods for inferring the type of the let construct. Section 3 introduces the concepts and terminologies needed for this paper. Section 4 gives an overview of Wand’s algorithm and states soundness and completeness theorems. Section 5 describes the changes needed for the extension. Section 6 gives an overview of the correctness proofs. Section 7 summarizes our current work.
2 Literature Review
We review some of the constraint-based algorithms in their handling of the let construct. A detailed survey of substitution-based algorithms is available in [Kot07]. Wand [Wan87] looked at the type inference problem as a type-erasure: whether it is decidable that a term of the untyped lambda calculus is the image under type-erasing of a term of the simply typed lambda calculus, and presented a type reconstruction algorithm. This was the ﬁrst successful attempt at separating constraint generation from constraint solving phase. However, extending the algorithm to handle the let construct remained a future work5. Heeren
2 Throughout this paper we term substitution-based algorithms as those algorithms which intermix constraint generation and constraint solving, whereas algorithms with a clear separation are termed constraint-based algorithms.
3 DTIME(2nO(1) ) 4 Personal communication from Bastiaan Heeren. 5 Personal communication from Mitchell Wand.

256 Sunil Kothari and James L. Caldwell
[Hee05] suggested three constraint representations to handle the let construct; each equally expressive but diﬀering in constraint solving. Approach 1. Qualiﬁcation of type constraints: Type schemes contain a constraint component as part of its type as shown by the following grammar:
σ ::= ∀−→α .σ | C ⇒ τ For example, an expression λx.x can be assigned a type τ1 → τ2 under the constraint τ1 ≡ τ2. So the type of the expression is ∀τ1, τ2.τ1 ≡ τ2 ⇒ τ1 → τ2. In many ways, this approach is similar to HM(X)[SOW97], Pottier and R´emy’s account of ML type inference [PR05], and Jones’s qualiﬁed types [Jon95]. Approach 2. Type scheme variables as placeholders: The type constraint language is extended to take into account generalization and instantiation of type schemes by means of type scheme variables. The constraint language is given by the following grammar:
C ::= τ1 ≡ τ2 | σ := GEN (Γ, τ ) | τ := IN ST (σ) Our approach uses a slightly modiﬁed version of the above constraint representation. We know of no published account of the constraint solving phase for this representation, although Heeren does mention in his thesis that a special substitution that maps type scheme variables to type schemes is needed for this representation. Our algorithm does not require any such substitution. Approach 3. Using implicit instance constraints: This approach merges the generalization and instantiation constraints in the above approach. The constraint language is given by the following grammar:
C ::= τ1 ≡ τ2 | τ1 ≤M τ2 This representation is described, in detail, in Heeren’s thesis [Hee05].
3 Preliminaries
In this paper, we assume familiarity with the notion of types, functional programming and the lambda calculus. The terms considered here are pure untyped lambda terms given by the following grammar:
Λ ::= x | M N | λx.M We follow the usual conventions for lambda calculus: arrow types associate to the right, function applications associate to the left and application binds more tightly than abstraction. The types for untyped lambda terms is given by the following grammar:
τ ::= α | τ1 → τ2 The type is either a type variable or a function type. We call any expression formed by following the above grammar as a type expression. We follow the convention that α, β denote type variables, whereas τ denotes a type expression. A type environment, denoted by Γ , maps type variables to type expressions. The set of free type variables of a type expression τ is denoted by FTV(τ ) and those of a type environment Γ is denoted by FTV(Γ ). A term and its type is related by an assertion, denoted by Γ M : τ , where M is a term, τ is a type and Γ is a type environment. We denote type environment where x is not in the domain of Γ by Γ \x. A derivation of an assertion Γ M : τ is a ﬁnite tree of assertions,

On Extending Wand’s Type Reconstruction Algorithm 257

where the root is Γ M : τ . Every interior node in the tree is related to its parent by an instance of one of the non-axiom type rules and every leaf node is an instance of an axiom type rule. In this paper, we consider three diﬀerent type systems: the Hindley-Milner type system illustrated in Fig. 1, Wand’s system illustrated in Fig. 2, and the extended Wand system illustrated later in Section 5. We denote a judgment in a particular system by a subscript. For example, Γ HM M : τ is a HM judgment.

Γ

x : τHM

where x : τ ∈ Γ

Γ \x ∪ {x : τ1} HM M : τ0

Γ HM λx.M : τ1 → τ0

Γ HM M : τ1 → τ

Γ HM N : τ1

Γ HM MN : τ

(HM-Var) (HM-Abs) (HM-App)

Fig. 1. Hindley-Milner type system

Γ, {α =e τ }

where x : τ ∈ Γ and α is fresh
W x:α

(W-Var)

(Γ \x) ∪ {x : α}, E W M : β Γ, E ∪ {τ =e α → β} W λx.M : τ where α, β are fresh

(W-Abs)

Γ, E1 W M : α → τ Γ, E2 W N : α

Γ, E1 ∪ E2 W M N : τ

where α is fresh (W-App)

Fig. 2. Wand’s type system

A substitution is a mapping from type variables to type expressions. Let ρ1, ρ2 be two substitutions then substitution composition ρ1 ◦ρ2 is deﬁned extensionally as ∀α.(ρ1 ◦ ρ2)αd=efρ1(ρ2α), where α is a type variable. Substitution composition is associative but non-commutative. A substitution ρ is idempotent if ρ ◦ ρ = ρ. We treat all the substitutions as idempotent substitutions. A type τ is a substitution instance of a type τ if and only if τ = ρτ , for some substitution ρ. Substitution application to a type environment Γ , denoted by ρΓ , is deﬁned as {x : ρτ | x : τ ∈ Γ }.
In Wand’s system, constraints plays a central role. Speciﬁcally, Wand’s algorithm generates equality constraints, referred to as e-constraints, and are denoted by τ1 =e τ2, where τ1, τ2 are type expressions. An e-constraint τ1 =e τ2 is solvable if there exists a substitution ρ such that ρτ1 = ρτ2. More formally, we denote solvability of a constraint by |= (read solve). We write ρ |= τ1 =e τ2, if ρτ1 = ρτ2. A type judgment in Wand’s system is given as Γ, E W M : α, where E denotes a constraint set. Sometimes we elide the constraint component of the judgment to simplify the presentation. In that case, we denote a judgment by Γ W M : τ , where E is implicit and so there is a substitution generated by solving E such that ρα = τ .

258 Sunil Kothari and James L. Caldwell
4 Wand’s Algorithm
Next we brieﬂy discuss Wand’s algorithm [Wan87] and state the soundness and completeness theorems. Central to Wand’s algorithm is the notion of action table, which takes as input a type system and a term and generates equational constraints. For untyped lambda calculus, the type system is shown in Fig. 2. Let G denote a set of assertions (also called goals) and E a set of equational constraints. Then the algorithm sketch is given as:
Input. A term M0 of Λ. Initialization. Set E = ∅ and G = {(Γ0, M0, α0)}, where α0 is a fresh type variable
and Γ0 is an empty environment. Loop Step. If G = ∅ then return E else choose a sub-goal (Γ, M, τ ) from G, delete
the sub-goal from G and add to E and G new veriﬁcation conditions and subgoals generated by the action table6. Solve Constraints. Unify constraints in E.
We can now describe the soundness and completeness results as stated by Wand, but before that we introduce some terminology. We denote a goal by a 3-tuple, i.e. (Γ, M, τ ), and we write ρ |= g i.e. ρ |= (Γ, M, τ ) if and only if ρΓ HM M : ρτ . We write ρ |= G if and only if ∀g ∈ G. ρ |= g. Similarly, if E is a set of e-constraints, we write ρ |= E if and only if ∀e ∈ E. ρ |= e. Finally, we say ρ solves (E, G), where (E,G) result from applying Wand’s algorithm, if and only if ρ |= E and ρ |= G.
(Soundness) ∀ρ. ρ |= (E, G) ⇒ ρΓ0 HM M0 : ρτ0 (Completeness) Γ HM M0 : τ ⇒ (∃ρ. ρ |= (E, G) ∧ Γ = ρΓ0 ∧ τ = ρτ0)
We have reformulated Wand’s statement of completeness and soundness and made them more abstract (by eliminating the goal set). The reformulated theorems are used later in the proofs of soundness and completeness of the extended proof system. Our statement of the soundness and completeness are given below:
Theorem 1 (Soundness). If there is a derivation of Γ0, E W M0 : τ0 generating constraint set E then, for any ρ such that ρ |= E, ρΓ0 HM M0 : ρτ0 is derivable.
Theorem 2 (Completeness). If there is a derivation of Γ HM M0 : τ , then for any ρ, τ0, Γ0, such that dom(ρ) = (F T V (Γ0) ∪ {τ0}), ρΓ0 = Γ , and ρτ0 = τ then there exists a derivation of Γ0, E W M0 : τ0, and there exists a substitution ρ such that ρ ⊆ ρ and ρ |= E.
Both these theorems are proved in [KC07].
5 Extended Wand’s Algorithm
In this section we describe the changes needed to incorporate the let construct. First, we enrich our term and type language. We call the extended language Core-ML [MH88], and it is deﬁned as:
Core-ML ::= x | M N | λx.M | let x = M in N
6 See Section 5 for action table behavior for untyped lambda calculus.

On Extending Wand’s Type Reconstruction Algorithm 259

The let expression let x = M in N is not merely a syntactic sugar for (λx.N )M .

For instance, the term let i = λx.x in i i is typable but the desugared term

(λi.i i)(λx.x) is not typable. This is true for both Haskell and ML type recon-

struction algorithms. To handle polymorphic types introduced by let-expressions,

the type syntax is extended with type scheme variable and type scheme as shown

below: σ ::= α∗ | ∀−→α .τ
A type scheme, denoted by ∀−→α .τ , is a type where zero or more type variables

are universally quantiﬁed. We denote a type scheme variable by annotating a

type variable with a “*”. Generalizing a type τ with respect to a type environ-

ment Γ entails quantifying over the free variables of τ that are not free in Γ . Thus gen(Γ, τ )d=ef∀−→α .τ , where −→α = F T V (τ ) − F T V (Γ ). On the other hand, in-

stantiation of a type scheme involves replacing the quantiﬁed variables by fresh type variables and is given by inst(∀−→α .τ )d=efτ [α1 := β1, . . . , αn := βn], where
β1, . . . , βn are fresh type variables.

Next, we extend the constraint language C to include two other kinds of

constraints as shown below: C ::= τ =e τ

| τ =s Γ α∗ | α∗ =i τ

(e-constraint) (s-constraint) (i-constraint)

A s-constraint τ =s Γ α∗ expresses the fact that α∗ denotes a type scheme obtained by generalizing a type τ with respect to the environment Γ . An i-constraint

α∗ =i τ expresses the fact that τ is constrained to be an instantiated value of the type scheme denoted by α∗. Wand’s type system is now extended with two rules

to account for i&s-constraints. The new type system is called extended Wand’s system and a judgment in the extended system is denoted by the subscript W +.

(W-Var-i)

where x : τ ∈ Γ

Γ, {α∗ =i τ } W + x : α∗

(W-Let)

Γ, E1 W + M : α1

(Γ \x) ∪ {x : α2∗}, E2 W + N : τ

Γ, E1 ∪ E2 ∪ {α1 =s Γ α2∗} W + let x = M in N : τ

where α1, α∗2 are fresh

Apart from extending the type rules, we also had to extend the notion of satisﬁability and substitution application to a constraint. First, we describe some notations used in the description of satisﬁability. We use the notation Eα∗ to denote a set of i-constraints related7 to a s-constraint τ0 =s Γ α∗. From this point onwards, we think of a s-constraint and related i-constraint as a pair (τ0 =s Γ α∗, Eα∗ ); the ﬁrst component being the s-constraint and the second component being the list of related i-constraint(s). We use the symbol ≤ to express the notion of an instance. Speciﬁcally, τ ≤ σ expresses the fact that τ is an instance of σ in the sense that τ is obtained by instantiating all the bound variables of σ. This notion can then be used to express the satisﬁability of i&s constraints. We say

7 A s-constraint is related to an i-constraint if they share the same type scheme variable.

260 Sunil Kothari and James L. Caldwell

ρ satisﬁes (τ0 =s Γ α∗, Eα∗ ) if ∀(α∗ =i τ1) ∈ Eα∗ . ρ τ1 ≤ ρ(gen(Γ, τ0)). Substitution application to a pair of s-constraint and related i-constraints is deﬁned as: ρ(τ0 =s Γ α∗, Eα∗ )d=ef(ρτ0 =s ρΓ α∗, {ρτ =i α∗ | (τ =i α∗) ∈ Eα∗ }).
Next, we sketch the constraint generation phase8 for Core-ML. The algorithm sketch remains the same except that E now is a list9 of equational constraints
instead of a set. The behavior of action table for Core-ML is same as that for
untyped lambda calculus except for the variable (a slight modiﬁcation) and the
let case as shown below:
Case (Γ, x, τ0). If x is bound to a type scheme variable α∗ in Γ , i.e. x : α∗ ∈ Γ , then add α∗ =i τ0 to E else add τ0 =e τ1 (where x : τ1 ∈ Γ ) to E.
Case (Γ, M N, τ0). Let α be a fresh type variable. Generate subgoals (Γ , M, α → τ0) and (Γ, N, α), and add to G.
Case (Γ, λx.M, τ0). Let α and β be two fresh type variables. Generate equation τ0 =e α → β and sub-goal ((Γ \x) ∪ {x : α}, M, β), and add to E & G respectively.
Case (Γ,let x = M in N, τ0). Let α1, α2∗ be fresh type variables. Append10 El@[α1 =s Γ α2∗]@Er to list E, where El, Er are obtained by recursively calling the extended algorithm on (Γ, M, α1) and ((Γ \x) ∪ {x : α2∗}, N, τ0) respectively.
The next few paragraphs highlight the constraint solving phase. This phase consists of two distinct uniﬁcation phases: Phase I and Phase II. We ﬁrst give an informal description of both the phases and follow it with a formal description. In the ﬁrst phase, e-constraints are uniﬁed. Note that if there are no i&s-constraints, i.e. the term is a pure lambda term, then our Phase I mirrors the constraint solving phase for Wand’s algorithm. In the second phase, a s-constraint and related i-constraints are chosen and transformed to e-constraints and uniﬁed using the Phase I uniﬁcation. Let E = Ee@Ei&s be the constraint list obtained from the constraint generation phase, where Ee denotes a list containing econstraints, Ei&s denotes a list containing i&s-constraints. The constraint solving algorithm, SOLVE, integrates the two uniﬁcation phases as follows:

SOLVE(E) = let ρ1 = unif y1Ee in ρ1 ◦ (unif y2 ρ1(Ei&s))
The ﬁrst phase, unif y1 is deﬁned as:

unif y1(E) =

unif y1 ((α =e β) :: E)

= if α = β then unif y1 E

unif y1 ((α =e τ ) :: E)

= if α occurs in τ then raise Failure

unif y1 ((α =e τ ) :: E)

= (α → τ ) ◦ unif y1 (E[α := τ ])

unif y1 ((τ =e α) :: E)

= unif y1 ((α =e τ ) :: E)

unif y1 ((τ1 → τ2 =e τ3 → τ4) :: E) = unif y1 ((τ1 =e τ3) :: ((τ2 =e τ4) :: E))

8 We denote this constraint generation phase by W and+ in Fig. 3. 9 This is needed to preserve the order of s-constraints. 10 This will ensure that we solve the leftmost innermost let ﬁrst.

On Extending Wand’s Type Reconstruction Algorithm 261

The second phase, unif y2, is deﬁned as:

unif y2 (E @E) = let ρ1 = unif y1E

in ρ1 ◦ unif y2 (ρ1E)

where E = {inst(gen(Γ, τ1)) =e τ2 | (α∗ =i τ2) ∈ Eα∗ } and E = [(τ0 =s Γ α∗),Eα∗ ]

unif y2 [ ]

= Id

The ﬁrst phase of our constraint solving algorithm is very similar to the algorithm proposed by Martelli and Montanari [MM82], which is known to be exponential11 in the size of the input in the worst case [BS01]. Therefore, the ﬁrst phase of the uniﬁcation is linear while our implementation, which does not use the eﬃcient DAG representation, is exponential in the size of the input in the worst case. The second phase of the algorithm involves calling Phase I algorithm as many times as the number of s-constraints, i.e., the number of let-bound variables and there can be only O(n) of those. Therefore, in the worst case, the total time required by the algorithm is O(2n), where n is the size of the term.

6 Extension Correctness

For the correctness result, we need a function to transform polymorphic lets

to pure lambda terms since Wand’s type system has no notion of let. We call

this function ptol. Note that ptol is a type and value preserving transformation

[KC07]. We introduce two additional notations before formally describing ptol.

First, we use N [x] to denote one hole (denoted by [ ]) in a context (denoted

by N ) ﬁlled with the let-bound variable (x in this case). Second, we use the

notation |F V (N )|x to denote the count12 of free occurrence of x in N . Notice

that we transform only the body of the let since we assume the let-binding is

let-free. This assumption is strictly not necessary since there is a type and value

preserving transformation mentioned by Mairson[Mai89], which can make a let-

binding let-free. We use the assumption to simplify our proofs but its certainly

not a restriction on our algorithm. With the above notations, we describe ptol

below: ptol(x)

=x

ptol(λx.M )

= λx. ptol(M )

ptol(M N )

= (ptol(M ) ptol(N ))

ptol(let x = M in N [x]) = let N1 = ptol(N ) in

if | F V (N ) |x ≤ 1

(λx. N1[x])M ptol(let x = M in N [x]) = let N1 = ptol(N ) in

if | F V (N ) |x > 1

ptol(let x1 = M in let x = M in N1[x1])

where x1 is a fresh variable.

11 Linear if the input and output are represented as directed acyclic graph (DAG)
[PW76]. 12 We use the conservative notion of occurrence of a let-bound variable rather than the
actual types to diﬀerentiate between a monomorphic and polymorphic let.

262 Sunil Kothari and James L. Caldwell
The correctness is given by the proof sketch in Fig. 3. The most complicated proof was showing that the extended type system was sound and complete with respect to the Hindley-Milner type system via Wand’s type system. The detailed arguments involved in the correctness, the proofs of various theorems and lemmas, and a detailed discussion on the novel desugaring of polymorphic lets to pure lambda terms can be found in [KC07].

Constraint Generation

W

and+(Γ,

M,

α)

=

−→ E

⇔

Γ, E W + M : α is derivable

Extended Type System

Constraint Solving

f ¢

¢

f
¢ f
¢

Γ, E W + M : α is derivable ⇒ −→ −→

f
¢ f

SOLVE( E )Γ HM ptol(M ) :SOLVE( E )α is derivable

¢

f

¢ f
¢ f
¢ Efx

c Main Result

Γ, E W + M : α is derivable ⇔ Γ, E W ptol(M ) : α is derivable ⇔
Γ HM ptol(M ) : τ is derivable

W and+(Γ, M, α) infers the principal type for M under Γ

Fig. 3. Correctness proof overview

7 Conclusions and Future Work
The extension of Wand’s algorithm is a non-trivial extension and requires careful handling of constraints. Our algorithm is a direct extension of Wand’s algorithm and our soundness and completeness proofs rely on completeness and soundness of Wand’s algorithm. The main idea behind our algorithm is to preserve the order of generated s-constraints (as described in the action table for Core-ML) and use a multi-phase uniﬁcation, while preserving this order, in the constraint solving phase. We have validated our approach by running the examples mentioned in this paper on Alg. W [Mil78,DM82], Alg. M [LY98] and Alg. J [Mil78]. An implementation of our algorithm and other popular type reconstruction algorithms in OCaml is available online at http://www.cs.uwyo.edu/ ∼skothari. We are working on a formalization of the proofs in CoQ.
Acknowledgments
Thanks to Bastiaan Heeren for a detailed response to author’s query regarding the constraint representations mentioned in his thesis. We also want to thank anonymous referees for their detailed comments and suggestions (on an earlier draft of this paper), which greatly improved the presentation of this paper.

On Extending Wand’s Type Reconstruction Algorithm 263
References
[AW93] A. Aiken and E. L. Wimmers. Type inclusion constraints and type inference. In Functional Programming Languages and Computer Architecture, pages 31– 41, 1993.
[BS01] F. Baader and W. Snyder. Uniﬁcation theory. In A. Robinson and A. Voronkov, editors, Handbook of Automated Reasoning, volume I, chapter 8, pages 445–532. Elsevier Science, 2001.
[DM82] L. Damas and R. Milner. Principal type-schemes for functional programs. In POPL ’82, pages 207–212, New York, 1982. ACM Press.
[Hee05] B. Heeren. Top Quality Type Error Messages. PhD thesis, Universitiet Utrecht, 2005.
[Hin69] J. R. Hindley. The principal type-scheme of an object in combinatory logic. Trans. American Math. Soc, 146:29–60, 1969.
[HLI03] B. Heeren, D. Leijen, and A. IJzendoorn. Helium, for learning haskell. In Haskell ’03: Proceedings of the 2003 ACM SIGPLAN workshop on Haskell, pages 62–71, New York, NY, USA, 2003. ACM Press.
[Jon95] M. P. Jones. Qualiﬁed types: theory and practice. Cambridge University Press, New York, NY, USA, 1995.
[KC07] S. Kothari and J. L. Caldwell. Wand’s Algorithm Extended for the Polymorphic ML-Let. Technical report, University of Wyoming, 2007.
[Kot07] S. Kothari. Type Reconstruction Algorithms: A Survey. Technical report, University of Wyoming, 2007.
[LY98] O. Lee and K. Yi. Proofs about a folklore let-polymorphic type inference algorithm. ACM Transactions on Programming Languages and Systems (TOPLAS), 20(4):707–723, 1998.
[Mai89] H. G. Mairson. Deciding ML typability is complete for deterministic exponential time. In Proc. of the 16th ACM Sym. Principles of Programming Languages, pages 382–401, 1989.
[MH88] J. C. Mitchell and R. Harper. The essence of ML. In POPL ’88: Proceedings of the 15th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 28–46, 1988.
[Mil78] R. Milner. A theory of type polymorphism in programming. Journal of computer and system sciences, pages 348–375, 1978.
[MM82] A. Martelli and U. Montanari. An eﬃcient uniﬁcation algorithm. ACM Trans. Program. Lang. Syst., 4(2):258–282, 1982.
[Pie02] B. C. Pierce. Types and Programming Languages. MIT Press, 2002. [PJ89] P.C.Kanellakis and J.C.Mitchell. Polymorphic uniﬁcation and ML typing.
In 6th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 105–115. ACM Press, 1989. [PR05] F. Pottier and D. R´emy. The essence of ML type inference. In Benjamin C. Pierce, editor, Advanced Topics in Types and Programming Languages, chapter 10, pages 389–489. MIT Press, 2005. [PW76] M. S. Paterson and M. N. Wegman. Linear uniﬁcation. In STOC ’76: Proceedings of the eighth annual ACM symposium on Theory of computing, pages 181–186, New York, NY, USA, 1976. ACM. [SOW97] M. Sulzmann, M. Odersky, and M. Wehr. Type inference with constrained types. In Fourth International Workshop on Foundations of Object-Oriented Programming (FOOL 4), 1997. [Wan87] M. Wand. A simple algorithm and proof for type inference. Fundamenta Informaticae, 10:115–122, 1987.

Simulations Between Tilings
Gr´egory Laﬁtte1 and Michael Weiss2,
1 Laboratoire d’ Informatique Fondamentale de Marseille (LIF), CNRS – Aix-Marseille Universit´e, 39, rue Joliot-Curie, F-13453 Marseille Cedex 13, France 2 Centre Universitaire d’ Informatique, Universit´e de Gen`eve, Battelle bˆatiment A
7 route de Drize, CH-1227 Carouge, Switzerland
Abstract. Wang tiles are unit size squares with colored edges. To know whether a given ﬁnite set of Wang tiles can tile the plane while respecting colors on edges is undecidable. Robinson’s tiling is a self-similar tiling where the computation of a Turing machine can be carried out. We introduce ﬁner notions of simulation between tilings, giving rise to a ﬁner notion of universality. We show how to use this construction to be able to simulate a tile set by another tile set through Turing machines. We study the computability of some problems related to simulation. Finally, we show how to build a tile set that can simulate all the periodic tile sets.
1 Introduction
Wang was the ﬁrst to introduce in [13] the study of tilings with colored tiles. A tile is a unit size square with colored edges. Two tiles can be assembled if their common edge has the same color. To tile consists in assembling tiles from a tile set (a ﬁnite set of tiles) on the grid Z2.
Wang conjectured that if a tile set tiles the plane, then it tiles it in a periodic way. In [3], Berger was the ﬁrst to construct an aperiodic tile set, i.e., a tile set which tiles the plane only in an aperiodic way. This proof showed that Wang’s conjecture is false and that the domino problem is undecidable. This domino problem asks if one can decide whether given a tile set, there exists a tiling of the plane generated by this tile set. Simpliﬁed constructions of aperiodic tile sets can be found in [12] and later [1].
The main argument of this proof was to simulate the behavior of a given Turing machine with a tile set, in the sense that the Turing machine M stops on an instance w if and only if the tile set τ M,w does not tile the plane. Therefore, one of the most important fact concerning tilings is that tilings can constitute a Turing equivalent computation model. This computation model is particularly relevant as a model of computation on the plane.
Tilings have been studied for diﬀerent purposes: some researchers have used tilings as a tool for studying mathematical logical problems [1], others have studied the diﬀerent kinds of tilings that one tile set can produce [5,6,12,10], or deﬁned tools to quantify the regular structure of a tiling [7,2].
This author has been supported by the FNS grant 200020-113257.

Simulations Between Tilings 265
The most used construction to simulate Turing machines with tile sets is Robinson’s tiling. In [12], Robinson has built a tile set that generates only selfsimilar aperiodic tilings. The construction lays on a hierarchy of squares of ever increasing sizes. In each of this square, some zones can be used to simulate the behavior of a Turing machine. We show how to use this construction to be able to simulate a tile set by another tile set through Turing machines. In [10], notions of simulation and reduction between tilings and tile sets have lead to notions of universality for tilings and completeness for tile sets. From these deﬁnitions of simulation, we deﬁne ﬁner notions of simulation between tile sets (totally and exactly). These new notions of simulation give rise to a ﬁner notion of universality. We use our construction to study the computability of problems related to simulation. Then we use this construction to build tile sets that generate tilings with speciﬁc properties, e.g., universality for a certain class.
The main result of the paper is to use Robinson’s construction to build a recursive aperiodic tile set that totally simulates each and every periodic tile set and only them, i.e., a tile set that is totally universal for periodic tile sets. This is an interesting tile set since the problem to know whether a tile set is periodic is undecidable. We then generalize this construction and give suﬃcient conditions for tile sets classes to be totally simulated by one tile set.
In section 2, we recall the basic notions of tilings and simulation between tile sets and give two deﬁnitions of simulation, the total and the exact simulations, and a deﬁnition of universality, which are ﬁner than the ones introduced in [10]. In section 3, we show how we can simulate a Turing machine which stops on every input independently of the time and space needed for the computation. We use our construction to simulate a tile set with another tile set through the computation of a Turing machine in Robinson’s tiling . In section 4, we study the computability of problems related to simulation and show that the problem to know whether a recursive tiling P reduces to another recursive tiling Q is Σ2-complete. In the last section, we build a tile set that totally simulates each and every periodic tile set and only them. We then generalize this result to give suﬃcient conditions for a tile set class to be totally simulated by one tile set.
2 Notions of simulation
We start by recalling the basic notions of tilings. A tile is an oriented unit size square with colored edges from C, where C is a ﬁnite set of colors. A tile set is a ﬁnite set of tiles. To tile consists in placing the tiles of a given tile set on the grid Z2 such that two adjacent tiles share the same color on their common edge. Since a tile set can be described with a ﬁnite set of integers, then we can enumerate the tile sets, and τi designates the ith tile set.
Let τ be a tile set. A tiling P generated by τ is called a τ -tiling. It is associated to a tiling function fP where fP (x, y) gives the tile at position (x, y) in P . When we say that we superimpose the tiles of a tile set τ on the tiles of a tile set τ , we mean that for any tile t ∈ τ and any tile t ∈ τ , we build a tile u = t × t where the colors of the sides of u are the cartesian product of the colors of the

266 Gr´egory Laﬁtte and Michael Weiss

sides of t and t . Then two tiles u1 = t1 × t1 and u2 = t2 × t2 match if and only if t1 and t2 match and t1 and t2 match.
Diﬀerent notions of reduction have been introduced in [10]. We recall some of the notions relative to these reductions and we refer the reader to this paper for detailed explanations and properties.
A pattern is a ﬁnite tiling. If it is generated by τ , we call it a τ -pattern. A ﬁnite set of rectangular τ -patterns of same size is a τ -pattern set. By analogy with tilings, to tile with a pattern set consists in placing the patterns on a regular subgrid of Z2 in such a way that the connection between two patterns respects the local constraint of color matching. We call a tiling P generated by a pattern set M , a M -tiling. If M is a set of τ -patterns, then for any M -tiling P , there exists a τ -tiling Q that is a representation of P at the unit tile level.
From this remark we obtain notions of simulation. We say that our pattern tiling P simulates a tiling P if there exists a mapping R from the patterns of P to the tiles of P such that if we replace the patterns of P by their corresponding tiles given by R, then we obtain P . In such a case, we write P R P and say that P reduces to P .
In [10], the following deﬁnitions were introduced: a tiling P is strongly universal if for any tile set τ , there exists a τ -tiling Q such that Q P and a tile set τ is complete if for any tile set τ and any τ -tiling Q there exists a τ -tiling P such that Q P .
The following deﬁnitions present ﬁner notions of simulation:

Deﬁnition 1. Let τ and τ be two tile sets. We say that τ totally simulates τ if there exist a, b ∈ Z and a reduction R from the a × b patterns of τ to the tiles of τ such that the two following conditions are respected:

1. for any τ -tiling Q, there exists a τ -tiling P such that Q R P , 2. for any τ -tiling P , there exists a τ -tiling Q such that Q R P .

We denote it by τ

t τ (or τ

R t

τ

to

specify

the

reduction

R).

If τ t τ , then there exists a reduction R such that any τ -tiling can be cut in rectangle patterns of size a × b such that if one replaces these patterns by their corresponding tiles given by R then one obtains a τ -tiling. And the set of all τ -tilings that reduce to a τ -tiling is exactly the set of all τ -tilings. The total simulation is thus more speciﬁc than the simulation introduced in [10]. In this way, τ can be seen as a tile set which computes in a same way than τ .
To be able to study these notions of simulation, we now describe some speciﬁc aspects of the classical Robinson construction that we will use later on.

3 Some speciﬁc aspects of Robinson’s construction
We study here the implication of the complexity of Turing machines on their simulation by Robinson’s construction.

Simulations Between Tilings 267
Since Berger’s proof of the undecidability of the domino problem, we know how to simulate a Turing machine with a tiling. For a detailed explanation of the construction, we refer the reader to [1].
We now show that the simulation in Robinson’s tiling of the computation of a given Turing machine, which stops on all input, does not depend on the time and space needed for the computation.
Notice that, a problem could occur if, given a Turing machine M with L(M ) recursive, we wanted to simulate in the nth square of Robinson’s tiling (the square of size 22n − 1) the computation of M on the nth word. Since in the nth square of Robinson’s tiling , we have a square of size 2n + 1 dedicated to the computation, if M uses more than 2n + 1 times or spaces to compute the result, then the space×time diagram of M on this nth input will not ﬁt in the nth square.
Without loss of generality, we can suppose that M stops after (2n + 1) × s(n) spaces and k × (2n + 1) × s(n) times. If we simulate this Turing machine in Robinson’s tiling stretched vertically by a factor k, i.e., where we have replaced the tiles of Robinson’s tile set by rectangular patterns of size 1 × k, then the space×time diagram of the computation of M on the nth word will enter exactly in the square of size ((2n + 1) × s(n)) × (k × (2n + 1) × s(n)). We have to force the computation to be carried out in this rectangle.
The ﬁrst line of a rectangle of computation in Robinson’s tiling is ﬁlled in a non-deterministic way with a number of 1’s followed by a 2 and then by ’s (representing the empty symbol). The goal is to check whether 1n is the greatest input on which computation can be carried out in this rectangle. To do this, we simulate in parallel the computation of M on the input 1n and the computation of a modiﬁcation of M , say M , which works as M except that a 2 is seen as a 1. Therefore, if M computes on 1n, then M simulates M computing on 1n+1. So, 1n is the greatest input whose computation can be simulated in this rectangle if and only if the computation on 1n enters in the rectangle and if the computation on 1n+1 does not.
To be sure that we are in this case, if the computation of M on 1n stops, then a special color is sent by the ﬁnal state to the north side of the rectangle in which the computation is made, which forces the whole border of this rectangle to be marked with this special color. Then we just have to add the condition that the computation of M on 1n2 matches with the special colored border if and only if it does not reach a ﬁnal state.
Therefore, a rectangle can be ﬁlled if and only if the input wrote on the ﬁrst line is the greatest input whose computation can be simulated in this rectangle. At the end of our tiling process, our tile set simulates the computations of M on every input.
Thus, the simulation of a Turing machine in a tiling does not depend on the space and time needed to stop.

268 Gr´egory Laﬁtte and Michael Weiss
4 Computability of simulation
In this section, we study the computability of problems related to simulation. We deﬁne the following problems:
Problem TSR (Tile sets reduction)
– Input: Tile sets τ, τ . – Question: Does a τ -tiling P and a τ -tiling Q exist such that P Q?
Problem C-ness (Completeness)
– Input: Tile set τ . – Question: Is τ complete?
Problem TR (Tilings reduction)
– Input: Two recursive tilings P, Q. – Question: Does P Q?
We ﬁrst show that the halting problem reduces to problem TR, using the above construction. The construction of this proof can be used to prove the same result for the other two problems. At the end of this section, we will show the exact location of problem TR in the arithmetical hierarchy.
Theorem 1. Problem TR is undecidable.
Proof. We show that the halting problem reduces to problem TR. Let P be a deterministic tiling, i.e., a tiling generated by a tile set that tiles
the plane in an unique way, such that P does not reduce to Robinson’s tiling. Let M be a one-tape Turing machine and w an input word. We build a recursive tiling Q such that P Q ⇔ M (w) ↓. We consider the one-tape Turing machine M which takes as input the empty word and has the following behavior: it writes w on the tape and simulates M .
Let τM be the tile set which simulates M . We superimpose τM on Robinson’s tiling in such a way that the computation of M is simulated in the free zones of the squares of size 22n − 1 in Robinson’s tiling with the special condition that if the computation reaches a ﬁnal state before reaching the square perimeter, then a special color is sent to the north side of the rectangle in which the computation is made, which forces the whole border of the rectangle to be marked with this special color.
Thus, a square is marked with the special color if and only if the square is big enough to allow the computation to reach a ﬁnal state. Of course, if M stops on w using time t and space s, then the square of size 22m − 1, where m max(log2(t), log2(s)), is big enough to allow the computation to reach a ﬁnal state.
Therefore, if M (w) ↓, then there exists an integer n0 such that any square of size 22n − 1 can be ﬁlled with the computation of M for all n ≥ n0 and the squares of size 22l − 1 are squares without computation for any l < n0.

Simulations Between Tilings 269
In [10], we have shown how to build a Turing machine M which generates space×time diagrams which are isomorphic to a tile set τ . We refer the reader to this paper for detailed explanations. Now, we consider the Turing machine MP which simulates the tiles of the tiling P and the tile set τP which simulates MP . We superimpose τP on τM with the condition that the computation of the tiles of P can begin if and only if the square in which the computation is made is marked with the special color. Then a square of size 22l − 1 can be a pattern that simulates a tile of P if and only if:
– the square is big enough to allow the simulation of M to reach a ﬁnal state, – and the square is big enough to compute a tile of P .
If these two conditions are respected, then the squares of size 22l − 1 are the core of the patterns that simulate the tiles of P . We denote by Q the tiling generated by the tile set we have build until now, with the special condition that any square big enough to allow the simulation of M does carry out the simulation. Since P is a deterministic tiling, then the tile can be assembled in an unique way and thus, if only one square can be ﬁlled with a pattern representing a tile of P , then Q simulates the tiling P . To be sure that P simulates Q, we impose that the pattern centered around (0, 0) in Q simulates the tiles of P at position (0, 0).
We have to check that Q is a recursive tiling, i.e., there exists a Turing machine which given as input two integers a, b, outputs the tile of P at position (a, b). This machine has the following behavior:
– Find the smallest square containing position (a, b) in Robinson’s tiling. – If this position is out of a square of size 22n − 1, output the tile of Robinson’s
tiling at position (a, b). – Otherwise, compute in the square of Robinson’s tiling the tile set which
simulates M superimposed on the tile set which computes the tiles of P . – If the computation reaches a ﬁnal state and gives a pattern simulating a tile
of P before reaching the square perimeter, then ﬁll any square of size 22n − 1 with the computation of the patterns simulating the tiles of P , beginning by the one centered around the origin until we arrive to the square containing the position (a, b). – Output the tile at position (a, b). – If the computation does not halt in the square, output the tile of Robinson’s tiling at position (a, b).
By using the same kind of construction, we show that the halting problem reduces to problems TSR and C-ness:
Corollary 1. Problem C-ness and TSR are undecidable.
We can precisely locate problem TR in the arithmetical hierarchy. The next result shows that in fact, problem 3 is equivalent to problem F in = { i | The Turing machine with code i accepts a ﬁnite language L(Mi) } which is Σ2-complete.

270 Gr´egory Laﬁtte and Michael Weiss
Theorem 2. Problem TR is Σ2-complete.
Proof. [Problem TR∈ Σ2]: Let P and Q be two recursive tilings. Then P Q if there exist two integers a, b such that for any integer n, the n × n pattern of P centered around the origin reduces to the na × nb pattern of Q centered around the origin.
Therefore, problem TR can be deﬁned as a ∃∀ arithmetical property. [F in ≤ Problem TR]: Let i be the code of a Turing machine. We transform it in a pair of recursive functions (f, g) which represent two tilings Pf and Pg such that Pf Pg if and only if L(Mi) is ﬁnite. Let Pf be a deterministic tiling. We explain the behavior of the tiling function g. We use a tile set, say τ , composed of a unicolor blue tile and four tiles with three sides blue and one red. In [10] it has been shown that this tile set can simulate any tile set with inﬁnitely many diﬀerent reductions. At step one, we begin to simulate Pf(i) with τ by computing the tiles closest to (0, 0) and in parallel of this computation, we enumerate the words accepted by Mi. If no word is accepted, then g keeps tiling the plane with patterns that simulate Pf . If a word is accepted, we pass to the next step. At step n, we have already tiled a rectangular pattern A that simulates a rectangular pattern B of Pf . We do not want anymore this pattern A to be a simulation of the pattern B of Pf . We thus consider a larger enough square around A that we tile with the unicolor tile to guarantee that A does not simulate B anymore. Let A be this square. We start the simulation of Pf , using the pattern A already tiled. In parallel, we still enumerate the words accepted by Mi. If no word is accepted, then g goes on simulating Pf . If a new word is accepted by Mi, we go on to the next step. Therefore, g produces a tiling that simulates Pf if and only Mi accepts a ﬁnite set of words. If L(Mi) is inﬁnite, the simulation never ﬁnishes and Pg does not simulate Pf(i).
In the following theorem, we give an upper bound on the computability of the C-ness and TSR problems.
Theorem 3. Problems C-ness and TSR are respectively Π3 and Σ2.
Proof. i) In [10], it has been shown that a tile set is complete if it can generate a weakly dense universal tiling, i.e., a tiling P such that for any tile set τ , there exist two integers a, b and a reduction function R from the a × b patterns of P to the tiles of τ , such that the tiling P , obtained by replacing the a × b patterns of P by their corresponding tiles of τ given by R, contains any τ -pattern. Thus, we have the following description of the problem C-ness: τ is complete if and only if for any tile set τ , there exist two integers a, b such that for any integer n and for any τ -pattern m of size n, there exists an na × nb τ -pattern to which m reduces. Therefore, problem C-ness can be deﬁned as a ∀∃∀ arithmetical property.

Simulations Between Tilings 271
ii) This problem can be deﬁned as follows: there exists a τ -tiling P which reduces to a τ -tiling Q if and only if there exist two integers a, b such that for any n, there exists an n × n τ -pattern which reduces to an na × nb τ -pattern. Therefore, problem TSR can be deﬁned as a ∃∀ arithmetical property.
In the next section, we show how this construction of simulating a tile set by another tile set through Turing machines can be a powerful tool to build tile sets that generate tilings with speciﬁc properties.
5 Total simulation of all periodic tile sets
In this section, we build a recursive aperiodic tile set that totally simulates each and every periodic tile set and only them. This tile set is thus totally universal for periodic tile sets. This is an interesting tile set since the problem to know whether a tile set is periodic is undecidable. We then generalize this construction and give suﬃcient conditions for tile sets classes to be totally simulated by one tile set.
Theorem 4. The set of periodic tile sets can be totally simulated by one tile set, i.e., there exists a tile set τ such that for any periodic tile set τ , τ t τ .
Proof. First of all, let f be the projection of a recursive one-one function from N to N2 such that f (n) ≤ n. Thus, the set {f (1), f (2), . . .} contains inﬁnitely many times any integer. We can make the assumption that f can be computed by a Turing machine Mf in time and space t with unary input.
We now build a one-tape Turing machine M with the following behavior: it takes as input a word written in unary (1n), computes 1f(n) by simulating Mf , and then simulates the tile set τf(n) to enumerate the rectangular patterns generated by this tile set. If a periodic pattern is found, then the computation stops and never halts otherwise.
We now simulate this Turing machine in Robinson’s tiling stretched vertically by a factor 2. We have seen that the simulation of the computation of a Turing machine does not depend on the time and space needed to stop. In the square of size 2n − 1 we simulate Mf on the greatest input 1m whose computation ﬁts in this square. It thus outputs 1f(m). Since Robinson’s tiling is stretched with a factor 2, then we still have remaining spaces where we can compute M .
We simulate M in this tiling with the special condition that if a ﬁnal state is reached, then a special color is sent to the north side of the rectangle in which the computation is made, which forces the whole border of the rectangle to be marked with this special color. If no ﬁnal state is reached, then the computation keeps going on until it matches with the north side of the rectangle and the special color cannot be added to the sides of this rectangle.
This special color triggers the beginning of the computation of a new Turing machine, say N , in this rectangle. The behavior of N is the following: an input is an integer x - the index of a tile set - and an integer y - the index of a color.

272 Gr´egory Laﬁtte and Michael Weiss
We check whether y is a valid color of the tile set τx. If yes, we choose in a non-deterministic way a tile of τx with south color y, and we simulate this tile, i.e., we build a space×time diagram which is a pattern representing this tile.
We simulate N in our rectangle with the following conditions:
– The computation of N can begin if and only if the rectangle is marked with the special color.
– The ﬁrst line of the rectangle marked with the special color is ﬁlled in a non-deterministic way with the letters of the alphabet. The computation of N will say whether the input was valid or not, and if it is not, then the computation stops and the tiling cannot be completed to constitute a full tiling of the plane.
– Since the choice of x and y are made in a non-deterministic way, we have to check that x is the code of the tile set that Mf has computed before (even if it was not actually computed before since in our tiling the computations are made simultaneously). To do this, the bits of x are sent to the north and have to match with the value of 1f(n) to let the computation go on.
Therefore, at the end, the computation of N is carried out in the rectangle if and only if M has stopped, i.e., if the tile set τf(n) is periodic, and if and only if N has simulated a tile of τf(n).
The ﬁnal stage consists in sending the colors of the simulation of the tiles of τf(n) outside the rectangle to be sure that two Robinson rectangles simulating the tiles of τf(n) match if and only if the tiles they represent match.
Since the index of any tile set is enumerated inﬁnitely many times by f , then if a tile set τ is periodic, then there exist inﬁnitely many squares big enough to carry out the computation of M to ﬁnd a periodic τ -pattern and to stop, and thus, to trigger the beginning of the simulation of the tiles of τ . Therefore, the periodic tile sets are totally simulated by our tile set.
In this proof, we have used the fact that the property is recursively enumerable, and that no tile set, that does not tile the plane, has the property, here being periodic. If a tile set that does not tile the plane has the property, then when we simulate the tiles of this tile set, we arrive at a step where no more tiles can be simulated and the tiling process goes unﬁnished. We say that a property is decent if the class of tile sets satisfying it is recursively enumerable and if this property can not be veriﬁed by any tile set that does not tile the plane. This is equivalent to having a recursively enumerable property such that if a tile set can generate a pattern having this property, then it tiles the plane. Thus, we have the following generalization:
Theorem 5. Let P be a decent property, then the set LP = { i | τi satisﬁes P } can be totally simulated by one tile set.
This is equivalent to say that there exists a tile set which is totally universal for LP . We ﬁnish this section by giving some classes of tile sets that can be totally simulated by one tile set:

Simulations Between Tilings 273
Corollary 2. The class of tile sets that generate a period using all of its tiles can be totally simulated by one tile set.
The next corollary shows that the class of tile sets that exactly reduces to a tile set that tiles the plane can be totally simulated by one tile set:
Corollary 3. Let τ be a tile set that tiles the plane. The following set can be totally simulated by one tile set:
L = { i | τi ≡ τ }
Proof. L is recursively enumerable, since it is enough to enumerate the patterns generated by τi and to ﬁnd whether they are isomorphic to the tiles of τ . Since τ tiles the plane, no tile set that does not tile the plane satisﬁes the property.
References
1. Allauzen (C.) and Durand (B.), The Classical Decision Problem, appendix A: “Tiling problems”, p. 407–420. Springer, 1996.
2. Ballier (A.), Durand (B.) and Jeandel (E.), Structural Aspects of Tilings, in Proceedings of the Symposium on Theoretical Aspects of Computer Science, Dagstuhl Seminar Proceedings no 08001, p. 61–72, 2008.
3. Berger (R.), « The undecidability of the domino problem », Memoirs of the American Mathematical Society, vol. 66, 1966, p. 1–72.
4. Cervelle (J.) and Durand (B.), « Tilings: recursivity and regularity », Theoretical Computer Science, vol. 310, no 1-3, 2004, p. 469–477.
5. Culik II (K.) and Kari (J.), « On aperiodic sets of Wang tiles », in Foundations of Computer Science: Potential - Theory - Cognition, p. 153–162, 1997.
6. Durand (B.), Levin (L. A.) and Shen (A.), « Complex tilings », in Proceedings of the Symposium on Theory of Computing, p. 732–739, 2001.
7. Durand (B.), « Tilings and quasiperiodicity », Theoretical Computer Science, vol. 221, no 1-2, 1999, p. 61–75.
8. Durand (B.), « De la logique aux pavages », Theoretical Computer Science, vol. 281, no 1-2, 2002, p. 311–324.
9. Hanf (W. P.), « Non-recursive tilings of the plane. I », Journal of Symbolic Logic, vol. 39, no 2, 1974, p. 283–285.
10. Lafitte (G.) and Weiss (M.), « Universal Tilings », in Proceedings of the Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science no 4393, p. 367–380, 2007.
11. Myers (D.), « Non-recursive tilings of the plane. II », Journal of Symbolic Logic, vol. 39, no 2, 1974, p. 286–294.
12. Robinson (R.), « Undecidability and nonperiodicity for tilings of the plane », Inventiones Mathematicae, vol. 12, 1971, p. 177–209.
13. Wang (H.), « Proving theorems by pattern recognition II », Bell System Technical Journal, vol. 40, 1961, p. 1–41.
14. Wang (H.), « Dominoes and the ∀∃∀-case of the decision problem », in Proceedings of the Symposium on Mathematical Theory of automata, p. 23–55, 1962.

Discrete Non Determinism and Nash Equilibria for Strategy-Based Games
St´ephane Le Roux⋆
E´cole normale sup´erieure de Lyon, Universit´e de Lyon, LIP, CNRS, INRIA, UCBL http://perso.ens-lyon.fr/stephane.le.roux/
Abstract. Not all strategic games have Nash equilibria, so Nash deﬁned probabilistic equilibria, which exist in all strategic games. However, the probability approach fails for the slightly more general abstract strategic games that are deﬁned in this paper. Instead, this paper uses discrete non determinism, which yields relevant notions of equilibrium with guaranteed existence. These equilibria are payoﬀ-wise eﬃcient and easy to ﬁnd. Moreover, the discrete approach still works for much more general games supporting both sequential and simultaneous decision-making.
Key words: Abstract strategic games, games in graphs, discrete non determinism, Nash equilibrium, ﬁxed-point, constructive existence, eﬃcient algorithm.
1 Introduction
Not all ﬁnite strategic games have (pure) Nash equilibria. To cope with this, Nash [8] deﬁned probabilistic equilibria and proved their existence, i.e. he derived from a ﬁnite game a continuous game that has a (pure) Nash equilibrium.
This paper summarises [5], but without the proofs: it generalises strategic games by replacing real-valued payoﬀ functions with abstract outcomes; it translates the notion of Nash equilibrium from the real-valued to the abstract setting, by replacing the usual order over the reals with one binary relation per agent in order to account for his preferences over outcomes, as in [9]. However like in the real-valued case, some abstract strategic games have no Nash equilibrium.
Unlike for real-valued games, using probabilities to weaken the notion of strategy is irrelevant in this abstract setting, as detailed in [5] or [6]. So, this paper says that an agent does not choose (resp. may choose) a strategy instead of saying that he chooses the strategy with probability 0 (resp. p > 0). The set of strategies that may be chosen by an agent is named a discrete non deterministic (nd for short) strategy, similarly to [4]. Unlike Nash’s probability approach, this weakening keeps things discrete and ﬁnite, i.e. easy to handle.
Five relevant extensions of Nash equilibrium, named nd Nash equilibrium, are deﬁned as follows on the nd strategy proﬁles: First, deﬁne nd games and their nd equilibria, and ﬁve embeddings of abstract strategic games into nd games. Then,
⋆ Now postdoc at INRIA-Microsoft Research. I thank Pierre Lescanne for his comments on the draft of this paper, and a reviewer for his suggestions.

Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 275
deﬁne the nd Nash equilibria as the preimages of the nd equilibria by the ﬁve embeddings respectively. In the ﬁve cases, nd Nash equilibrium existence follows nd equilibrium existence, which follows a preliminary pre-ﬁxed point result.
The constructive proofs of existence of this paper give (exact) algorithms computing nd Nash equilibria. Time complexity happens to be polynomial for the ﬁve embeddings (yet might be exponential for other embeddings). In the real-valued setting, playing according to the computed nd Nash equilibrium may give better average payoﬀ than playing randomly. Together with a robustness argument, it shows that these nd Nash equilibria can serve as recommendations, whereas an example shows that probabilistic Nash equilibria cannot.
This paper also deﬁnes multi strategic games, i.e. graphs whose nodes are abstract strategic games. A play in such a game is a sequence of local plays of strategic games each of whose outcome points to the next node. Multi strategic games can thus model within a single game both sequential and simultaneous decision-making mechanisms. Embedding them into nd games also provides them with a notion of non deterministic Nash equilibrium, with an existence result.
Section 2 deﬁnes abstract strategic games and their Nash equilibria; section 3 states a pre-ﬁxed point result; section 4 deﬁnes nd games and their nd equilibria, and states existence results; section 5 embeds abstract strategic games into nd games in ﬁve ways, thus providing the formers with ﬁve notions of nd Nash equilibrium, with existence results. An example and eﬃciency results follow; informal section 6 deﬁnes multi strategic games and embeds them into nd games, thus providing them with a notion of nd Nash equilibrium.
Conventions: the notation P =∆ Q deﬁnes P as coinciding with Q. Let E = i∈I Ei be a cartesian product. For e in E, let ei be the Ei-component of e. Let E−i denote j∈I−{i} Ej. For e in E, let e−i be (. . . , ei−1, ei+1, . . . ) the projection of e on E−i. For x in Ei and X in E−i, deﬁne X; x in E as (X; x)i =∆ x and for all j = i, (X; x)j =∆ Xj.
2 Abstract Strategic Games
This section deﬁnes abstract strategic games and their Nash equilibria. Abstract strategic games are strategic games whose real-valued payoﬀ func-
tions are replaced with abstract outcomes. In addition for each agent, an arbitrary binary relation accounts explicitly for the agent’s preference over outcomes. These relations replace the (implicit) usual total order over the real numbers.
Deﬁnition 1 (Abstract strategic games) Abstract strategic games are 4tuples A, S, P, (⊳a)a∈A where:
– A is a non-empty set of agents, – S = a∈A Sa is the Cartesian product of non-empty sets of strategies, – P : S → Oc is a function mapping strategy proﬁles to given outcomes, – ⊳a is a relation over outcomes, and o ⊳a o′ says that agent a prefers o′ to o.
For s and s′ in S, s ⊳a s′ stands for P (s) ⊳a P (s′), by abuse of notation.

276 St´ephane Le Roux

Below, the left-hand strategic game is real-valued, as the right-hand one is abstract. Both games involve agent v (with strategies v1 and v2) and agent h.

h1 h2 v1 v → 0, h → 2 v → 1, h → 1 v2 v → 0, h → 1 v → 2, h → 0

h1 h2 v1 oc1 oc2 v2 oc3 oc4

The following notion of happiness (ﬁrst item of deﬁnition 2) generalises the

traditional one, i.e. they coincide on real-valued strategic games. Happiness is also characterised using the notion of best response (second item). As usual,

Nash equilibrium is deﬁned through happiness for all agents (third item).

Deﬁnition 2 (Happiness, Nash equilibrium) Let g = A, S, P, (⊳a)a∈A .
Happyg(a, s) =∆ ∀s′ ∈ S, ¬(s′−a = s−a ∧ s ⊳a s′) Happyg(a, s) ⇔ sa ∈ BRa(s−a) =∆ {x ∈ Sa | ∀y ∈ Sa, ¬(s−a; x ⊳a s−a; y)}
Eqg(s) =∆ ∀a ∈ A, Happyg(a, s)
For instance in the abstract strategic game before deﬁnition 2, the proﬁle (v1, h1) is a Nash equilibrium iﬀ ¬(oc1 ⊳v oc3), ¬(oc1 ⊳h oc2), ¬(oc1 ⊳v oc1), and ¬(oc1 ⊳h oc1). Also, note that there is a natural embedding of real-valued strategic games into abstract ones that preserves and reﬂects Nash equilibria.

3 A Preliminary Pre-Fixed Point Result
This section proves a pre-ﬁxed-point result, i.e. the existence of a y such that y F (y) for all F and that comply with given constraints.
First, meet semi-lattices are deﬁned below like in the literature.
Deﬁnition 3 (Meet semi-lattice) A meet semi-lattice is a partially ordered set (S, ) each of whose 2-element subset has a greatest lower bound.
In the rest of this section, (S, ) is a generic meet semi-lattice with least element m; inf is the inﬁmum function related to ; F is a generic function from S to S. As deﬁned below, a meeting point is an x in the lattice such that every sequence decreasing from x is not too much ”scattered” by F .
Deﬁnition 4 (Meeting point) Let x be in S − {m}. If for all x1 . . . xn such that m = x1 · · · xn x, we have inf (F (x1), . . . , F (xn), x) = m, then x is said to be a F -meeting point, and one writes MF (x).
The F -meeting point predicate is preserved by the greatest lower bound operator used with the meeting point and its image by F , as stated below.
Lemma 5 MF (x) ⇒ MF ◦ inf (x, F (x)).
If there is a F -meeting point but no inﬁnite strictly decreasing sequence, iteration of lemma 5 yields a non-trivial pre ﬁxed point of F , as stated below.
Lemma 6 If is well-founded and if there exists a F -meeting point, then there exists a F pre ﬁxed point diﬀerent from m, i.e. y F (y) for some y = m.

Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 277

4 Non Deterministic Games

This section deﬁnes non deterministic games, nd games for short, and their nd equilibria. Three results of existence of nd equilibrium follow. In this paper, these deﬁnitions and results are merely general intermediate tools.
Informally, nd games generalise real-valued or abstract strategic games in three ways: First, a nd game involves a set I of ”strategic games”. Second, for each i in I, each agent a selects one or more ”pure strategies” in Sai . This discrete non determinism amounts to forgetting the probabilities of a probabilistic strategy while remembering whether or not they equal zero. Third, there is no payoﬀ function or abstract outcome, but each agent is given a best-response function telling what his best nd strategy against his opponents’ nd strategies is.
Deﬁnition 7 (Nd games) A nd game is a pair (Sai )ia∈∈IA, (BRa)a∈A where:
– I is a non-empty set of indices and A is a non-empty set of agents. – For all a in A, BRa is a function from Σ−a to Σa,
where Σ = a∈A Σa and Σa = i∈I P(Sai )−{∅} and Σ−a = a′∈A−{a} Σa′ .
Elements of Σa (resp. Σ) are called nd strategies for a (resp. nd strategy proﬁles).
In the rest of this section, g = (Sai )ia∈∈IA, (BRa)a∈A is a generic nd game, σ is a generic nd proﬁle in the corresponding Σ, and a is a generic agent.
In deﬁnition 8, agent a is happy with σ if his own nd strategy σa is included in his nd best response against his opponents’ combined nd strategies, i.e. BRa(σ−a). So, informally, an agent is happy if any ”pure strategy” that he selects is a ”pure best response” against others’ nd strategies. This inclusion generalises the membership that is used in happiness characterisation of deﬁnition 2. Like for Nash equilibria, nd equilibria amount to happiness for all agents.

Deﬁnition 8 (Happiness and non deterministic equilibrium)

Happyg(σ, a) =∆ σa ⊆ BRa(σ−a) ⇔ ∀i ∈ I, σa,i ⊆ (BRa(σ−a))i ∈ P(Sai )

Eqg(σ) =∆ ∀a ∈ A, Happyg(σ, a)

Eqg(σ) ⇔ σ ⊆ BR(σ) where BR(σ) =∆

a∈A BRa(σ−a)

Below, a meet lattice is identiﬁed within the nd game, and invoking lemma 6

yields a ﬁrst equilibrium existence result.

Lemma 9 The poset (Σ ∪ {∅}, ⊆) is a meet semi-lattice with least element ∅, and if there exists a BR-meeting point in Σ, there exists a nd equilibrium for g.

The next equilibrium existence result invokes lemma 9 and the mentionned σ being a BR-meeting point. Lemma 10 is invoked in the rest of the paper.

Lemma 10 Assume that for all agents a, for all γ1 . . . γn in Σ−a, if γn ⊆ · · · ⊆ γ1 ⊆ σ−a then ∩1≤k≤nBRa(γk)∩σa = ∅. Then, the game g has a nd equilibrium.

Also, equilibria are preserved when ”increasing” the best response functions.

Lemma 11 Let g′ = (Sai )ia∈∈IA, (BRa′ )a∈A be another nd game such that for all a in A, for all γ in Σ−a, BRa(γ) ⊆ BRa′ (γ). Then Eqg(σ) ⇒ Eqg′ (σ).

278 St´ephane Le Roux
5 Non Deterministic Nash Equilibria
Like real-valued strategic games, not all abstract ones have Nash equilibria; unlike for real-valued strategic games, using probabilities to weaken the notion of strategy is irrelevant for abstract ones, as detailed in [5]. This section invokes the concept of discrete non determinism of section 4 instead. Informally, a nd strategy for an agent of an abstract strategic game is a non-empty subset of the set of his (pure) strategies. Then, if a strategy belongs to the nd strategy, the agent may eventually choose it; if it does not, he will not. Conceptually, nd strategies lie between strategies and probabilistic strategies, while being simpler than the latters. Indeed, a probabilistic strategy induces a nd strategy, but it further speciﬁes that the agent may choose strategies with given probabilities.
By embeddings into nd games, this section provides abstract strategic games with ﬁve notions of non deterministic Nash equilibrium. Existence results follow. this section also discusses the complexity of ﬁnding such equilibria, gives an example, and shows eﬃciency of these equilibria.
Deﬁnitions and existence: The following deﬁnition extends orders over functions’ codomains to orders over functions, in a Pareto style.
Deﬁnition 12 Let f and f ′ be functions of type A → B, and let A′ be a subset of A. Let ≺ be an irreﬂexive relation over B, and let be its reﬂexive closure.
f A′ f ′ =∆ ∀x ∈ A′, f (x) f ′(x) f ≺A′ f ′ =∆ f A′ f ′ ∧ ∃x ∈ A′, f (x) ≺ f ′(x)
For instance if both functions are represented by vectors of naturals with the usual total order, (1, 1) < (1, 2), (0, 2) < (1, 2) and (0, 2) < (1, 3), but (2, 0) < (1, 3) . The extension preserves strict partial ordering, as stated below.
Lemma 13 If ≺ is a strict partial order over B, then the derived ≺A over functions of type A → B is also a strict partial order.
The following lemma is proved by induction on the mentionned n.
Lemma 14 Let E be a ﬁnite set of functions of type A → B. Let ≺ be irreﬂexive and transitive over B, and let be its reﬂexive closure. For any ∅ = A0 ⊆ · · · ⊆ An ⊆ A there is an f that is maximal in E wrt all the ≺Ai .
In the rest of this subsection g = A, S, P, (⊳a)a∈A is a generic abstract strategic game, a is a generic agent, and γ is a generic element in the corresponding Σ−a. Also, the ⊳a are strict partial orders, i.e. irreﬂexive and transitive.
Five embeddings of g into nd games are deﬁned below. The ﬁve images of g have nd equilibria by lemmas 10, 11 and 14. This provides abstract strategic games with ﬁve deﬁnitions of non-deterministic Nash equilibrium and guarantees of existence. In the notation s ⊳γa s′ below, the strategies s and s′ are seen as functions from Sa to the outcomes and γ corresponds to A′ in deﬁnition 12.

Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 279

Lemma 15 For i between 0 and 4, (Sa)a∈A, (BRai )a∈A is a nd game (derived from g), and it has a nd equilibrium that is named nd Nash equilibrium for g.

BRa0(γ) =∆ {s ∈ Sa | ∀s′ ∈ Sa, ¬(s ⊳aS−a s′) ∀s′ ∈ Sa, ¬(s ⊳γa s′)

∧ ∧

∃c

∈

γ,

∀s′

∈

Sa,

¬(s

{c}
⊳a

s′)

}

BRa1(γ) =∆ {s ∈ Sa | ∀s′ ∈ Sa, ¬(s ⊳aS−a s′)

}

BRa2(γ) =∆ {s ∈ Sa | ∀s′ ∈ Sa, ¬(s ⊳γa s′)

}

B Ra3 (γ )

=∆

{s ∈ Sa

|

∃c ∈ γ, ∀s′

∈ Sa,

¬(s

{c}
⊳a

s′

)

}

B Ra4 (γ )

=∆

{s ∈ Sa

|

∀s′

∈ Sa,

∃c ∈ γ,

¬(s

{c}
⊳a

s′

)

}

The BRai for i = 1, 2, 3 correspond tho the three conjuncts of BRa0. (Note that BRa1(γ) does not depend on γ.) Also, BRa4 is even more generous than BRa2. Depending on the application, a user of this theory may choose either of these deﬁnitions or design his own deﬁnition of nd Nash equilibrium. Note that BR2 and BR4 somewhat relate to the notions of dominated strategy, studied
in [2] and [7], and rationalizability, studied in [1] and [10]. These notions are
more recently discussed in [3], for instance.
The rest of the subsection discusses a few properties of these equilibria. First, the equilibria related to BR1 form a simple structure, as stated below.

Lemma 16 The nd equilibria related to BR1 are the elements of Σ that are included in a∈A{s ∈ Sa | ∀s′ ∈ Sa, ¬(s ⊳aS−a s′)}.
The following lemma states that the nd Nash equilibria related to BR3 or BR4 deﬁne a Cartesian union lattice.

Lemma 17 If equilibrium is deﬁned through either BR3 or BR4, then
Eqg(σ) ∧ Eqg(σ′) ⇒ Eqg(σ ∪× σ′), where σ ∪× σ′ is the smallest element of Σ containing both σ and σ′.

Algorithmic time complexity: The constructive proof of lemma 15 yields algorithms ﬁnding a nd Nash equilibrium for each BRi. Computations relate to iteration of lemma 5, i.e. successive calls to BRi starting with γ = S, the full nd strategy proﬁle. For the ﬁve BRi, overall complexity in calls to preferences is polynomial in the number of proﬁles |S|: indeed, a call to BRi dismisses at
least one strategy of one agent (unless the nd proﬁle is a nd Nash equilibrium), so there are at most |S| such calls; each of them calls the BRai which decides whether or not each strategy of agent a is a best response (if agent a has only one strategy, no need to call BRai ); there are at most |S| such decisions per call to BRi. For BR0, this decision requires at most 3 × |S| calls to a ⊳a. So, ﬁnding a BR0-equilibrium (similarly BRi-equilibrium) is at most cubic in |S|.

280 St´ephane Le Roux

Example: The strategic game below uses naturals: the proﬁle (v1, h2) induces payoﬀ 3 for agent v and 2 for h and preferences are the usual order over naturals. Let us apply the BR0-equilibrium algorithm to the game, starting with the full
nd proﬁle ({v1, v2, v3, v4, v5}, {h1, h2, h3, h4, h5}), as meant by the underlining.

h1 h2 h3 h4 h5

h1 h2 h3 h4 h5

h1 h2 h3 h4 h5

v1 0,0 3,2 2,2 2,1 3,0 v1 0,0 3,2 2,2 2,1 3,0 v1 0,0 3,2 2,2 2,1 3,0

v2 3,3 0,2 0,0 2,1 3,2 v2 3,3 0,2 0,0 2,1 3,2 v2 3,3 0,2 0,0 2,1 3,2

v3 2,2 1,0 0,1 3,1 0,2 v3 2,2 1,0 0,1 3,1 0,2 v3 2,2 1,0 0,1 3,1 0,2

v4 1,0 1,2 1,2 1,2 1,0 v4 1,0 1,2 1,2 1,2 1,0 v4 1,0 1,2 1,2 1,2 1,0

v5 2,0 1,0 0,0 0,1 0,0 v5 2,0 1,0 0,0 0,1 0,0 v5 2,0 1,0 0,0 0,1 0,0

A priori, v (resp. h) may play either of the vi (resp. hi). In this context, h5 (resp. row v5) is smaller than h1 (resp. v3) according to agent h (resp. v). So h5 (resp. v5) is not a best response of h (resp. v). Also, v4 is not a best response of v because for each hi, row v4 is smaller than some other row. So, the combined best responses against the full nd proﬁle are v1 to v3 and h1 to h4, as shown in the second picture above. Now, a priori v may play either v1 or v2 or v3. So h4 is not a best response of h since for each row v1 to v3, h4 is smaller than some other column. So the nd proﬁle ”shrinks” to the third one above.

h1 h2 h3 h4 h5

h1 h2 h3 h4 h5

v1 0,0 3,2 2,2 2,1 3,0 v1 0,0 3,2 2,2 2,1 3,0

v2 3,3 0,2 0,0 2,1 3,2 v2 3,3 0,2 0,0 2,1 3,2

v3 2,2 1,0 0,1 3,1 0,2 v3 2,2 1,0 0,1 3,1 0,2

v4 1,0 1,2 1,2 1,2 1,0 v4 1,0 1,2 1,2 1,2 1,0

v5 2,0 1,0 0,0 0,1 0,0 v5 2,0 1,0 0,0 0,1 0,0

Similarly, for h1, h2 and h3, row v3 is smaller than v1 or v2, hence the lefthand nd proﬁle above. Since h3 is smaller than h2 now, this yield the right-hand irreducible nd proﬁle above, which is a nd Nash (BR0-)equilibrium for the game.

Recommendation: Consider the following class of games G, where each ∗ can be either 1 or −1, and where the preferences are along the usual order −1 < 1.

h1 h2 v1 ∗, ∗ ∗, ∗ v2 ∗, ∗ ∗, ∗
For each agent, the arithmetic mean of its payoﬀ over its four payoﬀs for all games in G is 0, by a ”symmetry” argument. However for each agent, the arithmetic mean of its payoﬀ over its payoﬀ that are inside the nd Nash equilibrium for all games in the class is 3/8, by a few combinatorial arguments.

Lemma 18 For a game g in G, let eq(g) be the nd Nash BR2-equilibrium built by the proof of lemma 15.

1 |G|

×

1 |eq(g)|

×

P (s, v)

=

3 8

g∈G

s∈eq(g)

Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 281

Probabilistic Nash equilibria cannot serve as recommendations to agents on

how to play because even for strategic games over rational numbers there is no

algorithm that ﬁnds such an equilibrium and that is robust to strategy renaming.

Below, let v (resp. h) model a real-life situation with the game to the left (resp.

right). Each game has only two probabilistic Nash equilibria, namely ((v1 →

1 2

,

v2

→

1 2

),

(h1

→

1 2

,

h2

→

1 2

))

and

((v3

→

1 2

,

v2

→

1 2

),

(h3

→

1 2

,

h2

→

1 2

)).

However, there is non way for agents to collectively choose one of them in a

non-cooperative setting.

h1 h2 h3 v1 3, 2 2, 3 0, 0 v2 2, 3 3, 2 2, 3 v3 0, 0 2, 3 3, 2

h3 h2 h1 v3 3, 2 2, 3 0, 0 v2 2, 3 3, 2 2, 3 v1 0, 0 2, 3 3, 2

This paper’s equilibrium computation is robust. Since it is more eﬃcient than random play, it can serve as a recommendation.

6 Multi Strategic Games

This section introduces multi strategic games and related notions informally. Such a game is a graph whose nodes are ”abstract strategic games”. For each node, each agent has to choose a node strategy once for all. These combine into a node proﬁle that yields an abstract outcome and points to another node .
The 2-agent game below has three nodes (big rectangles). Consider a play starting at (or reaching) the central node. For instance, if v and h both choose their ﬁrst strategy at this node, the outcome is oc1 and the play reaches the central node again; if h chooses his second strategy instead, the outcome is oc2 and the play reaches the right node, hence the small rectangle and the arrow.

oc9 oc10

oc1 oc2 oc3 oc4

oc5 oc6 oc7 oc8

A strategy for an agent is the combination of his node strategies, and a proﬁle is the combination of agents’ strategies. Nd node strategies allow an agent to choose several node strategies at a same node. A nd proﬁle is depicted below. At the central node, underlining says h selects his ﬁrst strategy and v selects both.

oc9 oc10

oc1 oc2 oc3 oc4

oc5 oc6 oc7 oc8

From each node, a nd proﬁle induces a set of inﬁnite sequences of outcomes. Above, the nd proﬁles induces the sequence ocω9 at the left-hand node, and the sequences ock1oc3ocω9 , for k in N, at the central node.
As described in [5], embedding such games into nd games also provides a non-
trivial notion of nd Nash equilibrium with guaranteed existence. The constructive

282 St´ephane Le Roux
proof yields a search algorithm exempliﬁed below. The game involves agents v and h. Agent v chooses the rows and gets the ﬁrst ﬁgures of the payoﬀ functions.
At ﬁrst, all agents consider all of their options, as suggested by the underlining. At the bottom node, only agent h has a decision to make. If he chooses right, he gets 0ω (and v gets 3ω, but h does not take it into account). If h chooses left, he gets an inﬁnite sequences of positive numbers whatever v’s strategy may be, which is better than 0 at any stage of the sequence. So h dismisses for good his right strategy at the bottom node, as depicted on the right-hand side below.

2, 2 2, 1 0, 4 1, 4

2, 2 2, 4 4, 2 3, 3

2, 2 2, 1 0, 4 1, 4

2, 2 2, 4 4, 2 3, 3

0, 1 3, 0

0, 1 3, 0

Now agent v considers the top-left node. If he chooses his bottom strategy, induced sequences use only 0 and 1. If he choose his top strategy, induced sequences use numbers that are greater than 1, which is better. So v dismisses for good his top strategy at this node, as depicted on the left-hand side below.
Then agent h considers the top-right node. If he chooses his left (resp. right) strategy, induced sequences use only 1 and 2 (resp. 3 and 4). So agent h dismisses his left strategy at the top-right node, as depicted on the right-hand side below.

2, 2 2, 1 0, 4 1, 4

2, 2 2, 4 4, 2 3, 3

2, 2 2, 1 0, 4 1, 4

2, 2 2, 4 4, 2 3, 3

0, 1 3, 0

0, 1 3, 0

Eventually, agent v dismisses his top strategy at the top-right node, which yields the nd Nash equilibrium below. So, for each agent, fore each node, the agent cannot get a better set of sequences by changing his strategy.

2, 2 2, 1 0, 4 1, 4

2, 2 2, 4 4, 2 3, 3

0, 1 3, 0
The universal existence of such equilibria actually follows an embedding of multi strategic games into nd games. However, other more subtle notions of equilibrium with guaranteed existence may be deﬁned.

Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 283
7 Conclusion
Using probability to weaken the notion of strategy and guarantee existence of weak Nash equilibria works for real-valued strategic games but not for the slightly more general abstract strategic games. Using discrete non determinism works in both settings and enables (at least) ﬁve diﬀerent relevant notions of discrete non deterministic equilibrium. Some of these notions are actually related to the concept of dominated strategy. However, this concept had not led to discrete Nash equilibria in the past literature, perhaps because probabilities seemed more accurate than discrete non determinism or because no alternative to Nash’s approach seemed to be required. Since elimination of dominated strategies gives information about the probabilistic Nash equilibria of a strategic game, studying discrete Nash equilibria may actually give information about probabilistic Nash equilibria. In addition, the discrete approach is better in at least four respects: First, it still works in an even more general setting, namely multi strategic games, which combine sequential and simultaneous decision-making. Second, the constructive proof of existence builds one speciﬁc equilibrium that can serve as a recommendation since it seems payoﬀ-wise eﬃcient. However, more work is needed about payoﬀ-wise eﬃciency. Third, the proof/algorithm has a (low) polynomial time complexity for the ﬁve notions, which makes equilibria easy to ﬁnd among exponentially many candidates. Fourth, some other notions of discrete equilibrium might yield NP-complete problems, which suggests that ﬁnding discrete non deterministic equilibria also lies on the boundary of P.
References
1. B Douglas Bernheim. Rationalizable strategic behavior. Econometrica, 52:1007– 1028, 1984.
2. D Gale. A theory of n-person games games with perfect information. Proceedings of the National Academy of Sciences of the United States of America, 39:496–501, 1953.
3. J Hillas and E Kohlberg. Foundations of strategic equilibrium. In R.J. Aumann and S. Hart, editors, Handbook of Game Theory with Economic Applications. 2002.
4. S Le Roux. Non-determinism and Nash equilibria for sequential game over partial order. In Proceedings Computational Logic and Applications, CLA ’05. DMTCS Proceedings, 2006. http://www.dmtcs.org/pdfpapers/dmAF0106.pdf.
5. S Le Roux. Discrete non neterminism and nash equilibria for strategy-based games. Research report http://hal.inria.fr/inria-00195397/fr, INRIA, 2007.
6. S. Le Roux. Generalisation and formalisation in game theory. PhD thesis, Ecole Normale Sup´erieure de Lyon, 2008.
7. RD Luce and H Raiﬀa. Games and Decisions. John Wiley and Sons, New York, 1957.
8. J Nash. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36:48–49, 1950.
9. MJ Osborne and A Rubinstein. A Course in Game Theory. The MIT Press, 1994. 10. DG Pearce. Rationalizable strategic behavior and the problem of perfection.
Econometrica, 52:1029–1050, 1984.

Combining Concept Maps and Petri Nets to Generate Intelligent Tutoring Systems
Maikel León, Isis Bonet, and Zenaida García
Department of Computer Science Central University of Las Villas Highway to Camajuaní km 5.5
Santa Clara, Cuba Phone: (53) (42) 281515 Fax : (53) (42) 281608
{mle, isisb, zgarcia}@uclv.edu.cu
Abstract. The use of pedagogical methods with the technologies of the information and communications produce a new quality that favors the task of generating, transmitting and sharing knowledge. Such is the case of the pedagogical effect that produces the use of the Concept Maps, which constitute a tool for the management of knowledge, an aid to personalize the learning process, to exchange knowledge, and to learn how to learn. In this paper the authors present a new approach to elaborate Intelligent Tutoring Systems, where the techniques of Concept Maps and Artificial Intelligence are combined, using Petri Nets as theoretical frame, for the student model. The pedagogical model that controls the interaction between the apprentice and the generated Intelligent Tutoring Systems is implemented by Petri Nets. The Petri Nets transitions are controlled by conditions that refer to the apprentice model. The firing of these transitions produces actions that update this apprentice model. These conditions are automatically included into the pedagogical model and the teacher has only to specify the contents of the domain model.
Key words: Intelligent Tutoring Systems, Petri Nets, Student Model, Artificial Intelligence, Concept Maps.
1 Introduction
To construct and to share knowledge, to learn significantly, and to learn how to learn, are ideas on which researchers have been pondering for a long time as well as the use of tools that would allow taking these aspirations into practice. To achieve this, different techniques and strategies have been used. Concept Maps (CM) provide a schematic summary of what is learned and they order it in a hierarchic range. The knowledge is organized and represented in all the levels of abstraction, the general knowledge goes to the upper part and the most specific one goes to the lower [1]. Over the last few years the CM have increasingly got a great popularity and its integration with the technologies of the information and the communications have become a very important element in the plans for the improvement of the teachinglearning process. The main application of the CM has had effect in the teachinglearning process, which is its basic intention, they are based on an instrument that combines the scientific rigidity with simplicity and flexibility, producing a general

Combining Concept Maps and Petri Nets

285

approval in the audience of students and professionals; this represents an important nexus between pedagogy and technology in a huge field of applications. Also, it constitutes an important aid for those who generate, transmit, store, and divulge information and knowledge and it comprises an important tool to obtain a highest practical value in the systems of the teaching-learning process [2].
Thus, Petri Nets (PN) are a graphical and mathematical modeling tool. It consists of places, transitions, and arcs that connect them. Input arcs connect places with transitions, while output arcs start at a transition and end at a place. There are other types of arcs, e.g. inhibitor arcs. Places can contain tokens; the current state of the modeled system (the marking) is given by the number (and type if the tokens are distinguishable) of tokens in each place. Transitions are active components. They model activities which can occur (the transition fires), thus changing the state of the system (the marking of the PN). Transitions are only allowed to fire if they are enabled, which means that all the preconditions for the activity must be fulfilled (there are enough tokens available in the input places). When the transition fires, it removes tokens from its input places and adds some at all of its output places. The number of tokens removed/added depends on the cardinality of each arc. The interactive firing of transitions in subsequent markings is called token game [3]. PN are a promising tool for describing and studying systems that are characterized as being concurrent, asynchronous, distributed, parallel, nondeterministic, and/or stochastic. As a graphical tool, PN can be used as a visual-communication aid similar to flow charts, block diagrams, and networks. In addition, tokens are used in these nets to simulate the dynamic and concurrent activities of systems. As a mathematical tool, it is possible to set up state equations, algebraic equations, and other mathematical models governing the behavior of systems [4].
To study performance and dependability issues of systems it is necessary to include a timing concept into the model. There are several possibilities to do this for a PN; however, the most common way is to associate a firing delay with each transition. This delay specifies the time that the transition has to be enabled, before it can actually fire. If the delay is a random distribution function, the resulting net class is called stochastic PN. Different types of transitions can be distinguished depending on their associated delay, for instance immediate transitions (no delay), exponential transitions (delay is an exponential distribution), and deterministic transitions (delay is fixed). The concept of PN has its origin in Carl Adam Petri's dissertation Kommunikation mit Automaten, submitted in 1962 to the faculty of Mathematics and Physics at the Technische Universität Darmstadt, Germany.
Till now the Intelligent Tutoring Systems (ITS) have demonstrated its effectiveness in diverse domains. However, its construction implies a complex and intense work of engineering of knowledge, which prevents an effective use of it. In order to eliminate these deficiencies there appears the target to construct the author's hardware which facilitates the construction of these systems to all users (not necessarily expert) in the computer field, in particular to those instructors who master a certain matter of education [5]. The field of the ITS is characterized by the application of Artificial Intelligence techniques, to the development of the teaching-learning process assisted by computers. If the key feature of an ITS is the aptitude to adapt itself to the student, the key component of the system is the Student Model, where the information associated to the student is stored. This information must be inferred by the system depend-

286 Maikel León et al.
ing on the information available: previous data, response to questions, etc. This process of inference is identified as diagnosis, and is undoubtedly the most complicated process inside an ITS, who uses the Artificial Intelligence techniques to represent knowledge, to shape the human reasoning, to emphasize the learning by means of the action, to combine experiences of resolution and discovery, to be able to solve problems by their own, to formulate diagnoses and to provide explanations. So, they count on a bank of instruction strategies which helps to decide what and how to inform to the student to get an effective direction.
The aim of this paper is to present a new approach to elaborate ITS using a CM, questionnaires able to catch the cognitive and affective state of the student (Student Model) and by using PN to allow the student to sail in an oriented way according to their knowledge and not in a free way as the CM are conceived. And it is the capacity to adapt dynamically to the development of the student’s learning what makes this system intelligent. The pedagogical model is controlled by PN that access and updates the apprentice model, while presenting the contents of the domain model. The PN are automatically generated from specifications defined by the teacher using a graphical interface and compiled into a neural net that actually implement the ITS. An important point in the proposed approach is that the teacher does not need to specify the interaction with the apprentice model; this interaction is automatically included in the PN using the prerequisite and difficulty orders previously defined by the teacher.
2 Motivations for Concept Maps
Concept Maps provide a framework for capturing experts' internal knowledge and making it explicit in a visual, graphical form that can be easily examined and shared. CM constitute a tool of great profit for teachers, investigators of educational topics, psychologists, sociologists and students in general, as well as for other areas especially when it is necessary to manage with large volumes of information. They have become a very important element in the plans for the improvement of the ITS and they have also extended its use to other areas of the human activity in which both management and the use of knowledge take up a preponderant place. If to define a CM is relatively simple, it is simpler to understand the meaning of the definition. The Concept Maps were defined by Novak, his creator, as a skill that represents a strategy of learning, a method to get the gist of a topic and a schematic resource to represent a set of conceptual meanings included in a set of propositions [2].
It is necessary to point out that there is not only one model of CM, several may exist. The important point is the relations that are established between the concepts through the linking words to form propositions that configure a real value on the studied object. For such a reason, in a concept there may appear diversity of real values. In fact, it turns very difficult to find two exactly equal CM, due to the individual character of knowledge. The CM can be described under diverse perspectives: abstract, visualization, and conversation. Since significant learning is reached more easily when the new concepts or conceptual meanings are included under wider concepts, the most used CM are the hierarchic ones, the most general and inclusive con-

Combining Concept Maps and Petri Nets

287

cepts are placed in the upper part of the map, and the progressively more specific and less inclusive concepts, in the lower part. The subordinated relations among concepts may change in different fragments of learning, so in a CM, any concept may rise up to the top position, and keep a significant propositional relation with other concepts of the map. The use of CM designed by the professor increases both learning and retention of scientific information. The students produce maps as learning tools. Considering that the CM constitute an explicit and clear representation of the concepts and propositions, they allow both teachers and students to exchange points of view on the validity of a specific propositional link and to recognize the missing connections in the concepts that suggest the need of a new learning. For this reason, this skill has complemented so favorably with the practice of distance learning which presupposes that students and teachers are not physically in the same place at the same time.
Concept Maps have particular characteristics that make them amenable to smart tools [2]. These include: 1. Concept Maps have structure: By definition, more general concepts are presented
at the top with more specific concepts at the bottom. Other structural information, e.g. the number of ingoing and outgoing links of a concept, may provide additional information regarding a concept’s role in the map. 2. Concept Maps are based on propositions: every two concepts with their linking phrase forms a “unit of meaning”. This propositional structure distinguishes Concept Maps from other tools such as Mind Mapping and The Brain, and provides semantics to the relationships between concepts. 3. Concept Maps have a context: A Concept Maps is a representation of a person’ understanding of a particular domain of knowledge. As such, all concepts and linking phrases are to be interpreted within that context. 4. Concepts and linking phrases are as short as possible, possibly single words. 5. Every two concepts joined by a linking phrase form a standalone proposition. That is, the proposition can be read independently of the map and still “make sense”. 6. The structure is hierarchical and the root node of the map is a good representative of the topic of the map.

The students who analyze CM will have a wider basic knowledge; therefore, they will be more prepared to solve problems in comparison to those students who learn by memorizing. The CM have turned into a useful instrument for teacher training and for the student’s understanding of diverse subjects. CM constitute an instrument that merges the scientific rigidity with the simplicity and flexibility. It represents an aid for those who generate, transmit, store and spread information and knowledge. They also constitute an important tool to achieve a practical value, especially in the Artificial Intelligence systems.

288 Maikel León et al.
3 Proposed Model
The ITS that are created correspond with a CM, with the particular feature that in some of its nodes there appears a questionnaire, capable to get the cognitive and affective state of the student and able to guide his navigation, creating this way an “Intelligent” CM. At the first level of the authoring interface, the teacher must specify the set of pedagogical units that define each curriculum and, at the second level, the set of problems that define each pedagogical unit. The elements of these sets present a partial order and associated prerequisites [6]. To specify these partial orders and prerequisites the teacher disposes of a graphical interface that allows the construction of graphs. In these graphs, an edge from node n1 to node n2 means that n2 has n1 as a prerequisite. Each node n may have the following input edges:
• None: node n has no prerequisite and can be executed anytime. This is the case only for initial nodes.
• One: node n has only one node as prerequisite and this one must be executed before node n is available for execution.
• Two or more necessary edges: node n has several prerequisite nodes and all of them must be executed, before node n is available for execution.
• Two or more alternative edges: node n has several prerequisite nodes but only one of them must be executed before node n is available for execution.
Necessary and alternative edges may occur simultaneously in the same node. Nodes and the different types of edges may be combined in a complex graph, according to the intended course sequences, but they must satisfy one restriction: each graph must have only one initial node and only one final node. This restriction seems reasonable, because pedagogical units and problems should have one begin and one end point and it also assures that the graphs of the two levels can be combined into a one level PN. Figure 1 shows a prerequisite graph. Note the AND constraint (with nodes n5 and n6 before and node n6 after) associated with necessary edges and the OR constraint (with nodes n6 and n7 before and node n8 after) associated with alternative edges.
Fig. 1. Prerequisites Graph

Combining Concept Maps and Petri Nets

289

The first step in the generation of the final code of an ITS using the proposed authoring tool is to compile each prerequisite graph into a PN that represents the pedagogical model. The pedagogical model defines the control strategy of the interaction between the apprentice and the TA associated with each sub-domain and is specified by the teacher in the form of curriculum and pedagogical unit graphs. To represent the pedagogical model, we use an Object Petri Net (OPN) [4]. An OPN is defined by a control structure (places, transitions and arcs connecting places to/from transitions), by the data structure of its tokens and by its semantics, that defines how the token player should execute the OPN, given an initial marking. The OPN places can represent either pedagogical unit, if the graph corresponds to the first level of the interface, or problems, if it corresponds to the second level.
The net structure is directly obtained from the graphs specified by the teacher and the token data structure is defined by the adopted apprentice model. As the tokens have the class Apprentice the marking of the OPN is associated with the current activity of the apprentice and also with the possible future activities. A transition fires when the apprentice has finished the interactions associated with its input nodes. The token is then moved to the output places of the transition, meaning that the apprentice is ready to begin the activities associated with them. A prerequisite graph (curriculum or pedagogical unit) is automatically translated into an OPN structure. Each configuration of nodes and edges in a prerequisite graph is associated with an OPN fragment that implements it and these fragments are combined into a single Petri net that implements the graph.
For instance, a simple sequence of nodes, see figure 2(a), is implemented by one transition (t1 2), one input place (n1) and one output place (n2). One node with two output edges, see figure 2(b), is implemented by a fork transition (tf1) with one input place (n1) and two output places (n2 and n3). After the activity associated with the input place is finished, the apprentice can do either the activity associated with one output place or the activity associated with the other. A node with two input necessary edges, see figure 2(c), is implemented by a join transition (tj3) with two input places (b1 3 and b2 3) and one output place (n3). The places b1 3 and b2 3 are buffer places and do not correspond to pedagogical units or problems. They are necessary because after the activity associated with either one of the places n1 or n2 is finished, that information must be stored in the corresponding buffer and only when the activity associated with the other place is also finished (and also stored in the corresponding buffer) the activity associated with the output place (n3) can be performed. For this, we need two extra transitions (tb1 3 and tb2 3) connecting, respectively, the input places n1 and n2 to the buffers b1 3 and b2 3. Finally, see figure 2(d), a node with two alternative input edges is represented by two transitions (t1 3 and t2 3), with input places n1 and n2, respectively, and both with output place n3. Transitions t1 and t2 are used to remove the apprentice token from places n1 and n2, respectively; in the case the activities associated with n3 have already been performed, avoiding in this way the repetition of n3. Figure 3 shows the OPN obtained from the prerequisite graph of figure 1.

290 Maikel León et al.
Fig. 2. Graph Topologies and Petri Nets
Fig. 3. Structure of an OPN
The semantic of an OPN is the particular way the token player executes this net given an initial marking. An ordinary token player (used for PN with binary tokens) verifies, from an initial marking, whether there are enabled transitions. If there is just one, the token player fires it. If there is more than one, it chooses one of them according to a priority order, or arbitrarily if no priority order is specified. In an OPN token player, because the token is associated with a data structure, the transitions can have pre-conditions that refer to the token attributes and/or to external variables [4]. The apprentice token attributes that are relevant to the definition of algorithm are name, doing and tobedone and the relevant external variables are next, that contains the next activity to be performed, and answer, that receives the interaction support unit returned values. We say that the apprentice, identified by name, is doing a problem if the apprentice is interacting with one of its associated interaction support units. When the apprentice finishes the interaction with one support unit, the unit returns one of the following navigation commands: halt, continue, repeat, that is stored in the external variable answer.
The command halt means that the apprentice wants to interrupt the present session with the ITS, the command repeat means that the next activity should be another interaction support unit of the same problem and the command continue means that the current problem is finished and a next one must be chosen, either by the coordinator or by the apprentice. The apprentice is asked to choose the next problem, when there is more than one problem in tobedone. Because the apprentice must choose only

Combining Concept Maps and Petri Nets

291

one problem to do next, only one transition can fire at a given moment [7]. The coordinator initializes the associated PN with a marking where the apprentice token is stored in the place associated with the first problem of the first pedagogical unit and calls the OPN token player.

4 Petri Nets and Neural Networks. A possible solution
PN are powerful and versatile tools for modeling, simulating, analyzing and designing of complex workflow systems. This topic mainly discusses a hybrid approach using neural network and PN in the formal model of an ITS. Neural networks are used in different fields; classification is one of problems where they are commonly used [8]. PN are alternative tools for the study of non-deterministic, concurrent, parallel, asynchronous, distributed or stochastic systems [9]. They can model systems in an easy and natural way. Furthermore, the PN approach can be easily combined with other techniques and theories such as object-oriented programming, fuzzy theory, neural networks, etc [10]. These modified PN are widely used in computing, manufacturing, robotic, knowledge based systems, process control, as well as in other kinds of engineering applications. Since PN offer advantages to model systems and can interact with other techniques easily, it would be advantageous to model neural networks starting from PN models, which allow not only the design adjustment but also the initialization of the neural network weights [11]. Following the algorithm proposed by Xiaoou Li and Wen Yu in [4], we can model a neural network starting from a PN with the application of weighty production rules in the algorithm. See the following example (figure 3).
The following weighting production rules are obtained starting from the example of Figure 3 and following the algorithm described in [4]:
1. If P1 and P4 then P5 (w1, w4)
2. If P1 and P4 then P6 (w1, w4)
3. If P1 and P4 then P12 (w1, w4)
4. If P5 or P6 then P9 (μ4, μ5-9)
5. If P6 and P1 then P15 (w6, w1)
6. If P9 then P10 (μ6)
The learning algorithm of the neural networks obtained is the same as the backpropagation of multilayer neural networks. The main idea is that all layer weights can be updated through the backpropagation algorithm if certainty factors of all sink places are given. A complex neural network can be divided into several sub-networks starting from the modular design of an original PN. With this system a teacher can edit an ITS starting from a CM that will compiled into a PN, the neural network that control the system interaction is constructed starting from the PN. The initial modelling PN will allow us to design a feedforward neural network with the backpropagation learning algorithm. It is even possible to create several neural networks starting

292 Maikel León et al.
from only one PN, taking into account its possible modules [12]. To split neural networks in modules makes their training and performance easier.

P4 w4

T3 μ3-5 P5

w1-3 P1

μ3-12 μ3-6

P12 w1-5

P6 w6

T4 μ4

μ5-9 T5

P9

μ5-15 P15

Fig. 3. Example of Petri Net

T6 μ6 P10

w1-3

G5 α(P15)

α(P1)

G1 α(P5) w6

w1-3

w1-3 w4

w1-3
G2 α(P6)

μ4 μ5-9

α(P9) G4 μ6

G6

w4
α(P4) w4

G3

α(P12)

α(P10)

Fig. 4. A neural network model obtained from the Petri Net of Figure 3

5 Conclusions
With this work we propose a new Student Model that could be considered in the construction of ITS, where are combined the facilities of the Concept Maps for the organization of the knowledge and the potentiality of the Petri Nets that allows to the student to sail in an oriented way according to their knowledge and not in a free way as the CM are conceived. The proposed idea has been applied successful in the elaboration of ITS by users (not necessarily expert in the Computer Science field).
The suggested hybrid system combines PN and neural networks using the advantages of PN in order to overcome the neural network deficiencies concerning their original design and definition of their initial weighs.

Combining Concept Maps and Petri Nets

293

References

1. Cañas Alberto J., Marco Carvalho. (2004). Concept Maps and AI: an Unlikely Marriage? SBIE 2004: Simpósio Brasileiro de Informática na Educação Manaus, Brasil.
2. Martínez, N., León M. (2006). Mapas Conceptuales y Redes Bayesianas en los Sistemas de Enseñanza/Aprendizaje Inteligentes. Congreso Internacional de Informática Educativa. Costa Rica.
3. Xiao-Qiang Liu, Min Wu, Jia-Xun Chen. (2002). Knowledge aggregation and navigation highlevel petri nets-based in e-learning. In Proceedings International Conferenceon Machine Learning and Cybernetics, volume 1, pages 420– 425.
4. Li X., Yu W. (2000). Dynamic Knowledge Inference and Learning under Adaptive Fuzzy Petri Net Framework, IEEE Transactions on Systems, Man, and Cybernetics – Part C: Applications and reviews, Vol. 30, No. 4.
5. Esposito F., Ferilli S., Basile A., Di Mauro N. (2006). Inference of abduction theories for handling incompleteness in first-order learning. Knowledge and Information Systems (KAIS) Journal, 1-27, ISSN: 0219-1377.
6. Brusilovsky P. (2000). Adaptative hypermedia: From intelligent tutoring systems to web-based education. Lecture Notes in Computer Science. 5th International Conference on ITS. Montreal, Canada.
7. Murray T. (1999). Authoring intelligent tutoring systems: An analysis of the state of the art. In Journal of Artificial Intelligence and Education, volume 10.
8. Hilera, J. (1995). Redes neuronales artificiales: Fundamentos, modelos y aplicaciones. Ed. Addison-Wesley Iberoamericana, 1995.
9. Yu X., Chen G., Cheng S. (1995). Dynamic Learning Rate Optimization of the Backpropagation Algorithm, IEEE Transaction on Neural Networks, Vol. 6, No. 3, pp. 669-677.
10. Bello, R. (2002). Aplicaciones de la Inteligencia Artificial. Ediciones de la Noche, Guadalajara, Jalisco, México. ISBN: 970-27-0177-5.
11. Liu W., Hong J. (2000). Re-investigating Dempster's Idea on Evidence Combination. Knowledge and Information Systems: An International Journal, Springer-Verlag, Vol.2, No.2, pp.223-241, ISSN 0219-1377.
12. Cooley J. (1999). Data Preparation for mining World Wide Web browsing patterns. Journal of Knowledge and Information Systems.

Query-Optimal Oracle Turing Machines for Type-2 Computations
Chung-Chih Li
School of Information Technology Illinois State University Normal, IL 61790, USA
Abstract. With the notion of optimal programs deﬁned in terms of Blum’s complexity measure, the well known speed-up theorem states that there exist computable functions that do not have optimal programs. For type-2 computation, we argue that the same speed-up theorem can be obtained as long as the complexity measure satisﬁes Blum’s complexity measure under the Oracle Turing Machine model. In the present paper, we consider the oracle query as a dynamic complexity measure. Since the query complexity is not a Blum’s complexity measure, we have to characterize the notions of optimal programs in a very diﬀerent way. We give three notions of query-optimal programs in diﬀerent strength and argue that the stronger the notion of query-optimal programs we deﬁne, the more diﬃcult to have query-optimal programs. In other words, with a stronger deﬁnition of query-optimal programs, one can prove an easier version of the speed-up theorem.
Key words: Type-2 Complexity Theory, Type-2 Computation, Query Optimal Programs, Speed-up Theorems
1 Introduction
Searching for the best algorithm to solve a concerned computable problem is a primary task for computer scientists. However, the speed-up theorem tells us that, theoretically, we cannot succeed on all problems. In fact, the speedup phenomena have been extensively studied by mathematicians for more than a half century, which in logical form was ﬁrst remarked by G¨odel in terms of the length of theorem proving [5, 6, 13]. Blum [1, 2] re-discovered the speed-up phenomenon in the context of computational complexity.
Blum’s speed-up theorem is a result of his axiomatization of complexity measures. Fix a programming system for Turing machines and let ϕi denote the function computed by the ith Turing machine and Φi denote the cost function associated to it. Blum considers that dynamic complexity measures should meet the following two requirements: for any i, x, m ∈ N, (i) ϕi is convergent on x (denoted by ϕi(x) ↓) if and only if Φi is convergent on x (denoted by Φi(x) ↓) and (ii) Φi(x) = m is eﬀectively decidable. The two axioms have successfully lifted the study of complexity theory to an abstract level with rich results that

Query-Optimal Oracle Turing Machines for Type-2 Computations 295

are independent from any speciﬁc machine models. At type-1, the meaning of “more eﬃcient programs” is intuitive: For a computable function f , we say that program j is more eﬃcient than program i if ϕi = ϕj = f and for all but ﬁnitely many x we have Φj(x) < Φi(x). Precisely, Blum [1, 2] formulates the speed-up theorem as follows: For any recursive function r, there exists a recursive function f such that

∞
(∀ i : ϕi = f ) (∃j : ϕj = f ) ( ∀ x) r(Φj(x)) ≤ Φi(x) .

(1)

There are many sources for detailed proofs and discussions regarding this curious speed-up phenomenon, e.g., [1–4, 6, 11–14]. We shall skip all irrelevant details due to the space constraints.
When we try to lift the complexity theory to type-2 computation, we want to know whether or not the same speed-up phenomenon can be described in terms of some complexity measures that are sensible in the context of type-2 computation. Clearly, time and space remain meaningful in type-2 computation. Since time and space are Blum’s complexity measures, we have not faced too many diﬃculties in translating the original proofs of the speed-up theorem and its variants into type-2 (see [9]). For query complexity, on the other hand, the two Blum’s axioms fail. As a result, we can’t reuse the same concepts and techniques but develop our new ones in order to investigate this specular speed-up phenomenon.

2 Type-2 Computation Using Oracle Turing Machines
In this section we will provide necessary notations for type-2 computation; detailed discussion about the type-2 complexity theory can be found in [7–10]. We consider natural numbers as type-0 objects and functions over natural numbers as type-1 objects. For type-2 objects, they are functionals that take as inputs and produce as outputs type-0 or type-1 objects. We consider objects of lower type as special cases of higher type, i.e., type-0 ⊂ type-1 ⊂ type-2. Without loss of generality we restrict our type-2 functionals to the standard type T ×N N, where T is the set of total functions and means possibly partial.
We use the Oracle Turing Machine (OTM) as our standard computing formalism for type-2 computation. An OTM is a Turing machine equipped with a function oracle. We say that a type-2 functional F : T × N N is computable if there is an OTM that computes F as follows. Before the OTM begins to run, the type-1 argument should be presented to the OTM as an oracle. Every OTM has two extra tapes called query-tape and answer-tape for oracle queries and oracle answers, respectively. During the course of the computation, the OTM may enter a special state called query-state where the oracle will read the query left on the query-tape and return its answer to the answer-tape. We can ﬁx a programming system ϕi i∈N for OTM’s and let Me denote the OTM with index e that computes ϕe.
Let F denote the set of ﬁnite functions over natural numbers, i.e., σ ∈ F iﬀ the domain of σ is a subset of N and its cardinality is in N. Given F : T ×N → N,

296 Chung-Chih Li
F (f, x) ↓= y denotes that F is deﬁned at (f, x) and its value is y. Since if F is computable, F must be continuous (i.e., compact and monotone), it follows that if F (f, x) ↓= y, there must be σ ∈ F with σ ⊂ f such that for every extension η of σ (i.e., σ ⊆ η), we have F (f, x) ↓= F (σ, x) ↓= F (η, x) ↓= y. We say that (σ, x) and (η, x) are locking fragments of F on (f, x). For any σ ∈ F, let ((σ)) denote the set of total extensions of σ, i.e., ((σ)) = {f ∈ T σ ⊂ f }. Also, if (σ, x) ∈ F × N, let ((σ, x)) = {(f, x) f ∈ ((σ))}. If σ1 and σ2 are consistent, we have ((σ1))∩((σ2)) = ((σ1 ∪σ2)); otherwise, ((σ1))∩((σ2)) = ∅. The union operation ((σ1)) ∪ ((σ2)) is conventional.

3 Query-optimal Programs
Only in type-2 computation can we use the number of queries made throughout the course of the computation as a dynamic complexity measure. We are interested in whether or not there is a query-optimal program for any given type-2 computable functional; if there is not, can we have a speed-up theorem in terms of query-complexity. It turns out that the query-complexity fails to satisfy Blum’s two axioms. Consequently, the arguments in the original proof of the speed-up theorem are no longer valid. We thus need a new approach to understand this curious phenomenon. In this section we deﬁne a few alternative notions of query-optimal programs. These notions must be sensible and intuitive.

3.1 Unnecessary Queries

We shall argue that some intuitive notions of query-optimal programs are not ﬂexible enough to derive interesting results. We say that an oracle query is necessary if its answer returned from the oracle will aﬀect the result of the computation. Intuitively, a query-optimal program should be a program that does not make any unnecessary queries. Unfortunately, this notion of queryoptimal programs is too strong. It is easy to argue that there are functionals that always make unnecessary oracle queries. Consider the following functional F : T × N → N deﬁned by,

F (f, x) =

f (0) + 1 0

if ϕx(x) ↓ in f (0) steps; otherwise.

(2)

Clearly, F is computable and total. Fix any a such that ϕa is diverged on a (denoted by ϕa(a) ↑). Then, on input (f, a), the value of f (0) only aﬀects the speed of computing F (f, a). Thus, F (f, a) = 0 for any f ∈ T , and hence (∅, a) is the minimal locking fragment of F on (f, a). That means any queries made during the computation of F on (f, a) are unnecessary. By contradiction, if there were an OTM that would not make any unnecessary queries for F , one could modify such OTM to solve the halting problem, which is impossible. Therefore, the type-2 functional deﬁned in (2) can’t be computed by an OTM without making unnecessary queries. From this example, it is clear that to require query-optimal

Query-Optimal Oracle Turing Machines for Type-2 Computations 297

programs to be ones that do not make unnecessary queries is too restricted. We will loose the requirements by allowing a small amount of unnecessary queries. By “a small amount”, we means “a compact set” in some topology determined by the concerned computation. We formalize our idea in the next subsection.

3.2 Unnecessary Queries in Compact Sets

We use Q(s, e, f, x) to denote the collection of queries made during the run time of OTM Me on (f, x) up to s steps. Formally,
Deﬁnition 1. Let e be a ϕ-program. Deﬁne Q : (N × N × T × N) → F by: Q(s, e, f, x) = τ , where τ ∈ F with τ ⊂ f and dom(τ ) is the collection of queries made during the course of the computation of OTM Me on (f, x) in s steps.
Clearly, Q(s, e, f, x) is monotone in s, i.e., s ∈ N, Q(s, e, f, x) ⊆ Q(s + 1, e, f, x).

Deﬁnition 2. Let e, x ∈ N and f ∈ T . We say that Q(·, e, f, x) is deﬁned in the limit if and only if there exist s ∈ N and τ ∈ F such that, for every s ≥ s,

Q(s, e, f, x) = Q(s , e, f, x) = τ.

In the case above, we write lim Q(s, e, f, x) ↓ = τ.
s→∞

We wish to treat the set of queries an OTM made as a dynamic complexity

measure, but it fails to satisfy Blum’s two axioms for complexity measure. That

is,

lim Q(s, e, f, x)
s→∞

↓

does

not

mean

that

ϕe(f, x)

↓,

and

hence

Blum’s

ﬁrst

axiom fails. Moreover, for some σ ∈ F with σ ⊂ f , lim Q(s, e, f, x) = σ is not

s→∞

necessarily decidable, and hence Blum’s second axiom fails too.

Let F : T × N → N be computable. Since we mean to characterize some ϕ-

programs e1, e2, . . . , en that compute the same functional F in terms of queries, the topology T(· · · ) deﬁned in [7–10] cannot serve as the relative topology in
which we need a workable notion of compactness. This is because ϕe1 = ϕe2 = · · · ϕen = F and thus T(ϕe1 , ϕe2 , . . . , ϕen ) = T(F ), and hence the complexity of unnecessary queries cannot be described by the compact sets in T(F ). The product topology, T × N (Baire and the Discrete topologies), is ﬁne enough to
describe the needed compact sets but it suﬀers the same problem as discussed

in [7–10]. We thus introduce another family of topologies as follows.

Deﬁnition 3. Given e1, e2, . . . , en ∈ N, let B(e1, e2, . . . , en) ⊆ T ×N be deﬁned as follows: (σ, a) ∈ B(e1, e2, . . . , en) if and only if there exists some f ∈ T such that, for each i ∈ {1, . . . , n}, we have

lim Q(s,
s→∞

ei,

f,

a)

↓

=

σi,

and σ =

σi. Let Q(e1, . . . , en) be the topology on T × N deﬁned by the

i∈{1...n}

basic open sets in B, where

B = ((σ, x)) (σ, x) ∈ B .

298 Chung-Chih Li

Since a ϕ-program e may make unnecessary queries outside the domains of its minimal locking fragments, it follows that the topology Q(e) may be ﬁner than the topology T(ϕe) which is deﬁned by the minimal locking fragments of ϕe. That is, T(ϕe) ⊆ Q(e). In other words, if e is a ϕ-program for F : T × N → N, then T(F ) ⊆ Q(e). On the other hand, if Q(e) ⊂ T(F ), then we can conclude that e cannot be a ϕ-program for F . In general, we have
T(ϕi, ϕj) ⊆ Q(i, j).
For convenience, we deﬁne the following notations. Let card(S) be the cardinality of set S. Suppose i and j are two ϕ-programs that compute the same functional, i.e., ϕi = ϕj. Deﬁne

Q[i<j] =

(f, x)

card( lim Q(s, i, f, x)) < card( lim Q(s, j, f, x)) ,

s→∞

s→∞

Q[i=j] =

(f, x)

card( lim Q(s, i, f, x)) = card( lim Q(s, j, f, x)) ,

s→∞

s→∞

Q[i>j] =

(f, x)

card( lim Q(s, i, f, x)) > card( lim Q(s, j, f, x)) .

s→∞

s→∞

The following deﬁnition makes use of the compactness in the topology we just mentioned to deﬁne the meaning of “better” ϕ-programs in terms of querycomplexity.

Deﬁnition 4 (Query Speed-up). Let ϕi = ϕj.

1. We say that j is a weakly query sped-up version of i if and only if

Q[i<j] is compact and Q[i>j] is noncompact in Q(i, j).
If j is a weakly query sped-up version of i, we write ϕi ≺∗Q ϕj. 2. We say that j is a strongly query sped-up version of i if and only if

Q[i<j] ∪ Q[i=j] is compact in Q(i, j).
If j is a strongly query sped-up version of i, we write ϕi ≺≺∗Q ϕj.
Suppose that we are to write a new ϕ-program j for ϕi. Intuitively, if the new ϕ-program j can improve the query-complexity on a “large” (noncompact) set of inputs and leaves a “small” (compact) set of inputs on which j queries more than i does, we say that j is a weakly query sped-up version of i. Note that, we do not care how many inputs on which the query-complexity remains unchanged. Since T(ϕj) ⊆ Q(i, j), it follows that, if Q[i<j] is compact in Q(i, j), so is it in T(ϕj), and hence we can patch ϕ-program j on Q[i<j] so that the patched ϕprogram will never query more than i does. Whereas, if ϕ-program j is a strongly query sped-up version of i, then the set on which j does not improve its querycomplexity must be “small” (compact). Note that, if Q[i<j] ∪ Q[i=j] is compact in Q(i, j), then Q[i>j] must be noncompact in Q(i, j). Clearly, for ϕi = ϕj, we have the following implication:
ϕi ≺≺∗Q ϕj =⇒ ϕi ≺∗Q ϕj .

Query-Optimal Oracle Turing Machines for Type-2 Computations 299
Deﬁnition 5 (Query-optimal ϕ-programs). Let F : T × N → N be a computable functional. Let e be a ϕ-program for F .
1. We say that e is an absolute query-optimal ϕ-program for F if and only if, on every (f, x) ∈ T × N, the OTM Me does not make unnecessary queries during the course of computing ϕe(f, x).
2. We say that e is a strong query-optimal ϕ-program for F if and only if there is no ϕ-program i for F such that, ϕe ≺∗Q ϕi.
3. We say that e is a weak query-optimal ϕ-program for F if and only if there is no ϕ-program i for F such that, ϕe ≺≺∗Q ϕi.
Deﬁnition 5 gives us three versions of query-optimal ϕ-programs, where version 1 is the strongest notion and version 3 is the weakest one. Suppose that e is a ϕ-program for F : T ×N → N. The following implications are straightforward:
1. If e is an absolute query-optimal ϕ-program for F , then it is also a strong query-optimal ϕ-program for F .
2. If e is an strong query-optimal ϕ-program for F , then it is also a weak query-optimal ϕ-program for F .
Moreover, if T(ϕe) = Q(e), then e is an absolute query-optimal ϕ-program for ϕe. We shall argue that the stronger the notion of query-optimum, the easier we can obtain a speed-up theorem. In other words, it is more diﬃcult to obtain a query-optimal OTM when the notion of query-optimum becomes stronger. However, we do not know if the speed-up phenomenon still exists under our weakest notion of query-optimum; neither do we know how far can one weaken the notion of query-optimal programs to do away the speed-up phenomenon while keeping the notion nontrivial. We give partial results in the next section.
4 Query-optimal Programs
With a ﬁxed complexity measure that satisﬁes Blum’s axioms, the original speedup theorem states that there is a total computable function without optimal program for it. Although query-complexity does not satisfy the Blum’s axioms, each of the three versions of query-optimal programs gives rise to the following questions:
1. Does every computable functional F : T × N → N always have a queryoptimal ϕ-program for it?
2. If the answer to the ﬁrst question is negative, then can we construct the query-speed-able functional?
3. If a given F : T × N → N does have a query-optimal ϕ-programs for it, then can we uniformly construct a query-optimal program for F from an arbitrary ϕ-program for F ?
4. Suppose there is no query-optimal ϕ-program for ϕe0 : T × N → N. Can we eﬀectively construct an inﬁnite sequence of query-sped-up version of ϕprograms from e0? That is, can we have e0, e1, e2, . . . such that, for each i ∈ N, ei+1 is a query-sped-up version of ei?

300 Chung-Chih Li

We have a positive answer to the second question with respect to the absolute and strong versions of query-optimal programs. Thus, a negative answer to the ﬁrst question follows immediately. Compared to the original nonconstructive speed-up theorem, the speedable functional for the second question is very simple. However, we do not know if there is a computable F : T × N → N that can forever strongly speed-up, i.e., we do not know if there is a computable F : T × N → N without a weak query-optimal ϕ-program. The answer to the third question is negative given by Theorem 4. The fourth question is still open. That is, given ϕe0 : T × N → N that is known to be speedable, we do not know if there is an eﬀective way to construct a sequence of ϕ-programs such that,

ϕe0 ≺∗Q ϕe1 ≺∗Q ϕe2 ≺∗Q · · · · · ·

or similarly,

ϕe0 ≺≺∗Q ϕe1 ≺≺∗Q ϕe2 ≺≺∗Q · · · · · · .

Let ϕe(x) ↓s denote the case that the Turing Machine Me, on x, halts in s steps. We now deﬁne a useful functional K : T × N → N in the following.

Deﬁnition 6. Deﬁne K : T × N → N by

K(f, x) =

ϕx(x) + 1 if ϕx(x) ↓s, where s =

f (0) i=0

f

(i);

0 otherwise.

Let H denote the set {x|ϕx(x) ↓}. For any (f, x) ∈ T × N, if x ∈ H, then any ϕ-program that computes K, on input (f, x), may ask arbitrarily many unnecessary queries, where the number of queries depends on the value of f (0). It is easy to prove that there is no ϕ-program for K with all unnecessary queries removed (Theorem 1). Even if we lower the standard to strong query-optimal programs, we still cannot have a query-optimal ϕ-program for K (Theorem 2). Theorem 3 states that K does have a weak query-optimal program for it.
Theorem 1. K has no absolute query-optimal ϕ-program.
Proof: We show that from an absolute query-optimal ϕ-program for K, if any, one can construct a solution to the halting problem.
Suppose, by contradiction, ϕe is an absolute query-optimal ϕ-program for K. Clearly, if x ∈ H, then for any f ∈ T , K(f, x) = 0. On the other hand, if x ∈ H, there must be two distinct f, g ∈ T such that ϕe(f, x) = ϕe(g, x). Thus, some queries to the oracles f and g must be made. Therefore, given any x ∈ N, we can run ϕe(∅∼0, x) ﬁrst, where ∅∼0 is the zero everywhere function. Then, if no queries were made during the course of computation, we know that x ∈ H. Hence, N − H is recursively enumerable, a contradiction.
Theorem 2. K has no strong query-optimal ϕ-program.

Query-Optimal Oracle Turing Machines for Type-2 Computations 301
Proof: Suppose, by contradiction, ϕe is a strong query-optimal ϕ-program for K. Let i ∈ H such that, for any f ∈ T , the OTM, Me, on (f, i), will make oracle queries at points 0, 1, 2, . . . , f (0). Such i must exist, otherwise the halting problem can be solved by a similar argument given for the proof of Theorem 1. We construct a weakly sped-up version of e as shown in Figure 1.

Program input f : T , x : N; if x = i and f (0) is even then output 0; else output ϕbe(f, x);
End program

/* by running ϕb-program e on (f, x) */

Fig. 1. A Weakly Sped-up Version of ϕbe

Let the index of the ϕ-program in Figure 1 be p. Clearly, if x = i or if f (0)

is odd, then the two ϕ-programs, e and p, will perform the same computation,

and hence the two make the same oracle queries. The two computations diﬀer

on the following set:

(f, i) f (0) is even .

(3)

Clearly, the set above is noncompact in Q(e, p). According to the program p and the assumption on i, the following set is the same as (3),

(f, i) card( lim Q(s, e, f, i)) > card( lim Q(s, p, f, i)) .

s→∞

s→∞

Thus, Q[e>p] is noncompact in Q(e, p). Moreover, Q[e<p] = ∅, and hence Q[e<p]
is compact in Q(e, p). It follows that p is a weakly sped-up version of e, i.e., ϕe ≺∗Q ϕp. Therefore, e cannot be a strong query-optimal ϕ-program for K.

Theorem 3. There is a weak query-optimal ϕ-program for K.

Sketch of Proof: It is impossible to strongly speed-up any ϕ forever. Otherwise, we can reach two ϕ programs e and p such that, ϕe = ϕp = K and ϕe ≺≺∗Q ϕp. Then, we can compare the queries made during the course of the computations between ϕe and ϕp to solve the halting problem. Therefore, a weak query-optimal ϕ-program for K must exist. We skip details of the proof.
However, we do not know if there is a computable type-2 functional such that, there is no weak query-optimal ϕ-program for it. In other words, we do not if there is a speed-up phenomenon associated with the notion of weak queryoptimum.
Suppose we know that a query-optimal program for ϕe does exist. In general, there is no eﬀective way to construct a query-optimal version of e in any strength, i.e., absolute, strong, and weak query-optimum. We state this in the following theorem, which is the strongest version in the sense that the weakest version of query-optimal programs is considered.

302 Chung-Chih Li
Theorem 4. There is no recursive r : N → N such that, for every ϕ-program e, if ϕe is total and there is a weak query-optimal program for it, r(e) is a weak query-optimal ϕ-program for ϕe.
Proof: By contradiction, suppose such recursive r : N → N exists. Consider the following program shown in Figure 2. Suppose that, by the recursion theorem, the index of the ϕ-program is e.
Program e input f : T , x : N; Q ←− Q(f (0), r(e), f, x); if ϕbr(e)(f, x) converges in f (0) steps then output f (max(dom(Q)) + 1); else output f (1);
End program e
Fig. 2. A ϕ-program with index e for Theorem 4 b
According to the construction, it is clear that e is a total ϕ-program. By assumption, r(e) is a weak query-optimal version of e. On any (f, x) ∈ T × N, the OTM Mr(e) will never halt in f (0) steps, otherwise the OTM Mr(e) does not compute ϕe. This is because if the OTM Mr(e) on (f, x) halts in f (0) steps, Q must contain the complete collection of queries made by the OTM Mr(e) on (f, x). But ϕe(f, x) is the value of f at a point not in Q.
Thus, we conclude that for every (f, x) ∈ T × N, ϕe(f, x) = f (1). But if that is the case, f (0) will not be queried by the OTM Mr(e), namely, the value of f (0) does not aﬀect the computation of Mr(e) at all. Thus, if f (0) is suﬃciently large, ϕr(e)(f, x) will converge in f (0) steps, and hence ϕe(f, x) = f (max(dom(Q))+1). Thus, the OTM Mr(e) does not compute ϕe.
Therefore, r(e) cannot be a weak query-optimal version of ϕ-program e.
5 Conclusion
The oracle query is a unique dynamic complexity measure in type-2 computation. Although Blum’s complexity measure has created a rich chapter in machine independent complexity theory, it is not appropriate to impose the same requirement to query complexity. We therefore provide new notions in order to describe the concept of query-optimal programs in type-2 computation. We obtain a speed-up theorem under a reasonable notion of query-optimal programs, which means that there exists a computable type-2 functional that does not have a query optimal OTM for it. Our version of speed-up theorem is stronger than the classical one in a sense that the speedable functional, K, does not depend on the speedup factor, i.e., the computable function r in equation (1).

Query-Optimal Oracle Turing Machines for Type-2 Computations 303
Clearly, there are many open questions that deserve further invitation. For example, can we uniformly construct a query-sped-up program for our functional K? Do we have a speed-up theorem under our weaker notions of query-optimal programs? Some classical questions can also be asked. For examples, do we have a union theorem? gap theorem? compression theorem? Or do we have a complexity hierarchy characterized by the query-complexity? Since Blum’s two axioms cannot be applied to the query-complexity, the approach and results must be very diﬀerent from the classical ones. It seems to us that the framework proposed in this paper may be a feasible direction for computer science theorists to study the query complexity closely.
References
1. Manuel Blum. A machine-independent theory of the complexity of recursive functions. Journal of the ACM, 14(2):322–336, 1967.
2. Manuel Blum. On eﬀective procedures for speeding up algorithms. Journal of the ACM, 18(2):290–305, 1971.
3. Critian Calude. Theories of Computational Complexity. Number 35 in Annals of Discrete Mathematics. North-Holland, Elsevier Science Publisher, B.V., 1988.
4. Nigel Cutland. Computability: An introduction to recursive function theory. Cambridge, New York, 1980.
5. Martin Davis, editor. The Undecidable. Raven Press, New York, 1965. 6. Kurt G¨odel. U¨ ber die l¨ange der beweise. Ergebnisse eines Math. Kolloquiums,
7:23–24, 1936. Translation in [5], pages 82-83, “On the length of proofs.”. 7. Chung-Chih Li. Asymptotic behaviors of type-2 algorithms and induced Baire
topologies. In Proceedings of the Third International Conference on Theoretical Computer Science, pages 471–484, Toulouse, France, August 2004. 8. Chung-Chih Li. Clocking type-2 computation in the unit cost model. In Arnold Beckmann, Ulrich Berger, Benedikt L¨owe, and John V. Tucker, editors, Proceedings of Computability in Europe, CiE 2006: Logical Approaches to Computational Barriers, CSR 7-2006, pages 182–192, Swansea, UK, June/July 2006. 9. Chung-Chih Li. Speed-up theorems in type-2 computation. In S. Barry Cooper, Benedikt L¨owe, and Andrea Sorbi, editors, Proceedings of Computability in Europe, CiE 2007: Computation and Logic in the Real World, pages 478–487, Siena, Italy, June 2007. Springer, LNCS 4497. 10. Chung-Chih Li and James S. Royer. On type-2 complexity classes: Preliminary report. In Proceedings of the Third International Workshop on Implicit Computational Complexity, pages 123–138, Aarhus, Denmark, May 2001. 11. A. R. Meyer and P. C. Fischer. Computational speed-up by eﬀective operators. The Journal of Symbolic Logic, 37:55–68, 1972. 12. Joel I. Seiferas. Machine-independent complexity theory. In J. van Leeuwen, editor, Handbook of Theoretical Computer Science, volume A, pages 163–186. NorthHolland, Elsevier Science Publisher, B.V., 1990. MIT press for paperback edition. 13. P. Van Emde Boas. Ten years of speed-up. Proceedings of the Fourth Symposium Mathematical Foundations of Computer Science 1975, pages 13–29, 1975. Lecture Notes in Computer Science. 14. Klaus Wagner and Gerd Wechsung. Computational Complexity. Mathematics and its applications. D. Reidel Publishing Company, Dordrecht, 1985.

From Hilbert’s Program to a
Logic Tool Box
J. A. Makowsky
Department of Computer Science Technion – Israel Institute of Technology
Haifa, Israel janos@cs.technion.ac.il www.cs.technion.ac.il/∼janos
For Witek Marek, ﬁrst mentor, then colleague and true friend, at the occasion of his 65th birthday.
Abstract. In this paper I discuss what, according to my long experience, every computer scientists should know from logic. We concentrate on issues of modeling, interpretability and levels of abstraction. We discuss how the minimal toolbox of logic tools should look like for a computer scientist who is involved in designing and analyzing reliable systems. We shall conclude that many classical topics dear to logicians are less important than usually presented, and that less known ideas from logic may be more useful for the working computer scientist.
The following text is not a scientiﬁc paper. It is really a prose version of a set of slides in which I present my ideas on the subject. I have presented a ﬁrst version of these slides at the LPAR’07 conference in Yerevan, Armenia, in October 2007. I do hope that I will ﬁnally turn these thoughts into a proper scholarly paper. In these sketchy notes I mostly give references to monographs, and not the original papers.
The Students I Have in Mind
I want to examine what we should teach from logic to our non-specialized undergraduate students. I mean, what does every graduate of Computer Science have to learn in/from logic? The current syllabus is often justiﬁed more by the traditional narrative than by the practitioner’s needs. The practitioner’s needs are determined by what he needs to understand his own activity in dealing with his computing environment. As a computer/computing engineer he should be aware of the inherent diﬀerence between consumer products and life-critical hardware and software. The occasional failure of consumer goods is beneﬁcial to the functioning of the Fordistic consumer society in as it maintains the consumption cycles needed for its functioning. The failure of life-critical products is disastrous

From Hilbert’s Program to a Logic Tool Box 305
for all the parties involved. Life-critical products have to be properly speciﬁed, veriﬁed, tested and certiﬁed before they can be released. The practitioner therefore needs a basic understanding of what it means to properly specify,test, verify and possibly certify. a product.
Practically speaking,
– he should understand the meaning and implications of modeling his environment as precise mathematical objects and relations;
– he should understand and be able to distinguish intended properties of this modeling and side-eﬀects;
– he should be able to discern diﬀerent level of abstraction; – he should master the (non-formalized) language of sets and second order
logic which enables him to speak about the modeled objects; – he should understand what it means to prove properties of modeled objects
and relations; – but he should also understand the inherent limitations of what can be achieved,
and of his own activity.
1 Sets and the Logical Foundations of Mathematics
Whether we like or not depends on our philosophical position, if we at all have one, but it is a fact supported by a large social consensus, that the language of sets is the most used and most accepted way of modeling mathematical objects. A very convincing discussion, why sets are used that way, is given in [BG08]. We are used to model automata of all sorts including Turing machines as tuples of sets, functions and relations. We do the same when we discuss behavior of hardware and software, when we prove properties of modeled artefacts, and when we show that certain combination of properties of such artefacts cannot be achieved.
The emergence of the language of sets goes back to the work of G. Cantor and G. Frege, who both felt the need to put mathematics on new rigorous foundations upon which the growing ediﬁce of real and complex analysis could be built. Cantor initiated the use of sets for modeling natural, real and complex numbers and their functions, and Frege wanted to derive the rules of set formation from logic. Frege’s program intended to derive the foundations of mathematics from logical principles. It derived set theory as the universal data structure for modeling mathematical objects from logic. The history of logic in the years between 1850 and 1950 is the history of successes and failures of Frege’s program. This history forms the traditional narrative along which we are used to teach logic. I will argue that this narrative is misleading as far as the working mathematician or computer scientist or engineer is concerned.
Let us look at this traditional narrative the way I see it. We start by paraphrasing the history of Logicism from Frege to G¨odel, and further to the reevaluation of Frege’s program.

306 J. A. Makowsky
Act I: Cantors Paradise
– First G. Cantor (1874 - 1884) created the Paradise of Sets. – Then G. Frege (1879) created the modern Logical Formalisms, including the
correct binding rules for quantiﬁcation, and – set out to lay the Foundations of Mathematics with his Die Grundgesetze
der Arithmetik, Volume1 (1893), see [Bur05] – The book was not well received. Only G. Peano, author of The principles
of arithmetic, presented by a new method (1889), [Ken73] wrote a positive review of it.
Act II: Paradise lost
– On 16 June 1902, Bertrand Russell pointed out, with great modesty, that the Russell paradox gives a contradiction in Frege’s system of axioms.
– . . . and with Russel’s paradox started the crisis of the Foundations of Mathematics,
– G. Cantor had sensed this, when he noticed trouble with the ”set of all sets” and his notion of cardinality. Let V be the set of all sets. Then its power set P (V ) is a subset of V . But Cantor proved that the cardinality of the power set P (A) of a set A is always strictly bigger than the cardinality of A. On the other hand the cardinality of a subset of A is at most the cardinality of the set A, a contradiction.
Act III: Hilbert’s Program
D. Hilbert around 1920 designs a program to provide a secure foundations for all mathematics. In particular this should include1:
– Formalization of all mathematics: all mathematical statements should be written in a precise formal language, and manipulated according to well deﬁned rules.
– Completeness: a proof that all true mathematical statements can be proved in the formalism.
– Consistency: a proof that no contradiction can be obtained in the formalism of mathematics. This consistency proof should preferably use only ”ﬁnitistic” reasoning about ﬁnite mathematical objects.
– Conservation: a proof that any result about ”real objects” obtained using reasoning about ”ideal objects” (such as uncountable sets) can be proved without using ideal objects.
– Decidability: there should be an algorithm for deciding the truth or falsity of any mathematical statement.
1 This subsection is a quote from Wikipedia. Its author is unknown.

From Hilbert’s Program to a Logic Tool Box 307
Hilbert’s Logic Lectures
In 1928, D. Hilbert and W. Ackermann publish Grundzu¨ge der theoretischen Logik, [HA28,HA49,HA50]. Here are the points of interest to us:
– The Logic in question is Second Order Logic. – What we call First Order Logic, is called there the restricted calculus. – They prove soundness of the calculus, and ask the question of completeness.
The book is soon translated into English, French and Russian and remains the most widely used reference for more than thirty years. K. G¨odel, as a graduate student, reads the book in 1928. The original book contains several technical mistakes which are ﬁxed in subsequent editions. The ﬁrst English edition [HA50] gives credit to A. Church and W. Quine for pointing out some mistakes in the second German edition.
The book states as the main problem of Logic its axiomatization and proofs of the
– Independence of the axioms – Consistency of the axioms – Completeness of the axioms – The decision problem of the conse-
quence relation
Act IV: Rise and Fall of Hilbert’s Program
Initial successes: – Leopold L¨owenheim (1915), Thoralf Skolem (1920), Mojz˙esz Pressburger (1929), Alfred Tarski (1930), Frank Plumpton Ramsey (1930), L´aszl´o Kalm´ar (1939) and many others prove partial decidability results for fragments of Logic, and for Arithmetic, Algebra, Geometry. – In 1929 Kurt G¨odel proves the completeness of the Hilbert-Ackermann axiomatization of the the restricted (ﬁrst order) calculus.
Final blows: – 1931 K. G¨odel proves that every recursive theory which contains arithmetic is incomplete. – 1931 K. G¨odel proves that every recursive consistent theory which contains arithmetic cannot prove its own consistency. – 1936 Alonzo Church and Alain Turing show that already for the restricted calculus with free relation variables the set of tautologies is not computable (but is semi-computable). Hence, they gave a negative solution to the Decision Problem.
The most comprehensive account of solvable and unsolvable cases of the Decision Problem can be found in [BGG97].

308 J. A. Makowsky
Act V: Clariﬁcations and Repairs
Out of the ashes rise the four classical sub-disciplines of mathematical logic:
Set Theory arises from work by E. Zermelo, D. Mirimanoﬀ, J. von Neumann, A. Fr¨ankel, K. G¨odel and P. Bernays. Alternative approaches were developed by, among others, W. Quine, W. Ackermann, and J.L. Kelley and A.P. Morse, and more recently, by P. Aczel. Set theory, in contrast to using sets in mathematical practice, is mostly concerned with settling questions around the axiom of choice and cardinal arithmetic, or in formulating alternatives, such as the axiom of determinacy, and in clarifying their impact on questions in topology and analysis. Today, set theory is a highly specialized branch of technical mathematics with little impact on computer science.
Proof Theory arises from work by W. Ackermann, G. Gentzen, J. Herbrand, D. Hilbert and P. Bernays. It has developed into a full-ﬂedged theory of proofs, comprising the analysis of (transﬁnite) consistency proofs in terms of ordinals and fast-growing functions, program extraction from proofs, and resource analysis related to provability. It also plays an important role in all aspects of automated reasoning, an important branch of Artiﬁcial Intelligence.
Recursion Theory arises from work by E. Post, J. Herbrand, K. G¨odel, A. Church, A. Turing, H. Curry. Recursion theory developed at one side into degree theory, classifying non-computable functions according to their different levels of complexity, at the other side it developed into Computability Theory and has become one of the pillars of Computer Science education in its own right.
Model theory arises from work by T. Skolem, A. Tarski, A. Robinson, R. Fra¨ıss´e and A. Mal’cev. Two main directions evolve, classiﬁcation theory, and a more algebraic and geometric theory, linking model theory with algebraic geometry and number theory. Although ﬁnite model theory has its early origin, it was through Automata Theory, Database Theory and Complexity Theory that it evolved into its own discipline with a legitimate place in advanced Computer Science education.
. . . and for long this remained the classical divide of Mathematical Logic. J. Shoenﬁeld’s monograph [Sho67] is possibly the only monograph covering
all aspects of Mathematical Logic up to the boundaries of research of his time. Since then the four classical disciplines pursue their own paths, and among the younger generations of researchers it cannot be taken for granted that they have studied the four disciplines in depth.
Act VI: 100 years later - Fixing Frege
If only G. Frege had not been so scared by B. Russel’s letter. C. Wright, P. Geach and H. Hodes suggested, and G. Boolos proved (1987) that a modiﬁed

From Hilbert’s Program to a Logic Tool Box 309
Frege program actually is feasible, [Boo98a,Boo98c,Boo98b]. They noted that the famous contradiction stemmed from the axiom which states, roughly, that the extension of any concept is a set. However, this axiom is only used to derive an abstraction principle, called Hume’s principle, which states, again roughly, that two extensions have the same cardinality if and only there is a bijection.
So we have for the modiﬁed Frege system:
Frege: The Peano Postulates can be deduced in dyadic second order logic from Hume’s principle and suitable deﬁnitions of the natural numbers (Frege’s Arithmetic).
Boolos: Frege’s arithmetic is interpretable in second order Peano Arithmetic.
Some (the Neo-Logicists) argue that this justiﬁes a revival of Logicism. But it also creates new problems. A thorough discussion of the pros and cons of Neo-Logicism can be found in J. Burgess [Bur05]. A thorough discussion of abstraction principles similar to Hume’s Principle can be found in K. Fine [Fin02].
That much for the ”big crisis”.
At least the set theory needed for the foundations of Computer Science can be derived from logical principles.
2 The Foundations of Mathematics and Computer Science
What Frege and Russel and Whitehead had in mind, viz. to build the foundations of mathematics from scratch, was done in a more intelligible (but still not too user friendly) way by E. Landau in his Foundations of Analysis ﬁrst published in German in 1930, and in English in 1951, with many reprints, the latest in 1999 with a German-English vocabulary by the American Mathematical Society, [Lan99]. In this book he explicitely constructs the real and complex numbers from the standard model of Peano arithmetic. Landau’s style is very dry and concise, and the text was written for mature mathematicians. A more pedagogical version of the same constructions can be found in S. Feferman’s [Fef64], which I also would love to see reprinted. One can view such a foundation of Analysis as a pragmatic version of Frege’s program. Roughly, one proceeds as follows:
– One starts with a cumulative hierarchy of sets, based on the empty set alone (or with urelements) and natural set construction principles which allow to construct also inﬁnite sets.
– Then one deﬁnes inductively the natural numbers with a successor function, and the sets of ﬁnite words over a (not necessarily) ﬁnite alphabet (a set), with an append operation for each element of the alphabet.

310 J. A. Makowsky
– One then proceeds with deﬁning the number systems N, Z, Q and their arithmetic operations inductively and using quotient structures.
– Then one construct the reals R, using Dedekind cuts. – In similar ways one constructs other structures, say, groups, ﬁelds, topolog-
ical spaces, Banach spaces, Lie algebras, which are speciﬁed axiomatically. – Existence of axiomatically deﬁned objects had to be established by an explicit
sequence of set construction steps within the cumulative hierarchy. – Clearly, one can apply the same methods to model objects of the computing
world, such as automata, formal languages, programs, data structures, etc.
One should adapt Landau’s way for modeling the basic data structures of Computer Science. I have attempted to do this in the course Sets and Logic for Computer science, which we teach in the third semester in our Computer Science undergraduate program.
Set Theory in Computer Science
Besides using set theory for modeling purposes, the computer scientist uses only few ingredients of set theory:
1. The Cantor-Bernstein Theorem to prove equicardinality, 2. The fact that a countable union of countable sets is countable, 3. The fact that the cardinality of the power set of a set A is always bigger
than that of A, 4. The relationship between the termination of processes and well-orderings. 5. The Recursion Theorem. 6. Some Fixed-Point Theorems.
The ﬁrst three of these go ultimately back to Cantor’s original work and are very basic. The Recursion Theorem and Fixed Point Theorems should be taught in a more advanced course.
Recursion Theory vs Computation Theory
Recursion Theory got its name for a good reason: The computable functions over the natural numbers were deﬁned recursively, and early Recursion Theory consisted in studying the strength of various proposed recursion schemes. Recursion schemes can be replaced by register machines. Computability Theory studies usually the computable relations and functions over sets of words. The three approaches are inter-translatable, but they are not the same. It is a pity that teaching all these complementary notions of computability is not always part of the Computer Science curriculum. Be that as it may, Computation Theory has emancipated itself from Logic and in Computer Science the two are often taught independently. A good exception is the monograph [Pap94].

From Hilbert’s Program to a Logic Tool Box 311
Proof Theory
Proof theory evolved around the question what type of consistency proofs are at all possible. As a spin-oﬀ the ﬁeld of deduction-based automated reasoning and automated theorem proving came into being. Proof theory is also used in the foundations of programming languages, where it generated a rich literature of deep insights into the nature of programming. Some basic principles of automated reasoning do belong into a beginner’s course on Artiﬁcial Intelligence, and some basic facts about functional programming do belong into a basic course on Programming Languages. However, only little of this rich material is suitable for the undergraduate student I have in mind. A comprehensive survey of Proof Theory is [Bus98].
Model Theory
The main tools of classical model theory almost all derive from the compactness theorem and variations of the model existence theorem. Using these tools one proves preservation theorems, the omitting types theorem and develops a general understanding of the possible structure of models of ﬁrst order theories. I have described how to use these tools in the Computer Science context, in database theory, the foundations of Logic Programming, and the speciﬁcation of data types, [Mak84,Mak92]. It turned out, however, that the Compactness Theorem is mostly suitable when dealing with inﬁnite structures, and the most prominent application of the Compactness Theorem in Computing is Herbrand’s Theorem with its ramiﬁcations in Automated Reasoning and Logic Programming. The most important tools from Model Theory in algorithmic applications are the Ehrenfeucht-Fra¨ıss´e games and the Feferman-Vaught Theorem and its variations. The former is omnipresent in Finite Model Theory, cf. [EF95,Lib04] and the a survey of the uses of the latter can be found in [Mak04]. For other failures of classical theorems of First Order Logic when restricted to the ﬁnite case, cf. [Gur88].
One should add here that the combination of the Compactness Theorem with the Ehrenfeucht-Fra¨ıss´e games leads to Lindstr¨om’s characterization of First Order Logic. The attempts to develop an Abstract Model Theory are documented in the monumental [BF85]. This line of reasoning had a considerable impact on Finite Model Theory and Descriptive Complexity in providing techniques for deﬁning logics which capture complexity classes. For the advanced student applications of Finite Model Theory to Computer Science are surveyed in [GKL+07].
The classical textbooks in Logic
The available undergraduate texts of Logic for Computer Science follow too often the narrative of the em Rise and Fall of Hilbert’s Program. They emphasize the classical Hilbertian topics.
– Logic is needed to resolve the paradoxes of set theory.

312 J. A. Makowsky
– First Order Logic is THE LOGIC due to its completeness theorem. – The main theorems of logic are the
Completeness Theorem and the Compactness Theorem – The tautologies of First Order Logic are not recursive. – Arithmetic Truth is not recursive enumerable. – One cannot prove CONSISTENCY within rich enough systems.
This is NOT what a Practitioner of Computing Sciences NEEDS !
Other texts are often written with a very special agenda reﬂecting the author’s research interest or his particular tastes. Finally, there are texts which are really written with the undergraduate Computer Science student in mind. But then they are often either written speciﬁcally with programmers in mind, and do not deal with the data modeling issues2. Logic is ﬁrst of all language in which we express ourselves before we prove statements. We ﬁrst have to formulate a speciﬁcation, a database query, an intermediate assertion, a loop invariant, before we prove them to hold, to be valid, or for two of them to be equivalent or not. Logic deals with deﬁnability issues as much as with provability issues, something which in the Hilbertian tradition is easily forgotten. An introductory text in Logic for Computer Science should choose its topics in way, that the student meets the topics taught again in later courses. When we teach Linear Algebra in the ﬁrst year of a mathematics curriculum, most of the topics reappear in vector analysis, diﬀerential equations, physic, statistics etc. We have to build our syllabus of logic keeping this in mind.
3 So what Does a Practitioner of Computing Sciences Need?
We distinguish between knowledge of theoretical orientation and practical knowledge, which consists of tools and skills. In our case this means:
Theoretical orientation: – awareness that our domain of discourse is an idealized world of artefacts which models fairly accurately the artefacts which allow us to run and interact with computing machinery. – awareness of the diﬀerent levels of abstractions. – awareness that in this world of artefacts there are a priori limitations. Not everything is realizable, computable, etc.
2 The recent book by R. Bornat[Bor05] is a lovely introduction to Logic for programmers.

From Hilbert’s Program to a Logic Tool Box 313
Practical knowledge: – tools which allow us to model new artefacts, whenever they arise; – tools which allow us to prove properties of the modeled artefacts.
The student needs a carefully adapted blend of the practical Frege program, with the knowledge of its limitations. He needs both proﬁciency and performance in his practical knowledge.
4 Lessons from 150 years of History
I have spent so much space reviewing what I consider noteworthy in the evolution of the Logicists program because I do want to draw some lessons from it which are not foundational but practical. I would like the B.Sc. graduates of Computer Science to be familiar with the following:
Lesson 1. Modeling the world
Our scientiﬁc language: Natural Language enhanced by precise use of boolean operations, quantiﬁcation and the use of naive language of sets.
Our universal data structure: A cumulative world of sets. Modeling the world: We model all artefacts of our computing world by con-
structed objects in the world of sets. Modeling involves side eﬀects:
Modeled artefact have properties not intended. Digression: the ordered pair: Ordered pairs could be introduced via an ab-
straction principle: (x, y) = (x , y ) if and only if x = x and y = y . But usually the ordered pair is modeled directly: N. Wiener: (x, y)W := {{{x}, ∅}, {{y}}}, K. Kuratowski: (x, y)K = {{x}, {x, y}}, Simpliﬁed : (x, y)S = {x, {x, y}}. Now one has to verify that (x, y) = (x , y ) if and only if x = x and y = y . All the proposed versions do satisfy this, but the proofs diﬀer. The simpliﬁed version requires the axiom of foundation. Kuratowski’s version is the accepted deﬁnition today. But all deﬁnitions have side eﬀects, e.g., x is an element of {x, {x, y}} but not of {{x}, {x, y}}. Proving properties of objects which depend on the use of ordered pairs should not use these side eﬀects, but only the deﬁning property.
The distinction between speciﬁed properties and side eﬀects should be taught early on!
Fixing levels of abstraction: Introducing structures, and ﬁxing which sets are not further to be analyzed. A graph is a pair < V, E >. A ﬁnite automaton is a tuple < S, Σ, R, I, T >.

314 J. A. Makowsky
Like in the foundations of Analysis, as practiced by R. Dedekind, E. Landau and N. Bourbaki, we need the precise language mix of normalized natural language augmented by the language of sets to model the idealized artefacts of computer science. To model the artefacts we also need basic tools.
Artefacts: strings, concatenation, natural numbers, graphs, relational structures stacks, arrays; circuits, Turing machines, register machines; speciﬁcation and programming languages,
Tools: Inductive deﬁnitions, proofs by induction; enumerations, proving countability and uncountability; well-orderings (for termination)
Is this not ”too denotational” ? .... ... our friends may ask.
Yes, this approach does map everything into sets. But “truth” does not necessarily presuppose a world of sets. Truth in the sense of Frege’s world is deﬁned by the laws (introduction and elimination rules) of logic and of the Fregean constructs. It does leave your foundational options open ...
Lesson 2. Modeling Computability and its limitations (when modeled)
We have already said that computability is usually taught in a separate course, be it together with formal languages or with an introduction to basic complexity theory. Nevertheless, our student should understand that computability is modeled over diﬀerent domains, computing operations, resource restrictions.
Natural numbers and recursion: The original deﬁnition of the set of recursive functions.
Natural numbers and register machines: Close to early programming languages.
Turing machines and words: Close to assembly languages. Other models: Logic programs, Lambda calculus, cellular automata, quantum
computing
Showing their equivalence involves modeling also
– translation between the domains; – translations between programs (interpreters and compilers).
Here I want to stress The diﬀerent basic structures involved, and their biinterpretability. In terms of knowledge of orientation and practical knowledge we have:

From Hilbert’s Program to a Logic Tool Box 315
Orientation: Not everything is computable. Not everything is feasibly computable.
Tools: Using the non-solvability of the Halting Problem to prove non-computability. Using diﬀerent types of reducibilities (and simulations).
I have observed that even my colleagues are sometimes imprecise: The Church Turing Hypothesis is often carelessly invoked. There is also a trend to say computable when actually one means feasibly computable, where feasibly computable may mean computable in deterministic polynomial time, sometimes computable in polynomial time with randomized algorithms.
It is also important to distinguish between complexity classes deﬁned as equivalence classes of problems under certain reductions, and sometimes deﬁned as classes of decision (counting, approximation) problems solvable in a speciﬁc computational model.
Using polynomial time Turing reductions, the class [SAT ]T of problems reducible to SAT is of the ﬁrst type, N P is of the second type, and N P = [SAT ]T is a theorem. In the case of counting problems [#SAT ]T is of the ﬁrst type, #P is of the second type, and #P = [#SAT ]T is not true. Using reductions in First Order Logic we still have [SAT ]F OL = N P , but not every problem X, which is N P -complete with respect to polynomial time Turing reductions satisﬁes [X]F OL = [SAT ]F OL.
It is important to insist that slogans are replaced by precise deﬁnitions.
Lesson 3. Modeling Syntax and Semantics
We look at Propositional, First Order, Second Order Logic, or any other logic of assertions. Again we model them in our framework of sets.
Syntax: The syntax is an inductively deﬁned set of words, the well formed expressions.
Semantics: Structures are interpretations of the basic non-logical symbols. Assignments are interpretations of the variables. The meaning function associates with structures, assignments, and formulas a truth value.
What is the meaning of an assertion ?

316 J. A. Makowsky
Without free variables: The meaning of an assertion is a truth value.
But this is misleading!
With free variables: The meaning of an assertion is the set of interpretations of its free variables. In the case of ﬁrst order variables only it is a relation. As in Classical Geometry one speaks of the geometrical lieu of all points satisfying an equation, we can speak of the logical lieu deﬁned by a formula. In modern data base parlance this is called a query.
We deﬁne usually logical validity via truth values. It would be preferable to deﬁne validity and logical consequence directly for formulas with free variables.
Our student is more likely to meet in the sequel of courses formulas with free variables that just formulas without.
Do we need the Completeness Theorem?
For the practical knowledge we need:
– The semantic notion of logical consequence. – Enough basic logical equivalences to to prove the Prenex Normal Form The-
orem (PNF). – Introduction and elimination rules for quantiﬁers (via constants). – A game theoretic interpretation of formulas in PNF.
For the knowledge of orientation we might state (but not prove) the Completeness Theorem for our redundant set of manipulation rules.
Here are the arguments for and against proving the Completeness Theorem in the ﬁrst course of Logic.
The classical argument pro:
– Completeness and its corollary, Compactness is at the heart of logic.
My arguments against:
– None of these are part of the practical knowledge we aim at. – The proof of the Completeness Theorem is a waste of time at the expense of
teaching more the important skills of understanding the manipulation and meaning of formulas. – First Order Logic is not privileged in our context. We deal very often with ﬁnite structures, where the Completeness Theorem is not true. – Second Order Logic anyhow is the natural logic we work in, and not taking that seriously confuses the student.

From Hilbert’s Program to a Logic Tool Box 317
We should instead concentrate on understanding quantiﬁcation
As tools we need to
– Read, write and understand the meaning of First Order and Second Order formulas.
– Understand the relationship between projection of relations and ﬁrst order quantiﬁcation.
– Understand that Relational Calculus and First Order Logic are really the same (i.e., bi-interpretable).
– Introduce immediately after the proof of the Prenex Normal Form Theorem the Ehrenfeucht-Fra¨ıss´e Game, and proceed to show the easy direction of the Ehrenfeucht-Fra¨ıss´e Theorem, i.e., if a formula (say in Prenex Normal Form) of quantiﬁer rank k is true in one but not in another structure, then we can derive from the formula a winning strategy for player I (the spoiler) for the game with k moves.
– Play with the game interpretation of quantiﬁers to analyze the amount of quantiﬁcation needed to express, say ”there exists at least n elements x such that φ(x)”.
– One can even point out that (easy direction) of Ehrenfeucht-Fra¨ıss´e Theorem holds also for Second Order Logic.
Lesson 4. Limitations of formalisms: Deﬁnability
Before we ﬁnd time to prove the Completeness Theorem, I would like the students to understand the diﬀerence between First Order (FO) and Second Order (SO) Logic.
– Look at the statement ”There are an equal number of x with P (x) and with Q(x)”
where P, Q are unary predicate symbols. This is expressible in SO but not in FO, and we can even show the proof having the Ehrenfeucht-Fra¨ıss´e Games available. – We can even be daring, and show that connectedness on ﬁnite graphs is SO-deﬁnable, but not FO-deﬁnable. – In the natural numbers N, multiplication is SO-deﬁnable, but not FO-deﬁnable, using addition only. However, multiplications is FO-deﬁnable using addition and squaring. The negative result we cannot prove in an undergraduate course, as we need the decidability of FO Pressburger Arithmetic. But we can explain it.

318 J. A. Makowsky
Lesson 5. Interpretability and Reducibility
Again before we use our time prove the Completeness Theorem I would like the students to understand what it means that a structure is FO-interpretable in another structure. Let look at the case of the natural numbers N and the integers Z with their arithmetic operations.
The integers Z with their arithmetic are FO-interpretable inside the natural numbers N with their arithmetic.
To get the interpretation we deﬁne a new structure from N, called a transduction T (N), and which will be isomorphic to Z, as follows.
– The new universe consists of equivalence classes of pairs of natural numbers such that (x, y) ∼ (x , y ) iﬀ x + y = x + y.
– The new equality is this equivalence. – The new addition is the old addition on representatives. – Same for multiplication. T is a semantic map. Its syntactic counterpart is the interpretation S : F ormulas → F ormulas, deﬁned as follows:
For any SO-formula φ we let S(φ) be the result of substituting the new deﬁnitions of addition and multiplication and equality for the corresponding symbols. In the exact deﬁnition one has to be careful with the renaming of free variables.
S and T are intimately related:
Z = T (N) |= φ iﬀ N |= S(φ)
which is the Fundamental Property of Transductions and Interpretations. In the same way we can see that – The Cartesian product is interpretable in the disjoint union. – Many graph transformations are given as transductions. – All implementations of one data structure in another are of this form. – Transductions and interpretations are everywhere
5 The Fundamental Properties of SO and FO
In teaching our students to think and speak Second Order Logic, we should teach – that isomorphic structures satisfy the same SO sentences; – the Fundamental Property of Transductions and Interpretations; – the Prenex Normal Form Theorem and its visualization as a two person game.
and we should practice thinking in SO as the natural language of specifying properties of modeled artefacts.

From Hilbert’s Program to a Logic Tool Box 319
The Fundamental Properties of FO
Besides the properties of SO we have The Ehrenfeucht-Fra¨ıss´e Theorem:
Two structures can be distinguished by a sentences of quantiﬁer depth k iﬀ Player I (the spoiler) can force a win in the EF-game of length k.
or, equivalently
Two structures cannot be distinguished by a sentences of quantiﬁer depth k iﬀ Player II (the duplicator) can force a win in the EF-game of length k.
We say that two structures are k-isomorphic if Player II can force a win in the EF-game of length k.
Furthermore:
k-isomorphism is preserved under the formation of disjoint unions of structures.
Modiﬁed versions also hold for Monadic Second Order Logic, but not for SO.
Combining EF-Games and Interpretations
Combining games and interpretations gives a very powerful tool to compute the meaning function of a FO formula in a complex structure by reducing this computation to simpler structures.
If G is obtained from graphs H1, H2 by applying disjoint unions, Cartesian products, and ﬁrst order deﬁnable transductions T1, T2, say
G = T1(H1 × T2(H2))
then the truth of the formulas of quantiﬁer rank k in G is uniquely and eﬀectively determined by the the truth of the formulas of quantiﬁer rank k which hold in H1 and H2.
This is the Feferman-Vaught Theorem. It allows us to compute the meaning function for FO-formulas (or MSO-formulas) of composite structures by reducing its computation to the meaning functions of the formulas on the components. I have surveyed how to use the Feferman-Vaught Theorem in Computer Science in my paper in [Mak04].
6 My Logic Tool Box
So we ﬁnally come to the description of the Logic Tool Box I would like to give to our students. Tools to do what, you will ask. Tools to think rigorously in order to approach the disciplines of programming and information processing, tools to model accurately new artefacts, as they occur, tools to grasp the scope

320 J. A. Makowsky
of abstraction and modularity. Our students are not logicians. Logic per se is not their main interest. Logic is for Computer Science what Hygienics is to Medicine. They should learn rigorous informal reasoning before they learn to model this kind of reasoning as formalized proof sequences. Needless to say that each tool comes with a required skill how to use it.
My Logic Tool Box contains:
Modeling tools: – Basic set construction principles; – Inductive deﬁnitions; – Proofs by induction; – Basic cardinality arguments.
Logic tools: – Propositional Logic and its axiomatization. – Second Order Logic as the main formalism to express properties of the modeled artefacts. – The semantic notion of logical consequence and validity. – Validity over ﬁnite structures. – Quantiﬁer manipulation rules. – Skolem functions. – The Fundamental Property of Transductions and Interpretations. – First Order Logic as an amenable fragment of Second Order Logic. – The Ehrenfeucht-Fra¨ıss´e Theorem and its reﬁnements. – The Feferman-Vaught Theorem and its variations.
We said before that the Completeness Theorem for First Order Logic holds only, if we deﬁne validity over all First Order Structures. For Second Order Logic one would have to explain the diﬀerence between Henkin’s notion of validity and standard second order quantiﬁcation. Just stating the Completeness Theorem for First Order Logic misleads the student, and explaining its true subtleties may be beyond the undergraduate level.
Where these tools work
I have chosen the Logic Tool Box with a view on the courses our student has to take during his undergraduate studies. Ideally, the course I have in mind, Sets and Logic for Computer Science, should be taught in the second or third semester. The student should have studied already Discrete Mathematics and Algorithms and Data Structures, so the teacher can rely on the intuition of the students, and the examples developed in these courses. The course should play the same role as the course Number Systems used to play when it was still customary to teach it, cf. [Fef64],¯
The modeling skills taught in our course should help him in the following (usually compulsory) courses:
– Automata and Formal Languages – Introduction to Computability

From Hilbert’s Program to a Logic Tool Box 321
– Database Systems – Graph Algorithms – Principles of Programming Languages – Computer Architecture – Introduction to Artiﬁcial Intelligence
The more specialized topics of Logic should be taught in advanced courses: A course Advanced Topics in Logic could have three parts, covering the Completeness and Incompleteness Theorem, Ordinals and Termination, and Temporal and Modal Logic. Other topics belong there where they are rally used, in the courses on Veriﬁcation, Automated Theorem Proving, Principles of Logic Programming, Database Theory, Functional Programming and so forth.
7 What was omitted?
I have not included in my discussion what is called in the standard classiﬁcation Non-classical Logics. These logics can also be modeled using set-theoretic tools, and indeed they are. When they ﬁnd applications to Computer Science, as Temporal Logic [MP95] or the Logic of Knowledge, [FHMV95], they also ﬁnd there way into more advanced courses.
I have not included in my discussion two classical concerns of the debate around the Foundations of Mathematics and Computer Science: Epistemology and degrees of constructivism. A delightful and insightful presentation and discussion of these matters from a contemporary point of view can be found in [Sha00]. Although I tend to be a Platonist, viewing mathematical concepts as real, I am aware of the diﬃculties inherent in this position, cf. [Mad90,Mad98]. I am also aware of the social and cultural mechanisms at work which strongly inﬂuence how science evolves, cf. [Wil81]. However, I strongly object to the arguments which take the social and cultural mechanisms at work as a justiﬁcation for the erroneous claim that scientiﬁc truth is purely social and cultural.
From a more pragmatic point of view I tend to be a Formalist, viewing the observable part of the mathematical and logical enterprise as happening on (virtual) paper written with (virtual) pencils. Concerning the degrees of constructivism I subscribe to, I only want to remark that often non-constructive is confused with lack of detail. The axiom of choice is an example of lack of detail. I assume that the choice function exist and I want to proceed from there, ﬁlling in the details (implementation) later, or leaving them to others. Software engineering always proceeds like this and is not considered non-constructive even by the most extreme constructivists. Using a cardinality argument to prove the existence of, say, transcendental numbers, expander graphs or other combinatorial objects, is considered non-constructive, but can be explained in the same way.
Our students, however, should rather follow the advise of the Rabbinic Sages, who admonish us not to study Kabbala (Jewish Mysticism) before the mature age of forty years and before serious exposure to the more down-to-earth matters of Talmud and Torah. Our students should view the Philosophy of Mathematics

322 J. A. Makowsky
and of Computer Science as something to be left for later. Children do not question linguistic principles before they learn their ﬁrst language. Scientists should not question Science before they master the craft.
Acknowledgments
I would like to thank N. Francez, D. Giorgetta, S. Halevy and D. Hay for stimulating discussions and suggestions about how to teach Logic to Computer Science students.
I would like to thank the Trade Union of University Professors (Irgun HaSegel) of Israel for giving me time to prepare this paper. At the moment of completion we were in the forth week of our teaching strike.
References
[BF85] J. Barwise and S. Feferman, editors. Model-Theoretic Logics. Perspectives in Mathematical Logic. Springer Verlag, 1985.
[BG08] A. Blass and Y. Gurevich. Why sets? In A. Avron, N. Dershowitz, and A. Rabinowich, editors, Pillars of Computer Science: Essays Dedicated to Boris (Boaz) Trakhtenbrot on the Occasion of His 85th Birthday, volume 4800 of Lecture Notes in Computer Science, page In press. Springer, 2008.
[BGG97] E. B¨orger, E. Gr¨adel, and Y. Gurevich. The Classical Decision Problem. Springer-Verlag, 1997.
[Boo98a] G. Boolos. The consistency of Frege’s “foundations of arithmetic”. In Logic, Logic, Logic, pages 182–201. Harvard University Press, 1998.
[Boo98b] G. Boolos. Logic, Logic, Logic. Harvard University Press, 1998. [Boo98c] G. Boolos. On the proof of Frege’s theorem. In Logic, Logic, Logic, pages
275–90. Harvard University Press, 1998. [Bor05] R. Bornat. Proof and Disproof in Formal Logic. Number 2 in Oxford Texts
in Logic. Oxford University Press, 2005. [Bur05] J.P. Burgess. Fixing Frege. Princeton University Press, 2005. [Bus98] S. Buss, editor. Handbook of Proof Theory, volume 137 of Studies in Logic
and the Foundations of Mathematics. Elsevier Science Publishers, 1998. [EF95] H.D. Ebbinghaus and J. Flum. Finite Model Theory. Perspectives in Math-
ematical Logic. Springer, 1995. [Fef64] S. Feferman. The number systems: foundations of algebra and analysis.
Addison-Wesley, 1964. [FHMV95] R. Fagin, J. Halpern, Y. Moses, and M. Vardi. Reasoning About Knowledge.
MIT Press, 1995. [Fin02] K. Fine. The Limits of Abstraction. Oxford University Press, 2002. [GKL+07] E. Gr¨adel, P. Kolaitis, L. Libkin, M. Marx, J. Spencer, M. Vardi, Y Venema,
and S. Weinstein. Finite Model Theory and its Applications. Springer, 2007. [Gur88] Y. Gurevich. Logic and the challenge of computer science. In E. B¨orger,
editor, Trends in Theoretical Computer Science, Principles of Computer Science Series, chapter 1. Computer Science Press, 1988. [HA28] D. Hilbert and W. Ackermann. Grundzuge der theoretischen Logik. Springer, 1928. [HA49] D. Hilbert and W. Ackermann. Grundzuge der theoretischen Logik, 3rd edition. Springer, 1949.

From Hilbert’s Program to a Logic Tool Box 323

[HA50]
[Ken73]
[Lan99]
[Lib04] [Mad90] [Mad98] [Mak84]
[Mak92]
[Mak04]
[MP95]
[Pap94] [Sha00] [Sho67]
[Wil81]

D. Hilbert and W. Ackermann. Principles of Mathematical Logic. Chelsea Publishing Company, 1950. H.C. Kennedy. What Russel learned from Peano. Notre Dame Journal of Formal Logic, 14.3:367–372, 1973. E. Landau. Die Grundlagen der Analysis. American Mathematical Society, 1999. L. Libkin. Elements of Finite Model Theory. Springer, 2004. P. Maddy. Realisms in Mathematics. Oxford University Press, 1990. P. Maddy. Naturalisms in Mathematics. Oxford University Press, 1998. J.A. Makowsky. Model theoretic issues in theoretical computer science, part I: Relational databases and abstract data types. In G. Lolli and al., editors, Logic Colloquium ’82, Studies in Logic, pages 303–343. North Holland, 1984. J.A. Makowsky. Model theory and computer science: An appetizer. In S. Abramsky, D. Gabbay, and T. Maibaum, editors, Handbook of Logic in Computer Science, volume 1, chapter I.6. Oxford University Press, 1992. J.A. Makowsky. Algorithmic uses of the Feferman-Vaught theorem. Annals of Pure and Applied Logic, 126:1–3, 2004. Z. Manna and A. Pnueli. Temporal veriﬁcation of reactive systems. Springer, 1995. C. Papadimitriou. Computational Complexity. Addison Wesley, 1994. S. Shapiro. Thinking about Mathematics. Oxford University Press, 2000. J. Shoenﬁeld. Mathematical Logic. Addison-Wesley Series in Logic. Addison-Wesley, 1967. R.L. Wilder. Mathematics as a Cultural System. Pergamon Press, 1981.

From Program Veriﬁcation to Certiﬁed Binaries
The Quest for the Holy Grail of Software Engineering
Angelos Manousaridis, Michalis A. Papakyriakou, and Nikolaos S. Papaspyrou
National Technical University of Athens School of Electrical and Computer Engineering
Software Engineering Laboratory Polytechnioupoli, 15780 Zografou, Athens, Greece {amanous, mpapakyr, nickie}@softlab.ntua.gr
Abstract. The long tradition of formal program veriﬁcation and the more recent frameworks for proof-carrying code share a common goal: the construction of certiﬁed software. In this paper, mainly through a simple motivating example, we describe our vision of a complete hybrid system that combines the two approaches. We discuss the feasibility of such an ambitious project and report on progress made so far.
Key words: Formal methods, type systems and type theory, certiﬁed code, proof-preserving compilation.
1 Introduction
Program veriﬁcation aims at formally proving the correctness of a computer program, with respect to a certain formal speciﬁcation or property. As a research ﬁeld of computer science, program veriﬁcation is well into the fourth decade of its existence. However, it can hardly be argued that it is often adopted in practice by software engineers, except for verifying mission-critical systems. For the vast majority of software systems, quality assurance amounts to dynamic testing, which unfortunately can produce no guarantees. In this respect, software engineering, deﬁned as “the application of a systematic, disciplined, quantiﬁable approach to the development, operation, and maintenance of software” [1], is still very far from reaching the maturity of other branches of engineering.
Several formal logics, their majority greatly inﬂuenced by Hoare Logic [2], have been proposed in combination with programming languages as the vehicles for program veriﬁcation [3]. Most of the proposed approaches advocate a clear separation between the language in which speciﬁcations are given (e.g. ﬁrstorder predicate logic), the programming language, and the methodology and the tools—if any—that support the construction of proofs.
This work has been funded by the research programme ΠENE∆ (grant number 03E∆ 330), coﬁnanced by public expenditure (75% by the European Social Fund and 25% by the Greek Ministry of Education, General Secretariat of Research and Technology) and by the private sector, under measure 8.3 of the Operational Programme “Competitiveness” in the European Union’s 3rd Community Support Framework.

From Program Veriﬁcation to Certiﬁed Binaries 325
Proof-carrying code [4, 5] and foundational proof-carrying code [6] are general frameworks, expressing a relatively modern philosophy towards the veriﬁcation of low-level (e.g. machine language) programs. A certiﬁed binary is a value (a function, a data structure, or a combination of both) together with a proof that the value satisﬁes a given speciﬁcation. Certiﬁed binaries are essential in modern distributed computer systems, where executable code is transferred among computing devices that do not necessarily trust one another. The recipient of a certiﬁed binary does not need to trust the producer: the proof can be mechanically checked and, once found valid, it is known beyond doubt that the binary conforms to its speciﬁcation. Existing compilers that produce certiﬁed binaries have mostly focused on simple memory and control-ﬂow safety properties. Although the two frameworks are general enough to express arbitrary program properties, in the general case not much is known on how to construct certiﬁed binaries or how to automatically generate them from high-level source programs.
More recently, type-theoretic frameworks for constructing, composing and reasoning about certiﬁed software have been proposed [7, 8], based on the “formulae as types” principle [9]. The type-theoretic approach provides an embedding of logic in the type system of the programming language: program properties are encoded in types and proof checking is reduced to type checking. In analogy to a type-preserving compiler, which uses a typed intermediate [10] and a typed assembly language [11] and propagates type information from the source program down to the lower-level equivalent programs, a type-theoretic framework for certiﬁed binaries can support proof-preserving compilation. Provided that a common logic (type language) is used for expressing properties and proofs, from the source language to the target language, certiﬁed binaries can be generated by compiling previously veriﬁed source programs. This is important, because high-level programs are easier to reason about than low-level programs.
Still, in a type-theoretic framework such as that proposed by Shao et al. [7], constructing a proof of correctness even for a small program, written in an appropriate high-level source language, is far from simple. As the logic is part of the programming language (more accurately, part of the type language), the proof must be embedded in the code and has to be constructed at the same time with it. Although this has long been proposed as the “right way” to produce software [12, 13], it is not popular with programmers, who generally prefer to write down their algorithm ﬁrst and then (if ever) prove its correctness. Furthermore, if something in the speciﬁcation changes, the code has to change as well and, sometimes, the modiﬁcations can be substantial in size even if the code’s operational behaviour does not change.
Due to the complexity of the type languages used in the type theoretic frameworks that support proof-preserving compilation, type inference (or proof inference) is in general undecidable. The type system of the source language cannot do miracles. It can therefore be argued that, although the embedding of logic in the programming language is appropriate for the lower-level languages used by the compiler, it is not appropriate for the source language, in which the task of constructing the proof is—more or less—the programmers’ responsibility.

326 Angelos Manousaridis et al.
Fig. 1. Overview of a hybrid system for generating certiﬁed binaries.
In this paper, we register our dream of a hybrid system (half based on traditional program veriﬁcation and half type-theoretic). Although similar dreams must be common among computer scientists who advocate program veriﬁcation and proof-carrying code, it seems that they are still rather far from becoming reality. We outline our experience—limited, so far—in building such a system. If it turns out that, with the assistance of appropriate program veriﬁcation tools, programmers are able to prove the correctness of their programs (we want to be optimistic and believe this hypothesis to be true), such a system can be thought of as the Holy Grail of software engineering.
2 A Hybrid System for Generating Certiﬁed Binaries
A hybrid system for generating certiﬁed binaries from annotated source programs can be structured in two layers, as depicted in Fig. 1. First, a “programmerfriendly” program veriﬁcation layer assists programmers in constructing valid proofs for their source programs according to the speciﬁcations that they have set. In this layer, speciﬁcations and proofs are separate from the actual code and an ordinary, general-purpose programming language can be used.
The program veriﬁcation layer can follow the methodology suggested in the work of Filliˆatre et al. related to the Why software veriﬁcation platform [14, 15]. The source code, written in any from a variety of languages, must be annotated

From Program Veriﬁcation to Certiﬁed Binaries 327
with speciﬁcations (preconditions, postconditions, invariants, etc.) in some appropriate logic. It is then given to a tool serving two purposes: (i) to compile the source code into a lower-level intermediate language λ−H ; and (ii) to generate proof obligations that must be proved, in order to verify the correctness of the source code w.r.t. its speciﬁcations.
The language λ−H can be thought of as a typed intermediate language, such as λH in the paper by Shao et al. [7], with some proofs missing. The missing proofs are exactly those corresponding to the generated proof obligations. A variety of tools (from automatic theorem provers to human-driven proof assistants) can be used to discharge the proof obligations and generate the missing proofs. Subsequently, the intermediate program in λ−H can be automatically “linked” with the constructed proofs, resulting in a λH program. An additional type/proof checking step can be performed, to ensure the correctness of the “linked” program.
The second layer of the hybrid system consists of a type/proof preserving compiler, which transforms the program in λH and produces a certiﬁed binary. This compiler performs type/proof preserving program transformations that produce progressively lower-level code. Shao et al. have shown how to perform type preserving CPS transformation and closure conversion on λH (and call the intermediate languages λK and λC respectively). One or more type-preserving “code generation” steps are required to obtain a certiﬁed binary in the form of a (machine dependent or independent) typed assembly language.
It should be noted that the trusted computing base, i.e. the piece of software that the recipient of a certiﬁed binary must blindly trust (not shown in Fig. 1), consists only of a type/proof checker for the typed assembly language and a (type/proof erasing) translator to native assembly language. Both pieces of software are of moderate size and relatively easy to build.
3 A Motivating Example
In this section we present a small case study: the construction of a certiﬁed binary from a C function annotated with its speciﬁcation. The example is intentionally chosen to be very simple, so that self-contained equivalent programs in λH and λK can ﬁt in this paper.
Consider a function root that calculates the integer square root of an integer number n, i.e. the greatest integer r with the property r2 ≤ n. A na¨ıve C program that implements this function is the following:
int root (int n) { int y = 0; while ((y+1)*(y+1) <= n) y++; return y;
}
Following the notation used by the Why veriﬁcation platform and the veriﬁcation tool Caduceus for C programs [14, 15], the same program annotated with the function’s pre- and postcondition and the loop invariant is given in Fig. 2.

328 Angelos Manousaridis et al.

//@ predicate leRoot(int r, int x) { r >= 0 && r*r <= x } //@ predicate isRoot(int r, int x) { leRoot(r, x) && (r+1)*(r+1) > x }

/*@ requires n >= 0 @ ensures isRoot(\result, n) @*/
int root (int n) { int y = 0; //@ invariant leRoot(y, n) while ((y+1)*(y+1) <= n) y++; return y;
}
Fig. 2. The example program, annotated with its speciﬁcation.

root

∀ n : Z. ∀ n∗ : (n ≥ 0). sint n ∃ x : Z. ∃ x ∗ : isRoot x n. sint x

= poly n : Z. poly n∗ : (n ≥ 0). lambda n : sint n.

(fix loop : ∀ y : Z. ∀ y∗ : leRoot y n. sint y ∃ x : Z. ∃ x ∗ : isRoot x n. sint x.

poly y : Z. poly y∗ : leRoot y n. lambda y : sint y.

if [♣, ♣] ((y + cint [1])2 > n,

p1∗ . pack (y, pack (♣, y) as ∃ y∗ : isRoot y n. sint y) as

∃ x : Z. ∃ x ∗ : isRoot x n. sint x,

p2 ∗. loop [y + 1] [♣] (y + cint [1])))

[0] [♣] cint [0]

Fig. 3. The λ−H term with the missing proofs that correspond to proof obligations (♣).

The program in Fig. 2 is subsequently compiled to the λ−H program of Fig. 3. Readers not familiar with the syntax of λH will probably ﬁnd it hard to decipher the code. However, two things are obvious. First, the speciﬁcations in Fig. 2 have been translated to the types that are embedded in the term of Fig. 3. For instance, the type of root itself contains a direct translation of the function’s pre- and postconditions. Second, there are ﬁve parts of this code, marked with the symbol ♣, that are missing. The ﬁrst of these ﬁve is the predicate associated with the condition of the if expression. The remaining four are proofs that have to be constructed externally. Four proof obligations are therefore produced by the veriﬁcation condition generator.
The proof obligations must now be discharged, either by an automatic theorem prover or by a proof assistant. Suppose that the second alternative is used and the human prover provides the code given in Fig. 4 for the Coq1 proof assistant [16]. The missing parts of Fig. 3 can then be ﬁlled in, resulting in the λH program of Fig. 5.
1 Coq uses the Calculus of Inductive Constructions (CIC) as its type language and, for this reason, Coq proofs can be directly embedded in λH , which is also based on CIC. Other theorem provers or proof assistants can be used instead, but the resulting proofs would then have to be translated to CIC. It is worth mentioning that all proof obligations were easily proved automatically (by auto) in Isabelle/HOL.

From Program Veriﬁcation to Certiﬁed Binaries 329
Definition leRoot (r : Z) (x : Z) := (r >= 0 /\ r*r <= x)%Z. Definition isRoot (r : Z) (x : Z) := leRoot r x /\ ((r+1)*(r+1) > x)%Z.
Definition decidable (P : Prop) (b : bool) := if b then P else ~P.
Lemma geDecidablePrf: forall n m : Z, decidable (n >= m)%Z (Zge_bool n m). intros; unfold decidable, Zge, Zge_bool;
case (Zcompare n m); [ discriminate | auto | discriminate ].
Lemma gtDecidablePrf: forall n m : Z, decidable (n > m)%Z (Zgt_bool n m). intros; unfold decidable, Zgt, Zgt_bool;
case (Zcompare n m); [ discriminate | discriminate | auto ].
Lemma Z_ge_refl: forall n : Z, (n >= n)%Z. auto with zarith.
Lemma Zplus_ge_compat: forall n m p q : Z, (n >= m -> p >= q -> n + p >= m + q)%Z.
intros n m p q; intros H1 H2; apply Zle_ge; apply Zplus_le_compat; apply Zge_le; assumption. Fig. 4. Coq code, useful in discharging the proof obligations.

root

∀ n : Z. ∀ n∗ : (n ≥ 0). sint n ∃ x : Z. ∃ x ∗ : isRoot x n. sint x

= poly n : Z. poly n∗ : (n ≥ 0). lambda n : sint n.

(fix loop : ∀ y : Z. ∀ y∗ : leRoot y n. sint y ∃ x : Z. ∃ x ∗ : isRoot x n. sint x.

poly y : Z. poly y∗ : leRoot y n. lambda y : sint y. if [decidable ((y + 1)2 > n), gtDecidablePrf (y + 1)2 n] ( (y + cint [1])2 > n,

p1∗ . pack (y, pack (conj y∗ p1 ∗, y) as ∃ y∗ : isRoot y n. sint y) as ∃ x : Z. ∃ x ∗ : isRoot x n. sint x,

p2 ∗. loop [y + 1] [conj (Zplus ge compat y 0 1 0 (proj1 y∗)

(geDecidablePrf 1 0))

(Znot gt le (y + 1)2 n p2 ∗)] (y + cint [1]))) [0] [conj (Z ge reﬂ 0) (Zge le n 0 n∗)] cint [0]

Fig. 5. The λH term with the “linked” proofs.

330 Angelos Manousaridis et al.
Proof-preserving compilation phases can now be applied to the λH program. However, after CPS transformation, the size and complexity of the resulting program are too much for the human reader. The λK program obtained by the CPS transformation of the λH program of Fig. 5 and after some simple optimizations (such as constant propagation and beta contraction) is given in Fig. 6 at the end of this paper. To increase readability, the types of continuation parameters have been omitted from the λK program. In subsequent phases, still lower-level programs are obtained. The corresponding λC program, after a na¨ıve closure conversion, is a few hundred lines long when expressed in the same textual format. To obtain an eﬃcient implementation, it is essential to invent and implement proof-preserving compiler optimizations and to ﬁnd a compact representation for proofs.
4 Conclusion
The realization of a complete hybrid system for constructing certiﬁed binaries requires the implementation of the system’s two main software layers. Both for program veriﬁcation and for type-based proof-preserving compilation, there is still a long way to go. However, in order to exploit the feasibility of such a system, we have used existing techniques and tools. To verify high-level source programs and produce proof obligations, a platform such as Why/Caduceus can be used. The integration of such a platform with a compiler from the source language to λ−H and the implementation of a proof checker and linker are still future work.
So far, we have partially implemented a proof-preserving compiler in OCaml, manipulating programs in the set of languages described by Shao et al. [7]. As an implementation of the type language (CIC) we have used the Coq proof assistant, whose source code is freely available. In this way, we can build on Coq’s rich set of proof libraries. Our system is incompetent with long and complex source programs. There is much to be done before such an approach to software veriﬁcation can be applied to real software.
References
1. IEEE: Standard Glossary of Software Engineering Terminology. IEEE Standard 610.12-1990.
2. Hoare, C.A.R.: An axiomatic basis for computer programming. Communications of the ACM 12(10) (1969) 576–585
3. Cousot, P.: Methods and logics for proving programs. In van Leeuwen, J., ed.: Formal Models and Semantics. Volume B of Handbook of Theoretical Computer Science. Elsevier Science Publishers B.V., Amsterdam, The Netherlands (1990) 843–993
4. Necula, G.: Proof-carrying code. In: Proceedings of the 24th ACM Symposium on the Principles of Programming Languages. (1997) 106–119
5. Necula, G.: Compiling with Proofs. PhD thesis, Carnegie Mellon University (1998) 6. Appel, A.W.: Foundational proof-carrying code. In: Proceedings 16th IEEE Sym-
posium on Logic in Computer Science. (2001) 247–258

From Program Veriﬁcation to Certiﬁed Binaries 331

(lambda k. k

(poly n : Z. lambda k. k

(poly n∗ : (n ≥ 0). lambda k. k

(lambda xarg : sint n × Kc(∃ x : Z. ∃ x ∗ : isRoot x n. sint x).

let n = sel [N lt prop 0 2] (xarg, cnat [0]) in

let k0 = sel [N lt prop 1 2] (xarg, cnat [1]) in (fix loop [y : Z] (k : Kc(∀ x ∗ : leRoot y n. sint y (poly y∗ : leRoot y n. lambda k. k

∃ x : Z. ∃ x ∗ : isRoot x n. sint x)). k

(lambda xarg : sint y × Kc(∃ x : Z. ∃ x ∗ : isRoot x n. sint x).

let y = sel [N lt prop 0 2] (xarg, cnat [0]) in

let k1 = sel [N lt prop 1 2] (xarg, cnat [1]) in

let z1 = y + cint [1] in

let z2 = z1 ∗ z1 in

let z3 = z2 > n in if [decidable ((y + 1)2 > n), gtDecidablePrf (y + 1)2 n] (z3,
p1∗. k1 (pack (y, pack (conj y∗ p1∗, y) as K(∃ y∗ : isRoot y n. sint y)) as K(∃ x : Z. ∃ x ∗ : isRoot x n. sint x)),

p2∗. loop [y + 1] (lambda k. k [conj (Zplus ge compat y 0 1 0 (proj1 y∗)

(geDecidablePrf 1 0))

(Znot gt le (y + 1)2 n p2∗)]

(lambda k. let z1 = y + cint [1] in k z1, k1 )))))) [0] (lambda k. k [conj (Z ge reﬂ 0) (Zge le n 0 n∗)] (lambda k. k cint [0], k0 ))))))

Fig. 6. The λK term, after the proof-preserving CPS transformation and some optimizations.

7. Shao, Z., Trifonov, V., Saha, B., Papaspyrou, N.: A type system for certiﬁed binaries. ACM Transactions on Programming Languages and Systems 27(1) (2005) 1–45
8. Crary, K., Vanderwaart, J.C.: An expressive, scalable type theory for certiﬁed code. In: Proceedings of the 7th ACM International Conference on Functional Programming. (2002) 191–205
9. Howard, W.A.: The formulae-as-types notion of constructions. In Seldin, J.P., Hindley, J.R., eds.: To H. B. Curry: Essays on Computation Logic, Lambda Calculus and Formalism. Academic Press, Boston, MA (1980) 479–490
10. Harper, R., Morrisett, G.: Compiling polymorphism using intensional type analysis. In: Proc. 22nd ACM Symp. on Principles of Prog. Lang. (1995) 130–141
11. Morrisett, G., Walker, D., Crary, K., Glew, N.: From System F to typed assembly language. In: Proc. 25th ACM Symp. on Principles of Prog. Lang. (1998) 85–97
12. Dijkstra, E.W.: A Discipline of Programming. Prentice Hall (1976) 13. Gries, D.: The Science of Programming. Springer-Verlag (1981) 14. Filliˆatre, J.C.: Why: A multi-language multi-prover veriﬁcation tool. Research
Report 1366, LRI, Universit´e Paris Sud (March 2003) 15. Filliˆatre, J.C., March´e, C.: The Why/Krakatoa/Caduceus platform for deductive
program veriﬁcation. In: Computer Aided Veriﬁcation. Volume 4590 of LNCS. Springer (2007) 173–177 16. The Coq Proof Assistant Reference Manual, URL: http://coq.inria.fr/

Limiting Recursion, FM–representability, and Hypercomputations
Marcin Mostowski
Department of Logic Institute of Philosophy, Warsaw University
m.mostowski@uw.edu.pl
Abstract. We consider various methodologically well motivated notions which appear to be essentially diﬀerent, but surprisingly capture the same class of relations. These are: the notion of FM–representability (representability in Finite Models), statistical representability, limiting recursion or algorithmic learning with empty data, and decidability by accelerating Turing machines by computations of length ω. Finally, we give a new result characterizing FM–representability in poor ﬁnite arithmetics, weaker than divisibility arithmetic, but stronger than coprimality arithmetic.
1 Introduction
Two classical bounds determined by computations were given in the thirties by recursive — relations which can be decided by an algorithmic process, and recursively enumerable — relations which can be positively decided by ﬁnding proofs. Thirty years later — as a result of our computer experience — another important bound was postulated: practical computability. Edmond’s thesis (called also: the feasibility thesis) — which identiﬁes practically computable notions with those PTIME–computable — is still one of the crucial methodological motivations of our research in computational complexity.
However, in about the same time we were led to recognize another important bound falling far outside recursivity. It was motivated by our attempts of computerising some of our intellectual activities, namely learning from ﬁnite samples. In his classical paper [3] Mark Gold formulated fundamental ideas of algorithmic learning theory. However in his earlier paper [2] he gave an idea of limiting recursion. In the same issue of the Journal of Symbolic Logic [21] Hilary Putnam independently postulated a similar explication for the notion of learnability. As a matter of fact identifying learnability with limiting recursion was Putnam’s idea. Nevertheless this notion seems to be a good explication for the idea of algorithmic learning with no input data. Surprisingly this notion coincides with some other methodologically motivated notions. These are FM–representability (arithmetical expressibility in potentially inﬁnite domains), statistical representability, and decidability by hypercomputations of length ω.
Aristotle (in his Physics [1]) about 360 BC noticed the diﬀerence between actual and potential inﬁnities. Potentially inﬁnite are collections which are ﬁnite

Limiting Recursion, FM–representability, and Hypercomputations 333
but unbounded, in the sense that they can be always enlarged. Actually inﬁnite are collections containing inﬁnitely many members. More than 2000 years later in his talk Hilbert ([6]) recalled this idea stressing its importance in foundations of mathematics.1
Currently the idea is reﬂected — among others — in diﬀerence in ways of thinking in classical and ﬁnite model theory. One of the crucial problems in the latter is the question which notions can be expressed when we restrict possible interpretations to ﬁnite models. We try — under such restriction — to express various properties of computations, formulae, and some other combinatiorial objects. Then we need a general method of determining the class of the notions meaningful in ﬁnite models.
As an answer to this problem the author (in [14]) considered the notion of FM–representability. A relation on natural numbers is FM–representable by a given formula if each ﬁnite part of its characteristic function is corectly described by this formula in all suﬃciently large ﬁnite models. In Aristotelian spirite it can be treated as an answer to the question: what can be meaningfully described in a ﬁnite but potentially inﬁnite world?
***
In this paper we discuss the notion of FM–representability and other equivalent notions. Our research follow the ideas of the paper [14] and research done in collaboration with Konrad Zdanowski ([19], [18], and [27]).
The paper is partially self contained. Only the last part about FM–representability in poor arithmetics is dependent on other papers ([17] and [18]). Giving all the details would force us to repeat all the technical machinery which was very carefully elaborated there.
2 Finite arithmetics
We say that A is an arithmetical model if it is of the form A = (N, R1, . . . , Rs), where N is the set of natural numbers and R1, . . . , Rs are relations on N.
We consider ﬁnite initial fragments of arithmetical models. Namely, for every n ∈ N − {0}, by An we denote the model An = ({0, . . . , n − 1}, R1n, . . . , Rsn),
1 Hilbert claimed there that lack of actual inﬁnity is a characteristic feature of ﬁnitistic mathematics in opposition to ideal mathematics which should be grounded by representing it in the form of an axiomatic theory. His explanation of an idea of ﬁnitistic mathematics — from the point of our current knowledge — allows various nonequivalent explications. Later it was interpreted according to G¨odel’s results by identifying ﬁnitistic with expressible in primitively recursive arithmetic (see [24]). Nevertheless it would be interesting to investigate Hilbert’s idea just by taking lack of actual inﬁnity as the deﬁning feature of ﬁnitistic mathematics considered as the part of mathematics which is well deﬁned and free of inconsistent intuitions. In this way ﬁnitistic mathematics would be identiﬁed with mathematics restricted to FM–representable notions.

334 Marcin Mostowski
where Rin is the restriction of Ri to the set {0, . . . , n − 1}. The family of models {An : n = 1, 2, 3, . . .} is denoted by FM(A).
The most common arithmetical models are deﬁned as algebraic structures, e.g. with addition and multilication. However — as a rule — initial segments of natural numbers are not closed under arithmetical operations.2 Therefore we treat e.g. addition and multiplication as ternary relations.
Let ϕ(x1, . . . , xp) be an arithmetical formula and b1, . . . , bp ∈ N. We say that ϕ is satisﬁed by b1, . . . , bp in all suﬃciently large ﬁnite models of FM (A), what is denoted by FM(A) |=sl ϕ[b1, . . . , bp], if there is k ∈ N such that for all n ≥ k An |= ϕ[b1, . . . , bp]. When no ambiguity arises we will write |=sl ϕ[b1, . . . , bp] instead of FM(N ) |=sl ϕ(b1, . . . , bp), where N = (N, +, ×).
Traditionally an arithmetical model A is called an arithmetical domain — with possible qualiﬁcations, e.g. addition domain, multiplication domain, divisibility domain, or coprimality domain, with obvious meaning pointing at the relations considered. The corresponding class of ﬁnite models FM (A) is called FM–domain or a ﬁnite arithmetic possibly with appropriate qualiﬁcations.
The research area devoted to study logical properties of FM–domains is also called ﬁnite arithmetic. A report of our state of knowledge in this area can be found in [11].
3 Representing concepts in ﬁnite models
One of the main questions related to ﬁnite arithmetics is the problem of FM– representability in a given FM–domain.
Let ϕ(x1, . . . , xn) be a formula and S ⊆ Nn. We say that ϕ(x1, . . . , xn) FM– represents S in FM(A) if for all a1, . . . , an ∈ N the following two conditions are satisﬁed:
1. if S(a1, . . . , an) then FM(A) |=sl ϕ(a1, . . . , an), 2. if ¬S(a1, . . . , an) then FM(A) |=sl ¬ϕ(a1, . . . , an),
The idea of this deﬁnition is that a formula ϕ FM–represents a relation between natural numbers if for any given ﬁnite fragment of this relation ϕ correctly describes this fragment in all suﬃciently large ﬁnite initial segments of A (for both positive and negative cases). We say that a relation is FM–representable in a given FM–domain if it is FM–representable in this FM–domain by some formula.
This notion and its basic properties was presented in the paper [14]. Originally it was motivated by studying truth deﬁnitions in ﬁnite models (FM truth deﬁnitions).3 Tarski’s undeﬁnability of truth theorem (see [25]) essentially re-
2 In earlier papers (see e.g. [14]) we assumed that operations take as their value the greatest element MAX of the initial segment when they are not deﬁned. However this approach does not make things clearer.
3 See [14] and the later papers [15, 7]. Leszek Kolodziejczyk successfully applied the method for computational complexity questions in [8, 9]. See also recent discussion of the idea in [11].

Limiting Recursion, FM–representability, and Hypercomputations 335
quires expressibility of some syntactical relations in the model considered. Application of Tarski’s idea in ﬁnite models also requires expressibility of some relations in ﬁnite models. FM–representability just gives a proper notion of expressibility for ﬁnite models.
Let us observe that except for the trivial case initial segments of natural numbers are not closed on pairing function. Therefore we cannot restrict our attention to sets — unary relations. However some arguments can be given only for the unary case when no essential diﬀerence will follow from considering arbitrary arities.

3.1 FM representability theorem
In this section we consider the problem of characterizing of FM–representable relations. We begin with recalling classical characterizations of ∆02 arithmetical relations (for the proof see e.g. [22] or [23]).
Theorem 1 (The characterisation of ∆02 arithmetical relations). Let R be a relation on natural numbers. Then the following are equivalent:

1. R is recursive with recursively enumerable oracle; ( – in terms of oracle machines)
2. R is of degree ≤ 0 ; ( – in terms of Turing degrees) 3. R is recursive in the limit (see [2]), in the other words there is a recursive
sequence of recursive relations S0, S1, S2, . . . such that R = limn→∞ Sn; ( – in terms of limits) 4. R is ∆02 in arithmetical hierarchy. ( – in terms of arithmetical deﬁnability)
The equivalence with condition 3 is also known as the limit lemma.

R

=

lim
n→∞

Sn

means that the limit of the characteristic functions Sn exists, for all arguments, and it is 1 exactly when R holds for these arguments. In other words it is equivalent to the conjunction of the following two conditions:

∀a1, . . . , ak(R(a1, . . . , ak) ≡ ∃m∀n > m Sn(a1, . . . , ak))

and ∀a1, . . . , ak(¬R(a1, . . . , ak) ≡ ∃m∀n > m ¬Sn(a1, . . . , ak)).
Before stating the FM–representability theorem we consider its easier version — namely FM–representability of recursively enumerable relations.

Proposition 1. Each recursively enumerable relation is FM–representable.

Proof. Let us consider Kleene T predicate such that T (e, n, c) if and only if e is a number of a Turing machine and c is a code of a ﬁnished computation of this machine with the input n. It is known that a proper arithmetical formula

336 Marcin Mostowski
can be chosen in such a way that all quantiﬁers are bounded by “< c”. If R is recursively enumerable then R = We, for some e, where We is the set of inputs for which e halts. Then R is FM–represented by the formula ∃c T (e, n, c).4
Now we are ready to characterize just FM-representable relations.
Theorem 2 (The FM–representability theorem, M. Mostowski, [14]). Let R be a relation on natural numbers. Then all the conditions of theorem
1 are equivalent to the following: R is FM-representable (in FM(N, ×, +)).
Proof. Let us observe that both cases postive and negative for FM–representability are deﬁned by Σ20 formulae. It follows that all FM–representable relations are ∆02–deﬁnable.
Following [14] we will show that all relations recognized by oracle Turing machines with recursively enumerable oracles are FM–representable. Let M1 be an oracle deterministic Turing machine recognising n–ary relation5 R and using a recursively enumerable oracle S. Moreover, S is given by a deterministic Turing machine M2 such that M2 halts on a given input n exactly when n ∈ S. Our FM–representing formula ϕ(x1, . . . , xn) for R can be formulated as follows:
there is an accepting computation of M1 on input x1, . . . , xn such that each oracle question y is answered positively exactly when ψ(y),
where ψ(y) is an arithmetical formula FM–representing the set S, it exists by proposition 1.
For each given input a1, . . . , an we can ﬁnd a model of size suﬃcient to contain the unique M1–computation c and all required witnesses for oracle questions put by c for accepting them correctly as members of S by the formula ψ(y).
All the claims up to now have assumed that we consider only representability by ﬁrst order arithmetical formulae and FM–domain N . In [14] we have considered much stronger logic — namely ﬁnite order logic. However it was also observed there that taking any logic stronger than ﬁrst order logic we do not obtain more FM–representable relations, provided the logic has decidable “truth in a ﬁnite model” relation. A logic L has decidable “truth in a ﬁnite model” relation if the relation “M |= ϕ” is recursive for arguments: a ﬁnite model M and L–formula ϕ.
Theorem 3. Let A be a model on natural numbers having all relations recursive and L be a logic with decidable “truth in a ﬁnite model” relation. Then the relations FM–representable in FM(A) by L–formulae are ∆02 arithmetically deﬁnable.
4 Let us observe that by the Matiyasevich theorem recursively enumerable sets are exactly sets deﬁnable by purely existential arithmetical formulae. This gives slightly easier argument.
5 In ﬁnite models we cannot restrict to sets of natural numbers because ﬁnite models are not closed on pairing function.

Limiting Recursion, FM–representability, and Hypercomputations 337

3.2 Statistical representability

The notion of statistical representability was proposed by Konrad Zdanowski (see [19] and [27]) with the intention of comparing it with FM-representability.
By µn(ϕ(a1, . . . , ak)) we mean the density of the models satisfying ϕ(a1, . . . , ak) between models of size not greater than n, that is

µn(ϕ(a1, . . . , ak))

=

1 n

card{s

:

1

≤

s

≤

n

and

Ns

|=

ϕ(a1, . . . , ak)}.

A relation R ⊆ Nk is statistically representable if there is arithmetical formula ϕ(x1, . . . , xk) such that for each a1, . . . , ak ∈ N:

– the limit µ(ϕ(a1, . . . , ak)) = limn→∞ µn(ϕ(a1, . . . , ak)) exists;

–

if

R(a1, . . . , ak)

then µ(ϕ(a1, . . . , ak)) >

1 2

;

–

if

¬R(a1, . . . , ak)

then µ(ϕ(a1, . . . , ak)) <

1 2

.

Theorem 4. The statistical representability theorem, K. Zdanowski, [19] and [27]
Let R be a relation on natural numbers. Then all the conditions of theorem 1 and theorem 2 are equivalent to the following: R is statistically representable.

Proof. By the deﬁnition each FM–representing formula statistically represents
the same relation. On the other hand if a formula ϕ(x1, . . . , xk) statistically represents a relation R then a formula ψ(x1, . . . , xk) saying that “majority of x–s satisfy 0 < x ∧ ϕ≤x(x1, . . . , xk)” FM–represents the same relation, where the formula ϕ≤x(x1, . . . , xk) is obtained by bounding all quantiﬁers in ϕ(x1, . . . , xk) by the condition ≤ x. Obviously the logic with majority quantiﬁer has decidable
“truth in a ﬁnite model” relation. Then by theorem 3 the relation R is FM–
representable.

3.3 Learnable relations
In his currently classical paper Gold [3] considers learning algorithms supplying a natural framework for algorithmic modelling the phenomenon of learning grammar of a language given by ﬁnite samples. The algorithm in his sense has to identify on the basis of ﬁnite samples a grammar in such a way that the identiﬁcation has to stabilise on a correct grammar after ﬁnite number of guesses. In a similar way we can think of learning mathematical notions. However in this case no empirical data are required. What we need is more and more work. So at each stage t we have some answer but after ﬁnitely many stages the answer is stabilising.
We say that A is a mathematical learning algorithm for a relation R ⊆ Nk if
– A works with inputs a1, . . . , ak, t ∈ N, – R(a1, . . . , ak) if and only if there is s such that for all t > s A for the input
a1, . . . , ak, t answers “YES”,

338 Marcin Mostowski

– ¬R(a1, . . . , ak) if and only if there is s such that for all t > s A for the input a1, . . . , ak, t answers “NO”.
We say that a relation R is mathematically learnable if there is a mathematical learning algorithm for R.

Theorem 5 (Mathematical learnability theorem). Let R be a relation on natural numbers. Then all the conditions of theorem 1
and theorems 2, 4 are equivalent to the following: R is mathematically learnable.

Proof. Let us observe that a mathematical learning algorithm A for R gives a recursive sequence of relations S0, S1, S2, . . . such that

R

=

lim
n→∞

Sn,

where Sn is the relation computed when we ﬁx t = n.

3.4 Relations decidable by Zeno machines
Now we will consider a characterisation of ∆02 arithmetical relations in terms of hypercomputations. Currently hypercomputations have vast literature (see e.g. [5], [4], [20]). In spite of our idea of potential inﬁnity hypercomputations essentially use actual inﬁnity.6 We restrict our interest here to so called Zeno machines which are the simplest (hyper)computing devices.
A Zeno machine — called also accelerating Turing machine — is deﬁned in the same way as the simplest Turing machine. It has an inﬁnite tape consisting of ω cells. The ﬁrst cell (0 cell) is used for giving outputs: 0 or 1. It is accelerating because it carries out each next step of computation two times quicker than the previous one. So its computation consists of ω steps. If the ﬁrst step is done in 1 second then all the computation will be ﬁnished after 2 seconds. After that we look at the ﬁrst cell, if it contains 1 then the answer is “YES”, and if it contains 0 then the answer is “NO”. When the content of the ﬁrst cell stabilises after ﬁnitely many moves then the situation is clear — it contains the stabilised character. Otherwise, when this value is changed inﬁnitely many times, Potgieter [20]7 says that it does not halt, but Hamkins [4] says that it takes the supremum value = 1. These two approaches are not equivalent, and the Hamkins’ approach seems to be slightly artiﬁcial. Then we assume that if the machine inﬁnitely many times changes its output then the output is undeﬁned.
We say that a relation is Zeno decidable if its characteristic function can be computed by a Zeno machine. The following characterises Zeno decidable relations.
6 For discussion of these two views in the context of computational power see [16]. 7 The requirement for the halting condition in [20] that also the head stabilises is an
obvious mistake, because in this case Zeno machines would be equivalent to Turing machines.

Limiting Recursion, FM–representability, and Hypercomputations 339
Theorem 6 (Zeno decidability theorem). Let R be a relation on natural numbers. Then all the conditions of theorem
1 and theorems 2, 4, 5 are equivalent to the following: R is Zeno decidable.
Proof. It is easy to check by writing down the halting condition for Zeno machines that each Zeno decidable relation is ∆02 arithmetically deﬁnable.
Let us consider a relation R decidable by a Zeno machine Z. Firstly, let us observe that we can assume that in each computation of Z there are inﬁnitely many steps in which the machine writes something in the ﬁrst cell. This can be achieved by adding after each instruction a journey to the ﬁrst cell, writing the same value as that read, and then going back. So we consider an algorithm A taking as inputs all inputs of Z and additionally t. The algorithm simulates the behaviour of Z and counts how many times writing on the ﬁrst cell was done, if it was done t times then it halts and answers “YES” if in the ﬁrst cell is 1 and “NO” otherwise. So the deﬁned algorithm is — by the assumptions on Z — a mathematical learning algorithm for R.
The idea of Zeno machine can be easily generalised for computations of length deﬁned by any countable ordinal. It is observed in [5] that arithmetical truth can be computed by accelerating Turing machines in ω2 steps. An easy generalisation of the above theorem gives another proof for this fact.
4 FM–representability in poor arithmetics
In this section we consider FM–representability in some FM–domains with poor arithmetical notions. The weakest known such notion, which is suﬃcient for FM–representability of all ∆02 relations, is the relation of divisibility.
Theorem 7 (FM–representability for divisibility FM–domain, [17]). Let R be a relation on natural numbers. Then all the conditions of theorem 1 and theorems 2, 4, 5, 6 are equivalent to the following: R is FM–representable in the divisibility FM–domain.
Proof. We give here only a sketchy argument, for details we refer to [17]. The result is based on the observation that the product of any two coprime numbers can be deﬁned in terms of divisibility. Therefore we can deﬁne the standard ordering on initial segments of ﬁnite models by saying that the ﬁrst number can be multiplied by something by which the second number cannot be multiplied. In this way in each ﬁnite divisibility model we can reconstruct suﬃciently large model of divisibility and ordering. However, it is known (see [13]) that addition and multiplication are deﬁnable in ﬁnite models in terms of divisibility and ordering.
Now let us consider arithmetics which are too poor for obtaining full FM– representability theorem. We start with the observation by Michal Krynicki and Konrad Zdanowski.

340 Marcin Mostowski
Theorem 8 (FM–representability in arithmetic of addition, [12]). Relations which are FM–representable in FM((N, +)) are just the relations deﬁnable in (N, +).
Proof. It follows from more general fact (see [27]) that if A is an arithmetical domain with standard ordering then the FM–representable relations in the corresponding FM–domain are exactly the ∆02–deﬁnable relations in A. However in arithmetic of addition we have elimination of quantiﬁers then each deﬁnable relation is ∆02–deﬁnable.
Now we are going to characterise FM–representability in coprimality FM– domain. Firstly we need a few auxiliary notions.
Let ∼ be an equivalence relation on N and let R ⊆ Nk. We say that ∼ is a congruence relation for R if for all a1, . . . , ak, b1, . . . , bk ∈ N such that ai ∼ bi for, i = 1, . . . , k,
(a1, . . . , ak) ∈ R ⇐⇒ (b1, . . . , bk) ∈ R.
For a, b ∈ N, we deﬁne a ≈ b if a and b have the same prime divisors, that is ∀x (x⊥a ≡ x⊥b), where x⊥y means that x and y have no common prime divisors. A relation R ⊆ Nk is coprimality invariant if ≈ is a congruence for R.
We know that the structure (N, ⊥) has many automorphisms and only coprimality invariant relations are preserved by these automorphisms. In particular any two powers of the same prime can be interchanged. Moreover, this property determines deﬁnability also on initial segments. Therefore only coprimality invariant relations can be FM–represented. It foolows, by theorem 3, that only ∆02 arithmetically deﬁnable coprimality invariant relations can be FM–represented in FM–domain of coprimeness.
Theorem 9 (FM–representability in arithmetic of coprimeness, [18]). R is FM–representable in FM((N, ⊥)) if and only if R is FM–representable in FM(N ) and R is coprimality invariant.
Proof. We skip details from [18], giving only general idea and those technicalities which are needed for the next argument. By a similar trick as in the proof of theorem 7 we can deﬁne the standard ordering between primes on suﬃciently large initial segments. Then using properties of the distribution of primes we encode pairs of primes and compare lengths of sequences of primes. This allows to interpret relations R+ and R× in coprimality ﬁnite models. These relations are coprimality invariant versions of R+ and R× deﬁned as follows:
– R+(pi, pj, pk) if and only if i + j = k, – R×(pi, pj, pk) if and only if i × j = k,
where pi is the i–th prime. So we deﬁne R+(x, y, z) as ∃x ≈ x∃y ≈ y∃z ≈ z R+(x , y , z ) and similarly R×.
In this way we can FM–represent all FM–representable relations on indices of primes. We have to transfer them into other relations. For this let us observe

Limiting Recursion, FM–representability, and Hypercomputations 341
that our method of deﬁning ordering works also for products of ﬁnite sets of primes (of course up to equivalence ≈). By ind(x) we mean the index of x in this ordering. Then we deﬁne the relation W such that
(x, y) ∈ W if and only if y ≈ pind(x).
In [18] it is shown that the relation W is FM–representable in coprimality domain and this essentially ﬁnishes the proof.
Korec observed in [10] that between coprimality and divisibility we have inﬁnitely many relations ordered according to their deﬁnability power.
Let n ∈ N − {0} or n = ∞. We deﬁne the following relations on natural numbers:
– x|ny if and only if for each prime q and k ≤ n, if qk|n then qk|y. – x ≈n y if and only if x|ny and y|nx.
The relation |∞ is just divisibility | and ≈∞ is the identity. The relation |1 is mutualy deﬁnable with coprimality.
The following theorem generalizes the above results on FM–representability in FM((N, |)) and in FM((N, ⊥)).
Theorem 10. For any n > 0 or n = ∞, a relation R ⊆ Nr is FM–representable in FM((N, |n)) if and only if R is ∆02–deﬁnable and the relation ≈n is a congruence for R.
Proof. We will show only how to modify the proof of theorem 9 for obtaining this generalisation. We ﬁx n = 0, ∞. Of course the relation ⊥ is deﬁnable by |n, then we can assume that all required relations are deﬁnable provided they use only coprimality. Thus the ordering we deﬁne in a similar way, x ≺n y means that there is z coprime with both x and y such that there is w divisible by z and each power pi of prime p = z (i ≤ n) divides w exactly when pi divides x, but no such w exists for y and z. The main diﬀerence is that now the ordering is deﬁned up to ≈n instead of ≈.
The only relation which should be deﬁned essentially in a diﬀerent way is W . This is so because now ind(x) is the index of x in a more subtle ordering. However also in this case the argument from [18] can be repeated in extenso.
5 Summarizing FM representability theorem
In this ﬁnal section we collect together all the results about the notions equivalent to FM–representability in one theorem.
Theorem 11 (The FM–representability theorem and equivalent notions). Let R be a relation on natural numbers. Then the following are equivalent:
1. R is recursive with recursively enumerable oracle; ( – in terms of oracle machines)

342 Marcin Mostowski
2. R is of degree ≤ 0 ; ( – in terms of Turing degrees) 3. R is ∆02 in arithmetical hierarchy; ( – in terms of arithmetical deﬁnability) 4. R is recursive in the limit; ( – in terms of limits) 5. R is FM-representable (in FM(N, ×, +)); ( – just FM–representablity) 6. R is statistically representable; ( – in terms of density) 7. R is mathematically learnable; ( – in terms of algorithmic learning) 8. R is Zeno decidable; ( – in terms of hypercomputations) 9. R is FM–representable in the divisibility FM–domain.
References
1. Aristotle. Physics. about 360 BC. 2. E. M. Gold. Limiting recursion. The Journal of Symbolic Logic, 30:28–48, 1965. 3. E. M. Gold. Language identiﬁcation in the limit. Information and Control, 10:447–
474, 1967. 4. J. D. Hamkins. Inﬁnitary computability with inﬁnite time Turing machines. In
B. Cooper, B. Loewe, and L. Torenvliet, editors, Proceedings of the conference Computability in Europe, volume 3526 of Lecture Notes in Computer Science, pages 180–187. Springer, 2005. 5. J. D. Hamkins and A. Lewis. Inﬁnite time Turing machines. The Journal of Symbolic Logic, 65:567–604, 2000. 6. D. Hilbert. U¨ ber das Unendliche. Mathematische Annalen, 95:161–190, 1926. 7. L. Kolodziejczyk. A ﬁnite model-theoretical proof of a property of bounded query classes within ph. The Journal of Symbolic Logic, 69:1105–1116, 2004. 8. L. Kolodziejczyk. Truth deﬁnitions in ﬁnite models. The Journal of Symbolic Logic, 69:183–200, 2004. 9. L. A. Kolodziejczyk. Truth deﬁnitions and higher order logics in ﬁnite models. PhD thesis, Warsaw University, 2005. 10. I. Korec. A list of arithmetical structures complete with respect to ﬁrst–order deﬁnability. Theoretical Computer Science, 257:115–151, 2001. 11. M. Krynicki, M. Mostowski, and K. Zdanowski. Finite arithmetics. Fundamenta Informaticae, 81(1–3):183–202, 2007. 12. M. Krynicki and K. Zdanowski. Theories of arithmetics in ﬁnite models. Journal of Symbolic Logic, 70(1):1–28, 2005. 13. T. Lee. Arithmetical deﬁnability over ﬁnite structures. Mathematical Logic Quarterly, 49:385–393, 2003. 14. M. Mostowski. On representing concepts in ﬁnite models. Mathematical Logic Quarterly, 47:513–523, 2001. 15. M. Mostowski. On representing semantics in ﬁnite models. In A. Rojszczak†, J. Cachro, and G. Kurczewski, editors, Philosophical Dimensions of Logic and Science, pages 15–28. Kluwer Academic Publishers, 2003. 16. M. Mostowski. Potential inﬁnity and the Church Thesis. Fundamenta Informaticae, 81(1–3):241–248, 2007. 17. M. Mostowski and A. Wasilewska. Arithmetic of divisibility in ﬁnite models. Mathematical Logic Quarterly, 50(2):169–174, 2004. 18. M. Mostowski and K. Zdanowski. Coprimality in ﬁnite models. In Luke Ong, editor, Computer Science Logic: 19th International Workshop, CSL 2005, volume 3634 of Lecture Notes in Computer Science, pages 263–275. Springer, 2005.

Limiting Recursion, FM–representability, and Hypercomputations 343
19. M. Mostowski and K. Zdanowski. F M –representability and beyond. In B. Cooper, B. Loewe, and L. Torenvliet, editors, Proceedings of the conference Computability in Europe, volume 3526 of Lecture Notes in Computer Science, pages 358–367. Springer, 2005.
20. P. H. Potgieter. Zeno machines and hypercomputation. Theoretical Computer Science, 358:23–33, 2006.
21. Hilary Putnam. Trial and error predicates and the solution to a problem of mostowski. Journal of Symbolic Logic, 30(1):49–57, 1965.
22. J. R. Shoenﬁeld. Recursion Theory. Lectures Notes in Logic. Springer–Verlag, 1993.
23. R. Soare. Recursively enumerable sets and degrees. Springer-Verlag, 1987. 24. W. W. Tait. Finitism. Journal of Philosophy, 78:524–546, 1981. 25. A. Tarski. Poj¸ecie prawdy w j¸ezykach nauk dedukcyjnych. Nakladem Towarzystwa
Naukowego Warszawskiego, 1933. English version in [26]. 26. A. Tarski. The concept of truth in formalized languages. In J. H. Woodger, editor,
Logic, semantics, metamathematics, pages 152 – 278. Oxford at The Clarendon Press, 1956. translated from German by J. H. Woodger. 27. K. Zdanowski. Arithmetics in ﬁnite but potentially inﬁnite worlds. PhD thesis, Warsaw University, 2005.

Using Tables to Construct Non-Redundant Proofs
Vivek Nigam
INRIA & LIX/E´cole Polytechnique, Palaiseau, France nigam@lix.inria.fr ⋆
Abstract. Proofs containing more than one subproof for a common subgoal are less preferred in frameworks such as Proof Carrying Code, where proofs are stored and communicated, than proofs that don’t contain such redundancies. In this paper, we show how (cut-free) proofs can be transformed into proofs containing cuts and where no atom is proved twice, called non-redundant proofs. Two main questions arise when trying to construct these non-redundant proofs: First, which cut-formulas should be used; Second, where to perform cut rules. Some advances in proof theory, namely, our better understanding of focused proofs, allows us to propose the following answers: We use only atomic subgoals of the original proof; and we place cut rules only at the end of the asynchronous phases. The backbone of a non-redundant proof is a tree, called tree of multicut derivations (tmcd), where a node is a derivation containing only multicut rules, and an edge represents the provability dependency between a subgoal introduced by a node’s multicut rule and another (tree of) multicut derivation. We show how to obtain a tmcd from an existing proof.
1 Introduction
Frameworks such as Proof Carrying Code [9], where mobile codes are sent with proofs assuring that these codes satisfy certain properties, provide “real world” concerns, not only for provability, but also for the shape and format of proofs. In these frameworks, since proofs need to be stored and communicated, proofs have to attend certain engineering aspects; for instance, the size of proofs is relevant; more precisely, smaller proofs are preferred.
A redundant proof is a proof that contains more than one non-trivial subproof of the same atom. For example, consider the proofs that the 12th Fibonacci number is 144 (ﬁb 12 144) and obtained from the following logic speciﬁcation {ﬁb 1 1, ﬁb 2 1, ∀XY Z.[ﬁb X Y ∧ ﬁb (X + 1) Z ⊃ ﬁb (X + 2) (Y + Z)]}. Two types of proofs can be distinguished: one where a forward chaining behavior is
⋆ I thank Dale Miller, Miki Hermann, David Baelde, and anonymous reviewers for their helpful comments and discussions. This work has been supported in part by INRIA through the “Equipes Associ´ees” Slimmer and by the Information Society Technologies programme of the European Commission, Future and Emerging Technologies under the IST-2005-015905 MOBIUS project.

Using Tables to Construct Non-Redundant Proofs 345
adopted, and another where a backward chaining behavior is adopted. In the frameworks previously mentioned, the former linear size non-redundant proof is preferred to the latter exponential size redundant proof.
We propose a procedure to construct a non-redundant sequent calculus proof from a redundant sequent calculus proof. This is done by collecting (or tabling) from an existing proof a set of atomic lemmas, called table. These lemmas are then used as cut formulas to construct a non-redundant proof containing cuts. The use of cuts in non-redundant proofs is not surprising; it is well known that, when compared with cut proofs, cut-free proofs are potentially bigger (also known as the cut-elimination blow-up).
There are two main problems to be addressed: (1) which lemmas should be used; and (2) when to use these lemmas, that is, while constructing the nonredundant proof, when should a cut be used. Our answers for these question lie in the structure of focused proofs.
By distinguishing rules that are invertible, called asynchronous rules, from rules that are not invertible, called synchronous rules, focused proofs are organized in two alternating phases: asynchronous phases, where asynchronous rules are eagerly applied; and synchronous phases, where a formula is picked, or focused on, and synchronous rules are applied hereditarely to its subformulas (for more about focused proofs, we invite the reader to [1]). Some recent advances on our understanding about focused proofs in classical and intuitionistic logics [5] and about the eﬀect of atomic polarities1 in the shape of proofs [2, 7], provides us with the machinery necessary to propose the following answers to the previous questions:
1) We collect (or table), in the existing proof, all the atomic subgoals. This restriction allows us to construct non-redundant proofs with a polarized cutrule [7], where an atomic cut-formula has negative polarity in one branch of the proof tree and positive polarity on the other branch of the proof tree. As we investigate elsewhere [7], by using this polarized cut-rule, it is possible to mix a forward chaining behavior with a backward chaining behavior, what enforces some subgoals not to be re-proven;
2) The idea is to use the lemmas as close as possible to the root of the tree, so that if a lemma is to be proved again later in the tree, then it would already be available in the set of hypothesis of the sequent and allow to immediately complete the proof with an initial rule. However, it may happen that a lemma can’t be proved right from the bottom of the tree and should be introduced into the proof only when there is an increment in the set of hypothesis of a sequent (by for example, a right implication rule). We show that focusing provides the discipline necessary to identify the places in a proof tree where new lemmas should be introduced, namely, at the end of the asynchronous phases.
This paper is structured as follows: we introduce in Section 2 some key concepts related to focusing and introduce the intuitionistic system LJFt and the use of tables to specify multicut derivations (mcd). In Section 3, we specify
1 In a focused system, atoms are assigned either positive or negative polarity. This assignment is necessary to organize focused proofs.

346 Vivek Nigam
how to extract tree of multicut derivations (tmcd), that is the backbone to construct non-redundant proofs, from diﬀerent types of proofs, namely Horn Theory proofs, Uniform proofs, and LJFt proofs. In Section 4, we show and discuss some experimental results, and ﬁnally in Section 5, we ﬁnish with some concluding remarks.
2 Preliminaries
2.1 LJFt
In focused proof systems, formulas are classiﬁed as positive and negative. The formulas true, ⊥, or the formulas with main connective ∧, ∨, or ∃ are positive, while the remaining formulas are negative. This classiﬁcation is natural since for negative formulas, their right introduction rules are invertible, while this is not necessarily the case for positive formulas. Focused proof systems capitalize on this classiﬁcation by organizing focused proofs in two phases: the asynchronous phases, where only invertible rules are applied, and synchronous phase, where non-invertible rules are applied hereditarely to a formula and its subformulas. Notice that backtracking is only necessary in the synchronous phase. In many focused systems, such as LJF, proposed by Liang and Miller in [5], this classiﬁcation is extended to atoms by assigning arbitrarily their polarities.
The Figure 1 depicts the inference rules in LJF, where four diﬀerent types of sequents can be identiﬁed: (1) The sequent [Γ ]−A → is a right-focusing sequent (the focus is A); (2) The sequent [Γ ] −A→ [R]: is a left-focusing sequent (with focus on A); (3) The sequent [Γ ], Θ −→ R is an unfocused sequent. Here, Γ contains negative formulas and positive atoms, and R is either in brackets, written as [R], or without brackets; (4) The sequent [Γ ] −→ [R] is an instance of the previous sequent where Θ is empty.
Asynchronous phases use the third type of sequent above (the unfocused sequents): in that case, Θ contains positive or negative formulas. If Θ contains positive formulas, then an introduction rule (either ∧l, ∃l or falsel) is used to decompose it; if it is negative, then the formula is moved to the Γ context (by using the []l rule). The end of the asynchronous phase is represented by the fourth type of sequent. Such a sequent is then established by using one of the decide rules, Dr or Dl. The application of one of these decide rules then selects a formula for focusing and switches proof search to the synchronous phase or focused phase. This focused phase then proceeds by applying sequences of inference rules on focused formulas: in general, backtracking may be necessary in this phase of search. Moreover, according to which phase rules can be performed, we classify the rules ∧l, ∃l, falsel, ⊃r, ∀r, []l, []r, ∧−r as asynchronous rules, and the remaining rules as synchronous rules.
As pointed out elsewhere [5, 2, 7], the atomic polarities play an important role in the shape of the proofs, without aﬀecting in no way provability. For instance, if all atoms have positive polarity, only proofs with a forward chaining behavior are possible, and on the other hand, if all atoms have negative polarity, only proofs with a backward chaining behavior are possible, for example uniform proofs [6].

Using Tables to Construct Non-Redundant Proofs 347

[N, Γ ] −N→ [R] [N, Γ ] −→ [R]

Dl

[Γ ]−P → [Γ ] −→ [P ]

Dr

[Γ ], P −→ [R] [Γ ] −P→ [R]

Rl

[Γ ] −→ N [Γ ]−N→

Rr

[Γ ] −A→n [An]

Il

[Γ, Ap]−Ap→

Ir

[Γ, Na], Θ −→ R [Γ ], Θ, Na −→ R

[]l

[Γ ], Θ −→ [Pa] [Γ ], Θ −→ Pa

[]r

[Γ ], Θ −→ R [Γ ], Θ, ⊥ −→ R falsel [Γ ], Θ, true −→ R truel [Γ ]−true→ truer

[Γ ], Θ, A, B −→ R [Γ ], Θ, A ∧ B −→ R

∧l

[Γ ]−A→ [Γ ]−B→ [Γ ]−A∧B→

∧r

[Γ ]−A→ [Γ ] −B→ [R] [Γ ] A−⊃→B [R]

⊃l

[Γ ] −A→i [R] [Γ ] A1−∧→−A2 [R]

∧−l

[Γ ], Θ −→ A [Γ ], Θ −→ B [Γ ], Θ −→ A ∧− B

∧−r

[Γ ], Θ, A −→ B [Γ ], Θ −→ A ⊃ B

⊃r

[Γ ], Θ, A −→ R [Γ ], Θ, ∃yA −→ R

∃l

[Γ ]−A[t/x]→ [Γ ]−∃xA→

∃r

[Γ ] A−[t→/x] [R] [Γ ] ∀−x→A [R]

∀l

[Γ ], Θ −→ A [Γ ], Θ −→ ∀yA

∀r

Fig. 1. LJF: Here, Γ is a set of formulas, Θ is a list of formulas, An denotes a negative atom, Ap a positive atom, and P a positive formula, N a negative formula, Na a negative formula or an atom, and Pa a positive formula or an atom. All other formulas are arbitrary and y is not free in Γ, Θ or R.

LJF t capitalizes on the observation that atomic polarities can be arbitrarily assigned, and extends LJF in two ways. (1) Extends the LJF sequents with a polarity context, P, which speciﬁes all the positive atoms in a sequent. An atom in a sequent, P; [Γ ] −→ [R], is positive if and only if A ∈ P; (2) extends LJF with the following polarized multicut rule:

P; [Γ ] −→ [A1]

···

P; [Γ ] −→ [An]

P ∪ ∆; [Γ ∪ ∆] −→ [R] mc.

P; [Γ ] −→ [R]

Where ∆ = {A1, . . . , An} is a set of atoms. The multicut rule is the only rule that can change the polarity context in a proof.
Remarks: (1) The results in this paper could be easily applied to (focused) classic logics, such as LKF [5]. (2) Here we only consider focused proofs; however, it seems possible to apply the results obtained here to a more general setting where non-focused proofs are considered, by using methods such as in [8] to convert non-focused proofs to focused proofs, but this is left out of the scope of this paper. (3) We use interchangeably the same names for the rules in LJF and LJFt, but always being clear from the context to which system we refer to.
In the next subsection, we use this polarized multicut rule to construct multicut derivations that are the basic element used to construct non-redundant proofs.

2.2 Tables as Multicut Derivations We consider a table as a partially ordered ﬁnite set of atoms.

348 Vivek Nigam

Deﬁnition 1. A table is a tuple T = A, ≺ , where A is some ﬁnite set of atoms, and ≺ is a partial order relation over the elements of A.

In a table, each atom represents, intuitively, a provable sub-goal necessary in the proof of a sequent (say Γ −→ G), and the order relation the provability dependency between the atoms, that is, if A ≺ B then A is a subgoal used to prove the goal B.
The next deﬁnition speciﬁes a derivation composed only of multicut rules, represented by a table.

Deﬁnition 2. Let T = A, ≺ be a table. The multicut derivation for T and the
sequent S = Γ −→ G, written as mcd(T , S), is deﬁned inductively as follows:
if A is empty, then mcd(T , S) is the derivation containing just the sequent S.
Otherwise, if {A1, . . . , An} is the collection of ≺-minimal elements in A and if Π is the multicut derivation for the smaller table A \ {A1, . . . , An}, ≺ and the sequent Γ, A1, . . . , An −→ G, then mcd(T , S) is the derivation

Π

Γ −→ A1 · · · Γ −→ An

Γ, A1, . . . , An −→ G mc

Γ −→ G

Multicut derivations are always open derivations (that is, they contain leaves that are not proved). A proof of a multicut derivation is any (closed) proof that extends this open derivation.

Elsewhere [7], we investigated the use of tables for obtaining non-redundant proofs in the Horn fragment. We used the following observation:

Proposition 1. [7] Let Γ be a set of Horn clauses, A ∈ P ∩ Γ , and Ξ be an arbitrary LJFt proof tree for P; [Γ ]−G →. Then every occurrence of a sequent with right-hand side the atom A is the conclusion of an Ir rule.

If in a proof of Ξ, there are several non-trivial proofs for the subgoal A, that is, proofs that contain more than an inference rule, one could table A and construct the corresponding multicut derivation for this table and construct a non-redundant proof. For example, when comparing the two derivations below, the left derivation could have several non-trivial subproofs for A, while the right derivation must have only one non trivial proof for A: the proof of the cut’s left branch.

Γ −→ A Γ −→ G

P; [Γ ] −→ [A] P ∪ {A}; [Γ, A] −→ [A ∧ G] mc.

Γ −→ A ∧ G =⇒

P; [Γ ] −→ [A ∧ G]

In the next sections, we use mcds to construct non-redundant proofs. To represent a mcd when we specify the algorithms to extract non-redundant proofs, we use the following data type mcd ::= sequent * atom list2.
2 We use a list of atoms representing the topological sort of a table’s partial ordering.

Using Tables to Construct Non-Redundant Proofs
.. .. ..

349

M1 · · · Mi

Asynchronous + Synchronous phases

M0

M0

Fig. 2. The left ﬁgure illustrates of how the function buildTmcd extracts a tmcd from a proof tree. The right ﬁgure depicts the general architecture of a completed tmcd proof. The triangles represent mcds and the dashed lines represents end of asynchronous phases.

3 Tree of Multicut Derivations - tmcd
The backbone of a non-redundant proof is a tree, called tree of multicut derivations (tmcd), where a node is a derivation containing only multicut rules, and an edge represents the provability dependency between a subgoal introduced by a node’s multicut rule and another (tree of) multicut derivation. The architecture of a tmcd is depicted in Figure 2. The idea is that each multicut derivation is only used when the context is augmented with new atoms or new positive formulas. Since contexts can only be augmented by the asynchronous rules ∀r, ∃l, and ⊃r 3 and asynchronous rules are invertible, focusing provides a natural way to identify where to place a multicut derivation, namely, whenever an asynchronous phase ends and the context is augmented with a new positive formula or a new atom. While focused cut-free proofs are structured in two alternating phases, asynchronous phases and synchronous phases, focused cut proofs are structured with one more phase, called cut phase, appearing always between an asynchronous phase and the following synchronous phase.
In the following subsections, we specify algorithms to extract tmcds from proofs in Horn Theory, from Uniform Proofs [6], and from LJF proofs. We represent tmcds by the following data type: tmcd ::= atom * mcd * tmcd list.
3.1 Horn Theory
A characteristic of uniform Horn Theory proofs, such as the proofs generated by Prolog, is that contexts never changes. Therefore, all proved atoms in such a proof are provable from the initial context. This property enables us to construct a tmcd with only one node, obtained from the function buildTable shown in Figure 3. This function uses the function atomPOT that performs a postorder traversal (i.e., process a nodes premises before processing the node), and uses
3 The ﬁrst two rules augment the context with a new eigenvariable, and the last rule augments the context with some formula.

350 Vivek Nigam

buildTable : tree → mcd

input: Ξ

(rootOf(Ξ), eliRed(atomPOT Ξ))

getChT : context → form → tree list → (form * tree) list

where:

input: Γ ,F,Node(seq,branches)::list

atomPOT : tree → atom list

if seq = [Γ ] −→ [A]

input: Node(seq, b1, . . . , bn) if seq = [Γ ] −→ [A]

then (getChT Γ A branches) :: (getChT Γ F list) else if seq = [Γ ′] −→ [ ] and Γ = Γ ′

then atomPOT b1 :: · · · ::

then (F, Node(seq,branches)) :: (getChT Γ F list)

atomPOT bn :: A

else (getChT Γ F branches) :: (getChT Γ F list)

else atomPOT b1 :: · · · ::

buildTmcd : tree → tmcd

atomPOT bn

input: Ξ

getSCT : context → tree → tree

eliRed(tmcdAux Empty Ξ)

input: Γ ,Node(seq,branches)

where:

if Γ = contextOf(seq)

tmcdAux : (form * tree) → tmcd

then

input: (A, Ξ)

Node(seq, map (getSCT Γ ) branches)

(A, buildTable(getSCT (contextOf(Ξ)) Ξ),

else Nil

map tmcdAux (getChT (contextOf(Ξ)) A (Ξ :: [])))

Fig. 3. Functions used to extract a tmcd from a goal directed proof. Here the functions: contextOf returns the bracket context of a sequent; map applies a function to all the elements of a list.

the function eliRed that retains only the ﬁrst occurrence of any repeated atomic formula in a list of formulas.
The correctness of this algorithm can be shown by a simple induction and can be found elsewhere [7]. It is easy to show that buildTable extracts a mcd from a Horn Theory proof in time O(n), where n is the size of the input proof.
3.2 Uniform Proofs
We now specify how to extract a tree of multicut derivations from a ﬁnite (goaldirected) proof tree, that is, LJF proofs where all atoms are assigned negative polarity. In this more general class of proofs, it can happen that, after an asynchronous phase, the context is augmented with a new positive formula or with a new atom. Hence, diﬀerently from the Horn Theory case, not all atomic subgoals are provable from the initial context, and therefore, a tmcd with more than one node will be the backbone of the non-redundant proof of an uniform proof.
The function buildTmcd, shown in Figure 3, extracts a tmcd from an uniform proof. We use the illustration in Figure 2 to explain how this function works. The three diﬀerent kinds of nodes (ﬁlled squares, ellipses, and blank squares) represent sequents with diﬀerent bracket contexts. First, the subtree with the ﬁlled squares is extracted, by using the function getSCT, shown in Figure 3, and, from this subtree, the multicut derivation M0 is constructed by using the function buildTable. Second, by using the function getChT, also shown in Figure 3, the remaining children trees are extracted together with an atom, appearing in M0, representing the provability dependency of two nodes of the extracted tmcd. Third, buildTmcd is recursively applied to each child tree. Fourth, as done with in the Horn Theory case, we eliminate redundancies with the function eliRed as follows: let Ma be a multicut derivation containing the atom A, and let Md be a descendent multicut derivation of Ma. If Md has the base sequent [Γ ] −→ [A] then we obtain a new tmcd, by removing the tmcd’s subtree with

Using Tables to Construct Non-Redundant Proofs 351
root Md; or if A is in Md, then we obtain a new tmcd by removing A from Md and its possible descendent subtrees from the tree of multicut derivations. There is a ﬁnal step that is not shown here concerning the sequent’s polarity context, P and context Γ , which need to be augmented with the previous application of multicut rules. This can be done in a straightforward way, by traversing the obtained tmcd.
We prove the following correctness result by induction on the height of the input tree Ξ.
Proposition 2. Let Ξ be a uniform LJF proof and let τ =buildTmcd(Ξ) be the tree of multicut derivations obtained from Ξ. Then τ can be completed to a proof by adding derivations containing only one Dl rule.
As argued before, the synchronous rules are not invertible and hence, have an inherent don’t know non-determinism. In particular, the Dl rule has higher degree of don’t know non-determinism, since it is usually the case that the context of sequents contain several formulas and therefore, when applying the Dl rule, an interpreter might need to backtrack more often to this rule and choose another formula to focus on. Hence, the proposition above states that not only tmcds are correct, but also that one can complete a tmcd in an optimal way, since an interpreter only needs to search for derivations containing only one Dl rule.

3.3 LJF proof

In the previous subsection, we considered only uniform LJF proofs, that is, proofs

where all atoms have negative polarity. We now extend the results obtained

before to all LJF proofs where there can also be atoms with positive polarity.

Now, there are two main diﬀerences with respect to uniform LJF proofs: (1)

Initial Right Rule - Initial right rules can end the proof; (2) Reaction Left

with Atomic Formula - By allowing atoms with positive polarity, proofs can

perform forward chaining steps. For instance, consider the following derivation

where the atom A has positive polarity:

This type of derivation is not possible to occur in a uniform proof, since there, the only rule that introduces atoms focused in the left is the initial left rule.

[Γ, A] −→ [G′]

[Γ ]−G→

[Γ ] −A→ [G′] Rr, []r ⊃l

[Γ ] G−⊃→A [G′]

We change the functions atomPOT, getSCT, and getChT, to accommodate

these diﬀerences. For the ﬁrst diﬀerence, namely that initial right rules can

ﬁnish the proof, it suﬃces to table the positive atom used to ﬁnish the proof,

and therefore, in the extracted tmcd, this atom will have its polarity changed to

positive4. For the second diﬀerence, namely that atomic reaction left rules can

happen, we table the forward chained atom performing, in the extracted tmcd,

the following transformation:

4 Remember that at the base of any tmcd, all atoms are assigned negative polarity.

352 Vivek Nigam

Ξ

[Γ, A] −→ [G′]

[Γ ]−G→

[Γ ] −A→ [G′] Rr , []l ⊃l

[Γ ] G−⊃→A [G′]

P; [Γ ]−G→

P; [Γ ] −A→ [A] Il ⊃l

P; [Γ ] G−⊃→A [A]

P; [Γ ] −→ [A] Dl

P′; [Γ, A] −→ [G′]

[Γ ] −→ [G′] Dl

=⇒

P; [Γ ] −→ [G′]

mc

However, a new question arises: where in the tmcd should we perform a

cut with A as cut formula. The answer is to insert this cut before inserting the

cuts with the atomic subgoals appearing in Ξ because to prove these subgoals

it might be necessary to use A.

Accordingly, we add new cases, shown in Figure 4, to the functions atomPOT,

getSCT, and getChT. To atomPOT, the ﬁrst new case inserts a positive atom

focused on the left to the beginning of the list of atomic subgoals; the second

new case inserts to the list of atomic subgoals any positive atom that is used

to ﬁnish a proof with an initial right rule. The function getSCT, that is used to

extract subtrees used to construct tmcd’s nodes, is modiﬁed so that extracted

subtrees include atomic reaction left rules. Since the subtree extracted by getSCT

is modiﬁed, getChT is modiﬁed to extract correctly the children subtrees.

function: atomPOT = ...
else if seq = [Γ ] −A→ [G] and A ∈/ Γ
then A :: atomPOT b1 :: · · · :: atomPOT bn
else if seq = [Γ ]−A→ then A ...

function: getSCT = ... else if seq = [Γ ] −A→ [G] then Node(seq, map (getSCT (Γ ∪ {A})) branches) ...
function: getChT = ... else if seq = [Γ ] −A→ [G] then (getChT (Γ ∪ {A}) F branches) :: (getChT (Γ ∪ {A}) F list) ...

Fig. 4. New cases added to the functions shown in Figure 2 to handle all LJF proofs.

We prove the following correctness result by induction on the height of the input tree Ξ.
Proposition 3. Let Ξ be an LJF proof and let τ =buildTmcd(Ξ) be the tree of multicut derivations obtained from Ξ. Then τ can be completed to a proof by adding derivations containing only one Dl rule.
The complexity of buildTmcd lies in the comparison of two context (e.g. contexof(seq) = Γ ). Considering an upperbound, s, on the length of formulas, we can show that buildTmcd extracts a tmcd from a LJFt proof in time O(ns (log n)), where n is the size of the input proof. This is done by assigning an ordering to clauses, represented by strings; for instance by using the ASCII numbering. To determine that two sequents have the same context, it suﬃces to sort the contexts of the sequents and then, check one by one the equivalence of the clauses in the contexts.
4 Experiments
As the experimental results in the following table shows, by completing the tmcd extracted from the original tree (Ξ), we obtain a considerably smaller proof.

Using Tables to Construct Non-Redundant Proofs 353

Also, the execution time of buildTmcd is much lesser than the time needed to ﬁnd the original proof.

5th ﬁb 8th ﬁb 11th ﬁb

Size - number of nodes Ξ proof from tmcd 15 15 67 27 287 39

Time - ms

buildTmcd(Ξ) ﬁnd(Ξ) complete tmcd

2.8 45.0

50

3.1 85.0 120

4.8 330.0 170

In the case for the 8th Fibonacci, the time needed to complete a tmcd is more than of ﬁnding the original proof. However, we observed in our experiments that it takes a constant time of 20 ms to complete one of the open branches of a tmcd and there are only a linear number of them, that is, one open branch for each subgoal. Therefore, we expect that while the time to search for proofs exponentially grows with the Fibonacci number, the time of completing the extracted tmcd should increase linearly.

5 Conclusions
This paper presents an approach, from a proof theoretic point of view, for reducing size of proofs through redundancy elimination. By a careful study of focused proofs, we propose a new structure, called tree of multicut derivations, to be the backbone for the construction of non-redundant proofs. The theoretical and experimental results in this papers suggests that the procedure proposed can be used to obtain smaller proofs. Further research is needed to investigate the plausibility of this approach in the Proof Carrying Code framework.

References
1. Jean-Marc Andreoli. Logic programming with focusing proofs in linear logic. J. of Logic and Computation, 2(3):297–347, 1992.
2. Kaustuv Chaudhuri, Frank Pfenning, and Greg Price. A logical characterization of forward and backward chaining in the inverse method. In Proceedings of IJCAR’06, 2006.
3. Pierre-Louis Curien and Hugo Herbelin. The duality of computation. In Proceedings of ICFP ’00, 2000.
4. Radha Jagadeesan, Gopalan Nadathur, and Vijay Saraswat. Testing concurrent systems: An interpretation of intuitionistic logic. In Proceedings of FSTTCS, 2005.
5. Chuck Liang and Dale Miller. Focusing and polarization in intuitionistic logic. In Proceedings of CSL’07, 2007.
6. Dale Miller, Gopalan Nadathur, Frank Pfenning, and Andre Scedrov. Uniform proofs as a foundation for logic programming. Annals of Pure and Applied Logic, 51:125–157, 1991.
7. Dale Miller and Vivek Nigam. Incorporating tables into proofs. In Proceedings of CSL’07, 2007.
8. Dale Miller and Alexis Saurin. From proofs to focused proofs: a modular proof of focalization in linear logic. In Proceedings of CSL’07, 2007.
9. George C. Necula. Proof-carrying code. In Proceedings of POPL’97, 1997.

Classifying the Phase Transition Threshold for Unordered Regressive Ramsey Numbers
Florian Pelupessy and Andreas Weiermann
Vakgroep Zuivere Wiskunde en Computeralgebra Krijgslaan 281 Gebouw S22, 9000 Ghent, Belgium
{weierman, pelupessy}@cage.ugent.be
Abstract. Following ideas of Richer (2000) we introduce the notion of unordered regressive Ramsey numbers or unordered Kanamori-McAloon numbers. We show that these are of Ackermannian growth rate. For a given number-theoretic function f we consider unordered f -regressive Ramsey numbers and classify exactly the threshold for f which gives rise to the Ackermannian growth rate of the induced unordered f -regressive Ramsey numbers. This threshold coincides with the corresponding threshold for the standard regressive Ramsey numbers. Our proof is based on an extension of an argument from a corresponding proof in a paper by Kojman,Lee,Omri and Weiermann 2007.
Key words: regressive Ramsey numbers, Ackermann function, unordered canonical Ramsey theorem, Kanamori McAloon theorem
1 Introduction
There exist three basic and well known inﬁnitary Ramsey principles which give rise to independence results for PA: Ramsey’s theorem, the canonical Ramsey theorem (Erd¨os Rado) and the regressive Ramsey theorem (Kanamori McAloon) [8,14,13,12]. The latter two principles make essential use of a pre-existing order on the natural numbers in order to speak about min-colourings, max-colourings and min-homogeneous sets, etc. The three basic inﬁnitary Ramsey-principles give rise to ﬁnitary Ramsey principles which can be formulated in the language of arithmetic: the ﬁnite Ramsey principle with a suitable largeness condition (i.e. the Paris-Harrington principle [14]), the Kanamori McAloon principle [8] and the ﬁnite canonical Ramsey principle with a suitable largeness condition. It turns out that all these ﬁnite versions make essential use of the standard <relation and one might wonder if it is possible to ﬁnd strong principles which do not depend so intrinsically on the less than relation.
An interesting approach to this question can be obtained from a recent paper by Richer [17] about unordered canonical Ramsey numbers and their asymptotic classiﬁcation. It is quite natural to extend Richer’s approach to the context of strong Ramsey principles and in this paper we do this for the Kanamori McAloon Ramsey theorem.
We consider the phase transition for unordered regressive Ramsey numbers. For simplicity we limit ourselves to only colourings of pairs (graphs) and expect

Unordered Regressive Ramsey Numbers 355

that the result extends to higher dimensions (hyper graphs) using appropriate
bounds from the fast growing hierarchy. We also expect that our results gen-
eralize to the unordered canonical Ramsey theorem with a suitable largeness
condition. This and other related questions will be investigated jointly in a big-
ger research project with A. Bovykin, L. Carlucci, G. Lee et al.
This contributes to a general research program of the second author about
phase transitions in logic and combinatorics (see, for example, [19,20,21,22,23]
for more information).
We identify the natural numbers with their corresponding sets of predecessors and use C(u, v) instead of C({u, v}) for a colouring C : [R]2 → N. Here [R]2 denotes the set of pairs of unequal elements from R. No ordering of u, v is
implied here and when known we will give the relative order of the two elements.
Denote the collection of subsets of size m of a given set R with Pm(R). Given a number-theoretic function f let us call a colouring C of pairs f -regressive if C(u, v) ≤ f (u) for all u, v with u < v. Call an ordered set (H, ≺) min≺homogeneous for C if C(x, y) = C(x, z) for all x, y, z in H with x ≺ y, x ≺ z.
Then given m there exists a least number R := uKMf (m) such that for all f -regressive colourings C : [R]2 → N there exists an H ∈ Pm(R) and a linear ordering ≺ on H such that H is min≺-homogeneous for C.
The class of primitive recursive functions is the smallest class of functions Nd → N which contains the constant functions, projections and successor function and is closed under composition and recursion. We call a function Acker-
mannian if it eventually dominates every primitive recursive function. Deﬁne
the Ackermann function A as follows:

A0(i) := i + 1 An+1(i) := (An)(i)(i)
A(i) := Ai(i)

The Ackermann function is Ackermannian. It is easy to see that for constant

functions f the function uKMf is primitive recursive and so in between the

constant function and the identity function there will be phase transition from

being p√rimitive recursive to Ackermannian of uKMf . Roughly speaking, for f (i) = k i, the function uKMf is Ackermannian whereas for f (i) = log(i) t√he function uKMf is still elementary recursive. In a ﬁnal step we let k in k i

depend on i. if f (i) ≤ A−d 1

W√e i(i)

bshuotwbetchoamt efusnActcikoenrmuaKnMniafnisifstfi(lli)p≥rimAi−ti1v(e√i) rie. cursive

for

any

d

2 Upper Bounds
We use results from the Kanamori-McAloon principle to derive upper bounds on the unordered case.
Theorem 1 (uKM). For every m there exists an R such that for every fregressive colouring C : [R]2 → N there exists an H ∈ Pm(R) with linear order ≺⊆ H2 which is min≺-homogeneous for C.

356 Florian Pelupessy and Andreas Weiermann

Proof. Let KMf (m) be the minimal R¯ such that for every f-regressive colouring C : [R¯]2 → N there exists an H ∈ Pm(R) which is min<-homogeneous for C. (see [8] for proof of existence of such R¯ )
For given f take R = KMf (m) and an f -regressive colouring C. Then there exists H ∈ Pm(R) which is min-homogeneous for C, hence taking for ≺ the ordering < suﬃces to make H min≺-homogeneous for C.

Notation: uKMf (m) := the smallest such R. The proof of this theorem also delivers upper bounds on uKMf .

Theorem 2. uKMf is primitive recursive if f is:

1. a constant function,

2. 3.

i i

→ →

lAog−d 1i(,√i) i.

Proof. Because uKMf (m) ≤ KMf (m) and the primitive recursive functions are closed under bounded search it suﬃces that KMf is primitive recursive. For proof of that for all three cases see [2].

3 Lower Bounds

For the lower bound it is not possible to easily transfer earlier results. We modify the proofs for KMf from [2] and [10] to suit the problem that allowing diﬀering orderings on H gives. As a preliminary we begin with some results about primitive recursive functions. For s > 0, deﬁne the following sequence:

As0(i) := Asn+1(i) :=

i+1 (Asn)(

√s i

)(i)

As(i) := Asi (i)

Note that for s = 1 this is the Ackermann function. For Rc2(i) we take the minimal R such that for every colouring C : [R]2 → c there exists Y ∈ Pi(R) such that Y is C-homogeneous. (C is constant on [Y ]2).
Lemma 1. 1. The Ackermann function is Ackermannian. 2. If the composition of two non-decreasing functions functions is Ackerman-
nian and one of those is primitive recursive, then the other is Ackermannian. 3. (i, c) → Rc2(i) is primitive recursive.
Proof. A proof of the ﬁrst two statements can be found in [1] and [11], for a proof of the latter one see [7].

We now give a lower bound for uKMf which should ensure that it is Ackermannian. This proof rests on two ideas, namely the use of the particular colour-
ings similar to proofs of the ordered KMf and increasing the ’space’ in the D-homogeneous set to solve the problem caused by allowing any linear order to determine min≺-homogeneity. Fix s ∈ N.

Unordered Regressive Ramsey Numbers √
Lemma 2 (lower bound for roots). Let f : i → s i, then:

357

uKMf (Rc2(m + 4)) ≥ Asc+1(m)

for all c, m ∈ N.

Proof. Take k = Rc2(m + 4) and R = uKMf (k). Deﬁne a colouring C on R as

follows for x < y:

C(x, y) =

0 if Asc+1(x) ≤ y l else

where l is such that for the smallest p for which Asp+1(x) > y we have Asp(l)(x) ≤ y < Asp(l+1)(x). Taking p for x, y as above, deﬁne colouring D of R for x < y:

D(x, y) =

0 if Asc+1(x) ≤ y p else

Note that C is f -regressive (because Asp( f(x) )(x) = Asp+1(x) ). Let H ∈ Pk(R) with order ≺ be min≺-homogeneous for C, then by deﬁnition of k there exists Y ∈ Pm+4(H) which is D-homogeneous. Enumerate such a Y with a strictly <-increasing sequence Y = {y1, · · · , ym, x, y, z, z }. Then we have the following cases for the relative ≺-ordering of x, y, z, z :

1. x ≺ y, x ≺ z Claim: Asc+1(x) ≤ y. Assume for a contradiction that Asc+1(x) > y, then by deﬁnition of C we get C(x, y) = l = 0. Hence (by min≺-homogeneity of H) C(x, y) = C(x, z) = l. By deﬁnition of D and D-homogeneity of Y we also get D(y, z) = D(x, y) =
p = 0.
So the deﬁnition of C gives us:

Asp(l)(x) ≤ y < Asp(l+1)(x)

and Asp(l)(x) ≤ z < Asp(l+1)(x),

that of D delivers:

Asp(y) ≤ z.

Combining these inequalities, taking note that Asp is increasing, we get the contradiction:

z < Asp(l+1)(x) = Asp(Asp(l)(x)) ≤ Asp(y) ≤ z

2. z ≺ x, z ≺ y Claim: Asc+1(x) ≤ z. Assume Asc+1(x) > z, then by deﬁnition and min≺-homogeneity of C we

358 Florian Pelupessy and Andreas Weiermann

have C(x, z) = C(y, z) = l = 0, by deﬁnition and homogeneity of D we get: D(x, y) = D(x, z) = p. This gives us inequalities:

Asp(l)(x) ≤ z < Asp(l+1)(x),

Asp(l)(y) ≤ z < Asp(l+1)(y)
and Asp(x) ≤ y.
Combining these we get:

z < Asp(l+1)(x) = Asp(l)(Ap(x)) ≤ Asp(l)(y) ≤ z
3. y ≺ x, y ≺ z, we distinguish two possibilities: (a) y ≺ z Claim: Asc+1(y) ≤ z. Assume Asc+1(y) > z, then C(y, z) = C(y, z ) = l = 0 and D(z, z ) = D(y, z ) = p. So we have inequalities:

Asp(l)(y) ≤ z < Asp(l+1)(y),

and Combining these:

Asp(l)(y) ≤ z < Asp(l+1)(y) Asp(z) ≤ z .

z < Asp(Asp(l)(y)) ≤ Asp(z) ≤ z
(b) z ≺ y Claim: Asc+1(x) ≤ z . Assume Asc+1(x) > z , then C(x, z ) = C(y, z ) = l = 0 and D(x, y) = D(x, z ) = p. So we have:

Asp(l)(x) ≤ z < Asp(l+1)(x),

and Combining these:

Asp(l)(y) ≤ z < Asp(l+1)(y) Asp(x) ≤ y.

z < Asp(l)(Asp(x)) ≤ Asp(l)(y) ≤ z Examining the cases above allows us to conclude Asc+1(x) ≤ z . But then: Asc+1(m) ≤ Asc+1(ym) ≤ Asc+1(x) ≤ z ∈ Y ⊆ H ⊆ R. So we ﬁnally have:
Asc+1(m) ≤ R

Unordered Regressive Ramsey Numbers 359

For this lower bound to result in Ackermannian functions uKMf the As have to be Ackermannian as well:
Lemma 3. An(i) ≤ Asn+2s2+1(i) for any i ≥ 4s.
Proof. See [2], corollary 4.3. √
TA−h1e(o√i)rie.m 3. uKMf is Ackermannian for f = fs : i → s i and for f : i →

Proof. For the ﬁrst assertion combine lemmas 1, 2 and 3. For the second we claim that

N (i) := uKMf (Ri2+2i2+1(4i + 3)) > A(i)

for we

all i. have

Assume for A−1(l) ≤ i,

csoont√iral d≤ictAio−n1(√it)hla.tHNen(ic)e:≤

A(i)

for

some

i.

Then

for

l

≤

N (i)

uKMf (Ri2+2i2+1(4i + 3)) ≥ uKMfi (Ri2+2i2+1(4i + 3)) ≥ Aii+2i2+1(4i) > A(i)

Where the ﬁrst inequality is a consequence if the deﬁnition of uKMf , the second of lemma 2 and the third of lemma 3. The resulting inequality contradicts with our assumption. Now the claim with lemma 1 implies that uKMf is Ackermannian.

References
1. C. Calude.: Theories of computational complexity, Annals of Discrete Mathematics volume 35, North-Holland, Amsterdam (1988)
2. L. Carlucci, G. Lee, A. Weiermann: A Sharp Phase Transition for G¨odel Incompleteness and Finite Combinatorics. http://www.lix.polytechnique.fr/∼leegy/Publi/kmjams.pdf
3. P. Cholak, C. Jockusch, T. Slaman On the strength of Ramsey’s theorem for pairs. J. Symbolic Logic 66 (2001), no. 1, 1–55.
4. P. Erd˝os, A. Hajnal, A. M´at´e, and R. Rado. Combinatorial set theory: partition relations for cardinals, volume 106 of Studies in Logic and the Foundations of Mathematics. North-Holland, Amsterdam, 1984.
5. P. Erd˝os and R. Rado. Combinatorial theorems on classiﬁcations of subsets of a given set. Proc. London Math. Soc. (3), 2:417–439, 1952.
6. K. G¨odel. U¨ ber formal unentscheidbare S¨atze der Principia Mathematica und verwandter Systeme. Monatshefte f. Math. u. Phys., 38:173–198, 1931.
7. R. L. Graham, B. L. Rothschild, and J. H. Spencer. Ramsey theory. Second edition. John Wiley & Sons Inc., New York, 1990.
8. A. Kanamori and K. McAloon. On G¨odel incompleteness and ﬁnite combinatorics. Ann. Pure Appl. Logic, 33(1):23–41, 1987.

360 Florian Pelupessy and Andreas Weiermann
9. J. Ketonen and R. Solovay. Rapidly growing Ramsey functions. Ann. of Math. (2), 113(2):267–314, 1981.
10. M. Kojman, S. Shelah: Regressive Ramsey Numbers Are Ackermannian, Journal of Combinatorial Theory, Series A 86, 177-181 (1999)
11. M. Kojman, G. Lee, E. Omri and A. Weiermann. Sharp Thresholds for the Phase Transition between Primitive Recursive and Ackermannian Ramsey Numbers. To appear in Journal of Combinatorial Theory. http://www.math.bgu.ac.il/∼kojman/Thresh.pdf
12. J. Mileti, Partition theorems and computability theory, dissertation at UIUC (2004)
13. J. B. Paris. Some independence results for Peano arithmetic. J. Symbolic Logic, 43(4):725–731, 1978.
14. J. B. Paris and L. Harrington. A mathematical incompleteness in Peano arithmetic. In J. Barwise, ed., Handbook of Mathematical Logic, volume 90 of Studies in Logic and the Foundations of Mathematics, pages 1133–1142. North-Holland, 1977.
15. R. P´eter. Recursive functions. Third edition. Academic Press, New York, 1967. 16. F. P. Ramsey. On a problem of formal logic. Proc. London Math. Soc., 30:264–285,
1930. 17. D. Richer: Unordered Canonical Ramsey Numbers. Journal of Combinatorial The-
ory, Series B 80, 172-177 (2000) 18. S.G. Simpson: Subsystems of Second Order Arithmetic. 19. A. Weiermann: 2005: Analytic combinatorics, proof-theoretic ordinals, and phase
transitions for independence results. APAL 136, Issues 1-2 , 189-218. 20. A. Weiermann: Phasenu¨berg¨ange in Logik und Kombinatorik. MDMV 13 (3)
(2005), 152-156. 21. A. Weiermann: An extremely sharp phase transition threshold for the slow growing
hierarchy. MSCS 16 (5) (2006), 925-46. 22. A. Weiermann: Phase transition thresholds for some natural subclasses of the re-
cursive functions. Proceedings of CiE’06, LNCS 3988 (2006), 556-570. 23. A. Weiermann: 2007 Phase transition thresholds for some Friedman-style indepen-
dence results. MLQ. 53 (1), (2007) 4-18. 24. A. Weiermann. A classiﬁcation of rapidly growing Ramsey functions. Proc. Amer.
Math. Soc., 132(2):553–561, 2004.

Almost Partial m-Reducibility
Katya Petrova and Boris Solon
Ivanovo State University of Chemistry and Technology
Abstract. New reducibility (the so-called apm-reducibility) of enumeration type which is weaker than pm-reducibility is introduced in this paper. Initial segments of the upper semilattice of apm-degrees are studied here.
The notations and terminology similar to those of the monograph [3] are used. Let ω denote the set of positive integers; A, B, . . . , X, Y (with or without indices) are used to denote the subsets of ω and A = ω − A. We will use the letters V and W as variables which range over a set of all c.e. sets. Given a partial function α : ω → ω let domα, ranα and graphα = { x, α(x) : x ∈ domα} be the domain, the range and the graph of α respectively. We will write α(x) ↓ if x ∈ domα and α(x) ↑ if x ∈/ domα. Let α−1(X) = {x : x ∈ domα ∧ α(x) ∈ X}. We will use the letters ϕ and ψ as variables which range over a set of all p.c. functions.
Yu.L. Ershov introduced in [1] partial m-reducibility of sets:
A ≤pm B ⇐⇒ (∃ϕ)(∀x)[x ∈ A ⇐⇒ x ∈ domϕ ∧ ϕ(x) ∈ B] ⇐⇒
⇐⇒ (∃ϕ)[A = ϕ−1(B)].
In this case we will talk that A is pm-reducible to B via ϕ. In this paper we introduce a reducibility which slightly weakens pm-
reducibility :
A ≤apm B ⇐⇒ (∃W )(∃A1)[A = W ∪ A1 ∧ A1 ≤pm B] ⇐⇒
⇐⇒ (∃W )(∃ϕ)[A = W ∪ ϕ−1(B)].
In this case we will talk that A is apm-reducible to B via (W, ϕ). These new ideas of deﬁning the reducibility were proposed in 70-s by S.D.Zakharov.
It is clear that if A ≤pm B via ϕ then A ≤apm B via (∅, ϕ). It is easy to check that apm-reducibility is reﬂexive and transitive, hence it is a reducibility in ordinary sense. Also it is clear that A ≤apm B ⇒ A ≤e B for all A and B, i.e. apm-reducibility is a reducibility of enumeration type.
Let degapm(A) = {X : X ≤apm A ∧ A ≤apm X} be apm-degree of A and Dapm be the p.o. set of all apm-degrees.
Theorem 1 Dapm is an upper semilattice with least element 0apm = {Wt : t ∈ ω} in which the least upper bound of the apm-degrees a = degapm(A) and b = degapm(B) is a ∪ b = degapm(A ⊕ B).

362 Katya Petrova and Boris Solon

Proof. Obviously that W ≤apm A via (W, φ) where φ is p.c. function with domφ = ∅. Next let A ≤apm C via (V1, ψ1) and B ≤apm C via (V2, ψ2), we set V = V1 ∪ V2 and ψ = ψ1 ⊕ ψ2. Then it is clear that A ⊕ B ≤apm C via (V, ψ).
The following theorem gives an example of two sets such that A ≤pm B ∧ A ≤apm B, i.e. pm-reducibility is stronger than apm-reducibility.

Theorem 2 There are the sets A and B such that A ≤pm B and A ≤apm B.

Proof. At ﬁrst we construct c.e. sets W and V such that (i) V is inﬁnite; (ii) ∀y[|ranϕy| = ∞ ⇒ (∃x)[x ∈ W ∩ domϕy ∧ ϕy(x) ∈ V ]]. Step 0. Set W = V = ∅. Step z+1. Let z = k, y . Compute k steps in enumerations for each of the y + 1 c.e. sets domϕ0, . . . , domϕy. For each i = 0, 1, . . . , y see whether

(∃x)[x ∈ domϕi ∧ ϕi(x) > 2i].

(1)

If (1) is true then let x∗ be the least number x which satisﬁes (1). In this case we place x∗ in W and ϕi(x∗) in V .
The end of the construction.
As a result we have |{0, 1, . . . , 2y} ∩ V | ≤ y

for every y ∈ ω. Hence the set V is inﬁnitive. The construction is such that all steps z + 1 are computable, hence sets W and V are c.e. So the conditions (i) and (ii) are satisﬁed.
Now we construct a set B ⊂ V such that W ∪ { x, x : x ∈ B} is not c.e. Let Z = {z : Wz ⊇ W }, Z = {z0, z1, . . .} and V = {c0, c1, . . .}. (Both enumerations are not c.e.)
For every i = 0, 1, 2, . . . we see whether

ci, ci ∈ Wzi .

(2)

If (2) is true then let j > i such that Wzj = Wzi then we place cj in B and we do not place ci in B. If (2) is not true then we place ci in B. As a result we have W ∪ { x, x : x ∈ B} = Wz for all z ∈ ω, i.e. W ∪ { x, x : x ∈ B} is not c.e. set.
Let ϕ(z) be a p.c. function such that ∀z[ϕ(z) ↓ ⇐⇒ ∃x[ x, x = z]] and
∀x[ϕ( x, x ) = x]. As { x, x : x ∈ B} ≤pm B via ϕ then W ∪ { x, x : x ∈ B} ≤apm B via (W, ϕ).
Prove that W ∪ { x, x : x ∈ B} ≤pm B. We assume that

W ∪ { x, x : x ∈ B} ≤pm B then W ∪ { x, x : x ∈ B} = ϕ−y 1(B) for some y ∈ ω. If ranϕy is inﬁnite then
∃x[x ∈ W ∩ domϕy ∧ ϕy(x) ∈ V ].

In this case the construction of B guarantees that

∃x[x ∈ W ∩ domϕy ∧ ϕy(x) ∈/ B]

Almost Partial m-Reducibility 363
what contradicts the premise. If ranϕy is ﬁnite then W ∪ { x, x : x ∈ B} is a c.e. set that contradicts the
premise too. Let A = W ∪ { x, x : x ∈ B}, then it is clear that A ≤pm B and A ≤apm B
and the theorem is proved completely. Very simple properties of amp-reducibility are given in the following
Theorem 3 (i). If A diﬀers from B on a ﬁnite set then A ≡apm B; (ii). If V is c.e. set then A ∪ V ≤apm A for any A; (iii). For any A and c.e. V if there is c.e. W such that A ⊆ W and W ∩V = ∅
then A ≡apm A ∪ V .
A set A is called cohesive [2] if A is inﬁnite and A ∩ V is ﬁnite or A ∩ V is ﬁnite for any c.e. set V . In other words an inﬁnite set A is cohesive iﬀ it cannot be divided into two inﬁnite parts by a c.e. set.
Let (D, ≤) be a partial ordering, a, b ∈ D and b < a. The element a is called b-minimal if
∀x[x ∈ D ∧ x ≤ a ⇒ [x ≤ b ∨ a ≤ x]].
Let 0 be the least element in D; 0-minimal element a is called minimal. M.Rozinas proved that a pm-degree of cohesive set is a minimal element in
Dpm. The following theorem shows that this is true in Dapm.
Lemma 1 If A ≤apm B via (V, ψ) then A ≡apm (B ∪ V ) ∩ ranψ.
Proof. Let A ≤apm B via (V, ψ) then A = V ∪ψ−1(B). It is clear that ψ−1(B) = ψ−1(B ∩ ranψ) thus A = V ∪ ψ−1(B ∩ ranψ) = V ∪ ψ−1((B ∪ V ) ∩ ranψ). Then A ≤apm (B ∪ V ) ∩ ranψ via (V, ψ).
Conversely, let W = { y, x : x, y ∈ graphψ}. It is well known that there is p.c. ϕ such that domϕ = W 1 = ranψ and graphϕ ⊆ W . Let V −1 = ϕ−1(V ), now we will prove that
(V ∪ B) ∩ ranψ = V −1 ∪ ϕ−1(A).
We have for any y ∈ ω
y ∈ (V ∪ B) ∩ ranψ ⇐⇒ y ∈ V ∩ ranψ ∨ y ∈ B ∩ ranψ ⇒
⇒ ϕ(y) ↓ ∧ϕ(y) = x ∈ (V ∪ A) ⇒ y ∈ V −1 ∪ ϕ−1(A)
and y ∈ V −1 ∪ ϕ−1(A) ⇒ y ∈ V −1 ∨ y ∈ ϕ−1(A).
We consider each case separately:
y ∈ V −1 ⇐⇒ ϕ(y) ↓ ∧ϕ(y) = x ∈ V ⇒ y ∈ V ∩ ranψ ⇒ y ∈ (V ∪ B) ∩ ranψ
and
y ∈ ϕ−1(A) ⇐⇒ ϕ(y) ↓ ∧ϕ(y) = x ∈ A ⇒ y ∈ B ∩ ranψ ⇒ y ∈ (V ∪ B) ∩ ranψ.
So (V ∪ B) ∩ ranψ = V −1 ∪ ϕ−1(A), it implies (B ∪ V ) ∩ ranψ ≤apm A. Now let’s move to

364 Katya Petrova and Boris Solon

Theorem 4 There are minimal elements in Dapm. First we prove a lemma on cohesive sets.

Lemma 2 There is cohesive set B such that

∀n[|Wn| = ∞ ⇐⇒ |Wn ∩ B| = ∞]

(3)

Proof of the lemma. Let B−1 = ω, next by induction on n if |Bn−1 ∩Wn| = ∞ then set Bn = Bn−1 ∩ Wn and if |Bn−1 ∩ Wn| < ∞ then set Bn = Bn−1 ∩ Wn.
Let b0 ∈ B0, b1 ∈ B1, . . . , bn ∈ Bn, . . . such that b0 < b1 < . . . < bn < . . ., set B = {bi : i ∈ ω}. Prove that B is a cohesive set and satisﬁes (3).
Assume that B is not a cohesive set then |B ∩ Wm| = ∞ and |B ∩ Wm| = ∞ for some m. According our construction Bm ⊂ Wm or Bm ⊂ Wm. Since all but a ﬁnite number of members of B must lie in Bm, either |B ∩ Wm| < ∞ or |B ∩ Wm| < ∞ that contradicts the premise.
Now prove that (3) is true. Let |Wn| = ∞ and let m0 < m1 < . . . be an inﬁnite sequence such that Wn = Wmi for all i ∈ ω. It is clear that bmi ∈ B ∩ Wn for all i ∈ ω, hence |B ∩ Wn| = ∞. The lemma is proved.
Proof of the theorem. Let B be a cohesive set satisfying (3) and b =
degapm(B). It is suﬃcient to show that

∀x[x ∈ Dapm ∧ x ≤ b ⇒ [x = 0apm ∨ b ≤ x]].

Let a = degapm(A) and A ≤apm B via (V, ψ), i.e. A = V ∪ ψ−1(B). Lemma 1 implies A ≡apm (B ∪ V ) ∩ ranψ.
As B is cohesive then two alternative cases are possible: B ∩ ranψ is ﬁnite or B ∩ ranψ is ﬁnite. Let B ∩ ranψ be ﬁnite then (B ∪ V ) ∩ ranψ is c.e., hence we
have a = 0apm. Let B ∩ ranψ is ﬁnite. In this case as B is cohesive two alternative cases are
possible: V ∩ B is ﬁnite or V ∩ B is ﬁnite. Let V ∩ B is ﬁnite then V is a ﬁnite
set by (3). In this case we have

A ≡apm (B ∪ V ) ∩ ranψ = (B ∩ ranψ) ∪ (V ∩ ranψ) ≡apm B ∩ ranψ ≡apm B

and so we have a = b. If V ∩ B is ﬁnite then B ∪ V diﬀers from V on a ﬁnite set so in this case we
have a = 0apm. The theorem is proved completely.
Theorem 5 If B1, . . . , Bn, n ≥ 1 are cohesive sets satisfying (3) and their apmdegrees are diﬀerent in pairs then the initial segment [0apm, degapm(B1 ⊕ . . . ⊕ Bn)] ⊂ Dapm is isomorphic to Boolean algebra of subsets of a n-element set.
Proof. Let B = B1 ⊕ . . . ⊕ Bn and A ≤apm B, we show that in this case A is c.e. or A ≡apm i∈I Bi∗ where Bi∗ = {xn + i : x ∈ Bi} for some I ⊆ {1, . . . , n}, I = ∅.
Let A ≤apm B via (W, ψ) and V = ranψ. Let
I = {i : 1 ≤ i ≤ n ∧ |V ∩ Bi∗| = ∞}.

Almost Partial m-Reducibility 365

If I = ∅ then V ∩ Bi∗ is ﬁnite for all i = 1, . . . , n, thus V ∩ B is ﬁnite and then A is a c.e. set.
Let I = ∅. Since Bi is a cohesive set satisfying (3) then Bi∗ is a cohesive set satisfying (3) too, thus V ∩ Bi∗ is ﬁnite for all i ∈ I. Then
|V ∩ (∪i∈I Bi∗)| = | ∪i∈I Bi∗ − V | < ∞.
In this case the set i∈I Bi∗ diﬀers from i∈I (Bi∗ ∩ V ) on a ﬁnite set, thus

Bi∗ ≡apm (Bi∗ ∩ V ).
i∈I i∈I

(4)

Let i ∈ {1, . . . , n} − I then the set Bi∗ ∩ V is ﬁnite, thus the set B ∩ V = ∪ni=1(Bi∗ ∩ V ) diﬀers from ∪i∈I (Bi∗ ∩ V ) on a ﬁnite set. So

B ∩ V ≡apm ∪i∈I (Bi∗ ∩ V ).

(5)

By Lemma 1 A ≤apm B via (W, ψ) implies

A ≡apm (B ∪ W ) ∩ ranψ = (B ∩ V ) ∪ (W ∩ V ).

Taking into consideration the fact that the set V ∩ W may be ﬁnite or inﬁnite we conclude that A is c.e. or A ≡apm B∩V . In the last case we have A ≡apm ∪i∈I Bi∗, as every Bi∗ satisﬁes (3).
Now we show that for any I1 ⊆ {1, . . . , n} and I2 ⊆ {1, . . . , n}
I1 ⊆ I2 ⇐⇒ ∪i∈I1 Bi∗ ≤apm ∪i∈I2 Bi∗.
If I1 ⊆ I2 then ∪i∈I1 Bi∗ ≤apm ∪i∈I2 Bi∗ via (∅, α) where α(x) ↓ ⇐⇒ x ∈ ∪i∈I1 {nx + i : x ∈ ω} and x ∈ domα ⇒ α(x) = x.
Let ∪i∈I1 Bi∗ ≤apm ∪i∈I2 Bi∗. We assume that I1 ⊆ I2 and j ∈ I1 − I2. It is clear that {j} ⊆ I1 implies
Bj∗ ≤apm ∪i∈I1 Bi∗ ≤apm ∪i∈I2 Bi∗.
Let Bj∗ ≤apm ∪i∈I2 Bi∗ via (W ∗, ϕ) i.e. Bj∗ = W ∗ ∪ (∪i∈I2 ϕ−1(Bi∗)). Denote by Vi = ϕ−1({nx + i : x ∈ ω}) for all i ∈ {1, . . . , n}. As the Bj∗ is a cohesive set and Bj∗ ⊆ W ∗ ∪ (∪i∈I2 Vi) then there is l ∈ I2 (note that l = j) such that Bj∗ ∩ Vl is an inﬁnite set. Otherwise Bj∗ = W ∗ ∪ D for some ﬁnite set D and then Bj∗ is c.e. that contradicts the premise.
Let ϕ∗ be a restriction of ϕ to Vl. Obviously Bj∗∩Vl ≤apm Bl∗ via (W ∗∩Vl, ϕ∗). As Bj∗ ∩ Vl is a ﬁnite set then
Bj∗ ≡apm Bj∗ ∩ Vl ≤apm Bl∗ ⇒ Bj ≤apm Bl.
As the Bl∗ is a cohesive set satisfying (3)then by Theorem 4 the set Bj∗ is c.e. or Bl ≤apm Bj. But the ﬁrst is not possible as Bj is a cohesive set and

366 Katya Petrova and Boris Solon
the second is not possible as Bj and Bl should be in a distinct apm-degrees by hypothesis of Theorem. So we get a contradiction that proves
∪i∈I1 Bi∗ ≤apm ∪i∈I2 Bi∗ ⇒ I1 ⊆ I2.
Let Ξ be the mapping from the set of all non-empty subsets of {1, . . . , n} into Dapm, which is deﬁned by
Ξ(I) = degapm(∪i∈I Bi∗)
for all ∅ = I ⊆ {1, . . . , n} and Ξ(∅) = 0apm. We proved above that Ξ is an order-preserving embedding Boolean algebra of subsets of {1, . . . , n} to the initial segment [0apm, degapm(B1 ⊕ . . . ⊕ Bn)] ⊂ Dapm. The theorem is proved completely.
Corollary 1 For all n ≥ 1 Dapm contains a continuum of initial segments each of which is isomorphic to Boolean algebra of subsets of a n-element set.
In conclusion we would like to announce another weak pm-reducibility:
A ≤wpm B ⇐⇒ (∃k)(∃ψ1, . . . , ψk)[A = ψ1−1(B) ∪ . . . ∪ ψk−1(B)].
In this case we will talk that A is wpm-reducible to B via (ψ1, . . . , ψk). It is clear that ≤wpm is reﬂexive and transitive, hence it is a reducibility in
ordinary sense. It is also clear that A ≤apm B ⇒ A ≤wpm B and A ≤wpm B ⇒ A ≤e B (moreover A ≤wpm B ⇒ A ≤s B) for all A and B, i.e. apm-reducibility is a reducibility of enumeration type. We are able to construct two sets A and B such that A ≤apm B ∧ A ≤wpm B.
We invite to study this wpm-reducibility as a reducibility of enumeration type which has a good intuitive background.
References
1. Ershov Yu. L.: Numbering Theory. ”Nauka”. Moskou. 1977 2. Rogers H.,Jr.: Theory of Recursive Functions and Eﬀective Computability.
McGraw-Hill. New York. 1967 3. Soare Robert I.: Recursively Enumerable Sets and Degrees. Springer-Verlag. Berlin,
Heidelberg, New York, London. 1987

Two-Dimensional Cellular Automata Transforms for a Novel Edge Detection
Yongri Piao1, *Seok-Tae Kim1, Sung-Jin Cho2
1Department of Telematics, Pukyong National University, 599-1, Daeyeon 3-Dong, Nam-Gu, Busan, Republic of Korea (608-737)
pyr-bww@hanmail.net, setakim@pknu.ac.kr 2 Division of Mathematical Sciences, Pukyong National University, 599-1, Daeyeon 3-Dong, Nam-Gu, Busan, Republic of Korea (608-737)
sjcho@pknu.ac.kr
Abstract. In this paper, we propose a novel edge detection scheme using twodimensional cellular automata transforms (CAT). Cellular Automata (CA) is discrete dynamical system whose function is completely specified in terms of local relation. First, we get the gateway values such as wolfram Rule, number of cells in lattice, number of cells per neighborhood, initial configuration and boundary configuration. Second, we use the gateway values to generate a dualstate, two-dimensional cellular automata and dual-coefficients basis function. Finally, we transform images into cellular automata domain according to the basis function. Then we use the basis function and cellular automata transform coefficients to extract the edge of the image. The experimental results verify that the proposed scheme is a new attempt to process image with cellular automata model.
1 Introduction
Sharp variations of image intensity could be described by edges, which convey important information in an image. Edge detection is a pivotal technique in pattern recognition, image procession, and computer vision.
One edge detection algorithm can not be applied to all of the images. Based on the complexity in edge detection, each algorithm has positive and negative performance. For instance, Smith et al. [1] proposed low-level feature extraction-SUSAN operator. It attempted to provide robust signature for every edge point. The method proposed by Smith et al, however, is strongly influenced by the presence of edges and corners. On the other hand, noise contamination is always a problem and edge detection in noisy environment can be treated as an optimal linear filter design problem [2-6]. The Canny [3] edge detector is more accurate, though more complex. It employs Gaussian first order differential equation which is able to reach the balance between robust noise and edge detection. Unfortunately, the Canny detector failed to verify whether discontinuity is contributed by noise or true edges. Unlike previous works, we present
* Corresponding Author. Tel.: +82-51-629-6234; Fax: +82-51-629-6210

368 Yongri Piao et al.
a novel edge detection method in the CAT. The essence of CAT is that we can always find CA rules (and its associated neighborhood, initial/boundary configuration, lattice arrangement etc) which will result in basis function and transform coefficients.
CA was originally introduced by Ulam [7] and Von Neumann [8]. Afterwards, the cellular automata theory was developed by Stephen Wolfram [9-10]. Now cellular automata have generated much interest because of their diverse functions and usefulness as a discrete model for many processes [11-13]. Along with the development of the digital image technology, some efforts are expended in using CA in image processing. C. L. Chang et al. [14] proposed an image edge detection using CA. They defined a new kind of neighborhood of the CA. Then a suitable local rule of the CA is designed. However, their method only uses the relationship of the neighborhood to detect edges in the spatial domain and is weak at noise.
In this paper, we propose a novel edge detection method using two-dimensional cellular automata transforms. After we get the gateway values (such as Wolfram Rule, the number of cells in lattice, the number of cells per neighborhood, the initial configuration, and the boundary configuration), we use them to generate a dual-state, two-dimensional cellular automata dual-coefficients basis function. Then the basis function and their associated transform coefficients transform an image into cellular automata domain components to extract the edge of the image. The experimental results verify that the proposed method is not only a new attempt to detect edge detection with the cellular automata model but also is proved to be very efficient.

2 Cellular Automata Basics

CA is a dynamical system in which space and time are discrete. The cells, which are arranged in the form of a regular lattice structure, have a finite number of states. These states are updated synchronously according to a specified local rule of interaction. Using a specified rule, the values are updated synchronously in discrete time steps for all cells[15].
In general, for a K-state, m-site neighborhood CA, there are K Km rules. For ex-
ample, there are 28 rules for the two- state, 3-site neighborhood CA. The Wolfram Rule convention is to assign the integer R to rule generating the function F such that:

K m −1
∑R = Cn 2n n=0

(1)

where Cn is the Boolean value generated by the rule given the n-th configuration.
Consider a 3-site neighborhood dual-state one-dimensional CA. The quantity ait represents the state of the i-th cell, at discrete time t, whose two neighbors are in the following states: ai-1t , ai+1t. In general, we seek a rule that will be used to synchronously calculate the state ait+1 from the state of the cells in the neighborhood at the tth time level. The cellular automaton evolution can be expressed in the form:

Two-Dimensional Cellular Automata Transforms

369

ait+1 = F (ai−1t , ait , ai+1t )

(2)

where F is a Boolean function defining the rule. The number of cellular automata rules can be astronomical even for a modest lat-
tice space, neighborhood size, and CA state. Therefore, in order to develop practical applications, a system must be developed for addressing a subset of this infinitely large universe of CA rules. Consider, for example, a K-state, m-site neighborhood CA with m=2r+1 points per neighborhood. We define the rule of evolution of a cellu-

lar automaton by using a vector of integers

W (j j

= 0,1, 2, 3,L 2m )

such that

∑a ( r )( t +1 )

=

⎛ 2m −2 ⎜ W jα ⎝ j=0

j

⎞W 2m

+

W

2m

−1

⎟ ⎠

m od

K

(3)

where 0 ≤ W < K , r is a distance. α are made up of the permutations of the states of jj
the cells in the neighborhood. Hence, each set of W results in a given rule of evoluj
tion.

3 Cellular Automata Transforms

Given a process described by a function f, defined in physical space of lattice grid i, we seek basis function A and their associated transform coefficients c, defined in cellular automata space k [15]. We write

∑fi = ck Aik

(4)

k

Equation (4) represents a mapping of the process f (in the physical domain) in to c

(in the cellular automata domain) using the building blocks A as transfer functions.

In a 2-D square space consisting of N × N cells, the transform base

A

=

A , (i, ijkl

j, k,l

=

0,1,L, N

− 1) .

For

the

data

sequence

f ij

(i,

j

=

0,1,

2, L ,

N

− 1)

we can write:

N −1 N −1
∑ ∑fij = ckl Aijkl (i, j = 0,1,L, N −1) k=0 l=0

(5)

in which ckl are the transform coefficients. There are two approaches for generating two-dimensional CA transform bases:
1. Using the evolving states derived from two-dimensional cellular spaces.

Here,

Aijkl

is

calculated

from

a

≡

a ijt

, (i,

j, t

=

0, 1, L ,

N

− 1)

.

2. Calculating the canonical products of one-dimensional bases so that

Aijkl=AikAjl.

370 Yongri Piao et al.

4 Edge Detection Using CAT

In this section, we introduce the edge detection scheme using two-dimensional cellular automata transforms in details.

Step 1: We get the gateway values (wolfram Rule, number of cells in lattice, number of cells per neighborhood, initial configuration and boundary configuration etc.).

Step 2: We use the gateway values to generate a dual-state and two-dimensional cellular automata dual-coefficients basis function.
Consider, for example, a 2-state 8-node CA with 3points per neighborhood. In Table 1, we show the gateway values for generating a dual-state, two-dimensional CA dual –coefficient basis function.

Table 1. Gateway Values for 2-D dual-coefficient basis function

Wolfram Rule Number

46

Number of Cells per Neighborhood

3

Number of Cells in Lattice

8

Initial Configuration

00001001

Boundary Configuration

Cyclic

Basis Function Type

2

The cyclic boundary conditions imposed on the end sites are of the form equation (6):

a−1,k = aN −1,k

aN ,k = a0k

(6)

Then a dual-coefficient cellular automata basis function is described by equation (7):

Aik = 2aik aki −1

(7)

where aik is the state of the CA at the node i at time t=k. The states are obtained from N (N=8) cells evolved from a specific initial configuration for N time steps.

Step 3: The CA basis function is derived from the 1-D types in form equation (8):

Aijkl = Aik Ajl

(8)

2-D basis functions are derived from the evolving one-dimensional CA as below:

{( ) }Aijkl = Lw aik aki + a jl alj mod Lw − (Lw −1)

(9)

where L ≥ 2 is the number of states of the automaton. w

Two-Dimensional Cellular Automata Transforms

371

Step 4: The two-dimensional cellular automata basis function of the edge detection rule transforms images into cellular automata domain. Then we use the basis function and cellular automata transform coefficients to extract the edge of the image.
In Figure 1 we show the steps for generating two-dimensional dual-coefficient basis function. This method produces many types of these basis functions which can detect edges. Of the basis functions generated, Figure 2 graphically shows the one using the gateway values from Table 1. A00kl is the block at the extreme upper left corner. The top row represents 0 ≤ j < 8 ; i=0. The left column is j=0; 0 ≤ i < 8 . Aij00 is the upper left corner of each block. The white rectangular dots represent 1 while the black dots are -1.

Gateway Values

1D aik

1D Aik

Figure 1. Process of 2D basis function

Generate 2D Aikjl

Figure 2. 2D basis function of Table 1 gateway values
5 Experimental Results
The performance of the proposed scheme is tested on various types of images.
Here, the results are represented by three grayscale 8-bit images of size 256 × 256
(Figure 3 (a)-(c)). Table 2 shows the wolfram rule and its initial configuration of the dual-coefficient CAT filters in detecting the edge of tested images. The experimental results consist of two parts. Part one is the performance of the proposed method on the edge detection of real images, including noise and non-noise images. Part two is the performance of different edge detectors by using a synthetic image.

372 Yongri Piao et al.
5.1 Edge detection of clean and noisy images
The Sobel, Laplacian of Gaussian (LoG) and Canny detectors are the most common edge operators. Figure 3 (d)-(f) shows the extracted edge maps using 2D CAT filter (Figure 2). The edge maps of the test images obtained by the three detectors are shown in Figure 3 (g)-(o). Figure 3 (g)-(i), (j)-(l) and (m)-(o) show the extracted edge maps using Sobel, LoG and Canny detector in MATLAB, respectively. From the experimental results, we can find that the proposed method has good performance in the delicate region where the gray level is finely changing.
Figure 4 shows the proposed method compared with the Sobel, LoG and Canny detector under random noise. Figure 4 verifies that the Sobel, LoG and Canny edge detectors can detect the edges; though, noise pixels still exist in the extracted edge maps. The Sobel edge detector can remove the small speckles from the face and head. Using LoG and Canny edge detector to extract edge maps, it is difficult to find the true edges because of the noise. Even though the edges extracted by proposed edge detector include partial noise pixels, we can still easily find the true edges by using proposed CAT edge detector.

Table 2. Wolfram rule and initial configuration

Filter

Wolfram rule

Initial configuration

A 46

18

B 155

137

C 174

33

D 209

28

E 231

109

F 245

232

5.2 Edge detection of a synthetic image
On the other hand, noise contamination is always a problem and edge detection in noisy environment can be treated as an optimal linear filter design problem. The performance of the proposed method comparing with other methods (Sobel, LoG and Canny) is tested on the synthetic image (Figure 5 (a)). That image is an 8-bit gray-
scale image of size 256 × 256. The result of synthetic image added Gaussian noise as
shown in Figure 5(b). The FoM (Figure of Merit) [16] is used to evaluate the performance of the edge detection methods. The FoM is defined as follows

∑1
F= IN

1I A i=1 1 + β di2 ,

IN = max(I , I A )

(10)

where I is original pixels of edge, IA is detected pixels of edge, β (β=1/9) is scaling parameter, di is distance of i-th detected pixel of edge between original pixel of edge.

Two-Dimensional Cellular Automata Transforms

373

Figure 3. (a)-(c) Test images (d)-(f) CAT detector (g)-(i) Sobel detector (j)-(l) LoG detector (m)-(o) Canny detector.

374 Yongri Piao et al.

Figure 4. Extracted edge maps of Lena image with random noise (a) CAT detector (b) Sobel detector (c) LoG detector (d) Canny detector.

Figure 5. (a) Synthetic image (b) Synthetic image with Gaussian noise

Table 3. The proposed method compared with other methods

Detector

Power of Gaussian Noise

0.05 0.10 0.15 0.20 0.25

Sobel

0.8301

0.7059

0.3947

0.2706

0.2341

LoG

0.1776

0.1774

0.1768

0.1763

0.1753

Canny

0.3194

0.2945

0.2940

0.2778

0.2219

CAT

0.5320

0.2953

0.2117

0.1726

0.1520

Two-Dimensional Cellular Automata Transforms

375

Table 3 shows the proposed method compared with other methods. As shown in Table 3, the different gateway values generate two-dimensional CAT edge detection filters which are close to FoM results. Also, the performance of the proposed method was better than LoG and Canny methods in the region where the power of Gaussian noise was less than 0.15. Moreover, we found many basis functions which could detect edges. Furthermore, we expect them to be applied in various types of image processing in the future.

6 Conclusions
This paper presents a new edge detection scheme using two-dimensional cellular automata transforms. By using the gateway values (such as Wolfram Rule, the number of cells in lattice, the number of cells per neighborhood, the initial configuration, and the boundary configuration), we generated a dual-state, two-dimensional, and dual-coefficients cellular automata basis function. Then we transformed images into cellular automata domain with the basis function. Finally, we used the basis function and cellular automata transform coefficients to extract the edge of the image.
The experimental results verify that the proposed method is a new attempt to detect edge with cellular automata model as well as to maintain its high efficiency.
Although researchers continue to experiment on innovative edge detectors, they are still not able to define the ultimate suitable method for all conditions. However, with our proposed method, we can equally apply it to all images. We may collate many basis functions produced through the method and open possibilities for more developed edge detectors with novel features. Overall, our method will plays as a clue to solving such inconvenience.

References
1. S. Smith, M. Brady : SUSAN-A New Approach to Low Level Image Processing. Int. J. Comput. Vision 23 (1) (1997) 45-47
2. V. Torre, T. Poggio: On Edge Detection, IEEE Trans. Pattern Anal. Mach. Intell. PAMI 2 (1980) 147-163
3. J. Canny: A Computational Approach to Edge detection. IEEE Trans. Pattern Anal. Mach. Intell. PAMI 8 (1986) 679-698
4. T. D. Sanger : Optima Unsupervised Learning in A Single-Layer Feedforward Neural Network. Neural Networks 2 (1989) 459-473
5. B. S. Manjunath, R. Chellappa : A Unified Approach to Boundary Perception : Edges, Textures and Illusory Contours. IEEE Trans. Neural Network 4 (1993) 96-108
6. S. Sarkar, K. L. Boyer : On Optimal infinite impulse response edge detection filters. IEEE Trans. Pattern Anal. Mach. Intell. PAMI 13 (1991) 1154-1171
7. Ulam. S : Some Ides and Prospects in Biomathematics. Anm. Rev. Biophys. Bioengin, Vol. 1 (1963) 277-291
8. Von Neumann. J : Theory of Self-Reproducing Automata, University of Illinois Press, IL, (1966)
9. Wolfram S : Statistical Mechanics of Cellular Automata. Rev. Mod, Phys. 55 (1983) 601

376 Yongri Piao et al.
10. Wolfram S : Computation Theory of Cellular Automata. Commun. Math. Phys. 96 (1984) 15-57
11. S. K. Lee, S. T. Kim and S. J. Cho: A Potts Automata algorithm for Noise Removal and Edge detection. Journal of Korean institute of Communication Science, Vol. 28-3C (2003) 327-335
12. S.J. Cho, U.S. Choi, H.D. Kim, Y.H. Hwang, J.G. Kim and S.H. Heo: New Synthesis of One-Dimensional 90/150 Linear Hybrid Group Cellular Automata. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, Vol. 26(9) (2007) 1720-1724
13. S.J. Cho, U.S. Choi, H.D. Kim and Y.H. Hwang: Analysis of complemented CA derived from linear hybrid group CA. Computers and Mathematics with Applications, Vol. 53(1) (2007) 54-63
14. C. L. Chang, Y. J. Zhang, Y. Y. Gdong : CA for Edge detection of Images. IEEE International Conference on Machine Learning and Cybernetics, Shanghai (2004) 3830-3834.
15. Olu Lafe: Cellular Automata Transforms: Theory and Application in Multimedia Compression, Encryption, and Modeling. Kluwer Academic Publishers, Boston/ Dordrecht/London, (2000)
16. I. E. Abdou, W. K. Pratt : Quantitative design and evaluation of enhancement/thresholding edge detectors. Proc. IEEE, 69 (1979) 753-763.

Computable Counter-examples to the Brouwer Fixed-point Theorem⋆
Petrus H. Potgieter
Department of Decision Sciences, University of South Africa (Pretoria) PO Box 392, Unisarand, 0003, Republic of South Africa
php@member.ams.org, potgiph@unisa.ac.za, www.potgieter.org
Abstract. This paper is an overview of results that show the Brouwer ﬁxed-point theorem (BFPT) to be essentially non-constructive and noncomputable. The main results, the counter-examples of Orevkov and Baigger, imply that there is no procedure for ﬁnding the ﬁxed point in general by giving an example of a computable function which does not ﬁx any computable point. Research in reverse mathematics has shown the BFPT to be equivalent to the weak K¨onig lemma in RCA0 (the system of recursive comprehension) and this result is illustrated by relating the weak K¨onig lemma directly to the Baigger example.
Key words: Computable analysis, Brouwer ﬁxed-point theorem, weak K¨onig lemma
1 Introduction
We consider the Brouwer ﬁxed-point theorem (BFPT) in the following form, where the standard unit interval is denoted by I = [0, 1].
Theorem 1 (Brouwer). Any continuous function f : I2 → I2 has a ﬁxed point, i.e. there exists an x ∈ I2 such that f (x) = x.
A computable real number is a number for which a Turing machine exists that, on input n, produces a rational approximation with error no more than 2−n. A computable point is a point all the coordinates of which are computable reals. The notation
N0 for the non-negative natural numbers; Rc for the set of computable reals; Ic for I ∩ Rc; and δX for the boundary of a set X, being X ∩ Xc
is also used. The two examples discussed use distinct deﬁnitions of a computable function of real variables.
⋆ This work was supported in part by a grant under the South African-Hungarian Science and Technology Agreement (National Research Foundation of South Africa UID: 62110) and in part by a research grant from the College of Economic and Management Sciences of the University of South Africa.

378 Petrus H. Potgieter
Russian school In the Russian school of Markov and others, a computable function maps computable reals to computable reals by a single algorithm for the function that translates an algorithm approximating the argument to an algorithm approximating the value of the functions. It need not be possible to extend a function that is computable in the Russian school to a continuous function on all of the reals. These functions are often called Markov-computable.
Polish school In the Polish school of Lacombe, Grzegorczyk, Pour-El and Richards, and others, a function is computable on a region if it maps every every computable sequence of reals to a computable sequence of reals and it has a computable uniform modulus of continuity on the region [1].
2 Orevkov’s example for the Russian school
One can construct a Markov-computable function f through a computable mapping of descriptions of computable points x ∈ Ic2 to descriptions of f (x) ∈ Ic2, such that
f (x) = x ∀x ∈ Ic2.
That is, no computable point is a ﬁxed point for f . Unfortunately the f which is constructed in this way, cannot be extended to a continuous function on I2. This is the construction of [2], another instance of which can be found in [3].

···

Fig. 1. Basic contraction in the Orevkov counter-example

Lemma 1. Suppose Ak is a sequence of rectangles in I2 with computable vertices, disjoint interiors, and such that

(i) ∅ = δAj \ i<j Ai for all j; (ii) for each j there exists n > j such that δAj ⊂ (iii) Ic2 ⊆ i≥1 Ai

◦
i≤n Ai ; and

then there exists a Markov computable g, mapping Ic2 to δIc2 and ﬁxing δIc2.

Computable Counter-examples to the Brouwer Fixed-point Theorem 379
The conditions ensure that
(i) rectangles Aj, when added, have some part of their boundary in I2 \ i<j Ai;
(ii) each Aj is eventually closed oﬀ by new rectangles on all sides; (iii) all computable points lie in i≥1 Ai. The function f is obtained by composing g with a 90◦ rotation. It therefore remains only to prove the lemma and the existence of a sequence of rectangles which is as required. Suppose that g has been deﬁned on i<j Ai. For
∅ = δAj ∩ i<j Ai: let g on Aj consist of the simplest possible mapping to δI2, that ﬁxes δI2;
∅ = δAj ∩ i<j Ai: we extend g to Aj by using (i)—if g has already been deﬁned on the crosshatched set in Figure 1 then the deﬁnition can be extended to the solid gray set Aj by composing a contraction of the solid gray set in Figure 1 to 
δAj ∩ Ai ∪ δAj ∩ δI2
i<j
with the function g as it has already been deﬁned on the crosshatched set. This is always possible because, by construction of the Ai, our Aj will always have at least two sides non-contiguous with i<j Ai, at least one of which will not coincide with δI2.
So far only condition (i) has been used. Conditions (ii) and (iii) are necessary for showing that g is Markov-computable on I2. Let a description of any x ∈ Ic2 be given. We can ﬁnd a description of g(x) in the following way.
– Simultaneously, compute approximations of x using the given description and construct g on i≤n Ai for n = 1, 2, . . ..
– Together, (ii) and (iii) imply that for some n we will be able to verify that
 ◦
x ∈  Ai
i≤n
where the interior is with respect to the subset topology on I2, of course. – When such an n has been identiﬁed, we already know the deﬁnition of g for
i≤n Ai as well as the modulus of continuity of g on the same set. This is now used to describe g(x).
It remains to be shown that a suitable sequence of rectangles (An)n≥1 exists. This follows from the next fact, assumed without proof for now1.
Lemma 2 (see [4], for example). There exist computable sequences of rational numbers (an) and (bn) in the interval I = [0, 1] such that the intervals Jn = [an, bn] have the following properties.
1 Later we shall deduce the fact from the existence of a Kleene tree.

380 Petrus H. Potgieter
(i) If n = m then |Jn ∩ Jm] ≤ 1. (ii) If an = 0 then an ∈ {b0, b1, . . .} and if bn = 1 then bn ∈ {a0, a1, . . .}. (iii) Ic n Jn, i.e. the Jn cover the computable reals in I = [0, 1]. Now, let (An)n≥1 be any computable enumeration of the Jk × Jℓ. This completes the proof of the lemma, and the example.

3 Baigger’s example for the Polish school

Let a be any non-computable point in I2. Consider the function f which moves each point half-way to a,

f (x)

=

x

+

1 2

(a

−

x)

and has a single ﬁxed point, namely a itself. The function f is continuous and deﬁned on all of I2 and has no computable ﬁxed point. Nevertheless, this is not
really interesting since

– the ﬁxed point a has no reasonable description—since it is itself not computable; and therefore
– the function f has no reasonable description—it is not computable in any sense.

One would like to see a function which is computable, deﬁned (and therefore continuous) on all of I2 and yet avoids ﬁxing any of the computable points Ic2. The following example, having appeared in [5] and in [3], modiﬁes the construction of Orevkov to produce a computable f deﬁned on all of I2 having no computable
ﬁxed point. One uses the intervals Jn = [an, bn] of Orevkov’s example and sets

Cn =

Jk × Jℓ

k,ℓ≤n

after which one deﬁnes f progressively, using the sets Cn. The points

tn = (vn, vn)

where

vn

=

min {x
x∈I

|

(x,

x)

∈

Cn}

are used as “target point” at each stage of the construction, as in Figure 2. Note

that

v

=

lim
n→∞

vn

is not a computable number and (v, v) will be one of the ﬁxed points of f .

Deﬁnition 1. For any W ⊆ I2 we deﬁne

W ε = x ∈ W d x, δW \ δI2 ≥ ε

and W ε = x ∈ W d x, δW \ δI2 = ε .

Computable Counter-examples to the Brouwer Fixed-point Theorem 381 11

t2
0 0

C2 t5
0 10 Fig. 2. The “target points” tn

C5 1

One can deﬁne fn such that

1. fn moves every point in the interior of Cn 2−n but is the identity outside the

set, and is computable;

2.

fn+1

agrees

with

fn

on

Cn

2−n ·

3 2

and

therefore

3. f = limn→∞ fn is computable.

Every computable point eventually lies in some

Cn

2−n ·

3 2

⊂

Cn 2−n ◦

and is therefore moved by f . Clearly f (I2) ⊆ I2 and f will be as required. In fact, f has no ﬁxed point in

Cn =

Jk × Jℓ.

n k,ℓ≥1

Also, f has no isolated ﬁxed point—its ﬁxed points all occur on horizontal and vertical lines spanning the height and breadth of the unit square. Further details of the construction appear in Appendix A. The construction cannot be applied in the one-dimensional case because it is impossible to eﬀect a change of direction by continuous rotation.

4 BFPT and the K¨onig lemma
In reverse mathematics it is known that in RCA0, the system of recursive comprehension and Σ10 -induction, the weak Ko¨nig lemma, WKL0, is equivalent to the Brouwer FPT [6]. Lemma 3 (WKL0, K˝onig). Every inﬁnite binary tree has an inﬁnite branch. The Ko¨nig lemma does not have a direct computable counterpart.

382 Petrus H. Potgieter
Theorem 2 (Kleene [7]). There exists an inﬁnite binary tree, all the computable paths of which are ﬁnite.
The relation of the Kleene tree to the Baigger counterexample is reviewed in this section. The discussion is informal and attempts only to give the essential ideas that have been revealed by the approach of reverse mathematics. In RCA0, the weak Ko¨nig lemma WKL0 has been shown to be equivalent to a number of other results in elementary analysis, such as the fact that any continuous function on a compact interval is also uniformly continuous [8]. WKL0 and RCA0 can, furthermore, be used to prove G¨odel’s incompleteness theorem for a countable language [9].

4.1 From Baigger f to Kleene tree

Let f be a computable function, as in the Baigger example, mapping I2 to itself—with no computable ﬁxed point. The following auxiliary result will be used to construct the Kleene tree.

Lemma 4. Let a computable g : I2 → [0, 1] be given. Then there exists a Turingcomputable h : N90 → N20 such that for any (n1, n2, . . . , n8, k) with

0 ≤ n1 ≤ n3 ≤ 1 and 0 ≤ n5 ≤ n7 ≤ 1

n2 n4

n6 n8

we have h : (n1, n2, . . . , n8, k) → (m1, m2) with m1 ≤ m2 where

m1 ≤ min g m2

n1 , n3 × n5 , n7

n2 n4

n6 n8

≤ m1 + 1 . m2 k

Let g = ||f (x) − x|| and let h be as in the lemma. Note that g(x) = 0 if and only if x is a ﬁxed point of f . We shall use only the essential consequences that

– g(x) > 0 for all computable x; and – there exists a (non-computable) x0 such that g(x0) = 0.
As usual, {0, 1}∗ denotes the set of ﬁnite binary sequences and ab is the concatenation of a and b.
Deﬁnition 2. A binary tree is a function t : {0, 1}∗ → {0, 1} such that

t(ab) = 0 for all b whenever t(a) = 0.

An inﬁnite branch of a tree t is an inﬁnite binary sequence, on all of which ﬁnite initial segments t takes the values 1.
The tree is computable whenever the function t is Turing-computable and a computable branch is a computable binary sequence which is an inﬁnite branch. Deﬁne the Kleene tree as follows. Let
n
t(i1 . . . in) = s(i1 . . . im)
m=1

Computable Counter-examples to the Brouwer Fixed-point Theorem 383

where s is a function taking values in {0, 1}. This deﬁnition of t ensures that t is in fact a tree and if s is computable, t will be a computable tree. The function s will use h to estimate whether g gets close to zero on a speciﬁc square and if g has been bounded away from zero on the square, that branch of the tree will terminate.
Deﬁne s : {0, 1}∗ → {0, 1} for all sequences i1j1 . . . injn of even length by

where

s(i1j1 . . . injn) = χ{0}

m1 m2

(m1, m2) = h (i1 . . . in, 2n, i1 . . . in + 1, 2n, j1 . . . jn, 2n, j1 . . . jn + 1, 2n, n)

and binary strings have been interpreted as the natural numbers which they represent. Let s take the value 1 on sequences of odd length.
The tree t deﬁned in this way is obviously computable. It remains to show that t is

– inﬁnite; and – has no inﬁnite computable branch.

Let x0 be any point where g(x0) = 0. Then there exist inﬁnite sequences (in) and (jn) such that

x0 ∈

i1

... 2n

in

,

j1

... 2n

jn

×

i1

.

.

. in 2n

+

1

,

j1

.

.

. jn 2n

+

1

for all n

and therefore, for all n, s(i1j1 . . . injn) = 1 and so t(i1j1 . . . injn) = 1 which

proves the existence of an inﬁnite branch, hence that the tree t is inﬁnite.

Suppose that t had an inﬁnite computable branch. The branch would corre-

spond to a decreasing chain of closed squares, the intersection of which would

be non-empty. Let x1 be a point in the intersection. Since, by construction of

the

tree,

g(x1)

≤

1 n

for

all

n,

g(x1)

=

0

and

hence

x1

would

be

a

ﬁxed

point

of f . However, by the construction—the branch being computable—the point

x1 would also be computable, contradicting the fact that f has not computable

ﬁxed point. Therefore the tree t has no inﬁnite computable branch.

4.2 From Kleene tree to Baigger f
Suppose we are given a computable tree t with no inﬁnite computable branch. This tree can be used to construct a sequence of closed intervals with a computable sequence of end-points, covering all the computable real numbers in the unit interval and for which the corresponding open intervals are pair-wise disjoint.
Using the computable function t, one can enumerate all of the maximal ﬁnite branches of the tree. Say,
b(n) = b1(n) . . . bλ(n)(n)

384 Petrus H. Potgieter

and set

Jn,1 =

b1(n) . . . bλ(n) 2λ(n)

,

b1(n) . . . bλ(n) 2λ(n)

+

1 2

Jn,m =

b1(n)

.

.

.

bλ(n) + 2λ(n)

2−m+1

,

b1(n)

.

.

. bλ(n) 2λ(n)

+

2−m

for m ≥ 2.

It remains to show that the union of the intervals Jn,m covers all the computable points Ic but not all of the unit interval I. It is easy to see that

– for every computable x ∈ Ic there exists a computable binary sequence (xn)

such that

x1 . . . xn 2n

≤

x

<

x1 . . . xn 2n

+1

for all n

and since t has no inﬁnite computable branch t(x1 . . . xℓ) = 0 for some least ℓ, in which case x ∈ ∪mJn,m where b(n) = x1 . . . xℓ; – if (xn) is an inﬁnite branch of t then, since it is not computable, for all w
we have x1x2 . . . = w1111 . . . and therefore

lim
n

x1

. . . xn 2n

+

1

∈

Jℓ,m

m

for every ℓ.

The Baigger example f can now be constructed using the intervals Jn,m and by that construction one obtains a computable f with no computable ﬁxed point, as required.

5 Conclusion
The existence of the Kleene tree can quite easily be derived from the impossibility of ensuring the existence of a computable ﬁxed point for a computable function (in both Russian and Polish senses), in two dimensions (or higher). The ingenuous constructions of Orevkov and Baigger provide a way of deﬁning a computable function with no computable ﬁxed point from the set of intervals derived from the Kleene tree, in a constructive manner. This correspondence is, perhaps, more attractive for the “working mathematician” than the elegant derivation of the result in reverse mathematics. In one dimension, any computable f : I → I does have a computable point x ∈ Ic such that f (x) = x, which can be seen by fairly straight-forward reduction ad absurdum from the assumption that this is not the case.

References
1. Pour-El, M.B., Richards, J.I.: Computability in analysis and physics. Perspectives in Mathematical Logic. Springer-Verlag, Berlin (1989)

Computable Counter-examples to the Brouwer Fixed-point Theorem 385
2. Orevkov, V.P.: A constructive map of the square into itself, which moves every constructive point. Dokl. Akad. Nauk SSSR 152 (1963) 55–58
3. Wong, K.C., Richter, M.K.: Non-computability of competitive equilibrium. Economic Theory 14(1) (1999) 1–27
4. Miller, J.S.: Degrees of unsolvability of continuous functions. J. Symbolic Logic 69(2) (2004) 555–584
5. Baigger, G.: Die Nichtkonstruktivit¨at des Brouwerschen Fixpunktsatzes. Arch. Math. Logik Grundlag. 25(3-4) (1985) 183–188
6. Shioji, N., Tanaka, K.: Fixed point theory in weak second-order arithmetic. Ann. Pure Appl. Logic 47(2) (1990) 167–188
7. Kleene, S.C.: Recursive functions and intuitionistic mathematics, Providence, R. I., Amer. Math. Soc. (1952) 679–685
8. Simpson, S.G.: Which set existence axioms are needed to prove the cauchy/peano theorem for ordinary diﬀerential equations? The Journal of Symbolic Logic 49 (1984) 783–802
9. Simpson, S.G.: Subsystems of second order arithmetic. Perspectives in Mathematical Logic. Springer-Verlag, Berlin (1999)

Appendix A: details of the construction in Section 3

The constructions should guarantee that at each stage, the function fn moves

every point of

Dn =

Cn

2−n

\

Cn

2−n

·

5 4

◦

in the direction of tn by an amount proportional to its distance to Cn 2−n . The construction of f1 with this property is trivial. We proceed to construct fn+1
from fn.

(i) Extend and modify fn to Cn+2−1 n so that every point x of

Cn+2−1 n

\

C 2−n

·

5 4

n+1

◦

is moved in the direction of tn by an amount proportional to d x, Cn+2−1 n . (ii) Modify the resulting function so that each point in

Cn+2−1 n

\

C 2−n

·

9 8

n+1

is

mapped

a

non-negative

amount

proportional

to

its

distance

to

C 2−(n+1)
n+1

in the direction of tn.

(iii) By rotation of the direction of the mapping, extend the function to

C 2−(n+1)
n+1

such that every point x of

Dn+1 =

C \ C2−(n+1)
n+1

2−(n+1)

·

5 4

n+1

◦

386 Petrus H. Potgieter

2−n

C 2−n

·

5 4

n

Cn

C 2−n n

Dn

Fig. 3. Sets used in the construction

is mapped in the direction of tn+1 by an amount proportional to

d

x,

C 2−(n+1)
n+1

.

The ﬁnal step is the only one in which we use the fact that we are working in two dimensions as this step requires the continuous (computable) rotation of a vector in the direction of tn to a vector in the direction of tn+1.
A construction is given explicitly in [5] but it should be clear from the preceding that it can be done in many diﬀerent ways. The important part of the proof is that the construction is, at each stage, extended at the boundary to “look right” from the outside. This ensures that, eventually every point is in fact moved towards one of a sequence of points that converge to the non-computable ﬁxed point (v, v) on the diagonal. The Baigger construction is a somewhat delicate construction of a function that is in fact computable but that—somehow— mimics a simple mapping of every point in I2 in the direction of (v, v).

Polynomial Iterations over Finite Fields
Mihai Prunescu1;2
1 Brain Products, Freiburg, Germany 2 Institute of Mathematics “Simion Stoilow” of the Romanian Academy
Bucharest, Romania mihai.prunescu@math.uni-freiburg.de
Abstract. Consider the following natural algorithm: given a ﬁnite ﬁeld F and a polynomial f ∈ F[x, y, z] one produces the double sequence (ai,j ) deﬁned by a0,j = ai,0 = 1 und ai,j = f (ai,j−1, ai−1,j−1, ai−1,j ). If the polynomials f are linear, self-similarity arises. On the other hand, the class of double sequences (ai,j) generated by symmetric polynomials f (x, z) over arbitrary ﬁnite ﬁelds is Turing complete.
1 Introduction
The research reported here has a strong experimental back-ground. Let F be a ﬁnite ﬁeld and f ∈ F[x, y, z] some polynomial. Iterating f one gets a recurrent double sequence (ai,j) deﬁned by a0,j = 1, ai,0 = 1 and:
ai,j = f (ai,j−1, ai−1,j−1, ai−1,j ).
If one ﬁxes a correspondence between the elements of F and a set of colours, one can draw an image corresponding to the initial matrix (ai,j)0≤i,j<n of the recurrent double sequence. Such images will be sometimes called carpets. The starting point of this research was a conjecture of Lakhtakia and Passoja [3] telling that the polynomial f (x, y, z) = x + y + z generate self-similar carpets over the prime ﬁelds Fp.
In [5] the author proved that all polynomials f (x, y, z) = x + my + z produce self-similar (fractal) images over arbitrary ﬁnite ﬁelds and classiﬁed the occurring symmetries. The proofs uses elements of algebra and number theory, but also applies modern algebraic algorithms as those by Wilf and Zeilberger. In [6] the author proved that for arbitrary polynomials in only two variables f (x, z) the recurrent double sequences interpret instances of the Halting Problems and have undecidable properties, as for exemple the property of being ultimately zero.
The goal of this extended abstract is to present all deﬁnitions and results, but only (at most) sketches of proofs. It should be seen as a complementary text to [5] and [6] and has also the goal to emphasize their contrast.
2 The linear case
Deﬁnition 1. Let Fq be an arbitrary ﬁnite ﬁeld and ﬁx an element m ∈ Fq. The matrices occurring in this section are always indexed from 0 and have elements

388 Mihai Prunescu
in Fq, if not otherwise speciﬁed. Let the prime p be the characteristic of the ﬁnite ﬁeld, q = pk for some k. Let Md = (ai,j) be the pd × pd matrix constructed following the recurence ai,0 = a0,j = 1 and ai,j = ai−1,j + m · ai−1,j−1 + ai,j−1. The matrix M1 shall be denoted by F (p, m) and called fundamental block.
Deﬁnition 2. The black and white image Id is deﬁned as follows: one tiles the compact square [0, 1] × [0, 1] in pd × pd many equal squares Si,j, and excludes the interior of Si,j if and only if ai,j = 0.
Deﬁnition 3. The self-similar set in question shall be I = lim Id. The name of the variable d is chosen to mean the depth of the recursive approximation of I. The limit operator can be understood in the sense of the Hausdorﬀ metric for compact subsets of R2.

2.1 The recurrent function

Deﬁnition 4. Let K be an arbitrary ﬁeld and the element m ∈ K be ﬁxed. We consider the function f : N × N → K recursively deﬁned by the conditions f (n, 0) = f (0, k) = 1 and:

f (n, k) = f (n, k − 1) + m · f (n − 1, k − 1) + f (n − 1, k)

for n, k ≥ 1.

Lemma 1. The function f is symmetric and satisﬁes:

min(n,k)

f (n, k) =

ma

n

a

a=0

n+k−a k−a .

Proof. The symmetry follows from the symmetry of the recurrence formula and
of the initial conditions. To compute f , use the method of generating functions, see [12]. Deﬁne the generating function An(x) = f (n, k)xk. It follows:
k≥0

An+1(x) = An(x) + xAn+1(x) + mxAn(x).

This recurrence have the solution:

An(x) =

1

n+1
(1 + mx)n.

1−x

Using that (1 + mx)n =

n k

mk xk

and

that

k≥0

gets the Lemma.

n+1

1 1−x

=

n+k k

xk ,

one

k≥0

Polynomial Iterations over Finite Fields 389

2.2 Tensor powers and the automorphism of Frobenius
In this section we prove some properties of the fundamental block F (p, m) ∈ Mp×p(Fp). Recall the notation F (p, m) = (ai,j) with i and j = 0, . . . , p − 1.
Lemma 2. The last column and the last row of F (p, m) are exactly: 1, −m, (−m)2, . . . , (−m)p−1.

This works also for m = 0.

Proof. Take k ≤ n = p − 1 and work over Fq. For a < k the term:

t(a, p − 1, k) = ma p − 1 p − 1 + k − a = ma p − 1 · p · · · · = 0,

a k−a

a

so all these terms do not contribute in Fq. For the last term one has:
t(k, k, p − 1) = mk (p − 1) . . . (p − k) = mk(−1)k k! = (−m)k. k! k!
Deﬁnition 5. Let R be some commutative ring and A = (ai,j) ∈ Ms×t(R), B ∈ Mu×v(R) two matrices. Then the tensor product A ⊗ B is a matrix in Msu×tv(R) having the block-representation (ai,jB). If A1, A2, . . . , An are arbitrary matrices, we denote the tensor term:

((. . . ((A1 ⊗ A2) ⊗ A3) . . . ) ⊗ An−1) ⊗ An.

by: A1 ⊗ A2 ⊗ · · · ⊗ An−1 ⊗ An.
For all n ≥ 1 we deﬁne the tensor power A⊗n of A inductively by: A⊗1 = A and A⊗(n+1) = A⊗n ⊗ A.

Remark 1. (Principle of Substitution) For some n ≥ 2 consider a matrix A ∈
Mn×n({0, 1}) containing at least one zero and at least two ones. Let Id be the black and white image associated to A⊗d. Then Id is the d-th step in the
transﬁnite construction of a non-trivial self-similar set I = lim Id.

Deﬁnition 6. The automorphismus of Frobenius ϕ : Fq → Fq is deﬁned by ϕ(x) = xp. This automorphism generates the Galois group G(Fq/Fp).
Lemma 3. Let F = F (p, m) be a fundamental block for some m ∈ Fq. Consider the matrix in construction:
αF βF γF ·
with α, β, γ ∈ Fq. By application of the recurrent rule one gets:

αF βF γF δF

with δ = ϕ(m)α + β + γ.

390 Mihai Prunescu

Deﬁnition 7. For a matrix A = (ai,j) over Fq, let ϕ(A) be the matrix (ϕ(ai,j)).
Theorem 1. Recall that Md is the pd × pd matrix computed by the recurrent rule over the ﬁnite ﬁeld Fq and F = F (p, m) = M1 is the fundamental block. Then for all d ≥ 1:
Md = ϕd−1(F ) ⊗ ϕd−2(F ) ⊗ · · · ⊗ ϕ(F ) ⊗ F.
Proof. The proof works by induction and is a immediate application of the Lemma 3.
Corollary 1. For all ﬁnite ﬁelds Fq and all m ∈ Fq, if the fundamental block F (p, m) contains at least a zero, the black and white image Id of Md is the d-th step in the transﬁnite construction of a non-trivial self-similar set I.
Proof. Immediate application of the Principle of Substitution.
Lemma 4. If m ∈ Fp, the fundamental block F (p, m) contains zeros if and only if m = −1. In this situation it contains in the row i = 1 exactly one zero:
a1,k = 0 ↔ Fp |= k = −(m + 1)−1.
Note: in general there are many other zeros in the fundamental block.
Proof. The element a1,k = km + (k + 1) = k(m + 1) + 1 which is zero only for k = −(m+1)−1, existing for all m = −1 in Fp. Every such k has a representative between 1 and p − 1 inclusively. If m = −1 the matrix F (p, −1) contains only the repeated element 1.

Now from Remark 1 and from the Lemma 1 the main result follows:
Theorem 2. For all primes p and all m ∈ Fp \ {−1} the black and white image Id of Md is the d-th step in the transﬁnite construction of a non-trivial selfsimilar set I. For m = −1 the set I is the full square. The Pascal Triangle modulo p (got for m = 0) and the Passoja-Lakhtakia Carpets (got for odd q = p and m = 1) are non-trivial self-similar sets.
The following example shows the step M2 for p = 3 and m = 1, a step in the construction for the celebrated Sierpinski Carpet, used in [11]. The zeros are not displayed.

111111111

1 −1 1 −1 1 −1

1 −1 1 1 −1 1 1 −1 1

111

−1 −1 −1

1 −1

−1 1

1 −1 1

−1 1 −1

1 1 1 −1 −1 −1 1 1 1

1 −1 −1

1 1 −1

1 −1 1 −1 1 −1 1 −1 1

Polynomial Iterations over Finite Fields 391

2.3 Multiplicative inverse means mirroring

For studying the groups of symmetries of the black and white image I is enough to understand the symmetries for the fundamental block F (p, m). All groups of symmetries we are looking for are subgroups of the dihedral group of symmetries D8 of the square. We start with the least symmetric case, the case of Pascal’s Triangle:

Lemma 5. If m = 0 the group of symmetries consists of two elements: the identity and the reﬂection through the ﬁrst diagonal.

Proof. In F (p, 0) for 0 ≤ i, j ≤ p − 1 :

i+j

ai,j = 0 ↔ p | f (i, j) = i

↔ i + j ≥ p.

So exactly the elements strictly below the second diagonal are 0.

Deﬁnition 8. For a matrix A we deﬁne the mirrored image ΣA using the definition of a matrix as a family of column-vectors. If A = (a1, . . . , an) then ΣA = (an, . . . , a1).
Deﬁnition 9. For m = 0 we deﬁne the operator O acting over the fundamental block F (p, m) in the following way:
For i = 0 to p − 1, one divides the row i by (−m)i.

The result is denoted by OF (p, m).

Lemma 6. For all ﬁnite ﬁelds Fq and for all m ∈ Fq \ {0} the following identity
holds: OF (p, m) = ΣF (p, m−1).

Proof. The Lemma follows from the following claims: (1) The ﬁrst row and the last column of OF (p, m) consist only of ones. (2) For every connected 2 × 2 sub-block of OF (p, m):
AB CD
is true that C = m−1B + A + D. The ﬁrst claim follows from Lemma 2 and from the deﬁnition of the operator O: one divides exactly with the elements of the last column. We prove the second claim. Let (a, b | c, d) be the corresponding elements in F (p, m). They fulﬁll the equality:

d = ma + b + c.

Using the deﬁnition of OF (p, m), we see that:

A = µa, B = µb,

392 Mihai Prunescu

C = (−m)−1µc, D = (−m)−1µd, where µ = (−m)i for some i. It follows that:
C = (−m)−1µc = (−m)−1µ(d − ma − b) = = (−m)−1µd + µa − (−m)−1µb = D + A + m−1B.
Lemma 7. The following statements follow directly from Lemma 6: 1. For all 0 ≤ i, j ≤ p − 1:
ai,p−1−j = ai,j (−m)−i, ai,j (−m)−i = ap−1−j,p−1−i(−m)j+1−p. 2. If m ∈ Fq \ {0} then: δF (p, m) = δΣF (p, m−1) = ΣδF (p, m−1).

Moreover, the matrix δF (p, m) allows two diagonal symmetries; and so all its tensor powers. 3. Given m ∈ Fq \ {0} ﬁxed, some matrix Md contains zeros if and only if M1 = F (p, m) contains zeros. If this takes places, then

deg(m/Fp)

≤

p

− 2

1 .

The last condition occurring here is quite strong and implies that there cannot
be too much elements m generating non-trivial self-similar sets in arbitrary ﬁnite ﬁelds. Look at the case F192 = F361 seen as F19[x] where x2 + 1 = 0. Encode the element ax + b in the natural number 19a + b. I do not mention both m and m−1
because they produce mirrored carpets. Also, if m has been already mentioned, I don’t mention its Frobenius m19, because it produces the same carpet. So, up
to Frobenius and multiplicative inverse, one has non-trivial self-similar carpets
over F361 if and only if m is equal with one of the following 29 elements: 0, 1, 2, 3, 4, 6, 7, 8, 9, 14, 19, 21, 35, 47, 52, 53, 56, 63, 69, 76, 78, 88, 92, 102, 130, 136, 137, 148, 168. Values of m ∈ F361 which are not itself, inverses of, or Frobenius of elements in this list generate however interesting coloured images. Over the
prime ﬁelds Fp the situation looks better:

2.4 Fp as a ﬁeld of self-similar carpets
Theorem 3. Let p be a prime and m ∈ Fp. Exactly one of the following situations arrises:
1. m = 0. In this case I is a self-similar Pascal Triangle, I is only symmetric through the ﬁrst diagonal, and the group of symmetries of I is isomorphic with S2.

Polynomial Iterations over Finite Fields 393

2. m = ±1. In this case I is a full square (for m = −1) or a nontrivial selfsimilar set (for m = 1) and the group of symmetries of I is the full dyhedral group D8 of the square.
3. p ≥ 5, m ∈ Fp \ {−1, 0, 1}. In this case I is a non-trivial self-similar set and the group of symmetries of I is generated by the reﬂexions through the diagonals of the square. This group is isomorphic with Klein’s group K4.
Proof. Let now m ∈ Fp \ {0}, let K be the group generated by the symmetries through the both diagonals (isomorphic with Klein’s group K4) and let G be the group of symmetries of I. From Lemma 7 it follows that K ≤ G ≤ D8. If m = −1 then I is the full square and trivially G = D8. If m = 1 than it follows from Lemma 7 that:
δF (p, 1) = ΣδF (p, 1),
because 1−1 = 1 so G is strictly bigger than K which already has 4 elements, hence G = D8.
The converse is easy to prove using Lemma 4.

2.5 The special case m ∈ {−2, −2−1}: Diagonal Carpets
For m = −2 one has a1,1 = 0. Mirror-symmetric: for m = −2−1 one has a1,p−2 = 0. In fact, in these cases, all the elements of odd index on the corresponding diagonal are zero!

Deﬁnition 10. Call ﬁrst odd diagonal (respectively second odd diagonal) the following set of indexes:
D+ = {(i, i) | 0 < i < p − 1 ∧ 2 | i}.
D− = {(i, j) | i + j = p − 1 ∧ 2 | i}.
Theorem 4. Let p ≥ 5 be a prime. Following statements hold: D+ consists of zeros of F (p, −2) and D− consists of zeros of F (p, −2−1). Moreover, the elements of even index on the respective diagonals are = 0.

Proof. We prove that for m = −2 ∈ Z the recurrent function f : N × N → Z deﬁned in the second section has the property f (2s + 1, 2s + 1) = 0 for all k ∈ N. This follows from the following identity:

n
(−2)a

n

a

a=0

2n − a n−a

=

(−1)s

2s s

,

0,

if n = 2s, if n = 2s + 1.

This identity can be proved with Zeilberger’s Algorithm, see [7] and [1]. In fact, after running the software from [1], one gets the recurrent formula:

4(n + 1)S(n) + (n + 2)S(n + 2) = 0,

where S(n) is the sum on the left side of the equality. Starting with S(0) = 1 and S(1) = 0 one gets the result. D− follows from the case m = −2 and the
dualism from Lemma 6. Note that the corresponding values of f (n, k) are no
more 0 in Z but become 0 if projected in Fp.

394 Mihai Prunescu

2.6 The special case m = 1: Cross-carpets

The only one fully symmetric non-trivial case (where p is odd and m = 1) is worth for a closer look. This is exactly the case of the spectacular PassojaLakhtakia Carpets, described in [3]. It is worth to notice that the inﬁnite symmetric matrix (f (n, k)) for m = 1 is known as the double sequence of the Delanoy Numbers.

Deﬁnition 11. Let us call N = {(i, j) | ai,j = 0} the set of zeros of the fundamental cell F (p, 1). The set:

C

=

{( p

−

1 , i) ;

(i,

p

−

1 )|

0

≤

i

≤

p

−

1

∧

2

| i}

22

shall be called the Cross, and S = N \ C the set of sporadic zeros. We call the elements of the cross regular zeros.

Corollary 2. If p is an odd prime, the fundamental block F (p, 1) has the following properties:
1. If 0 ≤ n, k ≤ p − 1 then ap−1,k = (−1)k and an,k = (−1)nan,p−1−k. 2. The Cross C consists of zeros of F (p, 1).

Proof. This follows from Lemma 2. combined with Lemma 6.

The primes 3, 5, 7, 11, 19 have only regular zeros in F (p, 1). 13 is the ﬁrst odd prime with sporadic zeros, followed by 17. By all other primes tryed out by the author (from 23 to 599) there are lots of sporadic zeros in the fundamental block F (p, 1).
As ﬁnal remark: the general diagonally symmetric linear polynomial ax + by + az produce over Fp carpets consisting of overlappings of several periodic motives and one self-similar carpet of type x + my + z. However, the complete characterization of their behavior is an open problem.

3 General polynomials
The situation for the general polynomials is no more algebraic or combinatoric anymore. In fact according to the property of interpolation over ﬁnite ﬁelds, all functions are polynomial; and commutative binary operations can always be represented by symmetric polynomials in two variables.
Deﬁnition 12. Consider an arbitrary ﬁnite algebra A = (A, f, 0, 1) where f : A × A → A is a binary operation and 0, 1 are two constants. We call 1 start symbol. The recurrent double sequence associated to A is a function a : N × N → A deﬁned as follows:
1 if i = 0 ∨ j = 0, a(i, j) =
f (a(i, j − 1), a(i − 1, j)) if i > 0 ∧ j > 0.
If f is commutative, the recurrent double sequence is symmetric: a(i, j) = a(j, i).

Polynomial Iterations over Finite Fields 395
Deﬁnition 13. The recurrent double sequence a(i, j) is said to be ultimately zero if in A holds:
∃ N ∈ N ∀ i, j ∈ N i > 0 ∧ j > 0 ∧ i + j > N −→ a(i, j) = 0.
Theorem 5. It is undecidable if the recurrent double sequence deﬁned by an algebra A is ultimately zero. This question remains undecidable if is restricted to the class of commutative ﬁnite algebras.
Deﬁnition 14. An instance of the Halting Problem is a pair (M, w) where M = (Σ, Q, q0, qs, ¯b, δ) is a Turing machine and w ∈ Σ∗ is an input for M . Here the tape of M is inﬁnite in both directions, Σ is the alphabet of M , Q is M ’s set of states, q0 and qs are the start state and respectively the stop state, ¯b ∈ Σ is the blank symbol, and δ : Σ × Q → Σ × Q × {R, L, S} is the transition function.
Lemma 8. To every instance (M, w) of the Halting Problem one can algorithmically associate a ﬁnite algebra A = (A, f, 0, 1) such that A ∈ Z if and only if for input w the machine M stops and after stopping the tape is cleared.
Lemma 9. To every instance (M, w) of the Halting Problem one can algorithmically associate a commutative ﬁnite algebra A = (A, f, 0, 1) such that A ∈ CZ if and only if for input w: (the machine M stops with cleared tape without having done any step in the negative side of the tape) or (the machine M makes at least one step in the negative side of its tape and the ﬁrst time when M makes such a step the tape of M is cleared).
The Lemma 9 together with the Theorem of Rice implies in fact alone the Theorem 5, but is more natural to start considering the easy Lemma 8. Both Lemmas are proved by interpreting temporally succesive tape conﬁgurations of the Turing machine M in diagonals of the recurrent double sequence, given by (ai,j) with i + j = k and k constant. To prove the Lemma 8 one needs two alternating types of diagonals. The diagonals of type 0 encode the tape conﬁgurations. The diagonals of type 1 are needed only to transmit the information between two diagonals of type 0:
b δ (δ, b) a (a, δ) c
This idea of proof is also used in the more diﬃcult situation of the Lemma 9. There we cannot use ordered pairs of symbols because of the commutativity of the algebra to construct.
Deﬁnition 15. Let Γ = ∅ be a set and ≡ be the partition of Γ × Γ consisting of the following sets: for all a ∈ Γ the singleton sets {(a, a)} and for all a, b ∈ Γ with a = b the two-element sets {(a, b), (b, a)}. Then ≡ is an equivalence relation over Γ . Consider the set of equivalence classes:
Γ · Γ = (Γ × Γ )/ ≡

396 Mihai Prunescu
which is the set of unordered pairs of elements of Γ . We denote the equivalence class of (a, b) with [a, b] and call this the unordered pair of a and b.
For the proof of Lemma 9, the alphabet of Σ ∪ Σ × Q is extended by disjoint union with two copies of itself and all letters c are encoded in special words cc c . For the simulation of the Halting Problem in recurrent double sequences, one needs eight types of successive diagonals. Only the diagonals of type zero simulates the Turing tape. The alphabets used for the diagonals of type i > 0 is always Γi = Γi−1 · Γi−1 with Γ0 = Σ ∪ Σ × Q together with its two disjoint copies. The only thing to prove is that one can symmetrically deﬁne the function f over diagonals of type seven such that one gets the successor Turing machine conﬁguration on the next diagonal, which is indeed of type zero.
References
1. Wolfram K¨opf: Hypergeometric Summation. An Algorithmic Approach to Summation and Special Function Identities. Vieweg, Braunschweig/Wiesbaden, 1998. http://www.mathematik.uni-kassel.de/ koepf/hyper.html
2. Benoit B. Mandelbrot: The fractal geometry of nature. W. H. Freeman and Company, San Francisco, 1977, 1982.
3. Dann E. Passoja, Akhlesh Lakhtakia: Carpets and rugs: an exercise in numbers. Leonardo, 25, 1, 1992, 69 - 71.
4. Dann E. Passoja, Akhlesh Lakhtakia: Variations on a Persian theme. Journal of Recreational Mathematics, 24, 1, 1 - 5, 1992.
5. Mihai Prunescu: Self-similar carpets over ﬁnite ﬁelds. Submitted. 6. Mihai Prunescu: An undecidable property of recurrent double sequences. To
appear in The Notre Dame Journal of Formal Logic, 2008. 7. Marko Petkovsek, Herbert Wilf and Doron Zeilberger: A = B. A K
Peters. Ltd, 1997. http://www.cis.upenn.edu/ wilf/AeqB.html. 8. Gordon H. Rice: Classes of recursively enumerable sets and their decision prob-
lems. Transactions of the American Mathematical Society, 74, 358 - 366, 1953. 9. N. J. Rose: The Pascal triangle and Sierpinski’s tree. Mathematical Calendar,
Releigh, N. C, Rome Press, 1981. 10. Marjorie Senechal: Quasicrystals and Geometry. Cambridge University Press,
1995. 11. Waclaw Sierpinski: Sur une courbe cantorienne qui contient une image biuni-
voque et continue de toute courbe donne. C. R. Acad. Sci, Paris, Sr. 162, 629, 1916. 12. Herbert S. Wilf: Generatingfunctionology. Academic Press, 1990, 1994. 13. Stephen J. Willson: Cellular automata can generate fractals. Discrete Applied Mathematics, 8, 1984, 91 - 99.

Simulations of Quantum Turing Machines by Quantum Multi-Counter Machines
Daowen Qiu
Department of Computer Science, Zhongshan University, Guangzhou 510275, China {E-mail:issqdw@mail.sysu.edu.cn(D.Qiu)}
Abstract. We establish a kind of quantum multi-stack machines and quantum multi-counter machines, and use them to simulate quantum Turing machines. The major technical contributions are stated as follows: (i) We deﬁne quantum multi-stack machines (abbr. QMSMs) by generalizing a kind of quantum pushdown automata (abbr. QPDAs), and the well-formedness (abbr. W-F) conditions for characterizing the unitary evolution of the QMSMs are presented. (ii) By means of QMSMs we deﬁne quantum multi-counter machines (abbr. QMCMs) whose state transition functions are diﬀerent from the quantum counter automata (abbr. QCAs) in the literature. (iii) To simulate quantum Turing machines (abbr. QTMs), we show that any given QMCM allowed to count with ±n for n > 1 can be simulated by another QMCM that counts with 0, ±1 only. (iv) We demonstrate the eﬃcient simulations of QTMs in terms of QMSMs, and show that QTMs can be simulated by QMCMs as well.
Key words: Quantum Computation; Quantum Turing Machines; Quantum Multi-Counter Machines; Quantum Multi-Stack Machines.
1 Introduction
1.1 Motivation and purpose
Quantum computing is an intriguing and promising research ﬁeld, which touches on quantum physics, computer science, and mathematics [8]. To a certain extent, this intensive attention given by the research community originated from Shor’s ﬁndings of quantum algorithms for factoring large integers in polynomial t√ime [17] and Grover’s algorithm of searching in a database of size n with only O( n) accesses [7] which could also be sped up on a quantum computer.
Let us brieﬂy recall the work of pioneers in this area. In 1980, Benioﬀ [1] ﬁrst considered that the computing devices in terms of the principles of quantum mechanics could be at least as powerful as classical computers. Then Feynman
This work was supported by the National Natural Science Foundation under Grant 90303024 and Grant 60573006, the Research Foundation for the Doctorial Program of Higher School of Ministry of Education under Grant 20050558015, and Program for New Century Excellent Talents in University (NCET) of China.

398 Daowen Qiu
[4] pointed out that there appears to be no eﬃcient way of simulating a quantum mechanical system on a classical computer, and suggested that a computer based on quantum physical principles might be able to carry out the simulation eﬃciently. In 1985 Deutsch [3] re-examined the Church-Turing Principle and deﬁned quantum Turing machines (abbr. QTMs).
Quantum computation from the complexity theoretical viewpoint was studied systematically by Bernstein and Vazirani [2] and they described an eﬃcient universal QTM that can simulate a large class of QTMs. Notably, in 1993 Yao [19] demonstrated the equivalence between QTMs and quantum circuits. More exactly, Yao [19] showed that for any given QTM, there exists a quantum Boolean circuit (n, t)-simulating this QTM with polynomial time slowdown, where n denotes the length of input strings, and t is the number of move steps before the machine stops.
In the theory of classical computation [10], both 2-stack machines, as a generalization of pushdown automata, and 2-counter machines can eﬃciently simulate Turing machines [12, 5, 10]. However, as far as the author is aware, the simulations of QTMs in terms of QMSMs and QMCMs still have not been considered. Since Turing machines, circuits, multi-stack machines, and multi-counter machines are equivalent in classical computation, we naturally hope to clarify their computing power in quantum computers. Therefore, our focuses in this article are to introduce QMSMs and QMCMs that are somewhat diﬀerent from the quantum counter automata (abbr. QCAs) in the literature [11, 18], and particularly, to simulate QTMs by virtue of these two quantum computing devices.
Indeed, in quantum computing devices, the unitarity of evolution operators is generally characterized by the W-F conditions of the local transition function of the quantum models under consideration. Bernstein and Vazirani [2] gave the W-F conditions for the QTMs whose read/write heads are not allowed to be stationary in each move. In QTMs whose read/write heads are allowed to be stationary (called generalized QTMs, as in [2]), the ﬁrst suﬃcient conditions for preserving the unitarity of time evolution were given by Hirvensalo [9], and then Ozawa and Nishimura [14] further presented the W-F conditions for the general QTMs. For the details, see ([8], p. 173). Also, Yamakami [20] gave the simple W-F conditions for multiple-tape stationary-head-move QTMs. Golovkins [6] deﬁned a kind of QPDAs and gave the corresponding W-F conditions; Yamasaki et. al. [18] deﬁned quantum 2-counter automata and presented the corresponding W-F conditions, as well.
We see that those aforementioned W-F conditions given by these authors for corresponding quantum computing devices are quite complicated. Therefore, based on the QPDAs proposed in [16] where QPDAs in [16] and [13] are shown to be equivalent, we would like to deﬁne QMSMs that generalize the QPDAs in [16], and further deﬁne QMCMs. Also, we will give the W-F conditions for these deﬁned devices. Notably, these W-F conditions are more succinct than those mentioned above. In particular, motivated by Yao’s work [19] concerning the (n, t)-simulations of QTMs by quantum circuits, we will use QMSMs to (n, t)simulate QTMs, where n denotes that the length of input strings are not beyond

Simulations of Quantum Turing Machines 399
n, and t represents that the number of move steps of QTMs (time complexity) is not bigger that t for those input strings.
1.2 Main results
According to the above analysis, we state the main contributions in this article. In Section 2, we deﬁne QMSMs by generalizing QPDAs in [16] from one-stack to muti-stack and present the corresponding W-F conditions (Theorem 1) for the quantum devices.
In Section 3, by means of QMSMs we deﬁne QMCMs that are somewhat diﬀerent from the QCAs by Kravtsev [11] and Yamasaki et al. [18]; also, the W-F conditions (Theorem 2) are given for the deﬁned QMCMs. It is worth indicating that the state transition functions in QCAs deﬁned by Kravtsev [11] and Yamasaki et al. [18] have local property, since they are deﬁned on Q × {0, 1}×(Σ ∪{#, $})×Q×{0, 1}, but their W-F conditions are quite complicated, while in QMCMs deﬁned in this article, the state transition functions are on Q × Nk × (Σ ∪ {#, $}) × Q × Nk, and consequently, the corresponding W-F conditions are more succinct (see Theorem 2). To simulate QTMs, we deal with a number of properties regarding simulations between QMCMs with diﬀerent counters and diﬀerent counts (Lemmas 1 and 2). We show that QMCMs allowed to count with 0, ±1, ±2, . . . , ±n can be simulated by QMCMs that are able to count with 0, ±1 only but need more counters.
In particular, in Section 4, we present the simulations of QTMs in terms of QMCMs with polynomial time slowdown. More speciﬁcally, we prove that for any QTM M1, there exists QMCM M2 (n, t)-simulating M1, where n denotes the length of input strings not bigger than n, and t represents that the number of move steps of QTMs is not bigger than t for those input strings. Also, we show that QMCMs can be simulated by QMSMs with the same time complexity, and by this result it then follows the eﬃcient simulations of QTMs by QMSMs.
Due to the limit space, all proofs in the article are omitted and are referred to [15].
2 Quantum multi-stack machines
Here we will deﬁne quantum k-stack machines by generalizing the QPDAs in [16] from one stack to k stacks.
Deﬁnition 1. A quasi-quantum two-stack machine is deﬁned as M = (Q, Σ, Γ, δ, Z0, q0, qa, qr) where Q is the set of states, Σ is the input alphabet, Γ is the stack alphabet, Z0 ∈ Γ denotes the bottom symbol that is not allowed to be popped, q0 ∈ Q is the initial state, and qa, qr ∈ Q are respectively the accepting and rejecting states, and transition function δ is deﬁned as follows:
δ : Q × Γ ∗ × Γ ∗ × (Σ ∪ {#, $}) × Q × Γ ∗ × Γ ∗ → C
where Γ ∗ denotes the set of all strings over Γ , and δ(q, γ1, γ2, σ, q , γ1, γ2) = 0 if and only if (i) γ1 = γ1 or Xγ1 = γ1 or γ1 = Xγ1 for some X ∈ Γ \{Z0}; and (ii)

400 Daowen Qiu

γ2 = γ2 or Y γ2 = γ2 or γ2 = Y γ2 for some Y ∈ Γ \{Z0}. In addition, if γ1 = Z0, then γ1 = Z0 or γ1 = ZZ0 for some Z ∈ Γ with Z = Z0; similar restriction is imposed on γ2.
A conﬁguration of the machine is described by |q |γ1 |γ2 , where q is the current control state, γ1 and γ2 represent the current strings of stack symbols in two stacks, respectively, and the leftmost symbol of γi represents the top stack symbol of stack i for i = 1, 2. Therefore, the rightmost symbol of γi is Z0. We denote by CM the set of all conﬁgurations of M , that is,
CM = {|q |γ1Z0 |γ2Z0 : q ∈ Q, γi ∈ Γ ∗ \ Z0, i = 1, 2}.

Let HX represent the Hilbert space whose orthonormal basis is the set X, that is, HX = l2(X). Therefore, HQ ⊗ HΓ ∗ ⊗ HΓ ∗ is a Hilbert space whose orthonormal basis is CM , that is, l2(CM ) = HQ ⊗ HΓ ∗ ⊗ HΓ ∗ . In addition, we assume that there are endmarkers # and $ representing the leftmost and rightmost symbols for any input string x ∈ Σ∗, respectively. Therefore, any input string x ∈ Σ∗ is
put on the input tape in the form of #x$, and the read head of M begins with
# and ends after reading $.
For any σ ∈ Σ ∪ {#, $} we deﬁned the time evolution operators Uσ and Uσ from HQ ⊗ HΓ ∗ ⊗ HΓ ∗ to HQ ⊗ HΓ ∗ ⊗ HΓ ∗ as follows:

Uσ(|q |γ1 |γ2 ) =

δ(q, γ1, γ2, σ, q , γ1, γ2)|q |γ1 |γ2 ,

q ,γ1,γ2

(1)

Uσ(|q |γ1 |γ2 ) =

δ∗(q , γ1, γ2, σ, q, γ1, γ2)|q |γ1 |γ2 ,

q ,γ1,γ2

(2)

where δ∗ denotes the conjugate complex number δ. By linearity Uσ and Uσ can be extended to HQ ⊗ HΓ ∗ ⊗ HΓ ∗ .
Remark 1. Uσ is the adjoint operator of Uσ. Indeed, for any (qi, γi1, γi2) ∈ Q × Γ ∗ × Γ ∗, by means of Eqs. (1,2) we have

Uσ|q1 |γ11 |γ12 , Uσ|q2 |γ21 |γ22 = δ(q1, γ11, γ12, σ, q, γ1, γ2) × δ∗(q2, γ21, γ22, σ, q, γ1, γ2)
q,γ1 ,γ2
= |q1 |γ11 |γ12 , UσUσ|q2 |γ21 |γ22 .

(3)

Deﬁnition 2. Let M be a quasi-quantum two-stack machine with input alphabet Σ. If Uσ is unitary for any σ ∈ Σ ∪ {#, $}, then M is called a quantum two-stack machine.
Now we give the well-formedness conditions for justifying the unitarity of Uσ for any σ ∈ Σ ∪ {#, $}.
Theorem 1. Let M be a quasi-quantum two-stack machine with input alphabet Σ. Then for any σ ∈ Σ ∪ {#, $}, linear operator Uσ is unitary if and only if δ satisﬁes the following well-formedness conditions:

Simulations of Quantum Turing Machines

(I) For any σ ∈ Σ ∪ {#, $},

δ(q1, γ11, γ12, σ, q , γ1, γ2) × δ∗(q2, γ21, γ22, σ, q , γ1, γ2)

q ,γ1,γ2

=

1, if (q1, γ11, γ12) = (q2, γ21, γ22), 0, otherwise.

(II) For any σ ∈ Σ ∪ {#, $},

δ(q , γ1, γ2, σ, q1, γ11, γ12) × δ∗(q , γ1, γ2, σ, q2, γ21, γ22)

q ,γ1,γ2

=

1, if (q1, γ11, γ12) = (q2, γ21, γ22), 0, otherwise.

401 (4) (5)

3 Quantum multi-counter machines

As stated above, QCAs were ﬁrst considered by Kravtsev [11], and further developed by Yamasaki et al. [18]. In this section, we introduce a diﬀerent deﬁnition of quantum k-counter machines.
Deﬁnition 3. A quasi-quantum k-counter machine is deﬁned as M = (Q, Σ, δ, q0, qa, qr) where Q is a set of states with initial state q0 ∈ Q and states qa, qr ∈ Q representing accepting and rejecting states, respectively, Σ is an input alphabet, and transition function δ is a mapping from Q × Nk × (Σ ∪ {#, $}) × Q × Nk to C, where N denotes the set of all nonnegative integer and #, $ represent two endmarkers that begins with # and ends with $, and δ satisﬁes that

δ(q, n1, n2, . . . , nk, σ, q , n1, n2, . . . , nk) = 0

(6)

only if |ni − ni| ≤ 1 for i = 1, 2, . . . , k. Furthermore, let |q |n1 |n2 . . . |nk represent a conﬁguration of M , where q ∈ Q, ni ∈ N for i = 1, 2, . . ., and let the
set CM = {|q |n1 |n2 . . . |nk : q ∈ Q, ni ∈ N, i = 1, 2, . . . , k} be an orthonormal
basis for the space HCM = l2(CM ). For any σ ∈ Σ, linear operator Vσ on HCM is deﬁned as follows:

Vσ|q |n1 |n2 . . . |nk
= δ(q, n1, n2, . . . , nk, σ, q , n1, n2, . . . , nk)|q |n1 |n2 . . . |nk (7)
q ,n1,n2,...,nk
and Vσ is extended to HCM by linearity. Deﬁnition 4. We say that the quasi-quantum counter machine M = (Q, Σ,
δ, q0, qa, qr) deﬁned above is a quantum k-counter machine, if Vσ is unitary for any σ ∈ (Σ ∪ {#, $}). Also, we deﬁne linear operator Vσ on HCM as follows:
Vσ|q |n1 |n2 . . . |nk = δ∗(q , n1, n2, . . . , nk, σ, q, n1, n2, . . . , nk)|q |n1 |n2 . . . |nk .(8)
q ,n1,n2,...,nk

402 Daowen Qiu

Remark 2. Clearly Vσ is an adjoint operator of Vσ, which can be checked in terms of the process of Remark 1, and the details are therefore omitted here.
Now we give the W-F conditions for characterizing the unitarity of Vσ. Without loss of generality, we deal with the case of k = 2.
Theorem 2. Let M be a quasi-quantum two-counter machine with input alphabet Σ. Then for any σ ∈ Σ ∪ {#, $}, Vσ deﬁned as Eq. (7) is unitary if and only if δ satisﬁes the following W-F conditions:
(I) For any σ ∈ Σ ∪ {#, $},

δ(q1, n11, n12, σ, p, n1, n2) × δ∗(q2, n21, n22, σ, p, n1, n2)

p,n1 ,n2

=

1, if (q1, n11, n12) = (q2, n21, n22), 0, otherwise.

(9)

(II) For any σ ∈ Σ ∪ {#, $},

δ(p, n1, n2, σ, q1, n11, n12) × δ∗(p, n1, n2, σ, q2, n21, n22)

p,n1 ,n2

=

1, if (q1, n11, n12) = (q2, n21, n22), 0, otherwise.

(10)

In order to simulate QTMs by QMSMs, we need some related lemmas and deﬁnitions.
Deﬁnition 5. A quasi-quantum k-counter machine M = (Q, Σ, δ, q0, qa, qr) is called to count with ±r for r ≥ 1, if its k’s counters are allowed to change with numbers 0, ±1, or ±r at each step. In this case, if |ni − ni| ≤ 1 or |ni − ni| = r for i = 1, 2, . . . , k, then δ(q, n1, n2, . . . , nk, σ, q , n1, n2, . . . , nk) = 0 may hold; otherwise it is 0. We say that the quasi-quantum k-counter machine M is quantum if for any σ ∈ Σ ∪ {#, $}, Vσ is a unitary operator on l2(CM ), where
CM = {|q |n1 |n2 . . . |nk : q ∈ Q, ni ∈ N, i = 1, 2, . . . , k}.

It is ready to obtain that Theorem 2 also holds for quantum k-counter ma-
chines with count ±r for r ≥ 1.
Theorem 3. Let M = (Q, Σ, δ, q0, qa, qr) be a quasi-quantum k-counter machine that is allowed to count with a certain ±r for r ≥ 1. Then for any
σ ∈ Σ ∪ {#, $}, Vσ deﬁned as Eq. (7) is unitary if and only if δ satisﬁes Eqs. (9,10).
Deﬁnition 6. Let M1 and M2 be quantum k1-counter machine M1 and quantum k2-counter machine M2, respectively, and, M1 and M2 have the same input alphabet Σ. For any σ ∈ Σ ∪ {#, $}, Vσ(1) and Vσ(2) deﬁned as Eq. (7)
represent the evolution operators in M1 and M2, respectively. We say that M1 can simulate M2, if for any string σ1σ2 . . . σn ∈ Σ∗,

i1,i2,...,ik1 ≥0

ik1 | . . .

i1|

qa(1)

|V$(1)

Vσ(n1)

V (1)
σn−1

. . . Vσ(11)V#(1)|q0(1)

|0

. . . |0

2

Simulations of Quantum Turing Machines 403

=

jk1 | . . .

j1|

qa(2)

|V$(2)

Vσ(n2)

V (2)
σn−1

. . . Vσ(12)V#(2)|q0(2)

|0

. . . |0

2
(11)

j1,j2,...,jk1 ≥0

where q0(i) and qa(i) denote the initial and accepting states of Mi, respectively, i = 1, 2.
For convenience, for any quantum k-counter machine M = (Q, Σ, δ, q0, qa, qr), we deﬁne the accepting probability PaMccept(σ1σ2 . . . σn) for inputting σ1σ2 . . . σn as:

PaMccept(σ1σ2 . . . σn)
=
i1 ,i2 ,...,ik ≥0

2

ik| . . .

i1|

qa

|V$M

VσMn

VM
σn−1

. . . VσM1 V#M |q0

|0

. . . |0

, (12)

where VσM is unitary operator on l2(CM ) for any σ ∈ Σ ∪ {#, $}. Lemma 1. For any quantum k-counter machine M1 that is allowed to count
with ±r for r ≥ 1, there exists quantum 2k-counter machine M2 simulating M1 with the same time complexity, where M2 is allowed to count with 0, ±1, and ±(r − 1).
Lemma 2. For any quantum k-counter machine M1 that is allowed to count with 0, ±1, ±2, . . . , ±r, then there exists a quantum kr-counter machine M2 simulating M1 with the same time complexity, where M2 is allowed to count with 0, ±1 only.

4 Simulations of quantum Turing machines
To simulate QTMs in terms of QMCMs, we give the deﬁnition of QTMs in terms of Bernstein and Vazirani [2], in which the read-write head will move either to the right or to the left at each step. Indeed, generalized QTMs can also be simulated by QMCMs, but the discussion regarding unitarity is much more complicated. For the sake of simplicity, we here consider the former QTMs.
Deﬁnition 7. A QTM is deﬁned by M = (Σ, Q, δ, B, q0, qa, qr), where Σ is a ﬁnite input alphabet, B is an identiﬁed blank symbol, Q is a ﬁnite set of states with an identiﬁed initial state q0 and ﬁnal state qa, qr = q0, where qa and qr represent accepting and rejecting states, respectively, and the quantum transition function δ is deﬁned as
δ : Q × Σ × Σ × Q × {L, R} → C.
The QTM has a two-way inﬁnite tape of cells indexed by Z and a single readwrite tape head that moves along the tape. A conﬁguration of this machine is described by the form |q |τ |i , where q denotes the current state, τ ∈ ΣZ describes the tape symbols, and i ∈ Z represents the current position of tape head. Naturally, a conﬁguration containing initial or ﬁnal state is called an initial or ﬁnal conﬁguration. Let CM denote the set of all conﬁgurations in M , and therefore HCM = l2(CM ), that is a Hilbert space whose orthonormal basis can

404 Daowen Qiu

be equivalently viewed as CM . Then the evolution operator UM on l2(CM ) can be deﬁned in terms of δ: for any conﬁguration |c ∈ CM ,

UM |c =

a(c, c )|c ,

|c ∈CM

(13)

where a(c, c ) is the amplitude of conﬁguration |c evolving into |c in terms of

the transition function δ. UM is a unitary operator on l2(CM ). As in [2], we deﬁne that QTM halts with running time T on input x if after

the T ’s step moves beginning with its initial conﬁguration, the superposition

contains only ﬁnal conﬁgurations, and at any time less than T the superposition

contains no ﬁnal conﬁguration. Therefore, we assume that the QTM satisﬁes

this requirement.

Deﬁnition 8. For nonnegative integer n, T , let M1 = (Q1, Σ1, δ1, B1, q10, q1a, q1r) be a quantum Turing machine with initial state q10, and let M2 be a quantum k-counter machine with initial state q20 and the same input alphabet Σ1 as M1. We say that M2 (n, T )-simulates quantum Turing machine M1 with polynomial time O(n, T ) slowdown, if there exist some tape symbols added in

M2, say B2, B3, . . . , Bm such that for any input x = σ1σ2 . . . σl ∈ Σ2∗ (l ≤ n), if

the computation of M1 ends with t steps (t ≤ T ), then there is nonnegative inte-

gers kl1 ,

m1 i=1

kli

kl2 +

,

.

.mi.=,21kklmsi1

and ks1 , ≤ O(n, T

ks2 , . . . ), and

,

ksm2

that

are

related

to

l

and

t,

satisfying

PaM1 (x) = PaM2 (x),

(14)

where

PaM1 (x) =

i| τ | q1a|UMt 1 |q10 |τ0 |0 2

−T ≤i≤T ,τ ∈Σ[−T,T ]Z

where τ0 is deﬁned as: τ0(j) =

σj+1, if j ∈ [0, l − 1]Z, B1, j ∈ [−T, −1]Z ∪ [l, T ]Z,

and

(15)

PaM2 (x) =

nk| . . .

n1|

q2a

|V$

V ksm2
Bsm2

.

.

.

V ks1
Bs1

n1 ,n2 ,...,nk ≥0

Vσl Vσl−1

.

.

.

Vσ1

V klm1
Blm1

.

.

.

V kl1
Bl1

V#

|q0

|0

. . . |0

2
.

(16)

One of the main result is as follows:
Theorem 4. For any QTM M1 = (Q1, Σ1, δ1, B1, q10, q1a, q1r) with initial state q10 and accepting and rejecting states q1a, q1r, and for any nonnegative integer n, t with n ≤ t + 1, there exists a quantum (2t+2)-counter machine M2 that (n, t)-simulates M1 with most slowdown O(n + t).
QTMs can be also (n, t)-simulated by quantum multi-stack machine, since
quantum k-counter machine can be simulated by quantum multi-stack machine
in terms of the following Theorem 5.
Deﬁnition 9. We say that quantum k-stack machine M2 = (Q2, Σ2, Γ2, δ2, Z0, q20, q2a, q2r) simulates quantum k-counter machine M1 = (Q1, Σ1, δ1, q10, q1a, q1r)

Simulations of Quantum Turing Machines 405

that has the same input alphabet Σ1 = Σ2 with the same time complexity in

the sense of any eﬃcient overhead, if for any input string x = σ1σ2 . . . σn ∈ Σ1∗,

we have

PaMcc1ept(x) = PaMcc2ept(x)

(17)

where

PaMcc1ept(x) = | γk| γk−1| . . . γ1| q1a| U$Uσn . . . Uσ1 U#|q10 |0 . . . |0 |2 . (18)
γ1 ,γ2 ,...,γk

Also, the (n, t)-simulations of QTMs in terms of quantum k-stack machine
can be similarly deﬁned as Deﬁnition 8, and we leave out the details here.
Theorem 5. For any given quantum k-counter machine M1 = (Q1, Σ1, δ1, q10, q1a, q1r), there exists quantum k-stack machine M2 that simulates M1 with the same time complexity.
Corollary 1. For any n, t ∈ N, and any QTM M1, there exists QMSM M2 that simulates M1 with slowdown O(n + t).

5 Concluding remarks
The unitary evolution of quantum physics requires that quantum computation should be necessarily time reversible (unitary). This makes some simulations between quantum computing devices quite complicated. Indeed, the unitarity is reﬂected by the W-F conditions. The W-F conditions for these QMSMs and QMCMs deﬁned in this paper are more succinct than the W-F conditions for QCAs introduced by Yamasaki et al. [18], but we note that the transition functions in our quantum devices employ the whole property of the symbols in the stacks or counters at each move. An issue worthy of further consideration is to give also succinct W-F conditions but yet more local transition functions for characterizing the unitarity of these QMSMs and QMCMs deﬁned in this paper. Moreover, the relationships between QMCMs in the paper and QCAs by Yamasaki et al. [18] still need to be further clariﬁed. Finally, how to improve the (n, t)-simulations of QTMs by QMCMs and QMSMs towards more general simulations and how to decrease the number of counters of QMCMs for simulating QTMs are also worth studying.
The unitarity of the above models of computation leads to the complicated W-F conditions [14, 18], while, in this article, the W-F conditions are simpler but, the transition functions lose local property. Therefore, deﬁning the above models by means of measurement-based quantum computation [21] may be one of the feasible ways to solve this problem, since it is still physically allowed.

Acknowledgment
The author would like to thank the four anonymous reviewers for invaluable comments and suggestions.

406 Daowen Qiu
References
1. P. Benioﬀ, The computer as a physical system: a microscopic quantum mechanical Hamiltonian model of computers as represented by Turing machines, J. Statist. Phys. 22 (1980) 563-591.
2. E. Bernstein and U. Vazirani, Quantum complexity theory, SIAM J. Comput. 26 (1997) 1411-1473.
3. D. Deutsh, Quantum theory, the Church-Turing principle and the universal quantum computer, Proc. Roy. Soc. London A 400 (1985) 97-117.
4. R.P. Feynman, Simulating physics with computers, Internat. J. Theoret. Phys. 21 (1982) 467-488.
5. P.C. Fischer, Turing machine with restricted memory access, Information and Control 9 (4) (1966) 364-379.
6. M. Golovkins, Quantum Pushdown Automata, in: Proc. 27th Conf. on Current Trends in Theory and Practice of Informatics, Milovy, Lecture Notes in Computer Science, Vol. 1963, Spring-Verlag, Berlin, 2000, pp. 336-346.
7. L. Grover, A fast quantum mechanical algorithms for datdbase search, in: Proc. 28th Annual ACM Symp. Theory of Computing, Philadelphia, Pennsylvania, 1996, pp. 212-219.
8. J. Gruska, Quantum Computing, McGraw-Hill, London, 1999. 9. M. Hirvensalo, On quantum computation. PhD thesis, Turku Center for Computer
Science, 1997. 10. J.E. Hopcroft and J.D. Ullman, Introduction to Automata Theory, Languages, and
Computation, Addision-Wesley, New York, 1979. 11. M. Kravtsev, Quantum ﬁnite one-counter automata, in: SOFSEM’99, Lecture
Notes in Computer Science, Vol.1725, Springer-Verlag, Berlin, 1999, pp.431-440. 12. M.L. Minsky, Recursive unsolvability of Post’s problem of ‘tag’ and other topics
in the theory of Turing machines, Annals of Mathematics 74 (3) (1961) 437-455. 13. C. Moore and J.P. Crutchﬁeld, Quantum automata and quantum grammars, The-
oret. Comput. Sci. 237 (2000) 275-306. Also quant-ph/9707031, 1997. 14. M. Ozawa, H. Nishimura, Local transition functions of quantum Turing machines,
quant-ph/9811069, 1998. 15. D.W. Qiu, Simulations of quantum Turing machines by quantum multi-stack ma-
chines, quant-ph/0501176, 2005. 16. D.W. Qiu and M.S. Ying, Characterization of quantum automata, Theoret. Com-
put. Sci. 312 (2004) 479-489. 17. P.W. Shor, Algorithm for quantum computation: discrete logarithms and factoring,
in: Proc. 37th IEEE Annu. Symp. on Foundations of Computer science, 1994, pp. 124-134. 18. T. Yamasaki, H. Kobayashi, H. Imai, Quantum versus deterministic counter automata, Theoret. Comput. Sci. 334 (2005) 275-297. 19. A.C. Yao, Quantum circuit complexity, in: Proc. 34th IEEE Symp. on Foundations of Computer science, 1993, pp. 352-361. 20. T. Yamakami, A Foundation of Programming a Multi-Tape Quantum Turing Machine, in: MFCS’99, Lecture Notes in Computer Science, Vol.1672, Springer-Verlag, Berlin, 1999, pp. 430-441. 21. R. Jozsa, An introduction to measurement based quantum computation, quantph/0508124, 2005.

Optimal Proof Systems and Complete Languages (Extended Abstract)
Zenon Sadowski
Institute of Mathematics, University of Bialystok 15-267 Bialystok, ul. Akademicka 2, Poland sadowski@math.uwb.edu.pl
Abstract. We investigate the connection between optimal propositional proof systems and complete languages for promise classes. We prove that an optimal propositional proof system exists if and only if there exists a propositional proof system in which every promise class with the test set in co-NP is representable. Additionally, we prove that there exists a complete language for UP if and only if there exists a propositional proof system such that UP is representable in it. UP is the standard promise class with the test set in co-NP.
Key words: Optimal proof systems, promise classes, complete languages
1 Introduction
Although there are many diﬀerent formal systems for proving propositional tautologies in logic textbooks, they all fall under the concept of an abstract propositional proof system (a proof system for T AU T ) introduced by S. Cook and R. Reckhow [5]. In order to compare the relative strength of diﬀerent proof systems for T AU T we use the notion of simulation [10] and the notion of psimulation [5]. A proof system for T AU T is optimal (p-optimal) if and only if it simulates (p-simulates) any other proof system for T AU T . The still unresolved problem of the existence of an optimal (p-optimal) proof system for T AU T was posed by J. Kraj´ıˇcek and P. Pudl´ak [10] in 1989.
The notion of p-simulation between proof systems for T AU T is similar to the notion of reducibility between languages. Analogously, the notion of a p-optimal proof system for T AU T should correspond to the notion of a complete language.
Informally, a class of languages is a promise class if the languages in this class are accepted by nondeterministic polynomial-time clocked Turing machines which obey special conditions (promises). Common promise classes are UP, NP ∩ co-NP, and BPP. It is still open whether there exist complete languages for these classes. The reason lies in the undecidability of the problem of whether a given nondeterministic polynomial-time Turing machine indeed obeys the promise of any of these classes. Moreover, there exist relativizations for which these classes do not have complete languages (see [8]).

408 Zenon Sadowski
Recently, O. Beyersdorﬀ [3] introduced the notion of a disjoint NP-pair representable in a given propositional proof system f . The disjointness of such a pair is expressible by a sequence of propositional tautologies with short f -proofs. He also considered the complexity class of all disjoint NP-pairs representable in a proof system f . In this paper we extend these notions to any promise class with a propositionally expressible promise. It results in the notion of a language representable in a given proof system f and in the notion of a promise class representable in f . O. Beyersdorﬀ proved [2] that the class of all disjoint NP-pairs has a complete pair if and only if there exists a proof system for T AU T in which every disjoint NP-pair is representable. We prove the analogous theorem for the class UP. Namely, UP has a complete language if and only if there exists a proof system for T AU T such that UP is p-representable in it.
It turns out that there is a close connection between optimal proof systems and complete languages for promise classes, namely, the existence of optimal proof systems implies the existence of complete languages for various promise classes (see [9]). Let us mention two exemplary results of this type. A. Razborov [14] observed that the existence of an optimal proof system suﬃces to guarantee the existence of complete disjoint NP-pairs. J. Messner and J. Tor´an showed [12] that a complete language for UP exists in case there is a p-optimal proof system for T AU T . The converses of these implications probably do not hold [6] and in this paper we address the question of just why it is so.
It seems that the promise that a Turing machine computes a proof system for T AU T , or more precisely it produces only propositional tautologies, is the hardest one among those promises which are propositionally expressible. Therefore, the suﬃcient condition for the existence of an optimal proof system should be as strong as the existence of a complete language for every promise class with a propositionally expressible promise. At present, this intuition is only supported by the result of J. Messner [11] which states that a p-optimal proof system for T AU T exists if and only if every promise function class with a test set polynomial-time reducible to T AU T has a complete function (see also [9]). The analogous theorem in the setting of promise language classes instead of promise function classes is missing. The main result from this paper, that the existence of an optimal proof system for T AU T is equivalent to the existence of a proof system for T AU T in which any promise class with the test set in co-NP is representable, may be treated as the ﬁrst step in this direction.
2 Preliminaries
We assume some familiarity with basic complexity theory and refer the reader to [1] and [13] for standard notions and for deﬁnitions of complexity classes appearing in the paper. The class of all disjoint pairs (A, B) of NP-languages is denoted by DisNP.
The symbol Σ denotes a certain ﬁxed ﬁnite alphabet throughout the paper. The set of all strings over Σ is denoted by Σ . For a string x, |x| denotes the length of x.

Optimal Proof Systems and Complete Languages 409
Given two languages L1 and L2 (L1, L2 ⊆ Σ ), we say that L1 is polynomialtime many-one reducible to L2 if and only if there exists a polynomial-time computable function f : Σ −→ Σ such that x ∈ L1 if and only if f (x) ∈ L2 holds for any x ∈ Σ .
We use Turing machines (acceptors and transducers) as our basic computational model. We will not distinguish between a machine and its code. For a Turing machine M the symbol L(M ) denotes the language accepted by M .
We consider deterministic and nondeterministic polynomial-time clocked Turing machines (P T M and N P T M for short) with uniformly attached standard clocks that stop their computations in polynomial time (see [1]). We impose some restrictions on our encoding of these machines. From the code of any polynomial-time clocked Turing machine we can easily detect (in polynomial time) the polynomial pN which is its polynomial-time bound.
Let D1, D2, D3, ... and N1, N2, N3, ... be respectively standard enumerations of all deterministic and nondeterministic polynomial-time clocked Turing machines. For any class of languages C, we say that C has an uniform enumeration if and only if there exists a recursively enumerable list of nondeterministic polynomial-time clocked Turing machines Ni1 , Ni2 , Ni3 , ... such that {L(Nik ): k ≥ 1 } = C.
We consider only languages over the alphabet Σ (this means that, for example, boolean formulas have to be suitably encoded). The symbol T AU T denotes the set (of encodings) of all propositional tautologies over a ﬁxed adequate set of connectives. Finally, ., . . . , . denotes some standard polynomial-time computable tupling function.
3 Propositional proof systems
The concept of an abstract propositional proof system, subsuming all propositional proof systems used in practice, was introduced by S. Cook and R. Reckhow [5] in the following way:
Deﬁnition 1. A proof system for T AU T is a polynomial-time computable function f : Σ −on→to T AU T .
A string w such that f (w) = α we call an f -proof of a formula α. We write f ∗ αn if and only if {αn: n ≥ 1} is a sequence of tautologies with polynomialsize f -proofs. A polynomially bounded proof system for T AU T (which allows short proofs to all tautologies) exists if and only if NP=co-NP (see [5]).
Proof systems are compared according to their strength using the notion of simulation and the presumably stronger notion of p-simulation.
Deﬁnition 2. (Kraj´ıˇcek, Pudl´ak) Let h, h be two proof systems for T AU T . We say that h simulates h if there exists a polynomial p such that for any x ∈ T AU T , if x has a proof of length n in h , then x has a proof of length ≤ p(n) in h.

410 Zenon Sadowski
Deﬁnition 3. (Cook, Reckhow) Let h, h be two proof systems for T AU T . We say that h p-simulates h if there exists a polynomial-time computable function γ : Σ −→ Σ such that for every x ∈ T AU T and every w ∈ Σ , if w is a proof of x in h , then γ(w) is a proof of x in h.
In other words, γ translates h -proofs into h-proofs of the same formula. The notions of an optimal proof system for T AU T and a p-optimal proof system for T AU T were introduced by J. Kraj´ıˇcek and P. Pudl´ak [10].
Deﬁnition 4. A proof system for T AU T is optimal (p-optimal) if and only if it simulates (p-simulates) any proof system for T AU T .
We will study the problem of the existence of an optimal proof system and the problem of the existence of a p-optimal proof system from computationalcomplexity perspective.
4 Promise classes representable in a proof system
A nondeterministic polynomial-time clocked Turing machine which is the computational model of a given promise (semantic) class should obey the special condition, called the promise of the class. It can be illustrated by an example of the class UP. We call a nondeterministic Turing machine categorical or unambiguous if it has the following property: for any input x there is at most one accepting computation. We deﬁne UP={L(Ni): Ni is categorical}.
Let T be any formal theory whose language contains the language of arithmetic. We say that T is ”reasonable” if and only if T is sound (that is, in T we can prove only true theorems) and the set of all theorems of T is recursively enumerable. Let N be any N P T M . The notation T ”N is categorical” means that the ﬁrst order formula expressing the categoricity of N is provable in T . We say that UP is representable in T if and only if for any A ∈ UP there exists an N P T M N such that T ”N is categorical”. J. Hartmanis and L. Hemachandra [8] proved that UP has a complete language if and only if it has an uniform enumeration (see also [4]). It follows from Naming Lemma [7] that, the existence of a uniform enumeration of UP is equivalent to the existence of a ”reasonable” theory T such that UP is representable in T (see also [8]). Therefore, the problem of the existence of a complete language for UP can be characterized in terms of a uniform representability of UP in a ﬁrst order arithmetic theory T .
In this section we show that this problem can be also characterized in terms of a nonuniform representability of UP in a propositional proof system. In this case the promise of the class is expressed as the sequence of propositional tautologies with short proofs. We begin with the introduction of the necessary machinery.
Following J. Messner’s approach [11], we deﬁne promise classes in a very general way. A promise R is described as a binary predicate on the Cartesian product of the set of all N P T M s and the set of all strings, i. e. , R(N, x) means that N obeys a promise R on input x. An N P T M N is called an R-machine if and only if N obeys R on any input x ∈ Σ . For a given promise predicate R

Optimal Proof Systems and Complete Languages 411
we deﬁne the class of languages CR = {L(N ): N is an R-machine} and call it the promise class generated by R.
Deﬁnition 5. (Messner) A class of languages C is called a promise class if and only if C = CR for some promise predicate R.
The following notion of the test set for a promise class CR serves as a tool for estimating the complexity of the promise R of this class.
Deﬁnition 6. By the test set for a promise class CR we mean the set TR = { N, 0n, 0pN (n) : n is a natural number, N is an N P T M such that R(N, x) holds for any x such that |x| = n}
The notion of the test set for CR corresponds to the notion of the generic and length-only dependent test set from [9].
We are especially interested in the situation when the fact that a given N P T M is an R-machine can be expressed propositionally, as a sequence of propositional tautologies. It can be done only when the promise R has an appropriate complexity.
To any N P T M N we will assign the set DRN = { α1N , α2N , α3N ,... } of propositional formulas such that αnN is a propositional tautology if and only if R(N, x) holds for any x such that |x| = n. So, for any N P T M N it holds: N is an R-machine if and only if DRN ⊂ T AU T .
It should be possible to construct the formulas αnN for diﬀerent sets DRN , corresponding to diﬀerent N P T M s, easily and in an uniform manner. This leads to the following deﬁnitions:
Deﬁnition 7. By a propositional description of a promise R we mean a set DR = {αnN : N is an N P T M , n is a natural number} of propositional formulas fulﬁlling conditions (1) – (3):
(1) Adequacy: αnN is a propositional tautology if and only if R(N, x) holds for any x such that |x| = n.
(2) Uniform constructibility: There exists a polynomial time computable function f such that for any N P T M N and for any n natural
f ( N, 0n, 0pN (n) ) = αnN
(3) Local recognizability: For any ﬁxed N P T M N , the set DRN = {α1N , α2N , α3N , ...} is in P.
Deﬁnition 8. We say that a promise R is propositionally expressible if and only if there exists a propositional description of R.
The next lemma will be needed in Section 5 in the proof of the main result of the paper.

412 Zenon Sadowski
Lemma 1. Any promise R such that the class CR possesses the test set TR in co-NP is propositionally expressible.
Let R be a propositionally expressible promise and let DR = {αnN : N is an N P T M , n is a natural number} be its propositional description. Let h be a proof system for T AU T .
Deﬁnition 9. A language A is weakly DR-representable in h if and only if there exists an N P T M K such that conditions (1) – (2) are fulﬁlled:
(1) L(K) = A (2) h ∗ αnK
Deﬁnition 10. A language A is strongly DR-representable in h if and only if there exists an N P T M K such that conditions (1) – (2) are fulﬁlled:
(1) L(K) = A (2) There exists a polynomial time algorithm that on input 0n produces an h-
proof of αnK , for any n natural.
Finally, we have the following deﬁnitions:
Deﬁnition 11. A promise class CR is representable in h if and only if there exists a propositional description DR of R such that any language A ∈ CR is weakly DR-representable in h.
Deﬁnition 12. A promise class CR is p-representable in h if and only if there exists a propositional description DR of R such that any language A ∈ CR is strongly DR-representable in h.
Now we present the above mentioned characterization of the problem of the existence of a complete language for UP.
Theorem 1. There exists a complete language for UP if and only if there exists a propositional proof system h such that UP is p-representable in h.
Proof. (Sketch) (i) → (ii) The existence of the desired propositional proof system h follows from the existence of a uniform enumeration of the class UP.
(ii) → (i) Assume that there exists a propositional proof system h such that UP is p-representable in it. There exists a propositional description DR = {αnN : N is an N P T M , n is a natural number} of the promise of UP such that any language A ∈ UP is strongly DR-representable in h.
The following language L is the desired UP complete language.
L = { N, x, α|Nx|, P roof, 0pN (|x|) : x ∈ L(N )}
where N is an N P T M , pN is the polynomial that bounds the running time of N , x is a string, α|Nx| is a propositional formula from the propositional description of the promise of UP, P roof is an h-proof of α|Nx|, 0pN (|x|) is the sequence of zeros (padding).

Optimal Proof Systems and Complete Languages 413
5 Main results
J. Messner [11] considered the family of all proof systems for T AU T as a promise function class. He proved that a p-optimal proof system exists if and only if any promise function class with a test set polynomial-time reducible to T AU T possesses a complete function. Our intention was to ﬁnd an analogous theorem in the setting of promise language classes. In our results from this section we characterize the existence of optimal proof systems in terms of a nonuniform presentability of promise classes in a proof system. For most promise classes, having a complete language and a uniform enumeration (a uniform representation in an arithmetic theory) are equivalent. Similarly, it seems that for promise classes a nonuniform p-presentability in a proof system is close to the possession of a complete language.
In our chracterization of the problem of the existence of optimal and poptimal proof systems the families of all easy and all NP-easy subsets of T AU T play a very important role.
Deﬁnition 13. By an easy subset of T AU T we mean a set A such that A ⊂ T AU T and A ∈ P.
Deﬁnition 14. By an NP-easy subset of T AU T we mean a set A such that A ⊂ T AU T and A ∈ NP.
The next lemma shows that for every easy subset of T AU T there exists a proof system in which tautologies from this set have short and easily constructible proofs.
Lemma 2. (Messner, Tor´an [12]) If A is an easy subset of T AU T then there exists a proof system f : Σ −on→to T AU T and a polynomial-time computable function t that on input α produces f -proof of α, for any tautology α in A. That is for every α ∈ A, f (t(α)) = α.
Let us proceed to the main results of the paper.
Theorem 2. Statements (i) - (iii) are equivalent: (i) There exists an optimal propositional proof system. (ii) There exists a propositional proof system in which any promise class with
the test set in co-NP is representable. (iii) There exists a propositional prof system in which the class of all NP-easy
subsets of T AU T is representable.
The previous result can be translated to the deterministic case in the following way:
Theorem 3. Statements (i) - (iii) are equivalent: (i) There exists a p-optimal propositional proof system. (ii) There exists a propositional proof system in which any promise class with
the test set in co-NP is p-representable.

414 Zenon Sadowski
(iii) There exists a propositional prof system in which the class of all easy subsets of T AU T is p-representable.
We proved [15] that there exists a p-optimal proof system for T AU T if and only if the class of all easy subsets of T AU T is uniformly enumerable. It can be otherwise stated thus: there exists a p-optimal proof system for T AU T if and only if there exists a ”reasonable” arithmetic theory T such that the class of all easy subsets of T AU T is representable in it (see [7]). In this paper we replaced representability in an arithmetic theory by representability in a proof system, in the characterization of the existence of a p-optimal proof system in terms of easy subsets of T AU T . It is typical for proof complexity that an arithmetic theory coincides with a proof system for T AU T and the latter is a nonuniform version of the former.
References
1. J. Balcazar, J. D´ıaz and J. Gabarr´o, Structural Complexity I (Springer-Verlag, Berlin, 1995).
2. O. Beyersdorﬀ, Tuples of disjoint NP-sets, Thechnical Report TR 05-123, Electronic Colloquium on Computational Complexity, 2005.
3. O. Beyersdorﬀ, Disjoint NP-Pairs and Propositional Proof systems, PhD Thesis, Humbold-Universit¨at zu Berlin, July 2006.
4. H. Buhrman, S. Fenner, L. Fortnow and D. van Melkebeek, Optimal proof systems and sparse sets, Proc. Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science 1770, Springer - Verlag, Berlin, 2000.
5. S. Cook and R. Reckhow, The relative eﬃciency of propositional proof systems, Journal of Symbolic Logic 44 (1979) 36-50.
6. C. Glasser, A. L. Selman, S. Sengupta and L. Zhang, Disjoint NP-pairs, SIAM Journal on Computing, 33(6), (2004) 1369 –1416.
7. J. Hartmanis, Independence Results about Context-Free Languages and Lower Bounds, Technical Report, TR 84-606, Department of Computer Science Cornell University, May 1984.
8. J. Hartmanis and L. Hemachandra, Complexity classes without machines: On complete languages for UP, Theoretical Computer Science 58(1988) 129-142.
9. J. K¨obler, J. Messner and J. Tor´an, Optimal proof systems imply complete sets for promise classes, Information and Computation 184 (2003) 71 – 92.
10. J. Kraj´ıˇcek and P. Pudl´ak, Propositional proof systems, the consistency of ﬁrst order theories and the complexity of computations, Journal of Symbolic Logic 54 (1989) 1063-1079.
11. J. Messner, On the simulation order of proof systems, PhD Thesis, Universit¨at Ulm, December 2000.
12. J. Messner and J. Tor´an, Optimal proof systems for Propositional Logic and complete sets, in: Proc. 15th Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science 1373 (Springer,Berlin,1998) 477-487.
13. C. Papadimitriou, Computational Complexity, (Addison - Wesley, 1994). 14. A. Razborov, On provably disjoint NP-pairs, Technical Report 94–006, Electronic
Colloquium on Computational Complexity, 1994. 15. Z. Sadowski, On an optimal propositional proof system and the structure of easy
subsets of TAUT, Theoretical Computer Science, 288 (1), 2002, 181 – 193.

On the Complexity of Computing Winning Strategies for Finite Poset Games
Michael Soltys and Craig Wilson
Department of Computing and Software McMaster University
1280 Main Street West, Hamilton, Ontario, L8S 4K1, Canada soltys@mcmaster.ca
Abstract. We study the complexity of computing winning strategies for poset games; these are two player games on ﬁnite partially ordered sets. Given a poset game, we consider the corresponding language of those “board conﬁgurations” from which the ﬁrst player has a winning strategy. While it is reasonably clear that such a language is in PSPACE (as the answer can be determined by evaluating a quantiﬁed boolean formula), we give a simple and direct proof of this fact by reducing general poset games to the game of geography. We also show how to reason about poset games in Skelley’s theory W11 for PSPACE reasoning—we show that this theory can formalize the “strategy stealing argument” and show that the ﬁrst player has a winning strategy in the poset game Chomp. More than anything, this paper is an invitation to consider the problem whether poset games are PSPACE complete, and/or show under what conditions computing a winning strategy can be done eﬃciently.
Key words: Finite games, posets, PSPACE, proof complexity.
1 Introduction
A partially ordered set (a poset) is a set U together with an ordering relation on its elements, where is a subset of U × U . The relation must satisfy the following conditions: (1) if a b, then b a (anti-symmetry), and (2) if a b and b c, then a c (transitivity). Not all elements are necessarily comparable, in that there may be elements a, b such that a = b, where a b and b a. When two elements are incomparable, we write a||b.
Given a poset (U, ), a poset game (A, ) on (U, ) is played as follows: at ﬁrst A := U . Then, two players take turns making moves. On each move, a player picks an element x ∈ A, and removes all the elements y ∈ A such that x y. The player who is unable to move because A = ∅ loses.
An example of a poset game is Chomp, ﬁrst detailed by Gale in [1]. A game of Chomp is played on a “chocolate bar” divided into individual squares, with the bottom-left square being “poisoned”. This is usually represented by a grid of m rows and n columns, with the poisoned square residing at position (1, 1) (see Fig. 1). Two players take turns breaking oﬀ pieces of the chocolate by selecting a

416 Michael Soltys and Craig Wilson
square (i, j) from among the remaining squares and deleting all the squares (k, l) such that i ≤ k and j ≤ l. The game ends when a player selects the poisoned square. The player who does so loses. It is easy to show, by a “strategy-stealing argument”, that player 1 always wins ([1]). To transform Chomp to a poset
Fig. 1. An example 4 × 5 Chomp grid
game we delete the lower-left square, so now the player who ends up without any chocolate to chomp loses. Let Barm,n := {(i, j)|1 ≤ i ≤ m, 1 ≤ j ≤ n} − {(1, 1)}, and (i, j) (k, l) if and only if (i ≤ k ∧ j < l) ∨ (i < k ∧ j ≤ l). So, for any m, n we have the poset game (Barm,n, ).
Recall that PSPACE is the class of languages decidable in polynomial space in the length of the input on a Turing machine. A language L is PSPACE complete if it is in PSPACE, and for every language L′ in PSPACE, there exists a polynomial time function f : Σ∗ −→ Σ∗, such that x ∈ L′ ⇐⇒ f (x) ∈ L. See [2] or [3] for more background on PSPACE.
2 From Posets to Geography
In this section we present a polynomial time reduction from poset games to Geography. In order to study the complexity of the reduction, we assume that the posets are ﬁnite (i.e., U is ﬁnite). However, the reduction itself still works on inﬁnite posets, and yields inﬁnite instances of the game of Geography.
The game of Geography is played as follows: a directed graph is given (it does not have to be acyclic), and a starting node s is speciﬁed. The ﬁrst player selects s, then the second player selects an outgoing edge of s, and takes that edge to another node. The two players then traverse the graph, alternatively selecting outgoing edges of the current node. The player who is forced to revisit a node, or has no outgoing edges to select from, loses. It is a well known fact that Geography is PSPACE-complete (see [2] or [3]).
Let Posetgames = { (U, ) : player 1 has a winning strategy} and let Geography = { (G, s) : player 1 has a winning strategy}. Here (U, ) and (G, s) are the encodings of (U, ) and (G, s), respectively. Note that s is the speciﬁed starting node for the game of Geography on the graph G, so the game starts by player 1 selecting node s, then player 2 selects an edge out of s to a new node n. This is in contrast to poset games, where any element x ∈ U can be selected on the ﬁrst turn.

On the Complexity of Computing Winning Strategies 417
In the following paragraphs we will show how to transform (U, ) into (G = (V, E), s). More precisely, in what follows we describe a function f such that (U, ) →f (G = (V, E), s) , where is described as a list of pairs. Note that such a list has length at most O(|U |2 log(|U |)), and f can be computed in logarithmic space in | (U, ) |.
For every element x of U we have a node in V , and for every pair x, y ∈ U such that x y, we put the directed edge (y, x) in E. For every pair x, z ∈ U where x||z, we put (x, z) and (z, x) in E. This is not enough to simulate the poset game on G, however, because we have to keep track of all the elements of U that have been eliminated, that is, we need to keep track of A.
We are going to accomplish keeping track of A as follows: every time player 2 makes a move to a comparable node, player 1 can challenge that move. What is this challenge? It is player 1’s claim that player 2 moved to a node y such that x y and node x has already been visited. As well, we must ensure that challenging a legal move, or even one’s own move yields no beneﬁt to the challenger.
To be able to do these challenges, we are going to add four diﬀerent auxiliary nodes {x1, x2, x3, x4} to each node x ∈ V , as represented in Fig. 2. The edges are placed as follows: for each outgoing edge of x to a node z (comparable or incomparable to x), we have the edge (x3, z). For each incoming edge from a comparable node y such that x y, we add the edge (y, x1). Finally, we have the edges (x1, x), (x, x2), (x2, x3), (x2, x4), and (x4, x1).
Fig. 2. Reduction gadget: each x ∈ U is represented by ﬁve nodes in G.
We repeat this process for every x ∈ U . Finally, we add two more nodes, s, s′, with an edge from s to s′, and edges from s′ to every other non-auxiliary node. These nodes are required because in a poset game (U, ) we can select any x ∈ U to start the game, whereas in Geography the ﬁrst player must select node s on their ﬁrst turn. To emulate the free choice of the poset game, we have s as the initial node, which means player 2 must move to s′, then player 1 can select any non-auxiliary node in V . It is clear that this simple construction can be carried out in polynomial time (in fact, logarithmic space).
It is obvious that the Geography game on G mirrors a poset game on (U, ) if it is played correctly, in the sense that we never go to a node that corresponds to an element in U that was removed at a previous step. As mentioned previously,

418 Michael Soltys and Craig Wilson
the auxiliary nodes enable players to challenge each other’s moves. We examine below what occurs during these challenges.
Legitimate Challenge: Here player 2 moves to a node y where x y and node x was visited in the past. This means that player 2 moves to a y ∈/ A. Then player 1 challenges player 2 as follows: he moves from y to the challenging node x1 of x. Player 2 is now forced to revisit x, and loses. This ensures that players can only move to legal nodes.
False Challenge: What if a player challenges a legal move? That is, if player 2 moved to node y which was in A? Then the following happens: player 1 challenges player 2 by moving to node x1, and now player 2 moves to x and nothing happens as x has not been visited before. Then player 1 is forced to move to x2, and then player 2 moves to x4, and player 1 is now forced to move to x1, and thereby revisit a node, and player 1 loses. This not only shows that challenges to legal nodes yield no beneﬁt, but also challenges to incomparable nodes.
Self-Challenge: The ﬁnal type of challenge occurs when a player challenges their own move. Suppose player 2 arrives at node x without a challenge, i.e., directly through one of the incoming edges, then player 1 moves to x2. Now player 2 has a choice to move to x4, as if player 2 were going to do a counterchallenge. If indeed player 2 moves to x4, then player 1 moves to x1, and player 2 is forced to revisit node x, and player 2 loses. Thus, no gains are made if a player challenges their own move.
Therefore, if player 2 arrives at node x directly (without a challenge) he must continue (after the move of player 1) from x2 to x3, and then player 1 is free to select an outgoing edge from x3 to another node z. The above shows that there is a (correct) polynomial time reduction from Posetgames to Geography. As Geography itself is in PSPACE, this gives us that Posetgames ∈ PSPACE as well. Of course, this was expected as the existence of a winning strategy for a given “board conﬁguration” can be expressed as the satisﬁability of a quantiﬁed Boolean formula. On the other hand, we have given a simple and direct proof by reducing to Geography. The interesting (open) question is whether Posetgames is PSPACE-complete, and to characterize the poset games for which a winning strategy can be computed eﬃciently1.
3 W11 proves the existence of a strategy for Chomp
In this section we show how to extract a PSPACE algorithm for computing a winning strategy for Chomp from a W11-proof2 of the existence of a winning
1 For example, consider a chomp conﬁguration in the shape of an “L”. That is, we have the ﬁrst column and last row, with their intersection containing the poisoned square. Then as long as the two arms of the “L” have a diﬀerent length, the ﬁrst player has the following (simple) winning strategy: the ﬁrst player chomps the longer arm to be the same length as the shorter, and then copies symmetrically the moves of the second player on the opposite arm, to force the second player to be left with the poisoned square.
2 The system W11 was introduced in [4].

On the Complexity of Computing Winning Strategies 419
strategy3. We hope that a program consisting in formalizing proofs of existence of winning strategies in weaker and weaker fragments of Bounded Arithmetic can yield better algorithms (i.e., algorithms of lower complexity) for computing winning strategies for poset games.
W11 is a logical theory that captures PSPACE reasoning. It extends V1 (which captures polynomial time, the class P) presented by Cook and Nguyen in [5], but W11 is designed to reason over sets of strings, which themselves encode sets of sets of elements. W11 is a three-sorted (“third-order”) predicate calculus with free and bound variables of three sorts (the bound variables are given in parenthesis): a, b, c, . . . (x, y, z, . . .) for the ﬁrst sort, intended to denote natural numbers encoded in unary, A, B, C, . . . (X, Y, Z, . . .) for the second sort, intended to denote ﬁnite binary strings, and A, B, C, . . . (X, Y, Z, . . .) for the third sort, intended to denote sets of strings, and in particular functions from strings to strings.
The third order language we use is L3A = [0, 1, +, ·, | · |2, ∈2, ∈3, ≤, =] where 0, 1 are numbers, +, · is addition and multiplication of numbers, | · |2 denotes length of strings (note that size of third sort objects is not there), i ∈2 X is equivalent to X(i) (i.e., it is true if the i-th bit is turned on), X ∈3 X denotes the string X in X (we may omit the subscript 2 or 3 if it is clear from the context which type it is), and ≤, = are used to compare numbers (there is no equality symbol for the second and third sort; equality for these objects can be deﬁned in L3A). We are interested in sets of pairs deﬁning functions, and so we use the notation X(X) = Y , which formally denotes X, Y ∈ X. Our functions are going to be ﬁnite, thus they shall be deﬁned for X, Y such that |X|, |Y | ≤ n for some number term n. Of course, the intention is that we shall have a “strategy function” S(C1) = C2 which on conﬁguration C1 produces a conﬁguration C2.

Table 1. The set 2-BASIC

B1. x + 1 = 0

B7. (x ≤ y ∧ y ≤ x) → x = y

B2. (x + 1 = y + 1) → x = y B8. x lex + y

B3. x + 0 = x

B9. 0 ≤ x

B4. x + (y + 1) = (x + y) + 1 B10. x ≤ y ∨ y ≤ x

B5. x × 0 = 0

B11. x ≤ y ↔ x < y + 1

B6. x × (y + 1) = (x × y) + x B12. x = 0 → ∃y ≤ x(y + 1 = x)

L1. X(y) → y < |X|

L2. y + 1 = |X| → X(y)

SE. [|X| = |Y | ∧ ∀i < |X|(X(i) ↔ Y (i))] → X = Y

The theory W11 consists of axiom B1–B12 and L1, L2 (listed in table 1, from [5]) as well as two comprehension axioms (or rather axiom schemes) given below. We let ΣiB be the set of formulas over L3A containing formulas with arbitrarily
3 In fact, our method works for any poset game to which we can apply the “strategy steeling argument,” namely to any poset game with a supremum.

420 Michael Soltys and Craig Wilson

many bounded ﬁrst and second-order quantiﬁers, and exactly i alternations of third-order quantiﬁers. The purpose of the comprehension axioms is to allow the deﬁnition of new strings and functions (from strings to strings):

∃Y ≤ t x, X ∀z ≤ s x, X φ x, X, X, z ↔ Y (z) (∃Y) ∀Z ≤ s x, X φ x, X, X, Z ↔ Y (Z)

Σ0B-2COMP Σ0B-3COMP

In each of these schemes φ ∈ Σ0B is subject to the restriction that neither Y nor Y (as appropriate) occurs free in φ. Y (Z) abbreviates Z ∈3 Y, and similarly for Y (z).
Following [4, theorem 4] we know that if we can show that W11 proves the existence of a winning strategy for the ﬁrst player, where the formula asserting the existence of this strategy is in a proper syntactic form (i.e., it is a Σ1Bformula), then we can conclude directly that the strategy can be computed in
PSPACE. This is what we do next.
Any conﬁguration of chomp on an n × m board can be represented with a binary string X ∈ {0, 1}n+m. Following a suggestion of [6], a conﬁguration can
be represented with a string of 0s and 1s of length (n + m), where there are
exactly n 1s and m 0s. In this scheme, a board conﬁguration can be seen as a
path from the upper-left corner to the lower-right corner (the poisoned square
is in the lower-left corner), where we are only allowed to move right or down.
Reading the string left to right, every time we see a 0 we move right, and
every time we see a 1 we move down. The original conﬁguration would be
000000011111, and the ﬁnal conﬁguration would be the same but with all the 1s
moved to the left. Whoever moves the game into the ﬁnal conﬁguration loses. A
possible intermediate conﬁguration is given by Fig. 3. Each move is a “chomp”,

Fig. 3. A chomp board; it is represented by 011001000101.
which consists of picking a square, from among the surviving squares (i.e., to the left of the thick line in the picture above), and chomping oﬀ all the squares to the right and up. This means, that each move of chomp consists in moving one or more 1s one or more positions to the left, without ever overtaking the 1s in front.
All this can be expressed easily in W11. We are going to provide a formula Φ(X, n, m) which will assert that X is a valid chomp game on a n × m board. Here X is a long string, consisting of n × m many segments (the longest possible

On the Complexity of Computing Winning Strategies 421

chomp game, where each player takes just one square at a time) of length (n+m) each. We denote the i-th segment by X[i] (all valid notation in the language of W11). Φ is the conjunction of three formulas: φinit, φﬁnal, and φmove. φinit asserts that X[1] is the initial conﬁguration, i.e.
φinit(X[1], n, m) = ∀i ≤ (n + m)((i > m) → X[1](i) = 1)
φﬁnal asserts that X[n×m] is the ﬁnal conﬁguration:
φﬁnal(X[n×m], n, m) = ∀i ≤ (n + m)((i > n) → X[n×m](i) = 0)

φmove asserts that each segment of X can be obtained from one legal move on the previous segment:

φmove(X, n, m) = ∀i < (n × m)(X[i] “yields” X[i+1])
In order to deﬁne the “yields” portion of φmove, we ﬁrst take the high-level approach of determining what square was played between two consecutive conﬁgurations X[i] and X[i+1], and then express the conditions with W11 formulas. Note that for the remainder of this paper we will “invert” our coordinate system for Chomp conﬁgurations, as now the “origin” (1, 1) is the top-right square, while (m, n) is the bottom-right square.
First, we notice that in our string representation of conﬁgurations, a 0 occurring to the left of at least a single 1 corresponds to a column of playable squares, the height of which is the number of 1s to the right of the 0. Thus, a square (j, k) can be played on X[i] if and only if in reading X[i] from left to right, the kth 0 is encountered before the jth 1. To express this formally, we introduce two functions F0 and F1. F0 is deﬁned as
F0 (a, b, X) = c ↔ numones (1, X(a : c)) = b

or, the position in the string X where, starting from (and including) position a, there are b 0s. F1 is deﬁned analogously for b 1s:
F1 (a, b, X) = c ↔ |X(a : c)| − numones (1, X(a : c)) = b

Note that we deﬁne F0 (a, 0, X) and F1 (a, 0, X) to be 0 for any a, X. Also, if we reach the end of X before encountering b0s or 1s, we deﬁne F0 (a, b, X) and F1 (a, b, X) to be |X|. We now restate the conditions for the existence of playable square as a formula using these functions. This formula is the ﬁrst of many sub-formulas which will comprise the ﬁnal “yields” formula. Each of these sub-formulas will be marked with a label ψi.
Lemma 1 A square (j, k) can be played on a conﬁguration X if and only if

F0(1, k, X) < F1(1, j, X)

(ψ1)

422 Michael Soltys and Craig Wilson

Proof. We will argue by contradiction. Suppose square (j, k) exists on conﬁgu-

ration X, but F0(1, k, X) > F1(1, j, X) - note that we cannot have F0(1, k, X) = F1(1, j, X) as a position in X[i] can’t simultaneously be 0 and 1. This means that the kth 0 occurs after that jth 1, which in turn means that row j terminated

before column k. Thus, there are no squares beyond column k − 1 in row j, so

square (i, j) does not exist in X, and cannot be played.

⊓⊔

Now that we can identify playable squares, we need to determine the conﬁgurations which result from playing these squares. In general, playing a square (j, k) shifts at least a single 1 behind the 0 corresponding to the move, to delimit the row being shortened or eliminated. If a move aﬀects multiple rows, then multiple 1s will be shifted over. In order to ﬁgure out the substring of X which is aﬀected by playing a square (j, k), we calculate two values p and q which mark the beginning and ending of the substring (inclusive):

p = F0(1, k − 1, X[i]) + 1 q = F1(p, j, X[i])

(ψ2) (ψ3)

Intuitively, (p − 1) marks the location of the (k − 1)th 0, so p forms the left boundary of the substring. q is the position of X[i] in which we have seen j 1s
starting from p, so it designates the right boundary. Finally, to arrive at the new conﬁguration X[i+1] we replace X[i](p : q) with 1j0(q−p−k+1), i.e., the number of
1s corresponding to the number of rows aﬀected, then the number of 0s indicating
the new diﬀerence in row lengths. In order to express this new substring as a
formula we break it down into several sub-formulas. First, positions that are
outside the range of p and q remain unchanged:

(r < p) → (X[i+1](r) = X[i](r)) (r > q) → (X[i+1](r) = X[i](r))

(ψ4) (ψ5)

Next, for positions between p and q we assign values based on their placement relative to the 0 associated with the move. Positions before the 0 hold 1s aﬀected by the move, while positions after contain 0s.

(r < p + j) → (X[i+1](r) = 1) (r ≥ p + j) → (X[i+1](r) = 0)
(p ≤ r ≤ q) → (ψ6 ∧ ψ7)

(ψ6) (ψ7) (ψ8)

We now put formulas ψ4, ψ5 and ψ8 together to create one formula describing the changes between conﬁgurations X[i] and X[i+1]:

(∀r ≤ |X|)(ψ4 ∧ ψ5 ∧ ψ8)

(ψ9)

Finally, we combine formulas ψ2 and ψ3 with existential quantiﬁcation to describe the existence of valid p and q:

(∃p < |X[i]|)(∃q ≤ |X[i]|)(ψ2 ∧ ψ3)

(ψ10)

On the Complexity of Computing Winning Strategies 423

Thus, if we take the formula ψ9 for describing legal changes between conﬁgurations and combine it with formula ψ1 for the existence of a playable square, as well as ψ10 for the existence of values for p and q, we have the following formula for “yields”:

(∃j ≤ N umOnes)(∃k ≤ N umZeros) [ψ1 ∧ ψ10 ∧ ψ9]

where

N umOnes = |X[i]| − numones(1, X[i]) N umZeroes = numones(1, X[i])

Having completed the formula Φ(X, n, m) we can now restate the existence of a winning strategy for the ﬁrst player in W11.
A winning strategy in chomp is just a function S : X → Y , which maps
conﬁgurations to conﬁgurations, so that whoever plays by S, wins. It is well
known that the ﬁrst player has a winning strategy. How can this be stated? As follows: ∀Y , such that Y has nm/2 many segments of length (n + m) each,
there exists an X as above, such that every other segment of X is Y , and X[i+1] = S(Y [i]), and Φ(X, n, m), and also the ﬁrst time the ﬁnal conﬁguration
occurs, it is in an even segment (so second player loses).
We want to say that given any conﬁguration C as an initial conﬁguration,
either player 1 or player 2 has a winning strategy. This can be stated as follows:

∀C∃S[WinP1 (S, C) ∨ WinP2 (S, C)],

(1)

where WinPi (S, C) (i ∈ {0, 1}, and ¯i will denote the value in the set {0, 1} − {i})
asserts that if player i plays by strategy S, then player i wins. This can be easily state as follows: ∀Y (where Y is a sequence of moves of player ¯i), if it is player i’s move on conﬁguration C, and player i plays C′ = S(C), then player ¯i will end up with the poisoned square. Note that (1) is a Σ1B formula since the “∀C” is bounded by the size of the chomp grid—we omit the bound for clarity, and WinPi is a Σ0B formula.

Theorem 1. W11 ⊢ ∀C∃S[WinP1 (S, C) ∨ WinP2 (S, C)].

Proof. We prove it by induction on |C|, where we let |C| is the number of squares
in conﬁguration C (in particular, if C consists of the poisoned square only, then |C| = 1). The basis case is simple: if |C| = 1, then P1 loses, so in particular ∃SWinP2 (S, C) holds (and it does not matter what S is, since P2 will not use it anyways).
For the induction step, suppose that the claim holds for all C such that |C| ≤ n. Consider some C′ such that |C′| = n + 1. We want to show that

∃S[WinP1 (S, C′) ∨ WinP2 (S, C′)].

(2)

Let C′′ be the result of P1 making a move; since P1 must select some square, it follows that |C′′| < |C′|, and so we can apply the induction hypothesis to

424 Michael Soltys and Craig Wilson

it; in other words, ∃SWinP1 (S, C′′) ∨ ∃SWinP2 (S, C′′) (we used the fact here

that ∃x(α ∨ β) is equivalent to ∃xα ∨ ∃xβ). If no matter what ﬁrst move P1

makes (to obtain C′′) it is always the case that ∃SWinP2 (S, C′′), then we can

conclude that ∃SWinP2 (S, C′). If, on the other hand, for some move of P1 it is

the case that ∃SWinP1 (S, C′′), then deﬁne (using comprehension) S′ to be the

same as S, except S′(C′) = C′′, and we can conclude that WinP1 (S′, C′), and so

∃SWinP1 (S, C′). Therefore, in either case we obtain (2).

⊓⊔

Once we know that W11 proves (1), we can prove that P1 has a winning strategy for the full rectangle.

Theorem 2. W11 ⊢ “C is full rectangle” → ∃SWinP1 (S, C).

Proof. Suppose that C is a full rectangle (an r × c chomp grid). We know that

either P1 or P2 has a winning strategy. Suppose that it is P2; so P2 (playing

by some S) will win no matter what ﬁrst move P1 makes. So let P1 select the top-right square (i.e., the square (r, c)), and let C′ be the resulting conﬁguration

(i.e., C′ consists of all squares but it has a dent in the top-right square).

Now P2 makes a move according to S, i.e., P2 plays to obtain C′′ = S(C′). Now observe that whoever starts playing from conﬁguration C′′ loses, i.e., P1 loses on C′′, (by our assumptions). Also observe that P1 could have played to

obtain C′′ directly, since no matter what C′′ is, it must contain the top-right

corner, as it is the supremum of the grid. Thus, by taking the strategy S′ to

be S(C) = C′′ and otherwise S′ = S, P1 can win (note that we have used

comprehension to deﬁne S′ from S). Contradiction; so P2 cannot have a winning

strategy, and so P1 must have a winning strategy.

⊓⊔

Using the Witnessing Theorem for W11 ([4]) we can conclude from theorem 2 that the winning stragegy S can be computed in PSPACE (which, of course, we
know from the previous section—and the fact that there is a simple way of com-
puting the strategy by quering repeatedly whether one exists). The interesting question is: can we prove the same in a weaker theory than W11; in particular, is V1 too much to ask for?

References
1. Gale, D.: A Curious Nim-Type Game. The American Mathematical Monthly 81(8) (October 1974) 876–879
2. Papadimitriou, C.H.: Computational Complexity. Addison Wesley Longman (1995) 3. Sipser, M.: Introduction to the Theory of Computation. Second edn. Thomson
Course Technology (2006) 4. Skelley, A.: A Third-Order Bounded Arithmetic Theory for PSPACE. In: CSL.
(2004) 340–354 5. Cook, S., Nguyen, P.: Foundations of Proof Complexity: Bounded Arithmetic and
Propositional Translations. http://www.cs.toronto.edu/∼sacook/csc2429h/book. Last checked April 29, 2008 (2006) 6. Herman, G.: Private communication (2006)

A Statistical Mechanical Interpretation of Algorithmic Information Theory

Kohtaro Tadaki
Research and Development Initiative, Chuo University 1–13–27 Kasuga, Bunkyo-ku, Tokyo 112-8551, Japan.
tadaki@kc.chuo-u.ac.jp
Abstract. We develop a statistical mechanical interpretation of algorithmic information theory by introducing the notion of thermodynamic quantities, such as free energy, energy, statistical mechanical entropy, and speciﬁc heat, into algorithmic information theory. We investigate the properties of these quantities by means of program-size complexity from the point of view of algorithmic randomness. It is then discovered that, in the interpretation, the temperature plays a role as the compression rate of the values of all these thermodynamic quantities, which include the temperature itself. Reﬂecting this self-referential nature of the compression rate of the temperature, we obtain ﬁxed point theorems on compression rate.
Key words: algorithmic information theory, algorithmic randomness, Chaitin’s Ω, compression rate, ﬁxed point theorem, statistical mechanics, temperature

1 Introduction

Algorithmic information theory is a framework to apply information-theoretic and probabilistic ideas to recursive function theory. One of the primary concepts of algorithmic information theory is the program-size complexity (or Kolmogorov complexity) H(s) of a ﬁnite binary string s, which is deﬁned as the length of the shortest binary program for the universal self-delimiting Turing machine U to output s. By the deﬁnition, H(s) can be thought of as the information content of the individual ﬁnite binary string s. In fact, algorithmic information theory has precisely the formal properties of classical information theory (see Chaitin [3]). The concept of program-size complexity plays a crucial role in characterizing the randomness of a ﬁnite or inﬁnite binary string. In [3] Chaitin introduced the halting probability Ω as an example of random inﬁnite binary string. His Ω is deﬁned as the probability that the universal self-delimiting Turing machine U halts, and plays a central role in the metamathematical development of algorithmic information theory. The ﬁrst n bits of the base-two expansion of Ω solves the halting problem for a program of size not greater than n. By this property, the base-two expansion of Ω is shown to be a random inﬁnite binary string.
In [7, 8] we generalized Chaitin’s halting probability Ω to ΩD by

ΩD =

2−

|p| D

,

p∈dom U

(1)

426 Kohtaro Tadaki

so that the degree of randomness of ΩD can be controlled by a real number D with 0 < D ≤ 1. Here, dom U denotes the set of all programs p for U . As D becomes larger, the degree of randomness of ΩD increases. When D = 1, ΩD becomes a random real number, i.e., Ω1 = Ω. The properties of ΩD and its
relations to self-similar sets were studied in Tadaki [7, 8].
Recently, Calude and Stay [2] pointed out a formal correspondence between ΩD and a partition function in statistical mechanics. In statistical mechanics,
the partition function Z(T ) at temperature T is deﬁned by

Z(T ) =

e−

Ex kT

,

x∈X

where X is a complete set of energy eigenstates of a statistical mechanical system and Ex is the energy of an energy eigenstate x. The constant k is called the Boltzmann Constant. The partition function Z(T ) is of particular importance in equilibrium statistical mechanics. This is because all the thermodynamic quantities of the system can be expressed by using the partition function Z(T ), and the knowledge of Z(T ) is suﬃcient to understand all the macroscopic properties of the system. Calude and Stay [2] pointed out, in essence, that the partition function Z(T ) has the same form as ΩD by performing the following replacements in Z(T ):

Replacements 1
(i) Replace the complete set X of energy eigenstates x by the set dom U of all programs p for U .
(ii) Replace the energy Ex of an energy eigenstate x by the length |p| of a program p.
(iii) Set the Boltzmann Constant k to 1/ ln 2, where the ln denotes the natural logarithm.

In this paper, inspired by their suggestion above, we develop a statistical mechanical interpretation of algorithmic information theory, where ΩD appears as a partition function.
Generally speaking, in order to give a statistical mechanical interpretation to a framework which looks unrelated to statistical mechanics at ﬁrst glance, it is important to identify a microcanonical ensemble in the framework. Once we can do so, we can easily develop an equilibrium statistical mechanics on the framework according to the theoretical development of normal equilibrium statistical mechanics. Here, the microcanonical ensemble is a certain sort of uniform probability distribution. In fact, in the work [9] we developed a statistical mechanical interpretation of the noiseless source coding scheme in information theory by identifying a microcanonical ensemble in the scheme. Then, in [9] the notions in statistical mechanics such as statistical mechanical entropy, temperature, and thermal equilibrium are translated into the context of noiseless source coding.
Thus, in order to develop a statistical mechanical interpretation of algorithmic information theory, it is appropriate to identify a microcanonical ensemble in the framework of the theory. Note, however, that algorithmic information

A Statistical Mechanical Interpretation of Algorithmic Information Theory 427
theory is not a physical theory but a purely mathematical theory. Therefore, in order to obtain signiﬁcant results for the development of algorithmic information theory itself, we have to develop a statistical mechanical interpretation of algorithmic information theory in a mathematically rigorous manner, unlike in normal statistical mechanics in physics where arguments are not necessarily mathematically rigorous. A fully rigorous mathematical treatment of statistical mechanics is already developed (see Ruelle [6]). At present, however, it would not as yet seem to be an easy task to merge algorithmic information theory with this mathematical treatment in a satisfactory manner.
On the other hand, if we do not stick to the mathematical strictness of an argument and make an argument on the same level of mathematical strictness as statistical mechanics in physics, we can develop a statistical mechanical interpretation of algorithmic information theory while realizing a perfect correspondence to normal statistical mechanics. In the physical argument, we can identify a microcanonical ensemble in algorithmic information theory in a similar manner to [9], based on the probability measure which gives Chaitin’s Ω the meaning of the halting probability actually.1 In consequence, for example, the statistical mechanical meaning of ΩD is clariﬁed.
In this paper, we develop a statistical mechanical interpretation of algorithmic information theory in a diﬀerent way from the physical argument mentioned above.2 We introduce the notion of thermodynamic quantities into algorithmic information theory based on Replacements 1 above.
After the preliminary section on the mathematical notion needed in this paper, in Section 3 we introduce the notion of the thermodynamic quantities at any given ﬁxed temperature T , such as partition function, free energy, energy, statistical mechanical entropy, and speciﬁc heat, into algorithmic information theory by performing Replacements 1 for the corresponding thermodynamic quantities in statistical mechanics. These thermodynamic quantities in algorithmic information theory are real numbers which depend only on the temperature T . We prove that if the temperature T is a computable real number with 0 < T < 1 then, for each of these thermodynamic quantities, the compression rate by the program-size complexity H is equal to T . Thus, the temperature T plays a role as the compression rate of the thermodynamic quantities in this statistical mechanical interpretation of algorithmic information theory.
Among all thermodynamic quantities in thermodynamics, one of the most typical thermodynamic quantities is temperature itself. Thus, based on the results of Section 3, the following question naturally arises: Can the compression rate of the temperature T be equal to the temperature itself in the statistical mechanical interpretation of algorithmic information theory ? This question is rather self-referential. However, in Section 4 we answer it aﬃrmatively by prov-
1 Due to the 10-page limit, we omit the detail of the physical argument in this paper. It will be included in a full version of this paper, and is also available in Section 6 of an extended and electronic version of this paper at URL: http://arxiv.org/abs/ 0801.4194v1
2 We make an argument in a fully mathematically rigorous manner in this paper.

428 Kohtaro Tadaki

ing Theorem 9. One consequence of Theorem 9 has the following form: For every

T ∈ (0, 1), if ΩT =

2p∈dom U

−

|p| T

is

a

computable

real

number,

then

lim H(Tn) = T, n→∞ n

where Tn is the ﬁrst n bits of the base-two expansion of T . This is just a ﬁxed point theorem on compression rate, which reﬂects the self-referential nature of
the question. The works [7, 8] on ΩD might be regarded as an elaboration of the technique
used by Chaitin [3] to prove that Ω is random. The results of this paper may
be regarded as further elaborations of the technique. Due to the 10-page limit,
we omit most proofs. A full paper describing the details of the proofs is in preparation.3

2 Preliminaries
We start with some notation about numbers and strings which will be used in this paper. N = {0, 1, 2, 3, . . . } is the set of natural numbers, and N+ is the set of positive integers. Q is the set of rational numbers, and R is the set of real numbers. {0, 1}∗ = {λ, 0, 1, 00, 01, 10, 11, 000, 001, 010, . . . } is the set of ﬁnite binary strings, where λ denotes the empty string. For any s ∈ {0, 1}∗, |s| is the length of s. A subset S of {0, 1}∗ is called a preﬁx-free set if no string in S is a preﬁx of another string in S. {0, 1}∞ is the set of inﬁnite binary strings, where
an inﬁnite binary string is inﬁnite to the right but ﬁnite to the left. For any α ∈ {0, 1}∞ and any n ∈ N+, αn is the preﬁx of α of length n. For any partial function f , the domain of deﬁnition of f is denoted by dom f . We write “r.e.”
instead of “recursively enumerable.” Normally, o(n) denotes any function f : N+ → R such that limn→∞ f (n)/n =
0. On the other hand, O(1) denotes any function g : N+ → R such that there is C ∈ R with the property that |g(n)| ≤ C for all n ∈ N+.
Let T be an arbitrary real number. T mod 1 denotes T − T , where T is the greatest integer less than or equal to T . Hence, T mod 1 ∈ [0, 1). We identify
a real number T with the inﬁnite binary string α such that 0.α is the base-two
expansion of T mod 1 with inﬁnitely many zeros. Thus, Tn denotes the ﬁrst n bits of the base-two expansion of T mod 1 with inﬁnitely many zeros.
We say that a real number T is computable if there exists a total recursive function f : N+ → Q such that |T − f (n)| < 1/n for all n ∈ N+. We say that T is right-computable if there exists a total recursive function g : N+ → Q such that T ≤ g(n) for all n ∈ N+ and limn→∞ g(n) = T . We say that T is leftcomputable if −T is right-computable. It is then easy to see that, for any T ∈ R, T is computable if and only if T is both right-computable and left-computable.
See e.g. Weihrauch [12] for the detail of the treatment of the computability of
real numbers and real functions on a discrete set.
3 The details of the proofs are also available in an extended and electronic version of this paper at URL: http://arxiv.org/abs/0801.4194v1

A Statistical Mechanical Interpretation of Algorithmic Information Theory 429

2.1 Algorithmic information theory

In the following we concisely review some deﬁnitions and results of algorithmic information theory [3, 4]. A computer is a partial recursive function C : {0, 1}∗ → {0, 1}∗ such that dom C is a preﬁx-free set. For each computer C and each s ∈ {0, 1}∗, HC (s) is deﬁned by HC (s) = min |p| p ∈ {0, 1}∗ & C(p) = s . A computer U is said to be optimal if for each computer C there exists a constant
sim(C) with the following property; if C(p) is deﬁned, then there is a p for which U (p ) = C(p) and |p | ≤ |p| + sim(C). It is easy to see that there exists an
optimal computer. Note that the class of optimal computers equals to the class
of functions which are computed by universal self-delimiting Turing machines
(see Chaitin [3] for the detail). We choose a particular optimal computer U as
the standard one for use, and deﬁne H(s) as HU (s), which is referred to as the program-size complexity of s or the Kolmogorov complexity of s.
Chaitin’s halting probability Ω is deﬁned by

Ω=

2−|p|.

p∈dom U

For any α ∈ {0, 1}∞, we say that α is weakly Chaitin random if there exists c ∈ N such that n − c ≤ H(αn) for all n ∈ N+ [3, 4]. Then Chaitin [3] showed that Ω is weakly Chaitin random. For any α ∈ {0, 1}∞, we say that α is Chaitin random if limn→∞ H(αn) − n = ∞ [3, 4]. It is then shown that, for any α ∈ {0, 1}∞, α is weakly Chaitin random if and only if α is Chaitin random (see Chaitin [4] for
the proof and historical detail). Thus Ω is Chaitin random.
In the works [7, 8], we generalized the notion of the randomness of an inﬁnite
binary string so that the degree of the randomness can be characterized by a
real number D with 0 < D ≤ 1 as follows.

Deﬁnition 1 (weak Chaitin D-randomness and D-compressibility). Let D ∈ R with D ≥ 0, and let α ∈ {0, 1}∞. We say that α is weakly Chaitin D-random if there exists c ∈ N such that Dn − c ≤ H(αn) for all n ∈ N+. We say that α is D-compressible if H(αn) ≤ Dn + o(n), which is equivalent to limn→∞ H(αn)/n ≤ D.
In the case of D = 1, the weak Chaitin D-randomness results in the weak Chaitin randomness. For any D ∈ [0, 1] and any α ∈ {0, 1}∞, if α is weakly Chaitin D-random and D-compressible, then

lim H(αn) = D. n→∞ n

(2)

Hereafter the left-hand side of (2) is referred to as the compression rate of an
inﬁnite binary string α in general. Note, however, that (2) does not necessarily
imply that α is weakly Chaitin D-random. In the works [7, 8], we generalized Chaitin’s halting probability Ω to ΩD by
(1) for any real number D > 0. Thus, Ω = Ω1. If 0 < D ≤ 1, then ΩD converges and 0 < ΩD < 1, since ΩD ≤ Ω < 1.

430 Kohtaro Tadaki

Theorem 2 (Tadaki [7, 8]). Let D ∈ R.
(i) If 0 < D ≤ 1 and D is computable, then ΩD is weakly Chaitin D-random and D-compressible.
(ii) If 1 < D, then ΩD diverges to ∞.

Deﬁnition 2 (Chaitin D-randomness, Tadaki [7, 8]). Let D ∈ R with D ≥ 0, and let α ∈ {0, 1}∞. We say that α is Chaitin D-random if limn→∞ H(αn) −
Dn = ∞.

In the case of D = 1, the Chaitin D-randomness results in the Chaitin randomness. Obviously, for any D ∈ [0, 1] and any α ∈ {0, 1}∞, if α is Chaitin D-random, then α is weakly Chaitin D-random. However, in 2005 Reimann and Stephan [5] showed that, in the case of D < 1, the converse does not necessarily hold. This contrasts with the equivalence between the weakly Chaitin randomness and the Chaitin randomness, each of which corresponds to the case of D = 1.
For each real numbers Q > 0 and D > 0, we deﬁne W (Q, D) by

W (Q, D) =

|p|Q

2−

|p| D

.

p∈dom U

As the ﬁrst result of this paper, we can show the following theorem.

Theorem 3. Let Q and D be positive real numbers.
(i) If Q and D are computable and 0 < D < 1, then W (Q, D) converges to a left-computable real number which is Chaitin D-random and D-compressible.
(ii) If 1 ≤ D, then W (Q, D) diverges to ∞.

Thus, we see that the weak Chaitin D-randomness in Theorem 2 is replaced by the Chaitin D-randomness in Theorem 3 in exchange for the divergence at D = 1.

3 Temperature as a compression rate
In this section we introduce the notion of thermodynamic quantities such as partition function, free energy, energy, statistical mechanical entropy, and speciﬁc heat, into algorithmic information theory by performing Replacements 1 for the corresponding thermodynamic quantities in statistical mechanics.4 We investigate their convergence and the degree of randomness. For that purpose, we ﬁrst choose a particular enumeration q1, q2, q3, . . . of the countably inﬁnite set dom U as the standard one for use throughout this section.5
4 For the thermodynamic quantities in statistical mechanics, see Chapter 16 of [1] and Chapter 2 of [11]. To be precise, the partition function is not a thermodynamic quantity but a statistical mechanical quantity.
5 The enumeration {qi} is quite arbitrary and therefore we do not, ever, require {qi} to be a recursive enumeration of dom U .

A Statistical Mechanical Interpretation of Algorithmic Information Theory 431

In statistical mechanics, the partition function Zsm(T ) at temperature T is

given by

Zsm(T ) =

e−

Ex kT

,

(3)

x∈X

Motivated by the formula (3) and taking into account Replacements 1, we introduce the notion of partition function into algorithmic information theory as follows.

Deﬁnition 3 (partition function). For each n ∈ N+ and each real number

T > 0, we deﬁne Zn(T ) by

Zn(T ) =

n

2−

|qi
T

|

.

i=1

Then, the partition function Z(T ) is deﬁned by Z(T ) = limn→∞ Zn(T ), for each T > 0.

Since Z(T ) = ΩT , we restate Theorem 2 as in the following form.

Theorem 4 (Tadaki [7, 8]). Let T ∈ R.
(i) If 0 < T ≤ 1 and T is computable, then Z(T ) converges to a left-computable real number which is weakly Chaitin T -random and T -compressible.
(ii) If 1 < T ,then Z(T ) diverges to ∞.

In statistical mechanics, the free energy Fsm(T ) at temperature T is given

by

Fsm(T ) = −kT ln Zsm(T ),

(4)

where Zsm(T ) is given by (3). Motivated by the formula (4) and taking into account Replacements 1, we introduce the notion of free energy into algorithmic information theory as follows.

Deﬁnition 4 (free energy). For each n ∈ N+ and each real number T > 0, we deﬁne Fn(T ) by Fn(T ) = −T log2 Zn(T ). Then, for each T > 0, the free energy F (T ) is deﬁned by F (T ) = limn→∞ Fn(T ).
Theorem 5. Let T ∈ R.
(i) If 0 < T ≤ 1 and T is computable, then F (T ) converges to a right-computable real number which is weakly Chaitin T -random and T -compressible.
(ii) If 1 < T ,then F (T ) diverges to −∞.

In statistical mechanics, the energy Esm(T ) at temperature T is given by

Esm(T )

=

1 Zsm(T )

Exe−

Ex kT

,

x∈X

(5)

where Zsm(T ) is given by (3). Motivated by the formula (5) and taking into account Replacements 1, we introduce the notion of energy into algorithmic information theory as follows.

432 Kohtaro Tadaki

Deﬁnition 5 (energy). For each n ∈ N+ and each real number T > 0, we

deﬁne En(T ) by

En(T )

=

1 Zn(T )

n i=1

|qi

|

2−

|qi
T

|

.

Then, for each T > 0, the energy E(T ) is deﬁned by E(T ) = limn→∞ En(T ).

Theorem 6. Let T ∈ R.
(i) If 0 < T < 1 and T is computable, then E(T ) converges to a left-computable real number which is Chaitin T -random and T -compressible.
(ii) If 1 ≤ T , then E(T ) diverges to ∞.

In statistical mechanics, the entropy Ssm(T ) at temperature T is given by

1 Ssm(T ) = T Esm(T ) + k ln Zsm(T ),

(6)

where Zsm(T ) and Esm(T ) are given by (3) and (5), respectively. Motivated by the formula (6) and taking into account Replacements 1, we introduce the notion of statistical mechanical entropy into algorithmic information theory as follows.

Deﬁnition 6 (statistical mechanical entropy). For each n ∈ N+ and each

real

number

T

>

0,

we

deﬁne

Sn(T )

by

Sn(T )

=

1 T

En

(T

)

+

log2

Zn

(T

).

Then,

for each T > 0, the statistical mechanical entropy S(T ) is deﬁned by S(T ) =

limn→∞ Sn(T ).

Theorem 7. Let T ∈ R.
(i) If 0 < T < 1 and T is computable, then S(T ) converges to a left-computable real number which is Chaitin T -random and T -compressible.
(ii) If 1 ≤ T , then S(T ) diverges to ∞.

Finally, in statistical mechanics, the speciﬁc heat Csm(T ) at temperature T

is given by

d Csm(T ) = dT Esm(T ),

(7)

where Esm(T ) is given by (5). Motivated by this formula (7), we introduce the notion of speciﬁc heat into algorithmic information theory as follows.

Deﬁnition 7 (speciﬁc heat). For each n ∈ N+ and each real number T > 0, we deﬁne Cn(T ) by Cn(T ) = En(T ), where En(T ) is the derived function of En(T ). Then, for each T > 0, the speciﬁc heat C(T ) is deﬁned by C(T ) = limn→∞ Cn(T ).
Theorem 8. Let T ∈ R. (i) If 0 < T < 1 and T is computable, then C(T ) converges to a left-computable
real number which is Chaitin T -random and T -compressible, and moreover C(T ) = E (T ) where E (T ) is the derived function of E(T ). (ii) If T = 1, then C(T ) diverges to ∞.

A Statistical Mechanical Interpretation of Algorithmic Information Theory 433
Thus, the theorems in this section show that the temperature T plays a role as the compression rate for all the thermodynamic quantities introduced into algorithmic information theory in this section.
These theorems also show that the values of the thermodynamic quantities: partition function, free energy, energy, and statistical mechanical entropy diverge in the case of T > 1. This phenomenon might be regarded as some sort of phase transition in statistical mechanics.6
4 Fixed point theorems on compression rate
In this section, we show the following theorem and its variant.
Theorem 9 (ﬁxed point theorem on compression rate). For every T ∈ (0, 1), if Z(T ) is a computable real number, then the following hold:
(i) T is right-computable and not left-computable. (ii) T is weakly Chaitin T -random and T -compressible. (iii) limn→∞ H(Tn)/n = T .
Theorem 9 follows immediately from the following three theorems.
Theorem 10. For every T ∈ (0, 1), if Z(T ) is a right-computable real number, then T is weakly Chaitin T -random.
Theorem 11. For every T ∈ (0, 1), if Z(T ) is a right-computable real number, then T is also a right-computable real number.
Theorem 12. For every T ∈ (0, 1), if Z(T ) is a left-computable real number and T is a right-computable real number, then T is T -compressible.
Theorem 9 is just a ﬁxed point theorem on compression rate, where the computability of the value Z(T ) gives a suﬃcient condition for a real number T ∈ (0, 1) to be a ﬁxed point on compression rate. Note that Z(T ) is a strictly increasing continuous function on (0, 1). In fact, Tadaki [7, 8] showed that Z(T ) is a function of class C∞ on (0, 1). Thus, since the set of all computable real numbers is dense in R, we have the following for this suﬃcient condition.
Theorem 13. The set {T ∈ (0, 1) | Z(T ) is computable } is dense in [0, 1].
We thus have the following corollary of Theorem 9.
Corollary 1. The set {T ∈ (0, 1) | limn→∞ H(Tn)/n = T } is dense in [0, 1].
From the point of view of the statistical mechanical interpretation introduced in the previous section, Theorem 9 shows that the compression rate of temperature is equal to the temperature itself. Thus, Theorem 9 further conﬁrms the role of temperature as the compression rate, which is observed in the previous section.
In a similar manner to the proof of Theorem 9, we can prove another version of a ﬁxed point theorem on compression rate as follows. Here, the weak Chaitin T -randomness is replaced by the Chaitin T -randomness.
6 It is still open whether C(T ) diverges or not in the case of T > 1.

434 Kohtaro Tadaki
Theorem 14 (ﬁxed point theorem on compression rate II). Let Q be a computable real number with Q > 0. For every T ∈ (0, 1), if W (Q, T ) is a computable real number, then the following hold:
(i) T is right-computable and not left-computable. (ii) T is Chaitin T -random and T -compressible.
For the suﬃcient condition of Theorem 14, in a similar manner to the case of Theorem 9, we can show that, for every Q > 0, the set {T ∈ (0, 1) | W (Q, T ) is computable } is dense in [0, 1].
Acknowledgments
This work was supported both by KAKENHI, Grant-in-Aid for Scientiﬁc Research (C) (20540134) and by SCOPE (Strategic Information and Communications R&D Promotion Programme) from the Ministry of Internal Aﬀairs and Communications of Japan.
References
1. H. B. Callen, Thermodynamics and an Introduction to Thermostatistics, 2nd ed. John Wiley & Sons, Inc., Singapore, 1985.
2. C. S. Calude and M. A. Stay, “Natural halting probabilities, partial randomness, and zeta functions,” Inform. and Comput., vol. 204, pp. 1718–1739, 2006.
3. G. J. Chaitin, “A theory of program size formally identical to information theory,” J. Assoc. Comput. Mach., vol. 22, pp. 329–340, 1975.
4. G. J. Chaitin, Algorithmic Information Theory. Cambridge University Press, Cambridge, 1987.
5. J. Reimann and F. Stephan, On hierarchies of randomness tests. Proceedings of the 9th Asian Logic Conference, World Scientiﬁc Publishing, August 16-19, 2005, Novosibirsk, Russia.
6. D. Ruelle, Statistical Mechanics, Rigorous Results, 3rd ed. Imperial College Press and World Scientiﬁc Publishing Co. Pte. Ltd., Singapore, 1999.
7. K. Tadaki, Algorithmic information theory and fractal sets. Proceedings of 1999 Workshop on Information-Based Induction Sciences (IBIS’99), pp. 105–110, August 26-27, 1999, Syuzenji, Shizuoka, Japan. In Japanese.
8. K. Tadaki, “A generalization of Chaitin’s halting probability Ω and halting selfsimilar sets,” Hokkaido Math. J., vol. 31, pp. 219–253, 2002. Electronic Version Available: http://arxiv.org/abs/nlin/0212001v1
9. K. Tadaki, A statistical mechanical interpretation of instantaneous codes. Proceedings of 2007 IEEE International Symposium on Information Theory (ISIT2007), pp. 1906–1910, June 24-29, 2007, Nice, France.
10. K. Tadaki, The Tsallis entropy and the Shannon entropy of a universal probability. To appear in the Proceedings of 2008 IEEE International Symposium on Information Theory (ISIT2008), July 6-11, 2008, Toronto, Canada.
11. M. Toda, R. Kubo, and N. Saitˆo, Statistical Physics I. Equilibrium Statistical Mechanics, 2nd ed. Springer, Berlin, 1992.
12. K. Weihrauch, Computable Analysis. Springer-Verlag, Berlin, 2000.

Solving Tripartite Matching by Interval-valued Computation in Polynomial Time⋆
A´ kos Tajti and Benedek Nagy⋆⋆
Faculty of Informatics, University of Debrecen Hungary H-4010 Debrecen, P.O. Box 12
akos.tajti@gmail.com, nbenedek@inf.unideb.hu
Abstract. New computing paradigms are usually legitimated by showing their computing power. Hard, usually NP-complete problems are shown to be solved in eﬃcient way. One of the well known NP-complete problem is Tripartite Matching. In this paper this problem is solved by a polynomial Interval-valued computation.
Key words: new computing paradigms, interval-valued computing, NP-complete problems
1 Introduction
There are several problems shown to be computationally hard. Karp presented several computational problems that are proved to be NP-complete ([Karp 1972]). These intractable problems cannot be solved eﬃciently by the traditional computing way (deterministic Turing machines or similar devices) unless P=NP. The problem P=NP is one of the most important challenges of the (Theoretical) Computer Science. The fact that there is no known method to solve eﬃciently these problems by traditional computations leads to the phenomenon of developing several new paradigms of computation. Usually in these new paradigms one or more of the basic properties of the classical computing methodology are dropped. The traditional computation is sequential, uses discrete time-steps, the amount of data that can be processed in a step is ﬁnitely limited, etc. ([Tanenbaum 1984]) DNA and membrane computing, inspired by molecular biology, drop the sequential mode of computation. In his famous paper ([Adleman 1994]) Adleman showed a method to solve the NP-complete Hamiltonian Path problem in polynomial time by DNA computation. Several various methods are known to solve various NP-complete problems eﬃciently by membrane computing ([Paun 2002]), as well. The ways of parallelism of these (bio)computations are essentially diﬀerent ([Loos–Nagy 2007]).
Interval-valued computation is another computing paradigm initiated by B. Nagy in [Nagy 2005b,Nagy 2005c,Nagy–Va´lyi 2008] based on an interval-valued
⋆ The work is party supported by the O¨ veges programme of NKTH, Hungary. ⋆⋆ Corresponding author

436 A´ kos Tajti and Benedek Nagy
fuzzy logic ([Nagy 2005a]) instead of the the binary logic of traditional architectures ([Tanenbaum 1984]). In this paradigm the amount of data that can be processed at a step has not restriction. The paradigm keeps some features of the traditional computations and of classical computers. Classical computers work on ﬁnite sequences of bits, called bytes or memory cells. Instead of ﬁnite sequences of bits the interval-valued computations work on speciﬁc subsets of the interval [0, 1), more speciﬁcally, on ﬁnite unions of [)-type subintervals. In this way it is a straightforward extension of the classical paradigm. It also can be considered as a 1-dimensional version of the optical computing [Woods–Naughton 2005].
In this paper, after the formulation of the problem of tripartite matching (Section 2) and a formulation of the computing model (Section 3), a method is shown that solves the problem in the frame of the model in an eﬃcient way (Section 4). An example is also presented (Section 5), ﬁnally Conclusions close the paper.
2 Formulation of the Problem
Now the formal deﬁnition of the proposed problem is presented (based on [Papadimitriou 1994]).
Deﬁnition 1. ( The Tripartite Matching problem ) Let B = {b1, ..., bm}, G = {g1, ..., gm} and H = {h1, ..., hm} be three disjoint sets such that | B |=| G |=| H |= m (m > 1) and let T ⊆ B × G × H be a relation. The task is to ﬁnd a relation U ⊆ T containing each element of B, G and H exactly once. In other words no two distinct triplets in T contain the same bi ∈ B, gj ∈ G and hk ∈ H, moreover every element bi ∈ B, gj ∈ G, and hk ∈ H appears in one triplet of T .
One generalization of the problem is the simple d-partite matching problem. An instance of this problem consists of the sets A1, A2, ..., Ad (d ≥ 3), and the T ⊆ A1 × A2 × ... × Ad relation. The task is (similarly to Tripartite Matching) to ﬁnd a relation U ⊆ T containing each element of the sets Ai(i = 1, ..., d) exactly once. Further generalizing the problem we get the d-partite matching problem in which a weight ca is associated with every d-tuple and the task is to ﬁnd a matching with the minimal cost.
Our approach can be extended to solve these more general problems but in the remaining of this paper we are only concerned with the original Tripartite Matching problem.
The d-partite matching problem is also important in – mainly biological – applications. In [Singh–Xu–Berger 2008] the authors used the problem in an algorithm for global alignment of multiple protein-protein inter-action (PPI) networks. This is the ﬁrst known algorithm about the problem. A new computational method is invented for recognition of binding patterns common to a set of protein structures in [Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005a]. The dpartite matching is also used in that algorithm. By the same authors a method

Solving Tripartite Matching 437
of recognition of a set of common physico-chemical properties is presented in [Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005b].
In 1972 Karp proved that the Tripartite Matching problem is NP-complete ([Karp 1972], see also [Papadimitriou 1994]). Since there are not known eﬃcient algorithms to solve this problem in traditional computer, in our approach a new computing paradigm will be used. In the next section we brieﬂy describe the interval-valued computing.
3 A Brief Description of Interval-valued Computations
In this section we brieﬂy describe the model. For full description we refer to [Nagy 2005b] and [Nagy–Va´lyi 2008].
First we present an inductive deﬁnition of the interval-values.
Base of induction: Every interval of the form [a, b) with 0 ≤ a < b ≤ 1 is an atomic interval. The unit interval [0, 1) is an atomic interval. The interval-values can be considered as sets of real numbers (points) of the unit interval [0, 1). The atomic intervals are interval-values.
Induction steps: If A and B are interval-values, then A ∪ B and A \ B are interval-values (the operations union and diﬀerence can be applied on interval-values as sets). Every interval-value can be obtained by ﬁnitely many applications of the inductive steps from some atomic interval-values.
In this way, the interval-values are ﬁnite unions of atomic intervals. The empty interval-value is also allowed since it can be obtained as the diﬀerence of any atomic-interval and the unit interval.
The characteristic function of the interval-values A is a function [0, 1) → {0, 1} giving value 1 at exactly those points of the unit interval which belong to A.
The operations deﬁned on these interval-values are motivated by operations of the traditional computers. There are logical and non-logical operators.
The logical operators are naturally extended for interval-values. They work on the values of the characteristic functions. Table 1 shows the interval operators corresponding to some well known logical operators. All Boolean operators can be deﬁned and, as it was shown in [Nagy 2005b], the set of interval-values are closed under these operations. Usually the operations negation, disjunction, conjunction and implication are deﬁned. By the expressiveness of the operators using each other, it is suﬃcient to deﬁne only nand or the Sheﬀer stroke. These operators are well-known, they have theoretical importance, the number of deﬁned operators on interval-values can be reduced by using only one of them as logical operator.
Besides in this paper we need mostly the logical operators, we mention here that some non-logical operators such as shift and product are also deﬁned in

438 A´ kos Tajti and Benedek Nagy

Table 1. Deﬁnition of the logical operators on interval-values (A and B are interval-values)

Operator Negation Disjunction Conjunction Implication

Sign ¬A A ∨ B A ∧ B A ⊃ B

Value [0, 1) \ A A ∪ B

A ∩ B ¬A ∪ B

[Nagy 2005b] and used also in [Nagy–Va´lyi 2008]. For the completeness of this

paper we recall the deﬁnition of product:

kl
Let A = [ai1 , ai2 ) and B = [bj1 , bj2 ) be two interval values with k
i=1 j=1
and l atomic intervals, respectively. The value of C = A ∗ B is deﬁned as fol-

lows: the number of its atomic intervals is k · l. Their indices are given in the

form (i, j) from (1, 1) to (k, l). Then the atomic interval with index (i, j) is

[ ai1 + bj1 (ai2 − ai1 ), ai1 + bj2 (ai2 − ai1 ) ). An interval-valued computation is a deterministic sequence of operator ap-

plications starting by some atomic intervals. The operands of the operators can

be any initially deﬁned atomic interval and any previously computed interval-

value. In decision problems one may check if the interval-value obtained by the

computation is the empty interval-value. Note, that since the negation is al-

lowed operation, it has the same eﬃciency than the checking whether the result

is the unit interval [0, 1). The complexity is measured by the length of the com-

putation (i.e., the length of the computational sequence: the sum of the initial

atomic intervals and operator applications). We note here, that the interval-

valued computations proved to be very powerful using only 1 initially deﬁned

atomic interval, the

0,

1 2

, for details see [Nagy–Va´lyi 2008].

4 Theory: Solution in Polynomial Time
NP-completeness means that we cannot solve the problem in polynomial time by computers with traditional architecture, unless P=NP. At the same time, the paradigm of interval-valued computations makes it possible to execute the steps of the algorithms in a parallel way. More precisely, the real power comes from the fact that each logical operation is performed (in parallel) on every interval of the interval-value, independent of the number of these intervals. Due to this property of this computing architecture we can construct an algorithm to solve the Tripartite Matching problem in polynomial time using the inner parallelism.
For this reason, we must build an equivalent speciﬁcation of the problem using interval-values instead of triplets. To do this, we will engage the theory of logic: we will transform the elements of set theory to elements of logic. We know the solution of the SAT problem on interval computers ([Nagy 2005b]) and so, if we reduce our problem to the SAT problem, then we will be able to solve it eﬃciently.

Solving Tripartite Matching 439

4.1 Reduction to the SAT problem
We will present a method of constructing a logical formula for each instance of the Tripartite Matching problem. These logical formulas are satisﬁable if and only if the original problem instance is solvable; moreover if a solution exists, we can use these formulas to construct a solution.
We deﬁne the Xbgh Boolean variable as the following:

Xbgh =

true, f alse,

if the (b, g, h) triplet is in the set U otherwise,

where b ∈ B, g ∈ G and h ∈ H. This assignment can be done eﬃciently. (In a cubic complexity of m.)
Next, using Deﬁnition 1, the following logical formulas are constructed using these variables:











 







 







 







 



 



EB =

 b∈B 



g, h

 Xbgh ∧  

g′, h′



¬Xbg′

h′

 

,





   

 

(b,

g,

h)

∈

T

 

(b, g′, h′) ∈ T

 



  

g = g′ ∧ h = h′





f alse,

if every b∈B
appears in a triplet of T ;
otherwise.











 







 







 







 



 



EG =

 g∈G 



b, h

 Xbgh ∧



b′, h′



¬Xb′

gh′

 

,





   

 

(b,

g,

h)

∈

T

 

(b′, g, h′) ∈ T

 



  

b = b′ ∧ h = h′





f alse,

if every g∈G
appears in a triplet of T ;
otherwise.











 







 







 







 



 



EH =

 h∈H 



b, g

 Xbgh ∧  

b′, g′



¬Xbg′

h′

 

,





   

 

(b,

g,

h)

∈

T

 

(b′, g′, h) ∈ T

 



  

b = b′ ∧ g = g′





f alse,

if every h∈H appears in a triplet of T ;
otherwise.

440 A´ kos Tajti and Benedek Nagy

The signs of conjunction and disjunction can be used as n-ary operators. Since both the conjunction and disjunction are associative operations, their n-ary forms are well-deﬁned. For this reason it is allowed to use these operations as and in the way we used above. These formulas also can be built in a polynomial complexity.
The interpretation of the above formulas is as follows:

1. EB is true if and only if there are no two distinct triplets in the set U such that the two triplets are equal in their ﬁrst elements and b ∈ B is included in a triplet. To show this let b ∈ B, g1, g2 ∈ G and h1, h2 ∈ H such that g1 = g2 or h1 = h2. Suppose we have (b, g1, h1), (b, g2, h2) ∈ U , where U is a solution of the problem. In each of the m direct subformulas of EB there is exactly one Boolean variable which is not negative (i.e., not preﬁxed by the operator ¬). So, if both (b, g1, h1) and (b, g2, h2) are in U , then both Xbg1h1 and Xbg2h2 are true and so the whole subformula is false, thus EB is false. (The presented atomic conjunctions are true if and only if exactly one of the used Xbgh’s has value true.) On the other side, since EB is a conjunction, it can only be false if one of its conjunctional subformulas is false or there exists a b ∈ B such that it doesn’t appear in any triplets of T . In the ﬁrst case, from the deﬁnition of EB it follows that such a subformula is false if and only if there exists a b ∈ B such that (b, g′, h′) and (b, g′′, h′′) (g′, g′′ ∈ G, h′, h′′ ∈ H, g′ = g′′ or h′ = h′′) are in U , that is, the problem instance has no solution.
2. EG is true if and only if there are no two triplets in the set U such that the two triplets are equal in their second elements and every g ∈ G is included in a triplet. The correctness of this statement can be seen similarly as above.
3. EH is true if and only if there are no two triplets in the set U such that the two triplets are equal in their third elements and every h ∈ H is included in a triplet. This statement can be proved in the same way as in the previous cases.

If the above formulas are satisﬁable in the same evaluation, then the actual

problem instance has a solution. That is, the logical formula assigned to the

instance is the following:

EB ∧ EG ∧ EH

(1)

If the formula (1) is satisﬁable, then the problem is solvable and a solution
can be extracted from the satisfying evaluation. The formula can be built in complexity O(m5).

4.2 Constructing a solution
The solution can be divided into two phases. In the ﬁrst phase we construct a logical formula and check if it is satisﬁable. If it is satisﬁable, then in the next phase we can construct a solution.

Solving Tripartite Matching 441

Solution to SAT The solution of the SAT problem using interval-valued computations can be found in [Nagy 2005b]. We recall the main steps.
Let n be the number of the Boolean variables in the formula. Let the intervalvalue assigned to the i-th variable be given in the following form

2i−1 −1
Ai =

2j 2j + 1 2i , 2i

j=0

, 1 ≤ i ≤ n.

These values can be computed in a linear way (with respect to number of variables). One can easily construct the above interval-values, for instance, using the product operator:

A1 =

0,

1 2

and Ai+1 = (Ai ∗ A1) ∨ (¬Ai ∗ A1).

After constructing the n interval-values the logical expression (the given formula) must be evaluated using these interval-values and the deﬁnition of the Boolean operators on the interval-values. This is also a linear computations (with respect to the length of the formula). If the result interval-value is nonempty, then the formula is satisﬁable. Every non-empty interval-value contains a point x ∈ [0, 1). One can ﬁnd a satisfying evaluation by evaluating the logical variables according to the characteristic function of their interval-values at x.

Constructing the solution to Tripartite Matching Through the description of the algorithm we are referring to the notion of characteristic functions of interval-values.
If the actual problem instance is solvable, then the following algorithm builds a solution:
1. let U be the empty set. 2. let x be a point of the result interval-value (which satisﬁes all the formulas)
where its characteristic function is true. 3. for all Xbgh: if the characteristic function of the interval assigned to Xbgh on
x is true (i.e., cbgh(x) = true, where cbgh is the characteristic function of the appropriate interval value) then U = U ∪ (b, g, h), b ∈ B, g ∈ G and h ∈ H.
The set U is a solution of the Tripartite Matching problem instance.

5 Example
Let the sets be the following: B = {b1, b2}, G = {g1, g2} and H = {h1, h2},
and deﬁne the relation T on these sets as follows:

442 A´ kos Tajti and Benedek Nagy
T ⊆B×G×H and
T = {(b1, g1, h1), (b1, g1, h2), (b2, g2, h2), (b1, g2, h2), (b2, g1, h2)} Now we deﬁne ﬁve Boolean variables, Xb1g1h1 , Xb1g1h2 , Xb2g2h2 , Xb1g2h2 and Xb2g1h2 , one for each triplet in T . For each variable we construct an appropriate interval-value. These values are the following:
Xb1 g1 h1
Xb1 g1 h2

Xb2 g2 h2

Xb1 g2 h2

and

Xb2 g1 h2

We build the logical formulas:
EB = ( (Xb1g1h1 ∧ ¬Xb1g1h2 ∧ ¬Xb1g2h2 )∨ ∨(¬Xb1g1h1 ∧ Xb1g1h2 ∧ ¬Xb1g2h2 )∨ ∨(¬Xb1g1h1 ∧ ¬Xb1g1h2 ∧ Xb1g2h2 ) ) ∧ ∧( (Xb2g2h2 ∧ ¬Xb2g1h2 )∨ ∨(¬Xb2g2h2 ∧ Xb2g1h2 ) )
EG = ( (Xb1g1h1 ∧ ¬Xb1g1h2 ∧ ¬Xb2g1h2 )∨ ∨(¬Xb1g1h1 ∧ Xb1g1h2 ∧ ¬Xb2g1h2 )∨ ∨(¬Xb1g1h1 ∧ ¬Xb1g1h2 ∧ Xb2g1h2 ) ) ∧ ∧( (Xb2g2h2 ∧ ¬Xb1g2h2 )∨ ∨(¬Xb2g2h2 ∧ Xb1g2h2 ) )
EH = Xb1g1h1 ∧ ∧( (Xb1g1h2 ∧ ¬Xb1g2h2 ∧ ¬Xb2g2h2 ∧ ¬Xb2g1h2 )∨ ∨(¬Xb1g1h2 ∧ Xb1g2h2 ∧ ¬Xb2g2h2 ∧ ¬Xb2g1h2 )∨ ∨(¬Xb1g1h2 ∧ ¬Xb1g2h2 ∧ Xb2g2h2 ∧ ¬Xb2g1h2 )∨ ∨(¬Xb1g1h2 ∧ ¬Xb1g2h2 ∧ ¬Xb2g2h2 ∧ Xb2g1h2 ) )
Based on the interval-values assigned to the Boolean variables, we calculate
the interval-values corresponding to the formulas EB, EG and EH . For example, to calculate the interval-value of EB we calculate

Solving Tripartite Matching 443

(Xb1g1h1 ∧ ¬Xb1g1h2 ∧ ¬Xb1g2h2 )

then compute

¬Xb1g1h1 ∧ Xb1g1h2 ∧ ¬Xb1g2h2

and calculate

¬Xb1g1h1 ∧ ¬Xb1g1h2 ∧ Xb1g2h2

Now, if we apply disjunction on the above values, then we get ﬁrst part of EB :
Then, as above, we compute the second part of the formula EB:
and applying conjunction on these two subformulas, EB is obtained:
Similarly, the computation of EG will result:
and for EH we get:
Now, by computing EB ∧ EG ∧ EH the result is:
As we can see the formula is satisﬁable. Thus, there exists a set U ⊆ T with the properties mentioned in Deﬁnition 1. Evaluating the variables with the result interval-value will give us the following:
Xb1g1h1 = true, Xb2g2h2 = true, Xb1g1h2 = f alse Xb1g2h2 = f alse and Xb2g1h2 = f alse
Therefore the set U is the following: U = {(b1, g1, h1), (b2, g2, h2)}
One can easily check that U is a solution of the problem.

6 Conclusions
It was known ([Nagy 2005b]) that SAT can be solved in a linear interval-valued computation. Since the computing model is based on logic, it was natural to solve problems with strong relation to logic. In this paper another NP-complete problem was considered; the Tripartite Matching proved to be solvable by an algorithm of a polynomial length in the frame of interval-valued computation. This paper also proves that not only logical problems can be solved eﬃciently

444 A´ kos Tajti and Benedek Nagy
by the interval-valued computations. The method presented here is easily extendible to solve the simple d-partite matching problem. The solution will also be polynomial for any value of d ∈ N: O(m2d−1). We note here that by the method presented in [Jackson–Sheridan 2005] the logical formulas can be transformed to conjunctive normal form in complexity O(m3), in this way polynomial solution exists to the Tripartite Matching problem in other computing paradigms in which the SAT of formulas of conjunctive normal form is solvable in polynomial time.
References
[Adleman 1994] Adleman, L.: Molecular Computation of Solutions To Combinatorial Problems, Science 266 (1994) 1021–1024.
[Jackson–Sheridan 2005] Jackson, P. and Sheridan, D.: Clause Form Conversions for Boolean Circuits. in: SAT 2004, LNCS 3542, Springer, (2005) pp. 183–198.
[Karp 1972] Karp, R.: Complexity of Computer Computations. PlenumPress, NewYork, (1972).
[Loos–Nagy 2007] Loos, R. and Nagy, B.: Parallelism in DNA and Membrane Computing, in: “Computability in Europe 2007: Computation and Logic in the Real World”, (S. B. Cooper, T. F. Kent, B. L¨owe, A. Sorbi eds.), Siena, Italy, pp. 283–287.
[Nagy 2005a] Nagy, B.: A general fuzzy logic using intervals, in: Proceedings of the 6th International Symposium of Hungarian Researchers on Computational Intelligence, Budapest, Hungary, (2005), pp. 613–624.
[Nagy 2005b] Nagy, B.: An Interval-valued Computing Device, in: “Computability in Europe 2005: New Computational Paradigms”, (S. B. Cooper, B. L¨owe, L. Torenvliet eds.), ILLC Publications X-2005-01, Amsterdam, pp. 166–177.
[Nagy 2005c] Nagy, B.: U´ j elvu˝ sz´am´ıt´og´epek. (New computational paradigms) Lecture Notes, University of Debrecen, Hungary, (2005).
[Nagy–V´alyi 2008] Nagy, B. and V´alyi, S.: Interval-valued computations and their connection with PSPACE, Theoretical Computer Science 394 (2008) 208–222.
[Papadimitriou 1994] Papadimitriou, C. H.: Computational Complexity, AddisonWesley, (1994).
[Paun 2002] Paun, Gh.: Membrane Computing. An Introduction, Springer-Verlag, Berlin (2002).
[Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005a] Shatsky, M.; Shulman-Peleg, A.; Nussinov, R. and Wolfson, H.J.: Recognition of Binding Patterns Common to a Set of Protein Structures, in: Research in Computational Molecular Biology, LNCS 3500, Springer, (2005) pp. 440–455.
[Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005b] Shatsky, M.; Shulman-Peleg, A.; Nussinov, R. and Wolfson, H.J.: MAPPIS: Multiple 3D Alignment of ProteinProtein Interfaces, in: Computational Life Sciences, LNCS 3695, Springer, (2005) pp. 91–103.
[Singh–Xu–Berger 2008] Singh, R.; Xu, J. and Berger, B.: Global Alignment of Multiple Protein Interaction Networks, in: Proceedings 13th Paciﬁc Symposium on Biocomputing (2008) pp. 303–314.
[Tanenbaum 1984] Tanenbaum, A. S.: Structured Computer Organization, PrenticeHall, 1984.
[Woods–Naughton 2005] Woods, D. and Naughton, T.: An optical model of computation, Theoretical Computer Science 334 (2005) 227-258.

Probabilistic Machines vs. Relativized Computation
Hayato Takahashi1,2 and Kazuyuki Aihara1,2
1 Aihara complexity modelling project, ERATO, JST, 2 Institute of Industrial Science, University of Tokyo 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan.
Abstract. Computational power of probabilistic machines (Turing machines with random input) is studied. In particular, we extend the classical result of K. de Leeuw et al. (1956) to various distributions then, apply our result to analyze the computational power of analog machines.
Key words: probabilistic machine, relativized computation, analog computation, ergodic process
1 Introduction
There are two kinds of computational models of noisy environment: Turing machine with random input [1] and machine models that allow the transition of states to be stochastic, e.g., probabilistic (analog) automata [2, 3] and computable-stochastic machine [1]. In this paper, we call the former model (Turing machines with random input) as probabilistic machine. The diﬀerence of these two models is that the former is a model of Turing machine with external random input and the transition of states is deterministic; on the other hand the latter model does not have external random input but allows the transition of states to be stochastic (not deterministic). In [2], it is shown that a probabilistic automaton (the number of states is ﬁnite) is equivalent to a ﬁnite automaton under the condition isolated cut-oﬀ. In [1], a machine is called computable-stochastic if it has countably many states and the probability of the transition is computable, then it is shown that the class that is computable by computable-stochastic machines with positive probability is equivalent to that of Turing machines. Thus under these conditions, the former model includes the latter model.
In this paper, we study the former models (probabilistic machines). In [1], they showed that if the distribution of random input is Bernoulli with parameter p, then a set is enumerable with positive probability from random inputs iﬀ it is enumerable with oracle p. In Section 1,2, we extend the above result to various distributions and show some examples and counter-examples. We also study the class that probabilistic machines can compute with probability one
Present address: The Institute of Statistical mathematics, 4-6-7 Minami-Azabu, Minato-ku, Tokyo 106-8569, Japan. takahasi@ism.ac.jp E-mail: aihara@sat.t.u-tokyo.ac.jp

446 Hayato Takahashi and Kazuyuki Aihara

and show an interesting example (Sturmian sequence). In Section 3, we propose
analog machine models with noise and analyze their computational power. In
particular, we show that it is possible to increase its computational power by
simple feedback.
Let N and Q be the set of natural numbers and the set of rational numbers, respectively. Let A be a ﬁnite alphabet; and let A∗ and A∞ be the set of ﬁnite strings and the set of inﬁnite sequences of A, respectively. For x, y ∈ A∗, xy is the concatenation of x and y, and x y if x is a preﬁx of y. For x ∈ A∗, |x| is the length of x and x¯ := 0|x|1x. Let q : Q → A∗ be a computable bijection.
Throughout the paper, ξ and lowercase bold letters (e.g. p) denote elements of A∞.
We say that S is ξ-enumerable if S is recursively enumerable (r.e.) relative to
the set {(i, ξi)|i ∈ N}, where ξ = ξ1ξ2 · · · , ∀i ξi ∈ A, and ξ is called ξ-computable if {(i, ξi)|i ∈ N} is ξ-enumerable. Let M ⊆ N × A∗ be a r.e. set. For x ∈ A∗, we write M (x) := {n|y x, (n, y) ∈ M } and M (ξ) := ∪x ξM (x). Then S is ξ-enumerable iﬀ there is a r.e. M such that S = M (ξ). We write the class of
ξ-enumerable sets as M(ξ), i.e.,

M(ξ) = {S ⊆ N|∃M, S = M (ξ)}.

Throughout the paper, M denotes a r.e. set of N × A∗.

Let P be a probability on (B, A∞), where B is the Borel-ﬁeld generated by the

cylinder sets ∆(x) = {xξ|ξ ∈ A∞}, x ∈ A∗. For x ∈ A∗, let P (x) := P (∆(x)),

then ∀x ∈ A∗ P (x) = y∈A P (xy). P is called computable relative to ξ (ξcomputable) if there is a ξ-computable total function p : A∗ × N → Q such

that

∀x, k, |P (x) − p(x, k)| < 1/k.

(1)

Let p := q(p(n(1))) q(p(n(2))) · · · ∈ A∞, where n : N → A∗ × N is a computable bijection. Then the graph of p is p-enumerable and vice versa, so that a set S is r.e. relative to p iﬀ it is p-enumerable. p is called approximation of P if p satisﬁes (1). Note that 1) an approximation of P is not unique and 2) P is computable relative to its approximation. Let AP be the set of approximations of P and

M(AP ) := ∩p∈AP M(p).
Let M −1(S) := {ξ ∈ A∞|M (ξ) = S} for S ⊆ N. Note that M −1(S) is measurable for all S. S is called P-enumerable if there is a r.e. M such that P (M −1(S)) > 03. We write the class of P -enumerable sets as

M(P ) := {S ⊆ N|∃M P (M −1(S)) > 0}.
P is called eﬀectively estimated if there are computable functions e : A×A∗ → Q and r : N × N × A∗ → Q such that
∀x ∈ A∗∀k ∈ N, P (|P (x) − e(x, X1, X2, . . . , Xn)| > 1/k) < r(n, k, x) (2)

and r(n, k, x) → 0 as n → ∞ for each k, x. The following is the theorem of K. de Leeuw et al.[1].

3 In [1], it is called strongly enumerable.

Probabilistic Machines vs. Relativized Computation 447

Theorem 1 (K. de Leeuw et al.[1]). Let P be a probability on A∞. a) M(P ) ⊆ M(AP ). b) If P is eﬀectively estimated then, M(P ) = M(AP ).
Note that M(P ) always includes the class of r.e. sets (ignore the random input), so that part a) implies that if P is computable, M(P ) is the class of r.e. sets.
For example, let P be the Bernoulli process with parameter p, i.e.,

P (x1x2

·

·

·

xn)

=

P
p

xi

(1

−

p)n−P

xi ,

(3)

where p ∈ [0, 1]. Let p = 0.p1p2 · · · be the representation of base A and p := p1p2 · · · ∈ A∞. Then P is p-computable. Since p = P (1), we see that for any approximation p of P , p is p -computable. Conversely, from (3), there is an approximation p such that p is p-computable. Therefore M(AP ) = M(p). Since P is eﬀectively estimated, we have M(P ) = M(AP ) = M(p).
In general, the converse of a) of Theorem 1 does not hold. For example, let p ∈ [0, 1] be a non-computable real and P (100 · · ·) = p and P (00 · · ·) = 1 − p, where 100 · · · consists of 0s following 1 and 00 · · · consists of 0s. Then possible sequences generated by P are 100 · · · and 00 · · ·, so that M(P ) coincides with
the class of r.e. sets. On the other hand AP consists of approximations of noncomputable p, we see M(P ) = M(AP ). In the same way, if P (11 · · ·) = p and P (00 · · ·) = 1 − p and p is not computable, then M(P ) = M(AP ). Note that the ﬁrst example is not stationary and the second one is stationary but not ergodic.

Example 1. If P is ergodic, by ergodic theorem, there is a function r such that r(n, k, x) → 0 as n → ∞ for each k, x and

n−|x|+1

∀x ∈ A∗∀k ∈ N, P (|P (x) −

IXii+|x|−1=x/n| > 1/k) < r(n, k, x),

i=1

(4)

where I is the indicator function. If r is computable, then P is eﬀectively estimated and M(P ) = M(AP ). For example, in case of ergodic Markov processes, r decreases with exponential rate so that it is computable. However it is possible that r is not computable. Indeed, in pp.171 [4], it is shown that for any given decreasing function r, there is an ergodic process that satisﬁes

n
∀n, P (|P (1) − IXi=1/n| ≥ 1/2) > r(n).
i=1

In particular if r is chosen such that r decreases to 0 asymptotically slower than
any computable function then P is not eﬀectively estimated from sample mean.
For a similar example for a stationary process (not ergodic), see [5]. Note that this fact does not imply that M(P ) = M(AP ) for that P . The authors of the paper do not know whether M(P ) = M(AP ) for all ergodic processes or not. We study the weakened form of the equivalence in the next section.

448 Hayato Takahashi and Kazuyuki Aihara

In order to study the converse of a) of Theorem 1 for arbitrary distributions, we introduce some notations. Let xji := xixi+1 · · · xj for i ≤ j, and f : N → N.
P is called independent blocking process of (P, f ) if

P (xn1 ) = P (xf1(1))P (xf1+(1f)(+1f)(2)) · · · P (xn1+f(1)+···+f(k))

(5)

for xn1 ∈ A∗, where k is the number such that 1 +

k i

f

(i)

≤

n

≤

k+1 i

f (i).

P

is a probability on A∞.

Theorem 2. Let P be a probability on A∞ and f be an unbounded computable function. Let P be the independent blocking process of (P, f ). Then M(P ) = M(AP ) = M(AP ).

2 Ergodic process

First we study a weakened form of Theorem 1 for ergodic process. Let

n−|x|+1

rˆ(n, k, x) := P (|P (x) −

IXii+|x|−1=x/n| > 1/k),

i=1

(6)

for x ∈ A∗, k ∈ N. Let r : N × N × A∗ × N → Q be a total function such that ∀n, k, x, t, |r(n, k, x, t) − rˆ(n, k, x)| < 1/t. Let m : N → N × N × A∗ × N be a
computable bijection, and set r := q(r(m(1))) q(r(m(2))) · · ·. Then the graph of r is r-enumerable and vice versa. r is called approximation of rˆ. Let Mr ⊆ N×A∗
be r-enumerable. Let Arˆ be the set of approximations of rˆ and

M(P ∪ Arˆ) := ∩r∈Arˆ{S ⊆ N|∃Mr P (Mr−1(S)) > 0}.

Roughly speaking, M(P ∪ Arˆ) is the class of subsets of natural numbers that is computable with positive probability from rˆ and random inputs.

Corollary 1. If P is ergodic, M(P ∪ Arˆ) = M(AP ).
Next, we study the class of subsets of natural numbers that probabilistic machines can compute with probability one. Let

M1(P ) := {S ⊆ N|∃M P (M −1(S)) = 1},

SP := {x|P (x) > 0},
and M(SP ) be the class of r.e. sets relative to SP .
Theorem 3. If P is ergodic, M1(P ) = M(SP ).
Remark 1. Let ξ be the shifted sequence of ξ, i.e., ∀i, ξi = ξi+1. M is called shift invariant if M (ξ) = M (ξ ) for almost all ξ with respect to P . Then for a shift invariant M , the set {ξ|M (ξ) = S} is shift invariant with respect to P , so that if P is ergodic, we have P (M −1(S)) > 0 ⇔ P (M −1(S)) = 1, and M1(P ) = {S|∃ shift invariant M, P (M −1(S)) > 0}. Therefore if 1) SP is a r.e. set, 2) M(P ) = M(AP ), and 3) P is not computable, then M that computes an approximation of P with positive probability cannot be shift invariant.

Probabilistic Machines vs. Relativized Computation 449
For example if P is a ﬁnite state Markov process then SP is a r.e. set, and hence M1(P ) coincide with the class of r.e. sets. However, there is an example such that M1(P ) is beyond the class of r.e. sets.
Example 2 (Sturmian sequence). Let xn+1 = xn + θ mod 1 for θ ∈ [0, 1] and yn := 0 if xn ∈ [0, θ) else 1. If the initial distribution of x0 is uniform then y0, y1, . . . is an ergodic process. Let Pθ be such the process. Then Pθ is computable iﬀ θ is computable. In [6, 7], it is shown that there are a computable function e and a computable decreasing (to 0) function r such that |e(x)−θ| ≤ r(|x|), for all x generated by Pθ. Therefore we have M(Pθ) = M(APθ ) = M1(Pθ) = M(SPθ ).
3 Analog machines with noise
Certain kind of analog machines with noise can be modeled as follows (Model A):
f : [0, 1] → [0, 1], Yn := f (Yn−1) + n−1
D : R → A, Xn := D(Yn)
· · · X2X1 → M → s1s2 · · · ,
where { n} are i.i.d. random variables (noise) and S = {s1, s2, . . .} is an output of the machine. Then X1, X2, . . . are stochastic process on A∞, and if the distributions of Y0 and { n} are given, the distribution of X1, X2, . . . is determined. This machine is a combination of a dynamical system with noise and a Turing machine, and it is considered to be a probabilistic machine, see Fig. 1 and Remark 2. Thus the results of the previous section can be applied to this machine. For example, if 1. f : [0, 1] → [0, 1] is a polynomial with rational coeﬃcient (e.g. logistic map), 2. distribution of Y0 is computable, 3. distributions of { n} are i.i.d. and computable, and 4. D(x) = 0 if x ∈ [0, a) and D(x) = 1 if x ∈ [a, 1], where a is a rational number, then the distribution of X1, X2, . . . is computable. In such a case, Model A is equivalent to a Turing machine, which follows from the ﬁrst statement of Theorem 1.
In general, let P be the probability that X1, X2, . . . obey and AP be the set of approximations of P . Then M(P ) ⊆ M(AP ). However, unless P is eﬀectively estimated, in general, the converse does not hold. In order to increase the computational power of Model A when M(P ) = M(AP ), let us consider the following transformation (Model B): Let f : N → N be an unbounded computable function. If n = f (k) + 1, k = 1, 2, . . ., then Yn in Model A is reset i.e., Yn is drawn according to the initial distribution and it is independent from the past sequence, see Fig. 1. Model B corresponds to the independent blocking process P of (P, f ), and we have M(P ) = M(AP ) from Theorem 2.
Remark 2. Physically realizable analog machines always contain noise (uncertainty of initial value, noise eﬀect of transition, and so on), and some of them are modeled by discrete-time dynamical systems with noise, see [8]. {Yn} (and

450 Hayato Takahashi and Kazuyuki Aihara Y0

Y0 

? Yn = f (Yn−1) + n−1

? Yn = f (Yn−1) + n−1

? Xn = D(Yn)

? Xn = D(Yn)

?? MM

? s1, s2, . . .
Model A

Fig. 1. machine models

? s1, s2, . . .
Model B

{Xn}) in Fig. 1 is a discrete-time dynamical system with noise and it is considered to be an analog machine model with noise. Thus our model in Fig. 1 is a combination of noisy analog machine and Turing machine. In this paper, we are mainly interested in the distribution of {Xn}, and do not explore the dynamics of analog machines. For example, analog neural nets might be modeled by a transformation of a higher dimensional space, however it is not essential in this paper.
4 Proof
We need Lebesgue density theorem.
Lemma 1 (Lebesgue). Let D ⊆ A∞ be measurable and ID be the characteristic function of D. Let f (x) := P (ID ∩ ∆(x))/P (∆(x)). Then limx→ξ f (x) = ID for almost all ξ with respect to P .
Proof of Theorem 1) a: Assume that S is P -enumerable. Then there is a r.e. M such that P ({ξ ∈ A∞|M (ξ) = S}) > 0. From Lemma 1, we have
∀ > 0∃x ∈ A∗ P ({xξ|M (xξ) = S})/P (x) > 1 − .
Let x be a ﬁnite string that satisﬁes the above inequality for = 1/2. We see that
s ∈ S ⇔ P ({∆(xy)|s ∈ M (xy), y ∈ A∗})/P (∆(x)) > 1/2.
Since {(s, y)|s ∈ M (y)} is recursively enumerable and P is computable relative to p, S is enumerable from the ﬁnite string x and p.

Probabilistic Machines vs. Relativized Computation 451

b: By assumption, P is eﬀectively estimated and (2) holds. Let e and r are
computable functions in (2). Let N (x, k) be the least integer such that r(N (x, k), k, x) < 6/(πn−1(x, k))2, where n : N → A∗ × N is a computable bijection. Since r is computable and r(n, k, x) → 0 as n → ∞ for each k, x, we
see that N is computable for all x, k. Let

A(x, k) := {∆(y)| |P (x) − e(x, y)| ≤ 1/k, |y| = N (x, k), y ∈ A∗},

where |y| is the length of y. Then P (A(x, k)c) < r(N (x, k), k, x), where Ac is the

complement of A. Since x,k 6/(πn−1(x, k))2 = 1, we have P (∩x,kA(x, k)) ≥ 1 − x,k P (A(x, k)c) > 1 − x,k 6/(πn−1(x, k))2 = 0. Let

px,k := e(x, X1, X2, . . . , XN(x,k)) for all x, k and p := q(pn(1)) q(pn(2)) · · ·. Then,

p is an approximation of P with positive probability and it is P -enumerable.

Thus, if S ∈ M(AP ) then S ∈ M(P ). Proof of Theorem 2) First we show that M(AP ) ⊆ M(P ). In order to show

this, it is enough to show that P is eﬀectively estimated from random sampling according to P . Let X1, X2, . . . be random variables according to P and Xij := Xi · · · Xj for i ≤ j. Then blocks of random variables X1f(1), Xff((11))++1f(2), . . . are independent. Thus we see that P is eﬀectively estimated from X1∞. Formally let
N (l) := {k|f (k) ≥ l}. Since f is an unbounded computable function, N (l) is

computable and inﬁnite for all l. For each ﬁxed l, Yk := X1f+(1f)(+1·)·+·+··f·+(kf−(k1−)+1l), k ∈

N (l) are independent, where f (0) = 0. Let N (l) = {l1, l2, . . . , }, l1 < l2 < · · ·

and e(x, X1, X2, . . . , Xn) := f (1) + · · · + f (lm+1). Then for

mix=∈1 IAYl∗i

=x/m , |x| =

for f (1) + l, we have

· · · + f (lm E (IYk =x )

− =

1) + l P (x),

≤n≤ where

expectation is average with respect to P , and from Chebyshev inequality, we

have P (|e(x, X1, X2, . . . , Xn) − P (x)| > 1/k) ≤ k2/4m. Since m is computable from n and m → ∞ as n → ∞, we see that P is eﬀectively estimated from

random sampling according to P . Thus we have M(AP ) ⊆ M(P ). M(P ) ⊆ M(AP ) follows from Theorem 1. Since f is computable, by (5), we see that for

any approximation p of P there is an approximation p of P such that p is

p-computable, and hence M(AP ) ⊆ M(AP ). Proof of Corollary 1) From (6), we see that M(Arˆ) ⊆ M(AP ). From Theo-
rem 1 (a), we have M(P ∪Arˆ) ⊆ M(AP ). Since approximation of rˆ is given, as in

the same way of proof of Theorem 1 (b), we can show the converse inclusion.

Proof of Theorem 3) Let Sξ be the set of strings that appear in ξ, i.e.,
Sξ = {x|∃y∃ξ yxξ = ξ}. Since P is ergodic, by ergodic theorem, we have
Sξ = {x|P (x) > 0} almost surely. Thus M1(P ) ⊇ M(SP ). Conversely if P (M −1(S)) = 1 then s ∈ S ⇔ ∃x, P (x) > 0, s ∈ M (x). Thus we have

M1(P ) ⊆ M(SP ).

References
1. de Leeuw, K., Moore, E.F., Shannon, C.E., Shapiro, N.: Computability by probabilistic machines. In Shannon, C.E., McCarthy, J., eds.: Automata Studies. Princeton Univ. Press (1956) 183–212

452 Hayato Takahashi and Kazuyuki Aihara
2. Rabin, M.O.: Probabilistic automata. Information and Control 6 (1963) 230–245 3. Ben-Hur, A., Roitershtein, A., Siegelmann, H.T.: On probabilistic analog automata.
Theoret. Comput. Sci. 320 (2004) 449–464 4. Shields, P.: The ergodic theory of discrete sample paths. AMS (1996) 5. V’yugin, V.V.: Ergodic theorems for individual random sequences. Theoret. Com-
put. Sci. 207 (1998) 343–361 6. Takahashi, H., Aihara, K.: Algorithmic analysis of irrational rotations in a sigle
neuron model. J. Complexity 19 (2003) 132–152 7. Kamae, T., Takahashi, H.: Statistical problems related to irrational rotations.
Ann. Inst. Statis. Math. 58 (2006) 573–593 8. Maass, W., Orponen, P.: On the eﬀect of analog noise in discrete-time analog
computations. Neural Computation 10 (1998) 1071–1095

Quantum Query Algorithms for AND and OR Boolean Functions ⋆
Alina Vasilieva
Institute of Mathematics and Computer Science, University of Latvia Raina bulvaris 29, Riga, LV-1459, Latvia alina.vasilieva@gmail.com
Abstract. Quantum algorithms can be analyzed in a query model to compute Boolean functions where input is given in a black box and the aim is to compute function value for arbitrary input using as few queries as possible. We concentrate on quantum query algorithm designing tasks in this paper. The main aim of the research was to ﬁnd new eﬃcient algorithms and develop general algorithm designing techniques. In this article we propose quantum query algorithm constructing methods. Given algorithms for the set of sub-functions, our methods use them to build a more complex one, based on algorithms described before. Methods are applicable to input algorithms with speciﬁc properties and preserve acceptable error probability and number of queries. Methods oﬀer constructions for computing AND and OR kinds of Boolean functions.
1 Introduction
Let f (x1, x2, ..., xn) : {0, 1}n → {0, 1} be a Boolean function. We have studied the query model, where a black box contains the input (x1, x2, ..., xn) and can be accessed by questioning xi values. The goal is to compute the value of the function. The complexity of a query algorithm is measured by number of questions it asks. The classical version of this model is known as decision trees [1]. Quantum algorithms can solve certain problems faster than classical algorithms. The best-known exact quantum algorithm was designed for XOR function with n/2 questions vs. n questions required by classical algorithm [2, 3]. No analogous eﬃcient algorithms exist for AND and OR Boolean functions.
Quantum query model diﬀers from quantum circuit model [2, 5] and algorithm construction techniques for this model are less developed. The problem of quantum query algorithm construction is not that easy. Although there is a large amount of lower and upper bound estimations of quantum query algorithm complexity [2, 6, 7], examples of non-trivial and original quantum query algorithms are very few. Moreover, there is no special technique described to build a quantum query algorithm for an arbitrary function with complexity deﬁned in advance.
In our work we have tried to develop general constructions and approaches for computing Boolean functions eﬃciently in a quantum query settings.
⋆ Research supported by the European Social Fund

454 Alina Vasilieva
2 Notation and Deﬁnitions
Let f (x1, x2, ..., xn) : {0, 1}n → {0, 1} be a Boolean function. We use ⊕ to denote XOR operation. We use abbreviation QQA for ”quantum query algorithm”.

2.1 Quantum Computing

We apply the basic model of quantum computing. For more details see textbooks

by Gruska [4] and Nielsen and Chuang [5]. An n-dimensional quantum pure state

is a unit vector in a Hilbert space. Let |0 , |1 , ..., |n − 1 be an orthonormal basis

for Cn. Then, any state can be expressed as |ψ =

n−1 i=0

ai

|i

for some ai ∈ C.

Since the norm of |ψ is 1, we have

n−1 i=0

|ai|2

=

1.

States

|0

, |1

, ..., |n − 1

are

called basis states. Any state of the form

n−1 i=0

ai

|i

is called a superposition of

basis states. The coeﬃcient ai is called an amplitude of |i . The state of a system

can be changed by applying unitary transformation. Unitary transformation U

is a linear transformation on Cn that maps vector of unit norm to vector of unit

norm. The simplest case of quantum measurement is used in our model. It is

the full measurement in the computation basis. Performing this measurement

on a state |ψ = a0 |0 + ... + an−1 |n − 1 gives the outcome i with probability |ai|2. The measurement changes the state of the system to |i and destroys the

original state.

2.2 Quantum Query Model

Query algorithm is the model for computing Boolean functions. A black box contains the input (x1, x2, ..., xn) and can be accessed by questioning xi values. Algorithm must be able to determine the value of a function correctly for arbitrary input. The complexity of the algorithm is measured by the number of queries to the black box. We consider computing Boolean functions in the quantum query model. For more details, see the survey by Ambainis [6] and textbooks by Gruska [4] and de Wolf [2]. A quantum computation with T queries is a sequence of unitary transformations:

U0 → Q0 → U1 → Q1 → ... → UT −1 → QT −1 → UT .

Ui′s can be arbitrary unitary transformations that do not depend on the input

bits. Q′is are query transformations. Computation starts in the state

→
0

. Then

we apply U0, Q0, ..., QT −1, UT and measure the ﬁnal state. We use the following deﬁnition of query transformation: if input is a state

|ψ = i ai |i , then the output is |φ = i(−1)xk ai |i , where we can arbitrarily choose a variable assignment xk for each basis state |i .

Each quantum basis state corresponds to the algorithm output. We assign

a value of a function to each output. The probability of obtaining result j after

executing an algorithm on input X equals the sum of squares of all amplitudes,

which corresponds to outputs with value j.

Very convenient way of QQA representation is a graphical picture and we

will use this style when describing our algorithms.

Quantum Query Algorithms for AND and OR Boolean Functions 455
2.3 Query Algorithm Complexity
The complexity of a query algorithm is based on the number of questions it uses to determine the value of a function on worst-case input.
The deterministic complexity of a function f, denoted by D(f), is the maximum number of questions that must be asked on any input by a deterministic algorithm for f [1].
The sensitivity of f on input (x1, x2, ..., xn) is the number of variables xi with the following property: f (x1, ..., xi, ..., xn) = f (x1, ..., 1 − xi, ..., xn). The sensitivity of f is the maximum sensitivity of all possible inputs. It has been proved that D(f ) ≥ s(f ) [1].
A quantum query algorithm computes f exactly if the output equals f(x) with a probability 1, for all x ∈ {0, 1}n. Complexity is denoted by QE(f ) [1].
A quantum query algorithm computes f with bounded-error if the output equals f (x) with probability p > 1/2, for all x ∈ {0, 1}n. Complexity is denoted by Qp(f ) [1].
3 Basic Exact Quantum Query Algorithms
In this paper we will describe quantum query algorithm constructing methods, which use existing algorithms to construct more complex examples. To demonstrate methods we need at least few basic algorithms. The following exact QQAs were presented in [9] and we will use them as a base.
Algorithm 1. Exact QQA with 2 queries is presented in Fig. 1.

Fig. 1. Exact QQA for EQU ALIT Y3 with two queries

Boolean function: EQU ALIT Y3(X) = ¬(x1 ⊕ x2) ∧ ¬(x2 ⊕ x3).

Deterministic complexity: D(EQU ALIT Y3) = 3, by sensitivity on any accepting input.

To make reader more familiar with QQA model we show the computation for X = 111 (bra notation is used for convenience):

ψ| = (1, 0, 0, 0) U0Q0U1Q1U2 =

1 2

,

1 2

,

1 2

,

1 2

Q0U1Q1U2 =

=

−

1 2

,

−

1 2

,

−

1 2

,

−

1 2

U1Q1U2 =

−

1 2

,

−

√1 2

,

0,

−

1 2

Q1U2 =

= (1, 0, 0, 0) ⇒ [ACCEP T ]

1 2

,

√1 ,
2

0,

1 2

U2 =

456 Alina Vasilieva
Algorithm 2. Exact QQA for P AIR EQU ALIT Y4 is presented in Fig. 2.
Fig. 2. Exact QQA for P AIR EQU ALIT Y4 with two queries
Boolean function: P AIR EQU ALIT Y4(X) = ¬(x1 ⊕ x2) ∧ ¬(x3 ⊕ x4). Deterministic complexity: D(P AIR EQU ALIT Y4) = 4, by sensitivity on accepting input.
3.1 Exact Quantum Query Algorithm Classiﬁcation Two described exact QQAs have useful speciﬁc properties. They are designed in such a way that the ﬁnal amplitude distribution satisﬁes certain condition on any possible input. To describe these properties we introduce QQA classiﬁcation and deﬁne the following algorithm classes. Class 1. Exact QQA belongs to Class 1 IFF on any input system state before a measurement is such that for exactly one amplitude αi it is true that |ai|2 = 1. For other amplitudes it is true that |aj|2 = 0, for ∀j = i. Algorithm 1 and Algorithm 2 both belong to Class 1. Class 2+. Exact QQA belongs to Class 2+ IFF there is exactly one accepting basis state and on any input for its amplitude α ∈ C only two values are possible before the ﬁnal measurement: either α = 0 or α = 1. Algorithm 1 belongs to Class 2+. Class 2-. Exact QQA belongs to Class 2- IFF there is exactly one accepting basis state and on any input for its amplitude α ∈ C only two values are possible before the ﬁnal measurement: either α = 0 or α = −1. Lemma 1. It is possible to transform algorithm that belongs to Class 2- to algorithm that belongs to Class 2+ by applying additional unitary transformation. Proof. Let’s assume that we have QQA that belongs to Class 2- and k is the number of accepting output. To transform algorithm to Class 2+ instance apply the following quantum gate:
 0, if i = j  U = (uij) = 1, if i = j = k  −1, if i = j = k

Quantum Query Algorithms for AND and OR Boolean Functions 457
Class 3. Exact QQA belongs to Class 3 IFF it belongs to Class 1; and there is exactly one accepting basis state; and on any input accepting state amplitude value before measurement is α ∈ {−1, 0, 1}.
Algorithm 1 and Algorithm 2 both belong to Class 3.
3.2 Algorithm Transformation Methods
QQA transformation methods were introduced in [9]. These methods are useful for enlarging a set of exactly computable functions. By applying transformation methods to Algorithm 1 and Algorithm 2 it is possible to obtain two sets:
– QAlg3 - which consists of exact QQAs that compute 3-variable Boolean functions using 2 queries. |QAlg3| = 8.
– QAlg4 - which consists of exact QQAs that compute 4-variable Boolean functions using 2 queries. |QAlg4| = 24.
QAlg3 contains two algorithms that belong to Class 2+, two algorithms that belong to Class 2- and four algorithms that belong to Class 3. QAlg4 contains 12 algorithms that belong to Class 3. All these algorithms can be used as a base for constructions methods described in the next section.
We will use these results when calculating total number of algorithms that can be constructed by methods presented in this paper.
4 Algorithm Constructing Methods
In this section we will present two quantum query algorithm constructing methods. Each method requires explicitly speciﬁed exact QQAs on input, but as a result a bounded-error QQA for more complex function is constructed. The most important behavior is that methods do not increase overall algorithm complexity. No additional queries are necessary to compute complex function; input algorithms just have to be combined in a speciﬁc way. Methods maintain low quantum query complexity for complex function in comparison to increased deterministic complexity, thus enlarging a gap between classical and quantum complexities of an algorithm.
We oﬀer a general constructions for computing AND and OR kinds of Boolean functions.
4.1 Bounded-error QQA for 6-Variable Function
We consider composite Boolean function, where two instances of EQU ALIT Y3 are joined with logical AND operation: EQU ALIT Y3∧2(x1, ..., x6) = (¬(x1 ⊕ x2) ∧ ¬(x2 ⊕ x3)) ∧ (¬(x4 ⊕ x5) ∧ ¬(x5 ⊕ x6)) Deterministic complexity. D(EQ-LIT Y3∧2)=6, by sensitivity on X=111111.

458 Alina Vasilieva
Algorithm 3. Our approach in designing an algorithm for EQU ALIT Y3∧2 is to employ quantum parallelism and superposition principle. We execute algorithm pattern deﬁned by original algorithm for EQU ALIT Y3 in parallel for both blocks of EQU ALIT Y3∧2 variables. Finally, we apply additional quantum gate to correlate amplitude distribution. Algorithm ﬂow is depicted in Fig. 3.
Fig. 3. Bounded-error QQA for EQU ALIT Y3∧2
Quantum complexity. Algorithm 3 computes EQU ALIT Y3∧2 using 2 queries with correct answer probability p = 3/4: Q3/4(EQU ALIT Y3∧2) = 2.
4.2 First Constructing Method - AND(f1, f2) It is possible to generalize approach demonstrated in the previous section. It is evident that complex algorithm behaviour is based on a structure of subalgorithms. However, it does not depend on internal structure, but just on a properties of ﬁnal amplitude distributions. Therefore, described construction is not limited to use algorithm for EQU ALIT Y3 as a base, but can be applied to a partucular class of exact QQAs. Generalized method is described in Table 1.
4.3 Bounded-error QQA for 8-Variable Function Next step is to realize similar approach for OR operation. This time we take Algorithm 2 for P AIR EQU ALIT Y4 function as a base. We consider composite Boolean function, where two instances of P AIR EQU ALIT Y4 are joined with OR operation: P AIR EQLT Y4∨(X) = (¬(x1 ⊕ x2) ∧ ¬(x3 ⊕ x4)) ∨ (¬(x5 ⊕ x6) ∧ ¬(x7 ⊕ x8))
We succeeded in constructing quantum algorithm for P AIR EQU ALIT Y4∨ and this time algorithm structure is more complex than in AND operation case. However, this structure allows us to formulate generalized version of method, which will be applicable to QQAs that belong to Class 3.

Quantum Query Algorithms for AND and OR Boolean Functions 459

Table 1. First Constructing Method - AND(f1, f2)

Input. Two exact QQAs Alg1 and Alg2 that belong to Class 2+ or Class 2- and

compute Boolean functions f1(X1) and f2(X2).

Constructing steps.

1. If any algorithm belongs to Class 2-, transform it to Class 2+ algorithm as

described in Lemma 1.

2. If Alg1 and Alg2 utilize quantum systems of diﬀerent size, extend the smallest one

with auxiliary space to obtain equal number of amplitudes. We denote the dimension

of obtained Hilbert spaces with m.

3. For new algorithm utilize a quantum system with 2m amplitudes.

4. Combine unitary transformations and queries of Alg1 and Alg2 in the following

way: Ui =

Ui1 O O Ui2

, where O’s are m × m zero-matrices, Ui1 and Ui2 are either

unitary transformations or query transformations of Alg1 and Alg2.

5. Start computation from the state: ψ| = ( √1 , 0, ..., 0, √1 , 0, ..., 0). 22
6. Before the ﬁnal measurement apply additional unitary gate. Let us denote the

positions of accepting outputs of Alg1 and Alg2 by acc1 and acc2. Then the ﬁnal

gate is deﬁned as follows:

 

1,

√

if (i = j) AN D (i = acc1) AN D (i = (m + acc2))

   

1/√2,

if (i = j = acc1)

U

=

(uij )

=

 1/ 2, if ((i = acc1) AN D (j = (m + acc2)))

   

√ OR −1/ 2, if

((i = (m + acc2)) AN D (i = j = (m + acc2))

(j

=

acc1))



 0, otherwise

7. Deﬁne as accepting output exactly one basis state |acc1 .

Output. A bounded-error QQA A for computing a function F (X) = f1(X1)∧f2(X2)

with probability p = 3/4 and complexity Q3/4(A) = max(QE(Alg1), QE(Alg2)).

Algorithm 4. This time we use 4 qubit quantum system, so in total there are 16
amplitudes. First, we execute P AIR EQU ALIT Y4 algorithm pattern in parallel
on ﬁrst 8 amplitudes, and then apply two additional quantum gates. First gate USW AP swaps state amplitudes in the following way: 2nd ↔ 5th
and 6th ↔ 9th.

 1, if (i = 2 & j = 5) OR (i = 5 & j = 2)



USW AP

=

(uij )

=

 

1, 1,

if if

(i (i

= =

6 & j = 9) OR (i = 9 j) & (i ∈/ {2, 5, 6, 9})

&

j

= 6)



 

0,

otherwise

Second gate UOR is deﬁned as follows:

 H2

UOR

=

 O4×2

 

O4×2

O6×2

O2×4 H2 ⊗ H2
O4×4 O6×4

O2×4 O4×4 H2 ⊗ H2
O6×4

O2×6 

O4×6 O4×6

, 

I6

where

H2

=

√1 2

11 1 −1

, I6 is 6 × 6 identity matrix; Oi×j are zero matrices.

460 Alina Vasilieva Complete algorithm structure is presented in Fig. 4.

Fig. 4. Bounded-error QQA for P AIR EQU ALIT Y4∨

Quantum complexity. Algorithm 4 computes P AIR EQU ALIT Y4∨ using 2 queries with probability p = 5/8: Q5/8(P AIR EQU ALIT Y4∨) = 2.

4.4 Second Constructing Method - OR(f1, f2)
In this section we generalize approach for computing composite Boolean functions matching OR(f1, f2) pattern. The next lemma will be useful during method application.

Lemma 2. For any QQA on any computation step it is possible to swap amplitude values in arbitrary order by applying speciﬁc quantum gate.

Proof. Assume we need to swap amplitude values according to permutation σ =

α1α2...αn β1β2...βn

Then we can deﬁne quantum gate USW AP = (uij) elements as:

– ∀k ∈ {1..n} : uαkβk = 1; – uij = 0, in all other cases.

Now we are ready to formulate a method for computing OR(f1, f2) kind of functions. For simplicity we consider only input algorithms, which employ 2 qubit system. However, approach can be generalized for quantum systems of arbitrary size.

Quantum Query Algorithms for AND and OR Boolean Functions 461
Table 2. Second Constructing Method - OR(f1, f2)
Input. Two exact QQAs Alg1 and Alg2 that belong to Class 3, employ 2 qubit quantum systems and compute Boolean functions f1(X1) and f2(X2). Constructing steps. 1. Use 4 qubit quantum system for a new algorithm, in total 24 = 16 basis states. 2. Start from the state: ψ| = √1 , 0, 0, 0, √1 , 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 .
22
3. Combine Alg1 and Alg2 unitary and query transformations in the following way: Ui1 O4×4 O4×8
Ui = O4×4 Ui2 O4×8 , where I8 is 8 × 8 identity matrix; Oi×j - zero matrices. O8×4 O8×4 I8
4. Apply amplitude swapping gate USW AP , which was deﬁned in the proof of Lemma 2, to arrange amplitudes in the following order: − 1st amplitude ↔ accepting amplitude of the ﬁrst sub-algorithm; − 2nd amplitude ↔ accepting amplitude of the second sub-algorithm; − 3rd, 4th, 5th amplitudes ↔ rejecting amplitudes of the ﬁrst sub-algorithm; − 7th, 8th, 9th amplitudes ↔ rejecting amplitudes of the second sub-algorithm. 5. Apply the last quantum gate UOR that was precisely deﬁned in previous section. 6. Assign function values to an algorithm according to Fig. 4. Output. A bounded-error QQA A for computing a function F (X) = f1(X1)∨f2(X2) with probability p = 5/8 and complexity Q5/8(A) = max(QE(Alg1), QE(Alg2)).
5 Results of Applying Methods
We have applied constructing methods to the basic exact algorithms from sets QAlg3 and QAlg4 (see Sect. 3.2). In total we obtained 272 bounded-error QQAs. Each algorithm computes diﬀerent Boolean function and uses only 2 queries.
We have demonstrated, that by employing quantum computation features it is possible to calculate composite functions by combining algorithms for subfunctions, without additional queries. It is doubtful that similar eﬀect can be achieved in a classical model.
The important point is that invention of each brand-new exact QQA with required properties will at once signiﬁcantly increase a set of eﬃciently computable functions.

Table 3. Results of transformation and constructing methods application

Basic exact quantum algorithms

Set Size Number of function arguments Queries Probability

QAlg3

8

3

21

QAlg4

24

4

21

Constructed sets of algorithms

QAlg AND 16

6

2 3/4

QAlg OR 256

6,7,8

2 5/8

Total: 272

462 Alina Vasilieva
6 Conclusion
In this work we consider quantum query algorithm constructing problems. Main goal of research is to develop a framework for building eﬃcient ad-hoc quantum algorithms for arbitrary Boolean functions. We have tried to develop general approaches for designing algorithms for computing Boolean functions deﬁned by logical formula. In this paper we describe general algorithm constructions for computing AND and OR kinds of Boolean functions.
Suggested approaches allow building bounded-error quantum algorithms for complex functions based on already known algorithms. Signiﬁcant behavior is that overall algorithm complexity does not increase. Additional queries are not required to compute composite function. However, error probability is the cost for eﬃcient computing.
Further work in that direction is to improve general quantum query algorithm constructing techniques. Regarding already existing methods we would like to decrease existing limitations, such as restrictions on input basic algorithms. Another goal is to increase correct answer probability. From the other side, we need new methods to compute Boolean functions of diﬀerent logical structure, to raise overall framework ﬂexibility. Ultimate goal is to invent new eﬃcient quantum algorithms that exceed already known separation from classical algorithms.
7 Acknowledgments
This research is supported by the European Social Fund.
References
[1] H. Buhrman and R. de Wolf: Complexity Measures and Decision Tree Complexity: A Survey. Theoretical Computer Science, v. 288(1): 2143 (2002).
[2] R. de Wolf: Quantum Computing and Communication Complexity. University of Amsterdam (2001).
[3] R. Cleve, A. Ekert, C. Macchiavello, et al. Quantum Algorithms Revisited. Proceedings of the Royal Society, London, A454 (1998).
[4] J. Gruska: Quantum Computing. McGraw-Hill (1999). [5] M. Nielsen, I. Chuang: Quantum Computation and Quantum Information. Cam-
bridge University Press (2000). [6] A. Ambainis: Quantum query algorithms and lower bounds (survey article). Pro-
ceedings of FOTFS III. [7] A. Ambainis and R. de Wolf: Average-case quantum query complexity. Journal of
Physics A 34, pp 67416754 (2001). [8] A. Ambainis. Polynomial degree vs. quantum query complexity. Journal of Com-
puter and System Sciences 72, pp. 220238 (2006). [9] A. Dubrovska, ”Quantum Query Algorithms for Certain Functions and General
Algorithm Construction Techniques,” Quantum Information and Computation V, Proc. of SPIE Vol. 6573 (SPIE, Bellingham, WA, 2007) Article 65730F.

Space Complexity in Ordinal Turing Machines
Joost Winter
1 Introduction
In [Ko], Peter Koepke introduced a model for transﬁnite computation that goes beyond the earlier inﬁnite time Turing machine model described by Joel Hamkins and Andy Lewis in [HaLe]. Unlike ITTMs, which use a number of tapes of size ω, these ordinal Turing machines have a class-sized tape of length Ord. Also unlike ITTMs, these machines also give rise to a intuitive notion of space complexity, allowing us to deﬁne a variety of space complexity classes.
As it turns out, these machines are strictly more powerful than the earlier Hamkins-Kidder model of ITTMs: it has been shown in [L¨o] that the HamkinsKidder weak halting problem h is decidable by one of these Ordinal machines.
In this article, we will look at some issues of space complexity for these machines. Although in principle, here it is possible to look at classes of decidable ordinals (which need not even be sets!), in this article, the time and space complexity classes will be deﬁned as containing only sets of reals (where ‘reals’ are taken to be subsets of N) decidable within a certain time and space complexity.
2 Deﬁnitions
Ordinal Turing machines can be deﬁned as follows:
Deﬁnition 2.1. An ordinal Turing machine, or OTM, is deﬁned by a tuple of the form T = (Q, δ, qs, qf ), where Q is a set of internal states, δ is a transition function from (Q\{qf }) × {0, 1}3 to Q × {0, 1}3 × {L, R} and qs, qf ∈ Q are two special states called the initial and halting state, such that qs = qf holds.
In the next deﬁnition, we will recursively deﬁne three operations: (1) qαT (x) denotes the state a machine T is in at stage α, having started from the input x; (2) hTα (x) denotes the position of the head at stage α, having started from the input x; and (3) cTi,α(x), where i ∈ {1, 2, 3}, denotes the content of tape i at stage α, having started from the input x. Also, we will deﬁne another operation, cTi,n,α(x), which will be deﬁned as 1 if n ∈ cTi,α(x), and 0 otherwise.
Of the three tapes we have, the ﬁrst one will be called, and play the role of input tape, and the second one will be used as output tape. We will regularly use the term the snapshot at α to refer to the tuple of values (qαT (x), hTα (x), cT1,α(x), cT2,α(x), cT3,α(x)).
Deﬁnition 2.2. For any ordinal Turing machine T and any input x ∈ R, we deﬁne the operations qαT (x), hTα (x), cTi,α(x) as follows for all ordinals α, using transﬁnite recursion:

464 Joost Winter
– If α = 0, we set qαT (x) = qs, hTα (x) = 0, cT1,α(x) = x, and cTi,α(x) = ∅ for i ∈ {2, 3}.
– If there is some ordinal β < α such that qβT (x) = qf , then qαT (x), hTα (x), and cTi,α(x) for i ∈ {1, 2, 3} are undeﬁned.
– Otherwise, if α is a successor ordinal β + 1, and hTβ (x) = h, we look at the value of the transition function
(q , (a1, a2, a3), ∆) = δ(qβT (x), (cT1,h,β(x), cT2,h,β(x), cT3,h,β(x)))
and set: • qαT (x) = q • hTα (x) = h + 1 if ∆ = R; hTα (x) = h − 1 if ∆ = L and h is a successor ordinal; and hTα (x) = 0 if ∆ = L and h is either 0 or a limit ordinal. • For each i ∈ {1, 2, 3}, cTi,α(x) is deﬁned as cTi,β(x)\{h} if ai = 0, and as cTi,β(x) ∪ {h} if ai = 1.
– Otherwise, if α is a limit ordinal, we set qαT (x) = lim sup{qβT (x) : β < α}, hTα (x) = lim inf{hTβ (x) : β < α}, and for each i ∈ {1, 2, 3}, cTi,β(x) = {γ ∈ Ord : lim supα<β cTi,γ,α(x) = 1}.
As a result of deﬁning the position of the head at limit stages as the lim inf of the earlier head positions, it follows directly that the head position may, at some point, be an inﬁnite ordinal. This, in turn implies that inﬁnite ordinals may be written to, or again deleted from the tapes: as a result, for an arbitrary ordinal α, any x ∈ R, and any i ∈ {1, 2, 3}, cTi,α(x) need not be a real anymore, but can in principle be any set of ordinals. In principle, we could also remove the restriction that the input of a function be a real, and replace it with the weaker requirement that it be a set of ordinals. However, in this article we will not do this, and focus only on computations by ordinal Turing machines starting from reals.
Now we have deﬁned the ordinal Turing machines and their computation, we continue by deﬁning the complexity classes P and PSPACE1. The P classes for ITTMs were originally deﬁned in [Sc], and the P and PSPACE classes for OTMs both were originally deﬁned in [L¨o].
Deﬁnition 2.3. If T is a machine that eventually reaches the halting state qf , and α is the smallest ordinal such that qαT (x) = qf , then we say that time(x, T ) = α.
Deﬁnition 2.4. For any ordinal Turing machine T , we say that T is a time f machine if, for all x ∈ R, we have time(x, T ) ≤ f (x). For any ordinal ξ, we say that T is a time ξ machine if T is a time f machine for the constant function f such that f (x) = ξ for all x ∈ R.
1 It should be noted that is very little if anything at all polynomial about these classes: the P notation has been introduced in [Sc] originally, and later on the PSPACE was introduced in [L¨o]. The fact that these notions have stuck around might be regrettable, but is true nonetheless. The present author has decided to stick with the familiar, but strange, notation in this paper.

Space Complexity in Ordinal Turing Machines 465
Using these deﬁnitions, we can now deﬁne the classes PKf and PKα for functions f and ordinals α. Note that a set of reals A is decidable, if there is a OTM that terminates on all inputs r ∈ R, and which outputs 1 on exactly the reals r ∈ A. Also, following [L¨o], we will use the notations PHK and PSPACEHK for the corresponding classes for Hamkins-Kidder style inﬁnite Time Turing machines.
Deﬁnition 2.5. For any function f , we let PKf denote the class of all sets of reals that are decidable by a time f machine. For any ordinal ξ, we let PKξ denote the class of all sets of reals that are decidable by a time η machine for some η < ξ.2 Likewise, we use the similar notations PHf K and PHξ K for the corresponding complexity classes for ITTMs.
For the space complexity classes, we can likewise proceed by simply deﬁning space(x, T ) as the largest α that occurs as the position of the head at some stage of the computation:
Deﬁnition 2.6. If T is a machine that, at one point reaches the halting state qf , and time(x, T ) = α, we deﬁne space(x, T ) = sup{hTβ (x) : β ≤ α}.
Deﬁnition 2.7. For any OTM T , we say that T is a space f machine if, for all x ∈ R, we have space(x, T ) ≤ f (x). For any ordinal ξ, we say that T is a space ξ machine if T is a space f machine for the constant function f such that f (x) = ξ for all x ∈ R.
Deﬁnition 2.8. For any function f , we let PSPACEKf denote the class of all sets of reals that are decidable by a space f machine. For any ordinal ξ, we let PSPACEKξ denote the class of all sets of reals that are decidable by a space η machine for some η < ξ.
An immediate result of these deﬁnitions (see [L¨o]), is that a computation of a ordinal Turing machine can never take more space than it can take time:
Proposition 2.9. For all f and all α, we have PKf ⊆ PSPACEKf and PKα ⊆ PSPACEKα respectively.
We will now introduce the notions of clockable, writable, eventually writable, and accidentally writable ordinals. All these notions were originally deﬁned in [HaLe], from page 581 onward. Note that the these notions refer to writability by ITTMs, rather than by OTMs!
Deﬁnition 2.10. We call an ordinal α x-clockable if there is an ITTM that terminates at time α starting from input x. An ordinal α is x-writable if there is an ITTM that terminates with a code for α on the output tape, starting from input x. An ordinal α is eventually x-writable if there is an ITTM that, on
2 The < here and in Deﬁnition 2.8 is correct, and should not be a ≤. A justiﬁcation for this is found in the fact that using < instead of ≤ here allows for a larger variety of classes–a broader discussion on the exact nature of these deﬁnitions can be found in Appendix A of [Wi].

466 Joost Winter
input x, at one point writes a code for α on the out tape, never to change it afterwards, without necessarily terminating. An ordinal α is accidentally xwritable if there is an ITTM that, on input x, writes α on any of the tapes at some stage of the computation.
With clockable, writable, etc., we mean ∅-clockable, -writable, etc. We call the supremum of x-writable ordinals λx, the supremum of x-clockable ordinals γx, the supremum of eventually x-writable ordinals ζx, and the supremum of accidentally x-writable ordinals Σx.
It turns out that the notions of clockability, writability, eventually and accidentally writability are related in the following way:
Proposition 2.11. For all reals x, γx = λx, and λx < ζx < Σx.
Proof. See [We, Corollary 2.1], and for a corrected and more detailed version, [Wi, Proposition 3.16].
3 Variants on the ordinal Turing machine architecture
It turns out that the exact number of tapes used by an OTM is generally irrelevant, and that, as long as there is just one head used for these tapes, OTMs with more tapes can be simulated by OTMs with less tapes, and vice versa.
Proposition 3.1. For any n ∈ N, with n > 2, a set A ⊆ R is decidable by an OTM with n tapes if and only it is decidable by a ordinal Turing machine with 2 tapes.
Proof. Because we only are considering sets of reals here, we only have to deal with inputs that are real numbers. Assume that A is decidable by a ordinal Turing machine T with n tapes. We can now construct a ordinal Turing machine T with 2 tapes as follows:
– T starts out by stretching the input such that, if x is the input, after the stretching operation we ﬁnd {(n − 1) · k : k ∈ x} on the ﬁrst tape. We can use a signal state to recognize when we are done with this operation, and ensure this signal state only reads 1s at successor ordinals; at stage ω we will ﬁnd ourselves with the head at position ω, in this signal state, and read a 0, and move left so that the head position will again be 0.
– After this, we simulate the original n tape machine on our 2 tape machine, using the second tape to simulate the output tape, and the ﬁrst tape to simulate the n − 1 other tapes. We make sure that we have a ﬁxed number k such that each step by T is simulated by k steps of T , and moreover ensure that, for any ordinal α such that α = k · β for some k, hTα (x) = (n − 1) · η for some η.
Because the only tape being simulated on the second tape is the original second tape, and because ﬁrst cell of the second tape of T corresponds to the ﬁrst cell of the second tape of T , it follows that T and T will, on the same input values, always give the same output values.

Space Complexity in Ordinal Turing Machines 467
This gives us the following corollary:
Corollary 3.2. For any m, n ≥ 2, a set A is decidable by a ordinal Turing machine with m tapes if and only if it is decidable by a ordinal Turing machine with n tapes.
Furthermore, it has been conjectured that this is also true in the case where n = 1.
4 Time complexity for ordinal Turing machines
We can prove a number of theorems about time complexity for ordinal Turing machines: to start with, it turns out that for many recursive ordinals α, a ordinal Turing machine can compute exactly the same sets in time α that an ITTM machine can. The ﬁrst theorem is a modiﬁcation of [L¨o, Proposition 4].
It should be noted that, with suﬃciently closed α and/or f , PHαK ⊆ PKα and PHf K ⊆ PKf always follow. To simulate a Hamkins-Kidder machine by a Koepke machine, we only need to take care of ﬁnding a way to recognize when we are at a limit stage of the computation (which can be achieved through the use of ﬂags on an auxiliary tape), then move left, and move to a special limit state. This ‘moving left’ might take at most ω steps, and hence, the inequality holds for any ordinal α such that ω × α = ω.
Proposition 4.1. For any recursive ordinal α larger than ω2, where α is the successor of a limit ordinal β, and ω × β = β, we have PHαK = PKα .
Proof. Assume that A ∈ PKα . This means that A is decidable by a time η machine for some η < α, which, because α = β +1, here can be restated as: A is decidable by a time β OTM. Then A is, by Proposition 2.9, decidable by a time β OTM that uses β or less cells. Because β is a recursive ordinal, we can let an inﬁnite time Turing machine write a code for β on the tape in ω steps. After that, we can simulate the algorithm of the ordinal Turing machine, which needs β cells, on our ω-sized tape. Because every step of the simulated computation will be simulated by a ﬁnite number of steps of the simulating computation, and because the simulated computation takes less than β steps (the halting state cannot be reached at β itself, because β is a limit ordinal), the simulating computation will also take less than β steps. At limit stages, we still need to ﬁnd which the lim inf of the visited tape positions, which might take ω steps: hence, a single step might be simulated by ω steps in the simulation.
Because β ≥ ω2, the complete algorithm will take less than ω + (ω × β) = β steps in total.
This equivalence between the time complexity classes of OTMs and ITTMs also holds at the level λ, the supremum of all writable ordinals:
Proposition 4.2. PHλ K = PKλ .

468 Joost Winter
Proof. Assume that A is decidable by a time α OTM T , where α < λ (or, in other words, where α is a writable ordinal). This machine T is also a space α machine, so the computation uses at most α cells. Say, α is writable by an ITTM in time β: this implies that β is clockable and hence also writable. After writing a code for α on the tape in β steps, we can continue by simulating the OTM algorithm by simulating a tape of size α, in time less than α. Thus, we can decide A with an ITTM in time β + α, which is again a writable ordinal. Hence, A ∈ PHλ K.
It turns out, that in fact, an equality of the above type still holds up to the level Σx, the supremum of all ordinals accidentally writable on input x. Note here that PKΣx is taken to mean PKf for the function f such that f (x) = Σx for all x.
Proposition 4.3. PKΣx = PHλxK.
Proof. Assume that a set A is in PKΣx . Then it follows from Proposition 2.9 that A is in PSPACEKΣx , and we know that there is an accidentally x-writable ordinal α such that, if we can write α on the tape, we can compute A on an ITTM, simulating an α-sized fragment of the tape of the OTM within space ω, based upon the just-found coding of α.
The tactic now is to construct an ITTM, that simulates all other ITTMs on input x, and in doing so, performs an algorithm that, one by one, writes all reals that are accidentally writable from x on one of the scratch tapes. We can be sure that every real that is accidentally writable from x will be written on one of the scratch tapes at some point; and, each time a real is written on the scratch tape this way, we check if it codes an ordinal. If it does, we continue simulating the computation using this ordinal as tape length: eventually we will either run out of space, in which case we continue the algorithm writing accidentally writable reals on the tape, or it will turn out that we have suﬃcient space, in which case the computation will ﬁnish.
This computation will eventually halt, because a real coding an ordinal equal to or larger than α will be written on the tape at some point and, as all halting computations will halt before λx, we know that the computation will halt before λx. So, it follows that the set A is in PHλxK.
This gives us the following corollary:
Corollary 4.4. PKΣx = DecHK
Proof. This follows directly from DecHK = PHλxK and PKΣx = PHλxK.
5 Space complexity for OTMs
Now we will take a quick look look at space complexity issues for Koepke-type Ordinal machines.
Proposition 5.1. DecHK ⊆ PSPACEKω+2

Space Complexity in Ordinal Turing Machines 469
Proof. Assume that A ∈ DecHK. We now can simulate an algorithm deciding A on an OTM with tape length ω + 1 by moving left towards position 0 at limit states, and besides this, simply performing the algorithm used by the ITTM.
Proposition 5.2. PSPACEKΣx ⊆ DecHK
Proof. In the proof Proposition 4.3, it was shown that PSPACEKΣx ⊆ PHλxK. In combination with the fact that PHλxK = DecHK, this gives the desired result.
These two results combined give an interesting result: together they imply that it does not really matter how much extra space you use on an OTM, as long as it does not go beyond Σx. In particular, we have:
Corollary 5.3. For any ordinal α, such that α ≥ ω + 1 and α ≤ Σ + 1, and for any function f such that we have f (x) ≥ ω and f (x) ≤ Σx for all x, we have respectively that PSPACEKα = DecHK or PSPACEKf = DecHK.
So, it turns out that, even with space Σx at our disposal, we are unable to compute any functions that an ITTM cannot compute. Moreover, we can now establish a relationship between the PSPACEK-classes and the PK-classes in a few cases.
Proposition 5.4. For any ordinal α, such that α ≥ ω + 1 and α ≤ λ, we have that PKα PSPACEKα .
Proof. On one side, we know from Corollary 5.3 that PSPACEKα = DecHK, and on the other side, we know that PKα DecHK.
Proposition 5.5. For functions f such that, for all x, Σx > f (x) > λx, we have PSPACEKf = PKf = DecHK.
Proof. On one side, from Corollary 5.3 it follows that PSPACEKf = DecHK. On the other side, we know that PKf ⊇ PKλx , and from Proposition 4.3 we know that PKλx = PHλxK. From the fact that PHλxK = DecHK, it thus follows that PKf ⊇ DecHK. So we know that PSPACEKf = DecHK, as well as that PKf ⊇ DecHK. Finally, from Proposition 2.9 we know that PKf ⊆ PSPACEKf , so that we must have PKf = DecHK, completing the proof.
However, we will shortly show that Koepke machines are in fact more powerful than ITTMs, even when it comes to computing sets of reals. To see this, we will let ourselves be guided by the following question: are there any countable functions or ordinals, such that there are functions computable by an OTM, bounded in space by respectively that ordinal or function, that are not decidable by any ITTM? It turns out that this, indeed is the case. With Proposition 5.6, we can show that the weak halting problem h is indeed in PSPACEKΣ+2:
Proposition 5.6. For all reals x, and for any computation by an ITTM T from x that does not halt by time Σx, the snapshot at time ζx is the same as that at time Σx.

470 Joost Winter
Proof. See [Wi, Proposition 3.16] for a modiﬁed version of an earlier proof by Philip Welch.
Proposition 5.7. h ∈ PSPACEKΣ+2.
Proof. We will sketch a possible way to compute h in PSPACEKΣ+2 space. First we will simply check if the input codes a natural number n; if it does not, we simply output 0.
If it does, we know that either the computation of ITTM n on input n will ﬁnish before time λ, or, as a result of Proposition 5.6 the contents of the tape at stage ζ will be identical to that at stage Σ.
Now, we will simulate the (non-halting) computation of h on input n, but, using a scratch tape, we will keep track of all the ω-sized tapes at any stage. If we start a new step in the computation, we will ﬁrst check if the conﬁguration at this step is equal to one at an earlier step. If this is the case, we halt. Because the tape of the simulated computation has size ω, at any time α, we will have used at most ω · α tape. This leaves us with the following two possibilities:
– The ITTM n, on input n, ﬁnishes before time λ. In this case, n ∈ h, and we output 1, and we have used less than ω · λ < Σ space.
– The ITTM n, on input n, never ﬁnishes, and the content of stage Σ will be equal to that at time ζ due to Proposition 5.6. We will realize this during the checking stage at step Σ, and at that point the used part of the tape has size Σ.
In either case, we will use at most Σ space during the computation, and thus we have obtained a computation of h in PSPACEKΣ+2 space.
As a result, up to the level of Σx, OTMs bounded in space up to level Σx cannot compute any sets that ITTMs cannot compute. However, just above the level Σ this changes: the weak halting problem h is contained in PSPACEKΣ+2.
6 Relating the space classes to the ITTM degrees
The above results caused a few questions to be raised (initially by Benedikt L¨owe and Peter Koepke) about other variations on the inﬁnite time Turing machine architecture, with diﬀerent space constraints. One of these questions was whether the supremum of ordinals writable by OTMs that use at most Σ + 2 space is equal to the set λh, that is, the supremum of ordinals writable from the weak ITTM halting problem h. In this section a negative answer to this question will be provided. We will furthermore extend this question a bit further, and consider whether some connection can be made between OTMs with certain space constraints, and the ITTM degrees.
Deﬁnition 6.1. We let λκ denote the supremum of ordinals (on input 0) writable by an OTM that, on any real input, uses at most κ space. In particular, in this article we will be concerned with OTM computations constrained in length by Σ + 2.

Space Complexity in Ordinal Turing Machines 471
Following [HaLe, p. 588], we deﬁne the weak jump A∇ of a set A ∈ R as follows:
Deﬁnition 6.2. We deﬁne
A∇ = A ⊕ hA = A ⊕ {p : φAp (0) ↓}
We let A∇(n) denote the αth iteration of the weak operation. For successor ordinals this simply means that A∇(α+1) = A∇(α)∇. For limit ordinals α, in [HaLe] A∇(α) is deﬁned as A ⊕ wα, where wα = β<α wβ (here, for ordinals β < α, wβ is the set such that A∇(β) = A ⊕ wβ), using some code z for α to organize the information.
It is immediate from the deﬁnition that 0∇ = 0 ⊕ h, which is computable from h and vice versa.
Proposition 6.3. If x is eventually writable, and y is eventually x-writable, then y is eventually writable.
Proof. We perform the algorithm that eventually writes x. In parallel, we perform the algorithm that writes y from x on another set of tapes, using the tape on which x will eventually be written as the input. During each step of the ﬁrst computation, we perform a single step of the second computation if x remains unchanged during that step; and we restart, again copying the input from the tape on which x will eventually be output, if x has changed during that step.
As x will be eventually written, from some point onward the second computation will never be restarted, and continue forever, and eventually give y as output. This is the desired algorithm which eventually writes y.
Proposition 6.4. For all eventually writable ordinals α, we have ζ = ζ0∇(α) , and hence ζ > λ0∇(α) .
Proof. By the Eventual Jump Theorem, [HaLe, Theorem 6.14], if z is an eventually writable real, and α is an eventually writable ordinal, then the iterated jump z∇(α) is also writable. Applying this theorem to iterated jumps of 0, we obtain that 0∇(α) is eventually writable for every eventually writable ordinal α.
Assume for some ordinal α and some eventually writable ordinal β, that α < ζ0∇(β) . This means that some code for α is eventually writable from 0∇(β) , which is itself eventually writable.
But now, by Proposition 1, α itself is eventually writable, and hence α < ζ. So we have established that ζ0∇(α) ≤ ζ. The other direction, ζ ≤ ζ0∇(α) is trivial. By [Wi, Proposition 3.14], which gives us the unequality of λx and ζx for all x, the result ζ > λ0∇(α) now follows.
Proposition 6.5. ζ < λΣ+2

472 Joost Winter
Proof. We can compute ζ by a machine with tape length Σ +2 the following way: for every natural number n, we can, using the construction in the proof of [Wi, Proposition 6.21] decide if the computation by machine n on input 0 ﬁnishes, and moreover, we are able to decide if machine n on input 0 will eventually write some ordinal. The latter can be done by inspecting the repeating part once we have recognized it: if it remains unchanged and codes some ordinal, then the machine eventually writes some ordinal.
This way we can, using a pairing function, write down all codes for eventually writable ordinals together in a single real. Once we have done this, we can reduce this collection to a single ordinal by iteratively identifying the least elements from the combined set. The ordinal thus obtained is the supremum of all eventually writable ordinals, ζ. Because this is writable by a space Σ +2 machine, we obtain ζ < λΣ+2.
We not only showed that λΣ+2 is not equal to λh, but indeed that it is not equal to λ0∇(α) for any eventually writable ordinal α.

References

[HaLe] [Ko] [L¨o]
[DeHaSc] [Wi] [Sc] [We]

Joel David Hamkins and Andy Lewis: inﬁnite time Turing machines, Journal Of Symbolic Logic 65 (2000), pp. 567-604 Peter Koepke, Turing computations on ordinals, Bulletin of Symbolic Logic 11 (2005), pp. 377-397 Benedikt L¨owe: Space bounds for inﬁnitary computation, in Arnold Beckmann, Ulrich Berger, Benedikt L¨owe, John V. Tucker (eds.), Logical Approaches to Computational Barriers, Second Conference on Computability in Europe, CiE 2006, Swansea, UK, July 2006, Proceedings, SpringerVerlag, Berlin [Lecture Notes in Computer Science 3988] (2006), pp. 319329 Vinay Deolalikar, Joel David Hamkins, Ralf-Dieter Schindler, P = NP ∩ co−NP for inﬁnite time Turing machines, Journal of Logic and Computation 15 (2005), pp. 577-592 Joost Winter: Space Complexity in inﬁnite time Turing Machines, ILLC Scientiﬁc Publications, MoL-2007-14, 2007 Ralf Schindler, P = NP for inﬁnite time Turing machines, Monatshefte der Mathematik 139 (2003), pp. 335-340 Philip D. Welch, The Length of inﬁnite time Turing machine Computations, Bulletin of the London Mathematical Society 32 (2000), pp. 129-136

Reverse Mathematics for Fourier Expansion
Keita Yokoyama Department of Mathematics, Tokyo Institute of Technology 2-12-1 Oh-okayama, Meguro-ku, Tokyo 152-8551, JAPAN. yokoyama@math.titech.ac.jp or k yoko tautology@infoseek.jp

Abstract. This research is motivated by the program of Reverse Mathematics. We investigate some theorems for the convergence of Fourier series within some weak subsystems of second order arithmetic, in order to determine which set existence axioms are needed to prove these theorems. We show that uniform convergence of Fourier series for C1functions and L2-convergence of Fourier series for continuous functions are equivalent to WKL0 over RCA0. We also show that L2-convergence of Fourier series for bounded continuous functions is equivalent to WWKL0 over RCA0.
Key words: Reverse Mathematics, second order arithmetic, Fourier expansion, weak K¨onig’s lemma

1 Introduction

This paper is a contribution to the program of Reverse Mathematics, whose ul-
timate goal is to determine which set existence axioms are needed to prove theo-
rems of ordinary mathematics. The Reverse Mathematics program was initiated
by Friedman and carried forward most notably by Simpson. It is now known
that many of theorems of analysis, algebra and other branches of mathematics
are either proved in the system RCA0 or equivalent over RCA0 to particular set existence axioms.
In this paper, we deal with subsystems RCA0, WKL0 and WWKL0 of second order arithmetic. RCA0 is the system of recursive comprehension, which guarantees the existence of recursively deﬁnable sets, and Σ10 induction. WKL0 consists of RCA0 and a particular set existence axiom called weak K¨onig’s lemma, which asserts that every inﬁnite tree of sequences of 0’s and 1’s has an inﬁnite path.
WWKL0 consists of RCA0 and weak weak K¨onig’s lemma, which asserts that if a tree T has no path, then

|{σ ∈ T | lh(σ) = n}|

lim
n→∞

2n

= 0.

WWKL0 is weaker than WKL0. For details of the deﬁnitions of these subsystems, see [1]. For WWKL0, see also [4].
Reverse Mathematics for various parts of analysis were investigated by sev-
eral people. We study Reverse Mathematics for Fourier expansions. In the next

474 Keita Yokoyama

section, we prepare basic parts of diﬀerential calculus for Fourier expansions. In
section 3, we study some theorems for Fourier expansions within RCA0, WKL0 and WWKL0. We show that uniform convergence of Fourier series for periodic C1-functions and L2-convergence of Fourier series for periodic continuous functions are equivalent to WKL0 over RCA0. We also show that L2-convergence of Fourier series for bounded continuous functions is equivalent to WWKL0 over RCA0. Moreover, we give a local approximation for a continuous function within WWKL0.

2 Derivative and integral

Within RCA0, we can deﬁne the real number system R by Cauchy sequences, and can also deﬁne continuous functions on R, see [1]. We ﬁrst deﬁne C1-functions.
Deﬁnition 1 (C1-functions). The following deﬁnitions are made in RCA0. Let U be an open subset of R, and let f and f be continuous functions from U to R. Then a pair (f, f ) is said to be C1 if and only if

∀a ∈ U

f (x) − f (a)

lim
x→a

x−a

= f (a).

A continuous function f is said to be C1 if we can ﬁnd a (code for a) continuous
function f which is the derivative of f . To prove basic properties of C1-functions in RCA0, we construct diﬀeren-
tiable condition functions. A diﬀerentiable condition function for a C1-function
f expresses the condition of diﬀerentiability at each point of dom(f ). It also ex-
presses the continuity of the derivative f . Hence using a diﬀerentiable condition function, we can easily prove basic properties of C1-functions in RCA0.
Theorem 1. The following is provable in RCA0. Let U be an open subset of R, and let f be a C1-function from U to R. Then, there exists a continuous function ef from U × U to R such that

∀x ∈ U ef (x, x) = 0; ∀x, y ∈ U f (y) − f (x) = (y − x)(f (x) + ef (x, y)).

We call this ef a diﬀerentiable condition function for f .

Proof. See [3].

Note that we can eﬀectively ﬁnd a diﬀerentiable condition function for a C1function.
To integrate continuous functions eﬀectively, we introduce a modulus of integrability. Let f be a continuous function and let ∆ be a partition of [a, b], i.e. ∆ = {a = x0 ≤ ξ1 ≤ x1 ≤ · · · ≤ ξn ≤ xn = b}. Then, we deﬁne S[∆a,b](f ) as

n
S[∆a,b](f ) = f (ξk)(xk − xk−1)
k=1

and deﬁne |∆| as |∆| = max{xk − xk−1 | 1 ≤ k ≤ n}.

Reverse Mathematics for Fourier Expansion 475

Deﬁnition 2 (modulus of integrability). The following deﬁnition is made in
RCA0. Let f be a continuous function from [a, b] to R. A modulus of integrability on [a, b] for f is a function h from N to N such that for all n ∈ N and for all partitions ∆1, ∆2 of [a, b],

|∆1| <

2−h(n) b−a

∧ |∆2|

<

2−h(n) b−a

→ |S[∆a,1b](f ) − S[∆a,2b](f )| < 2−n+1.

f is said to be eﬀectively integrable if f has a modulus of integrability.

In RCA0, there might exist an unbounded continuous function on a closed interval. Note that an unbounded continuous function on a closed interval is not Riemann integrable and does not have a modulus of integrability. The next theorem shows that to integrate continuous functions requires WKL0.
Theorem 2. The following assertions are pairwise equivalent over RCA0.
1. WKL0. 2. Every continuous function on [a, b] is Riemann integrable. 3. Every continuous function on [a, b] has a modulus of integrability.

Proof. See [1, Theorem IV.2.7].

To integrate bounded functions, we only need WWKL0.
Theorem 3. The following assertions are pairwise equivalent over RCA0.
1. WWKL0. 2. Every bounded continuous function on [a, b] is Riemann integrable. 3. Every bounded continuous function on [a, b] has a modulus of integrability.

Proof. Within WWKL0, we can prove a weak version of Heine-Borel theorem appeared in [4]: if {Un}n∈N is an open covering of [0, 1], then there exists a sequence of ﬁnite sequences of intervals {[cni, dni]}i<ln | n ∈ N such that

∀n ∈ N [0, 1] ⊆ Uk ∪ [cni, dni];

k<n

i<ln

lim
n→∞

(dni, cni) = 0.

i<ln

By this theorem, we can prove 1 → 3 easily. The implication 3 → 2 is trivial. To show 2 → 1, we deﬁne some notation. For a tree T ⊆ 2<N, deﬁne a set
ST ⊆ 2<N and λTn ∈ N as

ST := {σ ∈ 2<N | σ ∈/ T ∧ ∀τ ⊆ σ(τ = σ → τ ∈ T )}; λTn := |{σ ∈ T | lh(σ) = n}|.

For a ﬁnite sequence σ ∈ 2<N, deﬁne aσ, bσ ∈ Q as

σ(i)

aσ :=

2i+1 ;

i<lh(σ)

1

bσ

:=

aσ

+

. 2lh(σ)

476 Keita Yokoyama

Thus, if σ, τ ∈ ST , then, bτ ≤ aσ or bσ ≤ aτ . Note that a tree T has a path if
and only if [0, 1] ⊆ σ∈ST [aσ, bσ]. Now, we show ¬1 → ¬2. We reason within RCA0. Assume ¬WWKL0. Then,
there exist q > 0 and a tree T which has no path such that λTn /2n > q for all n ∈ N. Since [0, 1] ⊆ σ∈ST [aσ, bσ], we can deﬁne a continuous function f from [0, 1] to [0, 1] as

f (x) :=

x−aσ cσ −aσ bσ −x bσ −cσ

x ∈ [aσ, cσ] ∧ σ ∈ ST , x ∈ [cσ, bσ] ∧ σ ∈ ST

where cσ := (bσ + aσ)/2. We show that this f is not Riemann integrable. Deﬁne partitions of [0, 1] ∆k
as

∆k :=

0

≤

1 2k

≤

2 2k

≤

···

≤

2k − 1 2k

≤

1

= {[aη, bη] | η ∈ 2<N ∧ lh(η) = k}.

Note that we can easily take Mσ := max{f (x) | x ∈ [aσ, bσ]} and mσ := min{f (x) | x ∈ [aσ, bσ]}. We show that for all k ∈ N,

(Mη − mη)2−k > q.
η∈2<N ∧lh(η)=k

If η ∈ T , then, there exists σ ∈ ST such that σ ⊇ η, thus, [aη, bη] ⊇ [aσ, bσ]. Therefore, η ∈ T implies Mη − mη = 1. Hence, for all k ∈ N,

(Mη − mη)2−k ≥

2−k ≥ λTn 2−n > q.

η∈2<N ∧lh(η)=k

η∈T ∧lh(η)=k

This completes the proof of 2 → 1.

Next, we construct series of continuous functions.

Theorem 4. The following is provable nonnegative real numbers whose series

in RCA0.

∞ n=0

αn

Let {αn}n∈N is convergent.

be a Let

sequence {fn}n∈N

of be

a sequence of continuous functions on [a, b] such that ∀x ∈ [a, b] |fn(x)| ≤ αn

for all n ∈ N. Then, there exists a continuous function f such that

∞
∀x ∈ [a, b] f (x) = fn(x).
n=0

Proof. See [1, Lemma II.6.5].

We can show termwise diﬀerentiation and integration by using a diﬀerentiable condition function and a modulus of integrability. For the proof of the following theorems, see [3].

Reverse Mathematics for Fourier Expansion 477

Theorem 5 (termwise diﬀerentiation). The following is provable in RCA0.

Let U be an open interval of R, and let

∞ n=0

an

and

∞ n=0

bn

be

nonnegative

convergent series. Let {(fn, fn)}n∈N be a sequence of C1-functions from U to R

such that ∀x ∈ U |fn(x)| ≤ an and ∀x ∈ U |fn(x)| ≤ bn for all n ∈ N. Then,

there exists a C1-function (f, f ) from U to R such that

∞∞

∀x ∈ U f (x) = fn(x) ∧ f (x) = fn(x).

n=0

n=0

Theorem 6 (termwise integration). The following is provable in RCA0. Let

∞ n=0

αn

be

nonnegative

convergent

series,

and

let

{fn}n∈N

be

a

sequence

of

eﬀectively integrable continuous functions from [a, b] to R such that ∀x ∈ [a, b]

|fn(x)| ≤ αn for all n ∈ N. By Theorem 4, we deﬁne f =

∞ n=0

fn.

Then,

f

is

eﬀectively integrable and

b ∞b

f (x) dx =

fn(x) dx.

a n=0 a

(We say that {fn}n∈N is a sequence of eﬀectively integrable continuous functions if there exists a sequence of functions {hn}n∈N such that each hn is a modulus of integrability for fn.)

By Theorems 4 and 5, we can construct trigonometric functions sin x and cos x as C1-functions within RCA0. We can also deﬁne π as a unique zero-point of sin x on [2, 4]. Note that we can prove basic properties of sin x and cos x in RCA0.

3 Fourier expansion

In this section, we show some results on Reverse Mathematics for some basic
theories of Fourier expansions. See e.g. [2] for the usual theory of Fourier analysis. We write f ∈ P2π if f is a continuous periodic function with period 2π. Let {ak}k∈N, {bk}k∈N be real sequences. Then, deﬁne Sn as

Sn [{ak }{bk }](x)

=

a0 2

+

n

ak cos kx + bk sin kx.

k=1

If f ∈ P2π is eﬀectively integrable, then, deﬁne

Sn[f ](x) = Sn[{ak}{bk}](x)

where ak and bk are Fourier coeﬃcients, i.e.,

1 ak = π
1 bk = π

π
f (x) cos kxdx;
−π π
f (x) sin kxdx.
−π

We ﬁrst prepare some lemmas.

478 Keita Yokoyama

Lemma 1. The following assertions are pairwise equivalent over RCA0.
1. WKL0. 2. Every periodic C1-function is bounded. 3. Every periodic C1-function is uniformly continuous.

Proof. Easy modiﬁcation of the proof of [1, Theorem IV.2.3] (by means of piecewise parabolic functions).

Lemma 2 (Bessel inequality). The following is provable in RCA0. Let f ∈ P2π be eﬀectively integrable and let ak and bk be Fourier coeﬃcients of f . Then,
nπ
2π (|ai|2 + |bi|2) ≤ f (x)2dx
i=0 −π
for all n ∈ N.

Proof. Straightforward imitation of the usual proof.

Lemma 3. The following is provable in RCA0. Let f ∈ P2π be eﬀectively integrable. If
ππ
f (x) cos kxdx = 0 ∧ f (x) sin kxdx = 0
−π −π
for all n ∈ N, then f ≡ 0.

Proof. Straightforward imitation of the usual proof.

Lemma 4. The following is provable in RCA0. Let f ∈ P2π be a C1-function and let f and f be eﬀectively integrable. Then the Fourier series Sn[f ] uniformly converges to f .

Proof. and bk

We reason be Fourier

within RCA0. Let ak and coeﬃcients of f and let

bk K

be =

Fourier coeﬃcients

π −π

f

(x)2dx.

Then,

of

f,

let

ak

πak =

π1 f (x) cos kxdx =
−π k

π f (x) sin kxdx = πbk ; −π k

πbk

=

πak . k

By Schwarz inequality and Lemma 2,

m
|ak| + |bk| ≤
k=n

m1 2 k2
k=n

m
|bk|2 + |ak|2 ≤
k=n

Km 1 π k2 .
k=n

Thus, that

∞ k=0

|ak| + |bk|

converges.

Then,

by

Theorem

4,

there

exists

g

∈

P 2π

such

g(x)

=

lim
n→∞

Sn[f

](x)

=

a0 2

+

∞

ak cos kx + bk sin kx.

k=1

Let f¯ = g − f . Then, by Lemma 3, f¯ ≡ 0. This means Sn[f ] uniformly converges to f . This completes the proof.

Reverse Mathematics for Fourier Expansion 479
The ﬁrst theorem is concerned with the uniform convergence of Fourier series.
Theorem 7. The following assertions are equivalent over RCA0.
1. WKL0. 2. If f ∈ P2π is a C1-function, then there exist real sequences {ak}k∈N and
{bk}k∈N such that Sn[{ak}{bk}] uniformly converges to f .
Proof. By Theorem 2 and Lemma 4, 1 → 2 holds. For the converse, we assume 2. By Lemma 1, we only need to show that every periodic C1-function (with period 2π) is uniformly continuous. Let f ∈ P2π be a C1-function. Then, there exist {ak}k∈N and {bk}k∈N such that Sn[{ak}{bk}] uniformly converges to f . Since sin x and cos x are uniformly continuous, we can easily show that f is also uniformly continuous. This completes the proof.
Next, we argue about L2-convergence of Fourier series.
Deﬁnition 3 (L2-convergence). The following deﬁnition is made in RCA0. Let {fn}n∈N be a sequence of functions in P2π. Then, we say that {fn}n∈N L2converges to f if for all i ∈ N there exists k ∈ N such that for all m ≥ k there exists a continuous function g such that g2 is eﬀectively integrable and
|fm(x) − f (x)| ≤ g(x),
π
g(x)2dx < 2−i.
−π
Lemma 5. The following is provable in RCA0. Let f ∈ P2π be eﬀectively integrable. Then, the Fourier series Sn[f ] L2-converges to f .
Proof. We reason within RCA0. Let h : N → N be a modulus of integrability for f on [−π, π]. We can construct a sequence of continuous functions on [−π, π] {f˜i}i∈N (by means of piecewise parabolic functions) which satisﬁes the following:
– f˜i is C1 and f˜i and f˜i are eﬀectively integrable; – f˜i(tij) = f (tij); – f˜i is monotone on [tj, tj+1]
where tij = −π + 2πj/2h(i) (j ≤ 2h(i)). Then, {f˜i}i∈N L2-converges to f . By Lemma 2,
π
(f (x) − Sn[f ](x))2dx
−π π
≤ {(f (x) − f˜i(x))2 + (f˜i(x) − Sn[f˜i](x))2 + (Sn[f˜i](x) − Sn[f ](x))2}dx
−π ππ
≤ 2 (f (x) − f˜i(x))2dx + (f˜i(x) − Sn[f˜i](x))2dx.
−π −π

480 Keita Yokoyama

Since {f˜i}i∈N L2-converges to f ,

π

lim
i→∞

(f (x) − f˜i(x))2dx = 0.
−π

By Lemma 4, {Sn[f˜i]}n∈N uniformly converges to f˜i. Thus,

π

lim
n→∞

(f˜i(x) − Sn[f˜i](x))2dx = 0.
−π

Hence,

π

lim
n→∞

(f (x) − Sn[f ](x))2dx = 0,
−π

and this completes the proof.

Theorem 8. The following assertions are equivalent over RCA0.
1. WKL0. 2. If f ∈ P2π, then there exist real sequences {ak}k∈N and {bk}k∈N such that
Sn[{ak}{bk}] L2-converges to f .
Proof. We reason within RCA0. By Theorem 2 and Lemma 5, 1 → 2 holds. For the converse, we show ¬1 → ¬2. Let ¬WKL0. Then, by Lemma 1, there exists an unbounded function f ∈ P2π. Thus, for any real sequences {ak}k∈N and {bk}k∈N, |Sn[{ak}{bk}] − f | is unbounded. Therefore, if |Sn[{ak}{bk}] − f | ≤ g, then g2 is not integrable. This means that ¬2 and this completes the proof of 2 → 1.

Theorem 9. The following assertions are equivalent over RCA0.

1. WWKL0. 2. If f ∈ P2π and |f | ≤ K for some K ∈ Q, then there exist real sequences
{ak}k∈N and {bk}k∈N such that Sn[{ak}{bk}] L2-converges to f .

Proof. We reason within RCA0. By Theorem 3 and Lemma 5, 1 → 2 holds. For the converse, we show ¬1 → ¬2. We use the notation ST and λTn deﬁned in the proof of Theorem 3. Let ¬WWKL0. Then, there exist q > 0 and a tree T which has no path such that λTn /2n > q for all n ∈ N. For a ﬁnite sequence σ ∈ 2<N, deﬁne aσ, bσ ∈ R as

σ(i)

aσ := −π + 2π

2i+1 ;

i<lh(σ)

2π

bσ

:=

aσ

+

. 2lh(σ)

Since T has no path, [−π, π] = σ∈ST [aσ, bσ]. Deﬁne a function f ∈ P2π as

 8(x−aσ)

f (x)

:=

 

bσ −aσ −8{x−(aσ

+bσ

)/2}

bσ −aσ

 

8(x−bσ

)

bσ −aσ

x ∈ [aσ, cσ] ∧ σ ∈ ST , x ∈ [cσ, dσ] ∧ σ ∈ ST , x ∈ [dσ, bσ] ∧ σ ∈ ST

Reverse Mathematics for Fourier Expansion 481

where cσ := (bσ + 3aσ)/4 and dσ := (3bσ + aσ)/4. Then, f (cσ) = 2, f (dσ) = −2

for any σ ∈ ST and |f | ≤ 2.

n

Now, we show ∈ N, if |Sn[{ak

that }{bk

for any }] − f |

real sequences

≤ g, then

π −π

{ak }k∈N g(x)2dx

and {bk}k∈N and for any > πq. Let {ak}k∈N and

{bk}k∈N be real sequences, let n ∈ N and let g be a continuous function such

that g2 is eﬀectively integrable and |Sn[{ak}{bk}] − f | ≤ g. Take M0 ∈ Q such

that M0 ≥ max{|a0|, . . . , |an|, |b0|, . . . , |bn|} and deﬁne M := (n + 1)2M0. Then,

|Sn[{ak}{bk}] (x)| ≤ M for any x ∈ [−π, π]. Thus, if σ ∈ ST and 2π/2lh(σ) ≤

1/M , then, |Sn[{ak}{bk}](x)| < 1 for all x ∈ [aσ, bσ] or |Sn[{ak}{bk}](x)| > −1

for all x ∈ [aσ, bσ]. Therefore, g(cσ) > 1 or g(dσ) > 1 for any σ ∈ ST such that

2π/2lh(σ) ≤ 1/M . Let h be a modulus of integrability for g2 on [−π, π]. Take

N ∈ N such that 2π/2N ≤ 1/M and 2−N < 2−h(i)/2π where i = min{j ∈ N |

2−j+2 < πq}. As in the proof of Theorem 3, if η ∈ T and lh(η) = N , then there

exists x ∈ [aη, bη] such that g(x)2 > 1. Take αη ∈ [aη, bη] | η ∈ 2<N ∧ lh(η) = N

such that g(αη) > 1 if η ∈ T . Then, as in the proof of Theorem 3,

π

g(x)2dx ≥

g(αη)2(bη − aη) − 2−i+2

−π η∈2<N∧lh(η)=N

≥ (bη − aη) − 2−i+2
η∈T ∧lh(η)=N

≥

2πλTN 2N

− πq

> πq.

This means that ¬2 and this completes the proof of 2 → 1.

Imitating the usual arguments for Fourier expansions in RCA0, we can show the following theorems.
Theorem 10 (Parseval equality). The following is provable in RCA0. Let f ∈ P2π be eﬀectively integrable, and let ak and bk be Fourier coeﬃcients of f . Then,
∞π
2π (|ai|2 + |bi|2) = f (x)2dx.
i=0 −π
Theorem 11 (Riemann-Lebesgue lemma). The following is provable in RCA0. Let f be an eﬀectively integrable continuous function on R. Then, for all a, b ∈ R,
b
lim f (x) cos nxdx = 0.
n→∞ a
Theorem 12 (pointwise convergence). The following is provable in RCA0. Let f ∈ P2π be of bounded variation on [−π, π], i.e., there exist monotone increasing functions g0, g1 such that f = g0 − g1. Then, f is eﬀectively integrable and Sn[f ] pointwise converges to f .

482 Keita Yokoyama

Theorem 13. Let f1, f2 ∈ P2π be eﬀectively integrable and let x0 ∈ R. Let f1 ≡ f2 on some neighborhood of x0. Then, Sn[f1](x0) converges if and only if Sn[f2](x0) converges. Moreover, if Sn[f1](x0) converges, then,

lim
n→∞

Sn[f1](x0)

=

lim
n→∞

Sn[f2](x0).

Finally, we argue about local approximation for continuous functions by trigonometric functions.

Theorem 14. The following assertions are equivalent over RCA0.

1. WWKL0. 2. If f is a continuous function on R and x0 ∈ R, then there exist real se-
quences {ak}k∈N and {bk}k∈N such that Sn[{ak}{bk}] L2-converges to f on
a neighborhood of x0.

Proof. We reason within RCA0. 1 → 2 is a straightforward consequence of The-
orem 9. For the converse, we show ¬1 → ¬2. We reason within RCA0. By
¬WWKL0, deﬁne a continuous function f on [−π, π] as in the proof of 2 → 1 of Theorem 9. Then, deﬁne continuous functions fi on [0, π/2i] as fi(x) = 2−if (2i+1x − π). Note that |fi| ≤ 2−i+1. Thus, we can deﬁne a continuous function f¯ on [−π, π] as

f¯(x)

:=

 fi+1(x 
fi+1(x

+ −

π 2i

)

)π
2i+1

0

x

∈

[

−π 2i

,

−π 2i+1

],

x

∈

[π
2i+1

,

π 2i

],

x = 0.

Let

U

be

a

neighborhood

of

0.

Then,

there

exists

i

∈

N

such

that

[

π 2i+1

,

π 2i

]

⊆

U.

As in the proof of Theorem 9, there are no real sequences {ak}k∈N and {bk}k∈N

such

that

Sn [{ak }{bk }]

L2-converges

to

f¯

on

[π
2i+1

,

π 2i

].

This

means

that

¬2

and

this completes the proof of 2 → 1.

References
1. S. G. Simpson. Subsystems of Second Order Arithmetic. Springer-Verlag, 1999. 2. Elias M. Stein and Rami Shakarchi. Fourier Analysis. Princeton Lectures in Anal-
ysis. Princeton University Press, 2003. 3. Keita Yokoyama. Standard and Non-standard Analysis in Second Order Arithmetic.
Doctoral thesis, Tohoku University, December 2007. 4. X. Yu and S. G. Simpson. Measure theory and weak K¨onig’s lemma. Archive for
Mathematical Logic, 30:171–180, 1990.

Induced Matchings in Graphs of Maximum Degree Three
Graz˙yna Zwoz´niak
Institute of Computer Science, Wrocław University, Poland grazyna@ii.uni.wroc.pl
Abstract. An induced matching is a matching M where each two edges e1, e2 ∈ M are at distance greater than one. In this paper we consider the problem of ﬁnding maximum induced matchings in graphs of maximum degree three. The problem is NP-hard. We present an algorithm which ﬁnds the maximum induced matching of size at least n/6+O(1), where n is the number of vertices in a graph. Using this bound we achieve an approximation ratio of 1.8 in cubic graphs.
Key words: induced matching, graphs, cubic, approximation algorithm
1 Introduction
Given a connected undirected graph G = (V, E), the maximum induced matching problem is to ﬁnd a maximum set of edges M which fulﬁls the following conditions: (a) no two edges of M share a common vertex (b) no two edges of M are joined by an edge of G. Induced matchings are also referred to as strong matchings [6, 7]. The problem has received considerable attention in the discrete mathematics community, since ﬁnding large induced matchings is a subtask of ﬁnding a strong edge-colouring in a graph [4, 5, 11, 14], a proper colouring of the edges, where no edge is incident to two edges of the same colour. There is also immediate connection between the size of an induced matching and the irredundancy number of a graph [7]. Stockmeyer and Vazirani [15] motivate the problem as the risk-free marriage problem, where each married person is compatible with no married person other than the one he (or she) is married to. On practical side, induced matchings have the applications for secure communication channels and network ﬂow problems [8].
Cameron [2] showed that the maximum induced matching problem is NP-hard for bipartite graphs. Ko and Shepherd [12] proved the NP-hardness for cubic planar graphs. The problem was shown to be polynomial for chordal graphs and for interval graphs by Cameron [2]. Golumbic and Laskar [7] gave a polynomial time algorithm for circular arc graphs. Golumbic and Lewenstein [8] constructed polynomial time algorithms for trapezoid graphs, interval-dimension graphs and cocomparability graphs, and a linear time algorithm for the maximum induced matching problem in interval graphs. The linear time algorithms for trees were presented by Fricke and Laskar [6], Zito [16], Golumbic and Lewenstein [8].
There are several papers that focus on ﬁnding maximum induced matching in dregular graphs. Zito [16] showed that for every k ≥ 1 there is a constant c > 1 such that the problem of approximating maximum induced matching within a factor of c on 4kregular graphs is NP-hard. He also showed that the problem is approximable within d.

484 Graz˙yna Zwoz´niak

Duckworth, Manlove and Zito [3] proved that for any ǫ > 0 it is NP-hard to approximate

maximum three and

induced within a

matching factor of

w7744i12th90in−aǫfainctocruboifc11g2256ra90p−hsǫ.

in graphs of maximum They also presented a

degree greedy

algoritm for approximating a maximum induced matching in d-regular graphs with a

factor of d − 1. Gotthilf and Lewenstein [9] provided a simple greedy approach that

yields an 0.8d approximation factor.

In our paper we concentrate on graphs of maximum degree three. We present an

algorithm which ﬁnds IM (G) – an induced matching of a graph G of size n/6 + O(1).

The approach to the problem is quite novel. Firstly a forest F with large number of

leaves is constructed. Then the algorithm processes small subtrees of the trees T ∈ F

and adds some edges to IM (G). If after this step IM (G) is not maximal then some

edges e ∈ E(G)\E(F ) are added to IM (G). The properties of the forest F let us

signiﬁcantly reduce the number of possible cases which we have to consider looking at

the subgraphs of G. It was shown [16] that the size of an optimum induced matching in

a cubic graph is at most 3n/10, so using our bound we achieve an approximation ratio

of 1.8 for the maximum induced matching problem in cubic graphs.

The paper is organized as follows. In Section 2 we introduce some notation. In

Section 3 we show how to construct the forest F and give some of its properties. In

Section 4 we present the second part of the algorithm – adding edges to IM (G). In

Section 5 we give the main contribution of our paper. The proofs of the facts, lemmata

and the theorem can be found in the full version of the paper.

2 Preliminaries
Let G be a connected undirected graph. We use V (G) to denote the set of vertices in G and E(G) to denote the set of edges in G. For a vertex v ∈ V (G) let ΓG(v) denote the set of vertices {w : (v, w) ∈ E(G)}. The degree of v in G, degG(v), is the number of edges incident to v in G. If T is a rooted tree, then we use LCAT (u, w) to denote the lowest common ancestor of vertices u, w in T , f (v) to denote the father of a vertex v ∈ V (T ), and h(T ) to denote the height of the tree T . By L(T ) we denote the set of leaves of T .
If u, v ∈ V (G) then dist(u, v) denotes the shortest distance between u, v. If e1, e2 ∈ E(G) then dist(e1, e2) denotes the shortest distance between e1, e2.

3 Construction of the forest F
In this section we consider a connected undirected graph G of maximum degree three, where at least one vertex v ∈ V (G) has degree three. We present the algorithm which constructs the forest F for G. This algorithm was also used in [13] and [17], were the forest F was the base structure for the presented algorithms.
In the ﬁrst step our algorithm builds successive trees T0, . . . , Tk of a forest F . Rule 1 puts to the tree Ti two vertices u, w ∈ V (F ) adjacent to a leaf v ∈ L(Ti), see Fig. 1(a). Rule 2 puts to Ti a vertex u ∈ V (F ) adjacent to a leaf v ∈ L(Ti) together with both further neighbours w1, w2 of u, where w1, w2 ∈ V (F ), see Fig. 1(b). We name the two leaves added by Rules 1 and 2 the left and the right son of their father.

Induced Matchings in Graphs of Maximum Degree Three (a) (b) (c) (d)

Ti Ti

v uw

v u

w1 w2

Ti v u1
us
w

xy

Ti v
u1
u2 us w
xy

485

Fig. 1. (a) Rule 1. (b) Rule 2. (c), (d) Rule 3.

Rule 3 initiates a new tree Tj, j > i, see Fig. 1(c)-(d). Let P be a path which starts at v ∈ L(Ti), goes through vertices u1, . . . , us ∈ V (F ), s ≥ 1 and ends at the ﬁrst vertex w ∈ V (F ), where degG(w) = 3, ΓG(w) = {us, x, y} and x, y ∈ V (F ). The vertex us precedes w on P . Rule 3 starts to build Tj rooted at us and adds us, w, x, y to V (Tj) and (us, w), (w, x), (w, y) to E(Tj). We refer to Ti as the father of Tj. This relation determines a partial order in F , so we use some other related terms as e.g. ancestor of the tree.
Let F = {T0, . . . , Tk}. In our algorithm and analysis we use the following notions:
Deﬁnition 1. A vertex v ∈ V (G) is an exterior vertex if v ∈ V (F ). Let EX denote the set of all exterior vertices in G.
Deﬁnition 2. Let r0 be a root of T0. Let R be the set of roots of the trees T1, . . . , Tk.
The algorithm works as follows.
CONSTRUCT F(G).
1. F ← ∅ 2. V (T0) ← {r0, v1, v2, v3}, where r0 is any degree three vertex of G and
v1, v2, v3 ∈ ΓG(r0); E(T0) ← {(r0, v1), (r0, v2), (r0, v3)}; let r0 be a root of T0; i ← 0 3. if it is possible: ﬁnd the leftmost leaf in Ti that can be expanded by the Rule 1 and expand it; go to step 3; else: go to step 4; 4. if it is possible: ﬁnd the leftmost leaf in Ti that can be expanded by the Rule 2 and expand it; go to step 3 else: F ← F ∪ Ti and go to step 5 5. if it is possible: ﬁnd the leftmost leaf v in Ti such that Rule 3 can be applied to v and apply this rule to v; i ← i + 1; let Ti be a new tree created in this step; go to step 3 with Ti else: if Ti has a father: go to step 5 with the father of Ti else: return F .

The properties of the forest F are described by Fact 1. The possibilities for edges e ∈ E(G)\E(F ) are presented in Fig. 1(c),(d) and 2.

486 Graz˙yna Zwoz´niak (a) (b) (c) (d)
Ti Ti Ti

(e)
Ti

(f)
Ti

(g)
Ti

(h)
Ti

(i) (j)
Ti Tj Tj

Tj Tj Tj Tj Tj Tj Tj Tj

Fig. 2. The edges (x, y) ∈ E(G)\E(F ), x, y ∈ EX ∪ L(F ) (dashed lines). Either Ti = Tj or Ti is an ancestor of Tj. Dotted lines denote the paths where all interior vertices have degree two in G.
Fact 1 Let v ∈ V (T ), T ∈ F .
1. If degT (v) = 2 and w ∈ ΓT (v) then degT (w) = 3. 2. If degT (v) = 2 and w ∈ ΓG(v) then w ∈ V (T ). 3. If v is adjacent to the vertex w ∈ EX ∪ R, u ∈ ΓG(v) and u = w then u ∈ V (T ).
The following facts let us consider the subtrees of the trees T ∈ F in some order described precisely in the next section.
Fact 2 Let degT (u) = 2, degT (w) = 2, T ∈ F , (u, w) ∈ E(G)\E(F ), a = LCAT (u, w) and let u be on the left of w in T . Then w is the right son of a, and there are no vertices of degree 2 on the path from u to a in T .
Fact 3 Let r be a root of a tree T ∈ F . Let u ∈ L(T ) and let w be the ﬁrst vertex of degree 2 on the path from u to r in T . If there is v ∈ V (T ) such that degF (v) = 2 and (u, v) ∈ E(G)\E(T ) then w is an ancestor of v in T .
4 Induced matching
Let G′ denote a current graph, F ′ = G′ ∩ F , EX′ = V (G′ )\V (F ′ ). At the beginning G′ = G, F ′ = F and EX′ = EX. In successive steps of our algorithm we add some edges (u, v) to IM (G) and remove from E(G′ ) these added edges with all edges (w, z) where (u, v), (w, z) are at distance at most one. A vertex u is removed from V (G′ ) when the last edge incident to u is removed from E(G′ ).
Now we present the idea of our algorithm, precise description is given later. There are two main phases of our algorithm. After the ﬁrst one all edges E(F ′ ) and some edges E(G′ )\E(F ′ ) are deleted from E(G′ ). Moreover, after this phase all vertices in V (G′ ) have degree at most two. After the second phase the remaining edges E(G′ )\E(F ′ ) are removed from E(G′ ). Each phase consists of the steps. In a single step some edges E¯ are added to IM (G) and appropriate edges Ed and vertices Vd are deleted from G′ . Our goal is to choose E¯ in such a way that |Vd| ≤ 6|E¯|. Since after the ﬁrst phase every connected component of G′ is either a path or a simple cycle, the way of adding some of their edges to IM (G) is obvious. Now we will describe more precisely the ﬁrst phase.

Induced Matchings in Graphs of Maximum Degree Three 487

Fig. 3. Vertices v1, v2, v3 and z would be removed from V (G′ ) if the edge (u, w) was added to IM (G). In the cases (b)-(d) there exist edges (vi, f (vi)), (f (vi), f (f (vi))) ∈ E(F ′ ).

Let T ′ = G′ ∩ T , where T ∈ F . Let Sx′ denote the subtree of T ′ rooted at some

vertex x ∈ V (T ′ ). We deﬁne Sx′+ as a tree, where E(Sx′+) = E(Sx′ ) ∪ E+ and E+ ⊆

E(G′ )\E(F ′ ) denotes some edges incident to V (Sx′ ) (precise deﬁnition of Sx′+ is

later). We {(u, w) :

process u, w ∈

bottom-up small V (Sx′+), u, w =

subtrees Sx′ x} such that

of if

T′ we

and we look added E¯ to I

for the edges M (G) then

given E¯ =

1. the set E(Sx′ ) would be empty; 2. the equality E(T ′ ) = E(Sr′ ) would hold, where r is the root of T ; 3. the vertices v ∈ EX′ with degG′ (v) = 3 and some neighbours in V (Sx′ ) would be
removed or would change degree into 1 or 2.

In the special cases when x ∈ {r0} ∪ R and h(Sx′+) = 1 we add to IM (G) an edge incident to x.

The special care is needed when u ∈ V (Sx′ ), v ∈ V (Sx′ ), degT ′ (v) = 2 and (u, v) ∈ E(G′ ), because if we added an edge incident to u to IM (G) then the equality E(T ′ ) = E(Sr′ ) might not hold (there could be two connected components: Sr′ and
the tree rooted at the son of v). That is why in our algorithm the vertices y, where

degT ′ (y) = 2 and degG′ (y) = 3, are treated as ”milestones”. More precisely, we consider in post-order subtrees Sy′ of T ′ , where (degT ′ (y) = 2 and degG′ (y) = 3) or (y is the root of T , T ′ = G′ ∩ T ), and process bottom-up subtrees Sx′ of Sy′ .

When we process Sx′+ some vertices v ∈ V (Sx′+) may be removed. To reduce the number of possible cases which we have to consider looking at these vertices we

make some preprocessing and add to IM (G) some edges e ∈ E(G′ )\E(F ′ ) which are

close After

to Sx′ . Later we will deﬁne precisely the sets this operation we process Sx′+ and add some

C1(Sx′ edges

) and E¯ to

C2(Sx′ ) of such IM (G). Now a

edges. vertex

z ∈ V (Sx′+) is removed if either z ∈ N or ΓG′ (z) ⊆ N , where N = {v : (u, v) ∈

E(G′ ), u ∈ V (Sx′+), v ∈ V (Sx′+) and (u, w) ∈ E¯ for some w ∈ V (Sx′+)}, see Fig. 3.

In our algorithm we use some kind of accounting analysis. We add some edges E¯

to IM (G) if they remove at most 6|E¯| vertices from V (G′ ). But in some cases we let

the number of removed vertices be larger, let us say 6|E¯| + x. In these cases we choose

x vertices which are still in V (G′ ) and mark them. Later these marked vertices will be

double counted in the process of adding edges to IM (G). Every chosen vertex may be marked at most once. It is possible to mark a vertex a ∈ V (G′ ) if

488 Graz˙yna Zwoz´niak

(a)
′ Sx c1

d1 ′ Sx
b1 c1 c2
a

d1 d2

d1d2d3

′

b1

b2

Sx b1 b2 c1c2c3

b3

aa

′ Sx a

(b)

d1 d2 b1 b2 f1 f2

d1 d1 d2 d1d2d3

b1 b1 b2b1 b2 b3

aa

aa

d1 d2 b1 b2 f1 f2

Fig. 4. A vertex a may be marked. (a) situations before adding E¯ to IM (G), (b) situations after adding E¯ to IM (G). It is possible that ci1 = ci2 , i1, i2 ∈ {1, 2, 3}.

1. before adding edges E¯ to IM (G) there are edges (ci, bi), (bi, a) ∈ E(G′ ), (bi, di) ∈ E(F ′ ), such that degG′ (a) = j, 1 ≤ j ≤ 3, 1 ≤ i ≤ j, and after adding edges E¯ to IM (G) all edges (ci, bi) would be removed, a ∈ V (G′ ), at least one bi ∈ V (G′ ), and if bi ∈ V (G′ ) then degG′ (bi) = 2, or
2. before adding edges E¯ to IM (G) there are edges (a, bi), (bi, fi) ∈ E(G′ ), (bi, di) ∈ E(F ′ ), degG′ (a) = 3, degG′ (fi) = 1, where i = 1, 2 and adding edges E¯ to IM (G) degG′ (a) = 2 and (a, bi), (bi, fi), (bi, di) ∈ E(G′ ), see Fig. 4.
The choice of vertices which can be marked is intentional. In most of these cases we
do not need to propagate the marking process, because we are sure that marked vertices
are in the trees, where it is possible to add y edges to IM (G) and remove less than 6y vertices from V (G′ ).
The algorithm starts with the procedure CONSTRUCT IM(G′ ), which realizes two main phases of our algorithm: the ﬁrst one which removes E(F ′ ) (step 1 of the procedure) and the second one which removes the remaining edges of G′ (steps 2 and 3 of
the procedure). The procedure MATCHING(T ′) applies the procedure MATCH() to the subtrees of
T′. The procedure MATCH(Sy′ ) starts with preprocessing steps. Then it tries to remove
the edges of the trees Sx′+, where x ∈ V (Sy′+) and h(Sx′+) = 2. If it is not possible, trees Sx′+ of height three are considered. If it is not possible, trees Sx′+ of height four are considered. If it is impossible to remove the edges of any tree of height two, three or four and there is a tree Sx′+ of height ﬁve then some edges are added to IM (G) and E(Sx′+) are removed. In the special case when x = r0, degG′ (x) = 3 and h(Sx′+) > 1 we split Sx′+ into two trees and successively process them.

y8

y1 y2

y4y5 y3

y7 y6

Fig. 5. The tree T ′ ∈ F ′ rooted at y8. The algorithm considers successively subtrees rooted at y1, y2, y3, y4, y5, y6, y7, y8.

Induced Matchings in Graphs of Maximum Degree Three 489

CONSTRUCT IM(G′ )

1. apply the procedure MATCHING() in post-order to T ′ ;

2. while there is a path {(v1, v2), . . . , (vk−1, vk)}: ADD((v3i+1, v3i+2)),

i = 0,

...,

k−2 3

;

3. while there is a cycle {(v1, v2), . . . , (vk, v1)}: ADD((v3i+1, v3i+2)),

i = 0,

...,

k−3 3

.

MATCHING(T ′ )
1. apply the procedure MATCH() in post-order to the subtrees Sy′ of T ′ such that (degT ′ (y) = 2 and degG′ (y) = 3) or (y is the root of T , T ′ = G′ ∩ T ).

MATCH(Sy′ )
1. if h(Sy′ ) > 0 apply the procedure MATCH() to the trees rooted at the sons of y; 2. while there is an edge (a, b) ∈ C1(Sy′ ): ADD((a, b)); 3. while there is an edge (a, b) ∈ C2(Sy′ ): ADD((a, b)); 4. for (i = 2; i ≤ 5; i + +): 5. if there is a tree Sx′+, where x ∈ V (Sy′+), x = r0 or degF ′ (x) = 3, h(Sx′+) = i
and M (Sx′+) = ∅: MAKE MATCH(Sx′+) and goto 2; 6. if h(Sy′+) = 1 and y ∈ {r0} ∪ R or |E(G′ )| = 1: ADD((y, u)) where u is a son of
y; 7. if y = r0 and degF ′ (y) = 3:
let Sy′∗ be a part of Sy′+ where E(Sy′∗) = E(Su′+1 ) ∪ E(Su′+2 ) ∪ (y, u1) ∪ (y, u2) and h(Su′+1 ) ≥ h(Su′+2 ) ≥ h(Su′+3 );
(a) MAKE MATCH(Sy′∗); (b) if E(Sy′+) = ∅: MAKE MATCH(Su′+3 ).

MAKE MATCH(Sx′+) 1. ADD(M (Sx′+)); 2. while there is an edge e ∈ I(T ′ ): ADD(e).
Now we will describe notations used in our procedures. Let Vu(E¯) (Vm(E¯)) denote the set of unmarked (marked) vertices which would be removed from V (G′ ) if the edges E¯ were added to IM (G). The process of adding edges to IM (G) is realized by the procedure ADD(E¯) which adds the edges E¯ to IM (G) and removes appropriate vertices and edges from G′ . Moreover, if |Vu(E¯)| + 2|Vm(E¯)| = 6|E¯| + x and x > 0 then the procedure ADD(E¯) marks x vertices which are still in V (G′ ). The only exception is the situation when the last tree is processed. In

490 Graz˙yna Zwoz´niak
this case we have x = 0, but there are some cases when the number of removed vertices may be larger than 6|E¯|. That is why the size of IM (G) is n/6 + O(1).
In the procedure MATCH(Sy′ ) subtrees Sy′ of T ′ ∈ F ′ are considered. Let C1(Sy′ ) and C2(Sy′ ) be the sets of the edges which are removed in preprocessing phase (steps 2 and 3 of MATCH(Sy′ )). C1(Sy′ ) contains the edges (v1, v2) ∈ E(G′ )\E(F ′ ) such that v1, v2 ∈ EX′ , degG′ (v1) = 3 and v1 is at distance at most two from some vertex w ∈ V (Sy′ ).
C2(Sy′ ) contains the edges e ∈ E(G′ )\E(F ′ ) such that
1. e is at distance one from some leaf of Sy′ or e is at distance two from some interior vertex of Sy′ ;
2. for every Sz′ ⊆ F ′ : e is at distance at least one from any leaf of Sz′ ; 3. for every Sz′ ⊆ F ′ : e is at distance at least two from any interior vertex of Sz′ .
Let P ′ be the set of the edges which can be processed together with Sy′ . P ′ contains the following paths:
1. P = {(v1, v2)}, where v1 ∈ L(Sy′ ), and v2 is not the endpoint of any edge e ∈ E(F ′ ).
2. P \(vk−1, vk), where P = {(v1, v2), . . . , (vk−1, vk)}, k ≥ 3, v1, vk ∈ V (Sy′ ), (vi−1, vi) ∈ E(G′ )\E(F ′ ) where i = 2, . . . , k, and degG′ (vj) = 2 where j = 2, . . . , k − 1.
We deﬁne a tree Sy′+ as Sy′ ∪ P ′′ , where P ′′ ⊆ P ′ and if e ∈ P ′ \P ′′ then Sy′+ ∪ {e} contains a cycle.
When we consider some tree Sx′+, where x ∈ V (Sy′+) then we select a set of the edges M (Sx′+) which can be added to IM (G). This set fulﬁl the following conditions:
1. if h(Sx′+) ≥ 2 and (a, b) ∈ M (Sx′+) then a, b ∈ V (Sx′+)\{x}; 2. if h(Sx′+) = 1 and f (x) does not exist or is not in V (F ′ ) then a, b ∈ V (Sx′+); 3. if e1, e2 ∈ M (Sx′+) then dist(e1, e2) > 1; 4. if |Vu(M (Sx′+))| + 2|Vm(M (Sx′+))| = 6|M (Sx′+)| + z and z > 0 then at least z
vertices can be marked; 5. if we added M (Sx′+) to IM (G) then the set E(Sx′+) would be empty.
If it is possible to ﬁnd such a set (M (Sx′+) = ∅) then we add M (Sx′+) to IM (G). The choice of M (Sx′+) depends on the shape of Sx′+ and the edges incident to Sx′+. When h(Sx′+) = 2 then in most of the cases it is possible to choose the edges presented in Fig. 7. Technical details are presented in the full version of the paper.
Let us introduce one more deﬁnition. We deﬁne I(T ′ ) as E(T ′ )\E(Sr′ ), where r is the root of T and T ′ = T ∩G′ . Such edges may appear only if before removing E(Sx′+) some vertex u of Sx′+ was connected by an edge with a vertex y of degree two in F ′ , and the tree rooted at y was not completely removed by MATCH(Sy′ ), see Fig. 6.
In our procedures we use the following notions: vertices ui are the sons of x, vertices vi, j are the sons of ui, vertices wi, j, k are the sons of vi, j, i ∈ {1, 2, 3} j, k ∈ {1, 2}.

Induced Matchings in Graphs of Maximum Degree Three 491
Fig. 6. (a) After MATCH(Sy′ ) one edge (y, u1) ∈ E(Sy′ ) may be still in E(G′ ). In such a case degG′ (u1) = 1. (b) After MATCH(Sy′ ) two edges (y, u1), (u1, v1,1) ∈ E(Sy′ ) may be still in E(G′ ). In such a case degG′ (u1) = 2 and there are edges (v1,1, bi), (bi, ai), (bi, ci), ∈ E(G′ ), i ∈ {1, 2} where degG′ (ai) = 1 and (bi, ci), ∈ E(F ′ ). (c) If we add (u, z) to IM (G) then (y, v) will be removed and (v, w) will be in E(T ′ )\E(Sr′ )
5 Main theorem
The main contribution of this paper is the following theorem.
Theorem 1. Let G be a connected graph of maximum degree three where |V (G)| = n. Our algorithm constructs for G an induced matching IM (G) of size at least n/6+O(1) in linear time.
The following lemmata are used in the proof of Theorem 1. Lemma 1. Let us consider the procedure CONSTRUCT IM(G′ ). 1. After step 1 E(F ′ ) = ∅ and ∀v∈V (G′ )degG′ (v) ≤ 2. 2. After step 2 ∀v∈V (G′ )degG′ (v) = 2. 3. After step 3 V (G′ ) = ∅. Lemma 2. After step 1 of the procedure MATCHING(T ′) E(T ′ ) = ∅. Lemma 3. Let us consider the procedure MATCH(Sy′ ). 1. If y ∈ R ∪ {r0} then after MATCH(Sy′ ) E(Sy′ ) = ∅. 2. If degT ′ (y) = 2 and degG′ (y) = 3 then after MATCH(Sy′ ) at most two edges of Sy′
may be still in E(G′ ), see Fig. 6(a)-(b). 3. If e ∈ C1(Sy′ ) and the procedure ADD(e) is called in step 2 of the procedure
MATCH(Sy′ ) then it removes at most 6 vertices from V (G′ ). All these vertices are unmarked. 4. If e ∈ C2(Sy′ ) and the procedure ADD(e) is called in step 3 of the procedure MATCH(Sy′ ) then it removes at most 6 vertices from V (G′ ). All these vertices are unmarked. Lemma 4. If h(Sy′+) ≥ 5 then it is possible to ﬁnd a tree Sx′+, where x ∈ V (Sy′+) and M (Sx′+) = ∅.

492 Graz˙yna Zwoz´niak
Fig. 7. Basic shapes of Sx′+ where h(Sx′+) = 2. The edges added to IM (G) are bold.
References
1. P. Alimonti, V. Kann Some APX-completeness results for cubic graphs, Theoretical Computer Science, 237, pages 123–134, 2000.
2. K. Cameron Induced matchings, Discrete Applied Mathematics, 24, pages 97-102, 1989. 3. W. Duckworth, D. F. Manlove, M. Zito On the approximability of the maximum induced
matching problem, The Journal of Discrete Algorithms, 3(1), pages 79-91, 2005. 4. P. Erdo˝s Problems and results in combinatorial analysis and graph theory, Discrete Mathe-
matics, 72, pages 81-92, 1988. 5. R. J. Faudree, A. Gya´rfas, R. H. Schelp, Z. Tuza Induced matchings in bipartite graphs,
Discrete Mathematics, 78, pages 83-87, 1989. 6. G. Fricke, R. Laskar Strong matchings in trees, Congressus Numerantium, 89, pages 239-
244, 1992. 7. M. C. Golumbic, R. C. Laskar Irredundancy in circular arc graphs, Discrete Applied Math-
ematics, 44, pages 79-89, 1993. 8. M. C. Golumbic, M. Lewenstein New results on induced matchings, Discrete Applied Math-
ematics, 101, pages 157-165, 2000. 9. T. Gotthilf, M. Lewenstein Tighter Approximations on Greedy for Maximum Induced Match-
ings in Regular Graphs, Third Workshop on Approximation and Online Algorithms, LNCS 3879, pages 270-281, 2005. 10. R. Greenlaw, R. Petreschi Cubic graphs, ACM Computing Surveys, 27, pages 471-495, 1995. 11. P. Hora´k, H. Qing, W. T. Trotter Induced matchings in cubic graphs, Journal of Graph Theory, 17(2), pages 151-160, 1993. 12. C. W. Ko, F. B. Shepherd Adding an identity to a totally unimodular matrix, London School of Economics Operations Research Working Paper LSEOR.94.14, 1994. 13. K. Lorys´, G. Zwoz´niak Approximation algorithm for the maximum leaf spanning tree problem for cubic graphs, Proceedings of the 10th Annual European Symposium on Algorithms, LNCS 2461, pages 686–697, 2002. 14. A. Steger, M. Yu On induced matchings, Discrete Mathematics, 120, pages 291-295, 1993. 15. L. J. Stockmeyer, V. V. Vazirani NP-completeness of some generalizations of the maximum matxhing problem, Information Processing Letters, 15(1), pages 14-19, 1982. 16. M. Zito Maximum induced matchings in regular graphs and trees, The 25th International Workshop on Graph-Theoretic Concepts in Computer Science, LNCS 1665, pages 89-100, 1999. 17. G. Zwoz´niak Small independent edge dominating sets in graphs of maximum degree three, Proceedings of the 32nd Conference on Current Trends in Theory and Practice of Computer Science, LNCS 3831, pages 556-564, 2006.

Subsystems of Iterated Inductive Deﬁnitions
Bahareh Afshari and Michael Rathjen
University of Leeds, Leeds UK
In 1963, G. Kreisel [5] initiated the study of formal theories featuring inductive deﬁnitions. Subsystems of the theories of iterated inductive deﬁnitions (IDn) such as the ﬁxed point theories IˆDn where investigated by Aczel and Feferman in connection with Hancock’s conjecture about the strength of Martin-L¨of type theories with universes. Another interesting type of theory lying between IˆDn and the usual IDn is IDn∗ . To illustrate this in the case n = 1, in contrast to IˆD1, ID1∗ has an induction principle for the ﬁxed points but it is restricted to formulas in which other ﬁxed points occur only positively. Results about the theories IDn∗ were obtained by Friedman, Feferman [4], and Cantini [2]. However, they did not settle the proof-theoretic strength of the theories IDn∗ . I would like to talk about our recent results revealing the strength of these theories.
References
1. Wilfried Buchholz, Solomon Feferman, Wolfram Pohlers and Wilfried Sieg. Iterated inductive deﬁnitions and subsystems of Analysis: Recent Proof-Theoretical Studies theories, Springer-Verlag, Berlin, Heidelberg, 1981.
2. Andrea Cantini. A note on a predicatively reducible theory of elementary iterated induction, Bollettino U.M.I., pp. 413-430, 1985.
3. Andrea Cantini. On the relation between choice and comprehension principles in second order arithmetic, Journal of Symbolic Logic 51, pp. 360–373, 1986.
4. Solomon Feferman. Iterated inductive ﬁxed-point theories: Application to Hancock’s conjecture, Patras Logic Symposion, pp. 171–196, North-Holland, Amsterdam, 1982.
5. G. Kreisel. Generalized inductive deﬁnitions. Tech. rep., Stanford University, 1963. 6. Michael Rathjen. Auwahl und Komprehension in Teitsystemen der Analysis, M.Sc.
thesis, University of Mu¨nster, Germany, 1985. 7. K. Schu¨tte. Proof Theory, Springer-Verlag, Berlin, Heidelberg, 1977. 8. Helmut Schwichtenberg. Proof Theory: Some Applications of Cut-Elimination,
Handbook of Mathematical Logic, pp. 868–895, North-Holland, 1977. 9. Stephen G. Simpson. Subsystems of Second Order Arithmetic, Springer-Verlag,
Berlin, Heidelberg, 1999.

Query Algorithms for Detecting Hamming and Reed-Solomon Codes
Rubens Agadˇzanjans
Institute of Mathematics and Computer Science University of Latvia, Rain¸a bulv. 29, R¯ıga, LV-1459, Latvia.
ruben.agadzanyan@gmail.com
In this talk we will compare quantum and classical query complexity of some Boolean functions. This is the model where the Boolean function is known, but its arguments are unknown. So, to compute the function, the query algorithm asks for values of particular arguments. The complexity of the algorithm is the number of queries. Up to now there have been discovered some, though few, Boolean functions whose quantum query algorithm is better than classical. Here we will talk about another group of such functions, based on Hamming and ReedSolomon error-correcting codes. There is a 25% improvement with the quantum algorithm for the Hamming codes and a 50% improvement for the Reed-Solomon codes (which repeats the previously best known achievement).
Key words: Boolean functions, polynomial degree, query algorithms, Hamming code, Reed-Solomon code.
References
1. R. Agadzanjans, J. Smotrovs. Eﬃcient Quantum Query Algorithms Detecting Hamming and Reed-Solomon Codes. In Proc. of the SOFSEM 06, 2006
2. A. Ambainis. Polynomial degree vs. quantum query complexity. In Proc. of the 44th IEEE FOCS, 2003.
3. H. Buhrman and R. de Wolf. Complexity Measures and Decision Tree Complexity : A Survey. Theoretical Computer Science, v. 288(1): 21–43, 2002
4. R. Freivalds, M. Miyakawa, H. Tatsumi. An Exact Quantum Query Algorithm for a Speciﬁc Boolean Function. In Proc. of the EQIS 04, 2004.
5. J. Gruska. Quantum Computing. McGraw-Hill, 1999. 6. R. W. Hamming. Error detection and error correcting codes. Bell System Technical
Journal, 26(2):147–160, 1950. 7. M. Nielsen, I. Chuang. Quantum Computation and Quantum Information. Cam-
bridge University Press, New York, 700pp., 2000. 8. C. Papadimitriou. Computational Complexity. Addison-Wesley, Reading, 500pp.,
1994. 9. I. S. Reed,G. Solomon. Polynomial codes over certain ﬁnite ﬁelds. J. Soc. Indust.
Appl. Math., v.8, p.300–304, 1960.
Research supported by the European Social Fund.

Expressive Power of Graph Logic
Timos Antonopoulos and Anuj Dawar
University of Cambridge
We present results on the expressive power of Graph Logic, a spatial logic for querying graphs introduced by Cardelli et al. [1] and studied further by Dawar et al. [2]. Graph Logic is an extension of First Order Logic with a second order quantiﬁer over edges of restricted form, and a ﬁrst order quantiﬁer over labels of edges. In particular, if G is some graph, all one can do with the second order quantiﬁer is express that there exists a set of edges X of the graph G such that some formula φ is satisﬁed by the subgraph of G containing exactly the edges in X, and some formula ψ is satisﬁed by the subgraph of G with exactly the remaining edges not in X.
It has been observed that GL is a sublogic of Monadic Second Order Logic with quantiﬁcation over edges and additional ﬁrst order quantiﬁcation over edge labels (MSO for short). Although it seems that GL is strictly less expressive than MSO, many interesting properties have been shown to be expressible in GL. Furthermore it was shown in [2] that GL is able to express complete problems on any level of the Polynomial Hierarchy and that GL and MSO are equi-expressive when restricted to words. Marcinkowski [3] showed that this richer form of MSO with quantiﬁcation over edge labels, is more expressive than GL.
As this richer form of MSO is not the one we usually deal with, the case where we omit the ﬁrst order quantiﬁcation over edge labels from both logics is a more interesting one. We show that this restriction of GL is indeed strictly less expressive than the one of MSO. Moreover we show that this is the case even when restricted to the class of forests.
References
1. L. Cardelli, P. Gardner and G. Ghelli, A spatial logic for querying graphs, ICALP 2002, Springer LNCS, 2380, 597–610.
2. A. Dawar, P. Gardner and G. Ghelli, Expressiveness and Complexity of Graph Logic, Information and Computation, 205 (2007) 263–310.
3. J. Marcinkowski, On The Expressive Power of Graph Logic, CSL 2006, Springer LNCS, 4207, 486–500.

The Lost Melody Theorem: Inﬁnite Time Register Machines
Merlin Carl and Peter Koepke
Mathematisches Institut, Universit¨at Bonn, Germany
In ([1]) it is shown that, for ITTMs there are sets which are computable, but not writeable. We ask whether an analogous theorem holds for ITRMs as introduced in ([2]). We start with some deﬁnitions. P x indicates that register machine program P is run with x as an oracle. A real r (a subset of ω) is ITRM-decidable (decidable) iﬀ there is a program P such that P x(0) = 1 ↔ x = r and the former is halts for every x. It is ITRM-computable (computable) iﬀ there is P such that P ∅(n) = 1 ↔ n ∈ r, P(n) a total function. A countable ∈-model M can be coded by a real by ﬁxing a surjection s : ω → |M | and setting: r := {i ∈ ω : ∃m, n ∈ ω[p(m, n) = i ∧ (b(m) ∈ b(n))M ]}, where p is the Cantor pairing function. Jα is the α-th level of the Jensen hierarchy of constructible sets. Now we can formulate our result: Theorem: There is a real r which is decidable but not computable. In fact, we can take r to be the <L-minimal code in the sense deﬁned above for the ∈-minimal model Jα of ZF −. Proof: Let r be as speciﬁed in the theorem statement. Then r is not computable, for if P computes r, the computation can be simulated in Jα and since ITRMcomputations are absolute between models ZF −, there is an ∈-formula φ such that φ(n) ↔ n ∈ r, so r ∈ Jα. But then r codes an element of Jα, and α is not minimal. But r is decidable: First, we check whether the ∈-relation given by r is wellfounded, as described in [2]. From now on, we suppose that this is the case. The ﬁrst surjection s : ω>1 → Jα is an element of Jα+2. So r ∈ Jα+2 and the question whether a real x satisﬁes the above conditions is equivalent to Jα+2 |= φ(x) for some φ. Now, elements of Jα+2 can be coded by terms of the form f (k1, ..., kn), where f is a composition of Goedel functions and ki ∈ ω for 0 < i < n + 1, which again can be compressed into a single natural number by an appropriate use of p. Questions of this form are then solved recursively by unfolding the constructions given by the f’s, ending up with questions whose answers can be directly read from r.
References
[1] Hamkins,J.D. and Lewis,A.: Inﬁnite Time Turing machines, J.of Symbolic Logic 65(2), (2000),567-604
[2] Koepke.P. and Miller,R.: An Enhanced Theory of Inﬁnite Time Register Machines. Logic and Theory of Algorithms, Fourth Conference on Computability in Europe, CiE 2008 (June 2008)

Comparing Notions of Fractal Dimension
Chris J. Conidis The University of Chicago, Chicago, IL USA We construct a countable Π10-class X ⊂ 2ω with eﬀective packing dimension 1. This answers a question of Athreya, Hitchcock, Lutz, and Mayordomo [1] who asked if there is a correspondence principle for eﬀective packing dimension, as in the case of eﬀective Hausdorﬀ dimension [4, 3].
References
[1] Athreya, K.B., Hitchcock, J.M., Lutz, J.H., Mayordomo, E.: Eﬀective strong dimension in algorithmic information and computational complexity. SIAM Journal on Computing 37, 671–705 (2007)
[2] Conidis, C.J.: Eﬀective Packing Dimension of Π10-classes. Proceedings of the American Mathematical Society (to appear).
[3] Hitchcock, J.M.: Correspondence Principles for eﬀective dimensions. Theory of Computing Systems 38, 559–571 (2005)
[4] Lutz, J.H.: The Dimensions of Individual Strings and Sequences. Information and Computation 187, 49–79 (2003)
This work has been published in The Proceedings of the American Mathematical Society [2].

Clockable Ordinals for Inﬁnite Time Register Machines

Tim Fischbach, Peter Koepke, Miriam Nasﬁ, and Gregor Weckbecker

Mathematisches Institut, Universit¨at Bonn, Germany

We do ordinal computability with the Inﬁnite Time Register Machine (ITRM) model of Miller and Koepke ([1]). In analogy with the theory of Inﬁnite Time Turing Machines (ITTM) ([2]) we introduce a notion of ITRM-clockable ordinals corresponding to the running time of a computation.

Deﬁnition 1. Let α ∈ Ord. α is ITRM-clockable iﬀ there is an ITRM program P and a halting computation on input (0, . . . , 0) with zero oracle Z

I : α + 2 → ω, R : α + 2 → (ωω)
All natural numbers and all ordinals of the form ωn−1 · cn−1 + . . . + ω · c1 + c0 for some n ∈ ω and a sequence (ci)i∈n are clockable. Using ﬁnite vectors of bits we can also show that ωω is ITRM-clockable. In contrast to the ITTM situation there are no gaps in the ITRM-clockable ordinals:

Theorem 1. CLOCK := {α | α ITRM-clockable} is a transitive initial segment of the ordinals and is properly contained in the ITTM-clockable ordinals.

The proof involves a halting criterion for ITRMs:

Lemma 1. Let

I : θ → ω, R : θ → (ωω)

be a inﬁnite time register computation by P with input (0, . . . , 0) and oracle Z. This computation does not stop iﬀ there is some constellation (I , R ) such that

otp({t < θ | (I(t), R(t)) = (I , R )}) ≥ ωω

Furthermore, a ﬁnite speed-up lemma is used:

Lemma 2 (Speed-up lemma). Let α + n be a clockable ordinal for some n ∈ ω, then α itself is clockable.

From theorem 1 we derive some closure properties of CLOCK and the following

Theorem 2. The class of ITRM-clockable ordinals coincides with the class of ITRM-computable ordinals.

References
1. Koepke, P., Miller, R.: An enhanced theory of inﬁnite time register machines. Logic and Theory of Algorithms, Fourth Conference on Computability in Europe, CiE 2008 (June 2008)
2. Hamkins, J.D., Lewis, A.: Inﬁnite time Turing machines. J. Symbolic Logic 65(2) (2000) 567–604

Embedding the Enumeration Degrees in the ω-Enumeration Degrees
Hristo Ganchev
Soﬁa University, Faculty of Mathematics and Informatics 5 James Bourchier blvd., 1165 Soﬁa, Bulgaria ganchev@fmi.uni-sofia.bg
The structure of the ω-enumeration degrees Dω = (Dω, ≤ω) is deﬁned in [1]. It is shown, that Dω is an upper semi-lattice and a jump operation is deﬁned. Furthermore it is shown, that there is a natural embedding of the structure of the enumeration degrees De = (De, ≤) in Dω, preserving l.u.b and jump operation. The image of the enumeration degrees under the natural embedding is denoted by D1 and it is shown in [2] that D1 is ﬁrst order deﬁnable in (Dω; ∪; ).
We shall be concerned with embeddings of De in Dω, not realizable as the composition of an endomorphism of De with the natural embedding. We prove that there are at least 2ℵ0 diﬀerent embeddings, preserving the least upper bound operation, and at least ℵ0 — preserving the jump operation. We prove a necessary and suﬃcient condition for the existence of an embedding preserving both operations. From it, we conclude that the algebraic closure of (De; ∪; ), with respect to the least jump-invert operation (see [2]), is only emebeddable in the algebraic closure Dn of (D1; ≤ω; ∪; ). So, Dn may play an important role in the structural properties of Dω. Finally, we show that Dn is ﬁrst order deﬁnable substructure of (Dω; ∪; ).
References
1. I. N. Soskov, The ω-enumeration degrees, Journal of Logic and Computation 17 (2007), 1193–1214.
2. I. N.Soskov H. Ganchev, The jump operator on the ω-enumeration degrees, to appear.

Computable Models Spectras of Ehrenfeucht Theories
Alexander N. Gavryushkin
Novosibirsk State University, Russia
This paper is connected with next common problem. To characterize the set of models of an Ehrenfeucht theory (a complete theory with ﬁnite number of countable models up to isomorphism) having computable presentation. Types of isomorphism of a prime model and of a saturated model are well known in classical model theory. In [3] there are examples of Ehrenfeucht theories the only computably presentable model of which is the prime one. In [2] there is an example of Ehrenfeucht theory such that only saturated model of the theory has computable presentation. In [1] there is an example of Ehrenfeucht theory such that only one model of the theory has a computable presentation, and that model is neither prime nor saturated. Also there exist an Ehrenfeucht theory T, whose prime and saturated models have computable presentations, and a model of T which lacks in such. The example is in [1].
In [4] there is a syntactic characterization for the class of Ehrenfeucht theories. In [5] there are examples of Ehrenfeucht theories guaranteeing that all possible parameters given in the characterization theorem in [4] are realizable.
The result I want to present is formulated in terms of mentioned characterization and it couldn’t be described using 1-page abstract. Then I have only to state that there are two groups of examples of Ehrenfeucht theories: 1) there are two models of a theory at the same level up to classiﬁcation from [4] one of which has computable presentation and another is not computably presentable; 2) there are levels of the classiﬁcation all models from which have computable presentations and another levels are without computably presentable models.
References
1. Gavryushkin, A. N., Spectra of Computable Models for Ehrenfeucht Theories, Algebra and Logic, 46, No. 3, 149–157, 2007.
2. Khoussainov, B., Nies, A., Shore, R., Computable Models of Theories with Few Models, Notre Dame Journal of Formal Logic, 38, 165–178, 1997.
3. Peretyat’kin, M. G., On Complete Theories with Finite Number of Countable Models, Algebra i Logika, 12, No. 5, 550-576, 1973.
4. Sudoplatov, S. V., Complete Theories with Finitely Many Countable Models. I, Algebra and Logic, 43, No. 1, 62-69, 2004.
5. Sudoplatov, S. V., Complete Theories with Finitely Many Countable Models. II, Algebra and Logic, 45, No. 3, 180-200, 2006.
The work was partially supported by President Grant – 335.2008.1

Deterministic Subsequential Transducers with Additional FIFO-memory
S. V. Gerdjikov
Department of Mathematical Logic and Applications Soﬁa University
st gerdjikov@abv.bg
Regular rewriting rules play a signiﬁcant role in text-processing techniques. It is well known that they can be expressed in terms of rational functions and consequently for each such rule one can eﬃciently construct a transducer or, equivalently a bimachine. Unfortunately the transducers, in general cannot be determinized and the bimachines do not allow to stream a text, and thus one cannot process a text on-line.
We propose a new formalism, which aims at preserving the determinism and still to be able to process the input sequentially, i.e. whithout disposing on the entire text in advance. To this end we incorporate an additional inﬁnite memory, represented as FIFO. We model it in a way to garantee the deterministic, linear traversal of an input text.
We call these machines FIFO-transducers and study their properties with respect to rational functions. Our main eﬀorts concern the composition problem. We show that we can eﬃciently compose FIFO-transducers with subsequentional transducers, but in general they are not closed under composition and they are unable to describe the class of all rational function. We also show a simple example of a function that is not rational but can be represented by such a machine.
Finally, we deﬁne a subclass of rational functions that can be recognized by FIFO-transducers but (in the general case) not by a subsequential transducer. We state suﬃcient conditions for a regular rule in order to be represented by a FIFO-transducer and show how to check these properties algorithmically.

Proof Fragments and Cut-Elimination
Stefan Hetzl
Institute of Computer Languages Vienna University of Technology Favoritenstraße 9, 1040 Vienna, Austria
hetzl@logic.at
Cut-elimination is a proof transformation of fundamental importance. Originally it was introduced by Gentzen together with the sequent calculus [2] and it builds the core of his consistency proof of Peano arithmetic [3]. It also plays an important role in the analysis of mathematical proofs and has deep connections to computation in functional programming languages.
Cut-elimination is usually presented as a set of local rewrite rules with some terminating strategy thus showing the existence of cut-free proofs for all provable sequents. The changes to the global structure of the proof that are caused by these local rewrite steps have traditionally been less investigated.
In this talk we consider proof skeletons (see e.g. [4]) which are abstract representations of the structure of proofs. We will describe the changes the skeleton of a proof undergoes during cut-elimination: Based on its skeleton, a proof with cuts can be split into several pieces (fragments) which are not broken up further by the local rewrite rules. The global eﬀect of cut-elimination on the structure of a proof is therefore shown to be a re-composition of instances of these fragments. The proof is carried out by relying on methods based on cut-elminiation by resolution [1].
This result allows to describe a certain kind of redundancy whose presence is a necessary condition for a cut-free proof to allow strong compression by introduction of cuts. From this characterization follows a lower bound on cutintroduction.
References
1. Matthias Baaz and Alexander Leitsch. Cut-elimination and Redundancyelimination by Resolution. Journal of Symbolic Computation, 29(2):149–176, 2000.
2. Gerhard Gentzen. Untersuchungen u¨ber das logische Schließen. Mathematische Zeitschrift, 39:176–210,405–431, 1934–1935.
3. Gerhard Gentzen. Die Widerspruchsfreiheit der reinen Zahlentheorie. Mathematische Annalen, 112:493–565, 1936.
4. Jan Kraj´ıˇcek and Pavel Pudl´ak. The Number of Proof Lines and the Size of Proofs in First Order Logic. Archive for Mathematical Logic, 27:69–84, 1988.

q-Overlaps in the Random Exact Cover Problem
Gabriel Istrate1 and Romeo Negrea2
1 e-Austria Institute, V.Paˆrvan 4, cam. 045B, Timi¸soara RO-300223, Romania gabrielistrate@acm.org.
2 Department of Mathematics, Universitatea Politehnica din Timi¸soara Victoriei 2, 300006, Timi¸soara, Romania negrea@math.uvt.ro
We prove lower and upper bounds for the threshold of the following problem: given q ∈ (0, 1) and c > 0 what is the probability that a random instance of the k-Exact Cover problem [KM05] has two solutions of overlap qn ± o(n) ? This problem is motivated by the study of phase transitions in Combinatorial Optimization problems has recently motivated (and brought to attention) the geometric structure of the solution space of a combinatorial problem. A remarkable recent advance in this area is due to M´ezard et al. [MMZ05], [DMMZ08]. These papers have provided rigorous evidence that for the random k-satisﬁability problem (with suﬃciently large k) the intuitions concerning the geometry of the solution space provided by the 1-RSB approach are correct. The approach in this paper is similar. We study the overlap distribution of the random k-Exact Cover problem. The phase transition in this problem has been studied in [KM05]. Zdeborov´a et al. [RSZ07],[MMR+07] have applied nonrigorous methods from Statistical Physics (the cavity approach) and have suggested that the 1-step Replica Symmetry Breaking assumption is valid. This motivates us to study the problem q-overlap k-Exact Cover and prove lower and upper bounds on its satisﬁability threshold.
References
[DMMZ08] H. Daud´e, M. M´ezard, T. Mora, and R. Zecchina. Pairs of SAT assignments and clustering in random boolean formulae. Theoretical Computer Science, 393(1-3):260–279, 2008.
[KM05] Vamsi Kalapala and Cris Moore. The phase transition in exact cover. Technical Report cs/0508037, arXiv.org, 2005.
[MMR+07] E. Maneva, T. Meltzer, J. Raymond, A. Sportiello, and L. Zdeborov´a. A hike in the phases of the 1-in-3 satisﬁability problem. In J.P. Bouchaud, M. M´ezard, and J. Dalibard, editors, Lecture Notes of the Les Houches Summer School 2006, pages 491–498. Elsevier, 2007.
[MMZ05] M. M´ezard, T. Mora, and R. Zecchina. Clustering of solutions in the random satisﬁability problem. Physical Review Letters, 94(197205), 2005.
[RSZ07] J. Raymond, A. Sportiello, and L. Zdeborov´a. The phase diagram of random 1-in-3 satisﬁability. Phys. Rev. E, 76(011101), 2007.
Corresponding author. Supported by a Marie Curie International Reintegration Grant within the 6th European Community FP.

Inductive Deﬁnitions over Domain Representable Spaces
Petter Kristian Køber
Department of Mathematics, University of Oslo, Norway petterk@math.uio.no
One of the main constructions in domain theory is the solution of recursive domain equations. Of particular interest are the positive equations, which can be solved iteratively within set theory. The domains we consider are separable Scott domains.
The class of topological spaces with an admissible domain representation has been characterised ([1,2]) as the T0 quotients of countably based spaces (qcb0 spaces). In this talk, we look at how canonical ﬁxed points can be constructed for strictly positive operators over qcb0 spaces, with the least solution of the equation
X = P [Q ⇒ X]
(where P and Q are ﬁxed qcb0 spaces) as the most natural example. The most useful approach to this problem is to consider partial equivalence
relations on domains (domain-pers). We generalise the strictly positive operators as well as the inductive limit construction to domain-pers, and consider an important subclass of domain-pers which is closed under these operations. A strictly positive operator over qcb0 spaces can be represented by a strictly positive operator over domain-pers and the latter admits a least ﬁxed point (w.r.t. certain well-behaved domain embeddings). Under certain natural initial conditions on the operator, this least ﬁxed point gives rise to an admissible representation.
A diﬀerent approach is to consider the operator over qcb0 spaces more generally as an operator over weak limit spaces. For the main example, this gives a least ﬁxed point (w.r.t. continuous, injective maps) and the sequential topological space associated turns out to be homeomorphic to the one derived from the least ﬁxed point of the operator over domain-pers. Combining these results, we obtain a least ﬁxed point for the operator on qcb0 spaces.
The method used works only in the case of strictly positive inductive definitions, thus it remains open whether qcb0 spaces can be deﬁned by positive induction in general. We also discuss whether it is possible to generalise the results by allowing the use of free algebra constructions in the deﬁning equation.
References
1. I.Battenfeld, M.Schr¨oder, A.Simpson, A Convenient Category of Domains. Electronic Notes in Theoretical Computer Science 172, 69-99, Elsevier 2007.
2. G.Hamrin, Admissible Domain Representations of Topological Spaces. U.U.D.M.Report 16, Uppsala University 2005.

The Computable Dimension of Free Projective Planes
Nurlan Kogabaev
Sobolev Institute of Mathematics Koptyug Prospect 4, Novosibirsk 630090, Russia.
kogabaev@math.nsc.ru
Shirshov (cf. [1]) suggested to treat projective planes as partial algebraic systems. In the framework of this approach a projective plane is a structure A, (A0, 0A), · with a disjunction of A into two subsets A0 ∪ 0A = A, A0 ∩ 0A = ∅ and commutative partial operation “·” which satisfy the following properties:
(1) a·b is deﬁned iﬀ a=b and a, b ∈ A0 (or a, b ∈ 0A) with the product a·b ∈ 0A (a·b ∈ A0 respectively);
(2) for all a, b, c ∈ A if a·b, a·c, (a·b)·(a·c) are deﬁned, then (a·b)·(a·c) = a; (3) there exist distinct a, b, c, d ∈ A such that products a·b, b·c, c·d, d·a are
deﬁned and pairwise distinct.
Any free projective plane is freely generated by the set of “points” {b1, b2} ∪ {ai|i ∈ I} and unique “line” c which is incident with ai for each i. From the results of [1] it follows that any countable free projective plane has a computable presentation.
In the present paper we investigate the question of possible computable dimension of free projective planes and the existence problem of computable list for the class of all projective planes (up to computable isomorphism). Applying the Unbounded Models Theorem (cf. [2]) we obtained the following results:
Theorem 1. Every countable free projective plane has computable dimension 1 or ω. Furthermore, such a plane is computably categorical if and only if it has ﬁnite rank.
Theorem 2. The class of all projective planes is not computable (up to computable isomorphism).
References
1. Shirshov, A.I., Nikitin, A.A.: On the theory of projective planes. Algebra and Logic. 20, 330–356 (1981)
2. Goncharov, S.S.: Autostability of models and Abelian groups. Algebra and Logic. 19, 13–27 (1980)

A Classiﬁcation of Theories of Truth
Graham Leigh and Michael Rathjen
University of Leeds, Leeds UK
We augment the ﬁrst-order language of Peano Arithmetic with a unary predicate T with T (x) intended to mean “x is the G¨odel number of a true sentence of arithmetic”. In [2] Friedman and Sheard constructed a list of twelve axioms and rules of inference concerning the predicate T , each expressing some desirable property of truth, and classiﬁed all subsets of the list as either consistent or inconsistent. This gave rise to a collection of nine theories of truth, two of which have been treated in the literature (see [2], Halbach [3], Sheard [4] and Cantini [1] for more details). We uncover the proof-theoretic strength of the remaining seven and in the process construct a proof-theory of truth allowing the systems to be subject to an ordinal analysis.
References
1. A. Cantini, A Theory of Formal Truth Arithmetically Equivalent to ID1, Journel of Symbolic Logic, 55, 1, 244–259, 1990.
2. H. Friedman and M. Sheard, An Axiomatic Approach to Self-referential Truth, Annals of Pure and Applied Logic, 33 1–21, 1987.
3. V. Halbach, A System of Complete and Consistent Truth, Notre Dame Journel of Formal Logic, vol. 35, 3, 311-327, 1994.
4. M. Sheard, Weak and Strong Theories of Truth, Studia Logica, 68, 89–101, 2001.

Monotonicity Conditions over Characterisations
of PSPACE
Bruno Loﬀ1,3 and Isabel Oitavem2,3
1 Dept. of Mathematics, Instituto Superior T´ecnico, Technical Univ. of Lisbon 2 Dept. of Mathematics, Faculdade de Ciˆencias e Tecnologia, Univ. Nova de Lisboa
3 Centro de Matem´atica e Aplica¸c˜oes Fundamentais (CMAF), Univ. of Lisbon
It is an open problem whether P = PSPACE. An algorithm working in polynomial space is allowed to erase previously used cells in order to reuse space, and if we observe the well-known algorithms for PSPACE-complete problems, they always seem to rely heavily on this possibility. Our intuition then indicates that this is the crucial diﬀerence between P and PSPACE. In this presentation, we begin by making this intuition rigourous, proving that the additional power of PSPACE arises exactly from the fact that we are allowed to reuse space. We consider a “write-only” Turing machine, given polynomial space, which is not allowed to erase (or re-write over) cells which have been previously written on; we then prove that under this restriction, polynomial-space-bounded Turing machines decide exactly P.
This will lead us to consider weaker forms of this “write-only” restriction. Consider the natural partial order over binary strings , where w v if |w| < |v| or if |w| = |v| and every symbol of w is less or equal to the corresponding symbol of v in the same position. Then by a detailed observation of the tape conﬁgurations of a write-only machine we ﬁnd that consecutive tape conﬁgurations are monotonically increasing according to this order. It turns out that this monotonicity ensures that any set decided by a monotone polynomial-space bounded computation can also be decided in P. Then P vs. PSPACE is equivalent to asking whether a polynomial-space-bounded computation can be made monotone.
So in the second part of our presentation we apply these results to implicit characterisations of PSPACE. We introduce two multi-sorted function algebras A and B. A is, essentially, the characterisation of PSPACE given by Isabel Oitavem in 97, and B is A with the input-sorted primitive recursion replaced by input-sorted primitive iteration. We show that, analogously to A, B characterises PSPACE. When we restrict the recursion and iteration operators to functions obeying a monotonicity condition for , we obtain two algebras Aˆ and Bˆ. Aˆ is obtained by restricting input-sorted primitive recursion, and Bˆ by restricting input-sorted iteration. Our results are that Bˆ characterises P, Aˆ characterises the polynomial hierarchy, and if we consider a hierarchy Aˆ1, ..., Aˆn, ... by counting the rank of the operator of restricted input-sorted primitive recursion, then Aˆn corresponds to the functions computable with oracles in Σn ∩ Πn. The whole work suggests a new machine-based characterisation of PH and of each level Σn ∩ Πn by polynomial-space write-only deterministic machines coupled with clocks.

Some Results on Local LR-degree Structures
Anthony Morphett University of Leeds
A natural direction of study arising from recent work in algorithmic randomness is to investigate connections between the information content of a set (in the sense of its Turing degree) and the notion of relative randomness that is obtained by adding the set as an oracle. One approach to this is the LR(low for random)-reducibility: an set A is LR-reducible to B if the class of reals MartinL¨of random relative to oracle B is contained in the class of randoms relative to A. The associated degree structure is the LR-degrees. An LR-degree is c.e. (∆02 respectively) if it contains a c.e. (∆02) set.
Although many questions exist about the global and local LR-degree structures, some results have been obtained. For instance, Barmpalias, Lewis and Soskova [1] prove a splitting theorem for c.e. sets, and Barmpalias, Lewis and Stephan [2] prove a weak density theorem for the c.e. LR-degrees. It is not known if full density holds for the c.e. or ∆02 LR-degrees. We describe some additional results about the c.e. and ∆02 LR-degrees, including upward density results. We will also discuss some similarities and diﬀerences between these structures and the c.e. or ∆02 Turing degrees.
References
1. George Barmpalias, Andrew E. M. Lewis, Mariya Soskova, Randomness, Lowness and Degrees, Journal of Symbolic Logic vol.73, Issue 2, pp. 559-577 (2008)
2. George Barmpalias, Andrew E. M. Lewis, Frank Stephan, Π10 classes, LR degrees and Turing degrees, to appear in Annals of Pure and Applied Logic
Supported by MEST-CT-2004-504029 MATHLOGAPS Marie Curie Host Fellowship.

The Axiomatic Derivation of Absolute Lower Bounds
Yiannis N. Moschovakis1,2
1 Department of Mathematics, University of California in Los Angeles 2 Graduate Program in Logic, Algorithms and Computation (MPLA)
Department of Mathematics, University of Athens
I will outline a method for deriving lower bounds for the complexity of problems (especially in arithmetic) which are absolute (universal), i.e., they apply to all algorithms; the key idea is to derive lower bounds from three axioms for algorithms, which are natural and easily shown to apply to all known models of computation.

Exploring the Computational Contribution of a
Non-constructive Combinatorial Principle
Diana Ratiu and Trifon Trifonov
Mathematics Institute, University of Munich, Germany {ratiu,trifonov}@math.lmu.de
We regard Π20-formulas as speciﬁcations of the sort “Given an input x satisfying some property D, is there an algorithm producing an output y from x so that a given requirement G is met?” and look at proofs by contradiction of such statements. Here we will consider a particular combinatorial problem expressed as ∀x D → ∀y(G(x, y) → ⊥) → ⊥ and analyze the algorithms that we extract from its classical proof by two distinct methods — the reﬁned A-Translation and G¨odel’s functional (Dialectica) interpretation.
A-Translation is a combination of G¨odel-Gentzen double negation translation and Friedman’s trick [2], which substitutes ⊥, viewed as a predicate variable, by ∃y G(x, y). [1] proposed a reﬁnement of the method, such that unnecessary double negations are avoided for certain classes of formulas D and G. By coupling this with Kreisel’s (modiﬁed) realisability, one can now associate with the Atranslated proof an extracted term in a Curry-Howard fashion.
G¨odel’s Dialectica interpretation [3] presents an alternative for collecting computational content from non-constructive proofs, namely by tracking counterexamples, i.e., terms instantiating universal assumptions. By employing simple ﬁnite types we can translate every formula A to a decidable relation AD(z, y) between a solution z and a counterexample y. Furthermore, from a derivation of A we can extract a term t, which satisﬁes AD(t, y) for every counterexample y.
We investigate a combinatorial result used to prove Ramsey’s foundational theorem, namely the Inﬁnite Pigeon Hole (IPH) principle. IPH states that any ﬁnitely colored inﬁnite sequence has an inﬁnite monochromatic subsequence. In general, there is no computable functional producing such an inﬁnite subsequence. However, IPH can be used to give a simple classical proof of the Π20statement that a ﬁnite monochromatic subsequence of any given length exists. We compare the programs extracted from this corollary using the aforementioned methods and discuss how they reﬂect the computational meaning of IPH.
References
1. Berger, U. and Buchholz, W. and Schwichtenberg, H., Reﬁned Program Extraction from Classical Proofs, Annals of Pure and Applied Logic, 114, pp 3–25, (2002)
2. Friedman, H., Classically and intuitionistically provably recursive functions, D.S. Scott and G.H. Mu¨ller, 669, Lecture Notes in Mathematics, pp 21–28, (1978)
3. G¨odel, K, U¨ ber eine bisher noch nicht benu¨tzte Erweiterung des ﬁniten Standpunktes, Dialectica, 12, pp 280–287, (1958)
The authors gratefully acknowledge ﬁnancial support by MATHLOGAPS (MESTCT-2004-504029), a Marie Curie Early Stage Training Site.

Autostability of Automatic Linear Orders
Alexandra Revenko
Novosibirsk State University, Russia
The class of automatic structures is the easiest class of computable structures from an algorithmic point of view.
Our investigations concern the problem of existence a computable isomorphism between two automatic presentations of a structure. We say that a structure is autostable under automatic presentations if there is a computable isomorphism between any two automatic presentations of this structure [1]. Here we deal with linear orders.
Some description of automatic linear orders was found by B. Khoussainov, S. Rubin and others. Let L = (L, ≤) be a countable linear order. We say that two elements x, y ∈ L are equivalent if there is a ﬁnite number of elements between x and y. The quotient structure with respect to this equivalence relation is a linear order L1 (ordering is induced from the original structure). One can continue this procedure by transﬁnite induction. So the least ordinal α for which Lα = Lα+1 is a F C-rank of L. Then the F C-rank of automatic linear order is ﬁnite [2].
It was earlier showed that all automatic ordinals and automatic scattered linear orders with FC-rank less than 3 are autostable under automatic presentations. We proved that moreover there exist an algorithm which, given two automatic presentations of a scattered linear order with FC-rank less than 3, constructs the computable isomorphism between them. Then every two automatic presentations of scattered linear order with F C-rank 3 are computable isomorphic.
References
1. Ershov, Yu. L., Goncharov, S. S., Constructive Models, Siberian School of Algebra and Logic, 1999.
2. S. Rubin. Automatic Structures. A thesis submitted in partial fulﬁlment of the requirements for the Degree of Doctor of Philosophy. The University of Auckland, 2004.
The work was partially supported by President Grant – 335.2008.1

Quantiﬁers on Automatic Structures
Sasha Rubin
Department of Computer Science, University of Auckland rubin@cs.auckland.ac.nz
An automatic structure is one that has a presentation consisting of ﬁnite or inﬁnite words or trees so that the coded domain and atomic operations are computable by ﬁnite automata operating synchronously on their inputs [KN95] [Blu99]. The fundamental fact about these structures is that they are eﬀectively closed under ﬁrst-order interpretations [BG00]. This result has been strengthened by extending ﬁrst-order logic with certain generalised quantiﬁers (for instance, ‘there exist inﬁnitely many’ and ‘there exist k modulo m many’) [BG00] [KRS04] [Col04] [KL05] [BKR07] [BKR08] [KL08].
Say that a generalised quantiﬁer Q preserves regularity for a class of automatic structures C if C is closed under FO + Q interpretations. In this talk I will present some steps towards a fuller understanding of these quantiﬁers.
I will introduce a natural collection Q of quantiﬁers, each preserving regularity for the ﬁnite-word automatic structures Cfw. The collection Q includes the known quantiﬁers that preserve regularity for Cfw; and every unary quantiﬁer that preserves regularity for Cfw is in Q.
This is part of ongoing work with Valentin Goranko and Moshe Vardi.
References
[BG00] A. Blumensath and E. Gr¨adel. Automatic structures. In 15th Symposium on Logic in Computer Science (LICS), pages 51–62, 2000.
[BKR07] V. B´ar´any, L. Kaiser, and A. Rabinovitch. Eliminating cardinality quantiﬁers from MLO, 2007. manuscript.
[BKR08] V. B´ar´any, L. Kaiser, and S. Rubin. Cardinality and counting quantiﬁers on omega-automatic structures. In STACS ’08: Proceedings of the 25th Annual Symposium on Theoretical Aspects of Computer Science, 2008.
[Blu99] A. Blumensath. Automatic Structures. Diploma thesis, RWTH Aachen, 1999. [Col04] T. Colcombet. Properties and representation of inﬁnite structures. PhD the-
sis, University of Rennes I, 2004. [KL05] D. Kuske and M. Lohrey. First-order and counting theories of omega-
automatic structures. Technical Report Fakult¨atsbericht Nr. 2005/07, Universit¨at Stuttgart, Fakult¨at Informatik, Elektrotechnik und Informationstechnik, 2005. [KL08] D. Kuske and M. Lohrey. Hamiltonicity of automatic graphs. FIP TCS 2008, 2008. [KN95] B. Khoussainov and A. Nerode. Automatic presentations of structures. Lecture Notes in Computer Science, 960:367–392, 1995. [KRS04] B. Khoussainov, S. Rubin, and F. Stephan. Deﬁnability and regularity in automatic structures. In STACS 2004, volume 2996 of LNCS, pages 440–451. Springer, Berlin, 2004.

The Almost Zero ω-Enumeration Degrees
Ivan N. Soskov
Faculty of Mathematics and Computer Science, Soﬁa University 5 James Bourchier Blvd., 1164 Soﬁa, Bulgaria, soskov@fmi.uni-sofia.bg
The jump operator “ ” on the ω-enumeration degrees possesses a surprising inversion property that for every n and every degree a ≥ 0ω(n) there exists a least degree In(a) among the degrees x such that x(n) = a, see [4].
A degree x is almost zero (a.z.) if x ≤ 0ω and for all n, In(x(n)) = x. There exist non-zero a.z. degree. Moreover, there exist incomparable a.z. degrees. Another nice property of the a.z. degrees is that for every pair a and b of a.z. degrees we have (∀n)((a ⊕ b)(n) = a(n) ⊕ b(n)). The a.z. degrees form an ideal called Az which has no minimal upper bound below 0ω. Suppose that C = (C, ≤) is a degree structure and let “ ”be a jump operator deﬁned on the elements of C. Given elements a and b of C, let a b if for some n, a(n) ≤ b(n). Let a ∼ b if a b and b a. Finally for every a ∈ C, set
a∗ = {b : a ∼ b}.
Let a∗ b∗ if a b and denote by JC the partial ordering ({a∗ : a ∈ C}, ). The ordering JR based on the r.e. Turing degrees is studied by Lempp [2]
who shows that it contains a minimal pair over 0∗ and a splitting of (0 )∗. Jockusch, Lerman, Soare and Solovay [1] show that this ordering is dense. The ordering JG based on the Σ20 enumeration degrees is of no particular interest since by a result of McEvoy [3] it is isomorphic to JR.
In the talk we shall present some results about the orderings JAz and JGω based on the almost zero degrees and on the Σ20 ω-enumeration degrees respectively. We shall show that JAz is isomorphic to (Az, ≤ω) and that the orderings JAz and JGω are dense.
References
1. R. I. Soare R. M. Solovay C. G. Jockusch, M.Lerman, Recurseively enumerable sets modulo iterated jumps and extensions of Arslanov’s completeness criterion, J. Symbolic Logic 54 (1989), 1288–1323.
2. S. Lempp, Topics in recursively enumerable sets and degrees, Ph.D. thesis, University of Chicago, 1986.
3. K. McEvoy, Jumps of quasi-minimal enumeration degrees, J. Symbolic Logic 50 (1985), 839–848.
4. I. N. Soskov and H. Ganchev, The jump operator on the ω-enumeration degrees, Ann. Pure Appl. Logic, to appear.

A Sequent Calculus for Intersection and Union Logic

Anastasia Veneti1 and Yiorgos Stavrinos2
1 Department of Computer Science, National Technical University of Athens GR-15773 Zografou, Greece, tassiana98@gmail.com
2 MPLA, Department of Mathematics, University of Athens GR-15784 Zografou, Greece, g.stavrinos@math.ntua.gr

We present a logical formalism for the intersection and union type assignment system IUT [1]. A ﬁrst attempt to this end is the intersection and union logic IUL, a logic whose main structures are binary trees called kits [3, 4]. The rules of IUL are in natural deduction style and are categorized as global or local according to whether they aﬀect all leaves of the kits involved or not. While rules for the intersection and union are meant to be local, the complex notation of kits conceals a certain kind of globality inherent in the union elimination rule. This becomes explicit if we abandon kits and resort to the linear structures employed in [2]. These structures are multisets of judgements (atoms) called molecules. A logic for IUT using molecules, but still in natural deduction style, incorporates a union elimination rule

[(Γi ϕi)]i<n [(Γ σ ∪ τ )]

[(Γi, ϕi ψi)]i<n [(Γ, σ ρ), (Γ, τ ρ)] (∪E)

[(Γi ψi)]i<n [(Γ ρ)]

which has both a global and a local behaviour. Here globality and locality refer
to the number of atoms modiﬁed by the rule in the premise molecules. In the present work we describe IUL with molecules in sequent calculus style.
The main advantage of this formulation is that globality and locality of the union elimination rule are separated in the cut rule and left union rule, respectively.

[(Γi ϕi)]i<n

[(Γi, ϕi ψi)]i<n (cut)

[(Γi ψi)]i<n

M [(Γ, σ ρ), (Γ, τ ρ)] (L∪)
M [(Γ, σ ∪ τ ρ)]

These improvements lead the way in yielding the relation of IUT to the logic IUL.

References
1. Barbanera F., Dezani-Ciancaglini M., and de’Liguoro U., Intersection and Union Types: Syntax and Semantics, Information and Computation 119, 202–230 (1995)
2. Pimentel E., Ronchi Della Rocca S., and Roversi L., Intersection Types from a prooftheoretic perspective, 4th Workshop on Intersection Types and Related Systems, Torino (2008)
3. Ronchi Della Rocca S. and Roversi L., Intersection Logic, Proceedings of CSL’01, LNCS 2142, 414–428 (2001)
4. Veneti A. and Stavrinos Y., Towards an intersection and union logic, 4th Workshop on Intersection Types and Related Systems, Torino (2008)

Anhomomorphic Logic: The Logic of Quantum Realism
Petros Wallden
Raman Research Institute, Sadashivanagar, Bangalore 560-080, India petros.wallden@gmail.com or petros@rri.res.in
Anhomomorphic logic, is a novel interpretation of Quantum Theory (initiated by Sorkin) that comes as a development of the consistent histories approach and is an attempt to retain realism. When using logic to describe (classical) physics, we have a set of possible histories Ω, a set of truth values ( e.g. {True, False}) and the possible maps (φi) that are homomorphisms between the Boolean algebra of subsets of Ω and of the truth values. These maps give rise to diﬀerent realizations (here is where the measure and thus the dynamics enter the picture). It is well known that the above picture cannot hold in quantum theory. One can either restrict the subsets of Ω allowed (standard consistent histories) or change the set of truth values to some Heyting algebra (Isham) or ﬁnally, weaken the requirement that the map is homomorphism. This latter approach is taken in “Anhomomorphic Logic”. The weakening of the requirement to be a homomorphism is replaced by other conditions, that guarantee that most structure is preserved and the basic inference law (modus ponens) still aplies. Thus we have a deductive logic (which is not the case in what is usually referred to as “quantum logic”). In this talk, we will ﬁrst introduce anhomomorphic logic in some detail. Then we will deal with some recent developments on the emergence of classicality and the closely related issue of recovery of probabilistic predictions. The former, essentially means that in some scale of coarse graining, we know that classical (boolean) logic applies. Thus we have to show that the anhomomorphic logic, upon some coarse grainings, results to homomorphic logic (i.e. classical). Finally, the issues of how probabilities arise, is present in several attempts to retain realism in quantum theory (such as many worlds), and in this approach it is resolved with the use of the concept of “approximate preclusion” and by taking a frequentist’s view on probability rather than treating it as propensity.

Author Index

Afshari, Bahareh . . . . . . . . . . . . . . . . . 493 Agadˇzanjan, Ruben . . . . . . . . . . . . . . 494 Aihara, Kazuyuki . . . . . . . . . . . . . . . . 445 Almeida, Marco . . . . . . . . . . . . . . . . . . . . 3 Aman, Bogdan . . . . . . . . . . . . . . . . . . . . 15 Antonopoulos, Timos . . . . . . . . . . . . 495 Antunes, Luis . . . . . . . . . . . . . . . . . . . . . 25 Ayala-Rinc´on, Mauricio . . . . . . . . . . 137 Azevedo, Tiago . . . . . . . . . . . . . . . . . . . 35
Beggs, Edwin . . . . . . . . . . . . . . . . . . . . . 45 Benevides, Mario . . . . . . . . . . . . . . . . . . 35 Benferhat, Salem . . . . . . . . . . . . . . . . .205 Bonet, Isis . . . . . . . . . . . . . . . . . . . . . . . 284
Caldwell, James . . . . . . . . . . . . . . . . . . 254 Calhoun, William . . . . . . . . . . . . . . . . . 55 Carl, Merlin . . . . . . . . . . . . . . . . . . . . . 496 Chiarabini, Luca . . . . . . . . . . . . . . . . . . 64 Cho, Sung-Jin . . . . . . . . . . . 77, 165, 367 Choi, Un-Sook . . . . . . . . . . . . . . . 77, 165 Ciobanu, Gabriel . . . . . . . . . . . . . . . . . . 15 Conidis, Chris . . . . . . . . . . . . . . . . . . . 497 Costa, Antˆonio Carlos . . . . . . . . . . . . . 87
Dawar, Anuj . . . . . . . . . . . . . . . . . . . . . 495 de Miguel Casado, Gregorio . . . . . . . 97 Devlin, Keith . . . . . . . . . . . . . . . . . . . . . . .1 Dimuro, Gra¸caliz . . . . . . . . . . . . . . . . . . 87 Durand-Lose, J´erˆome . . . . . . . . . . . . 107

Hetzl, Stefan . . . . . . . . . . . . . . . . . . . . . 502 Horihata, Yoshihiro . . . . . . . . . . . . . . 157 Hwang, Yoon-Hee . . . . . . . . . . . . 77, 165
Irrgang, Bernhard . . . . . . . . . . . . . . . . 175 Istrate, Gabriel . . . . . . . . . . . . . . . . . . 503
Jain, Sanjay . . . . . . . . . . . . . . . . . . . . . 185 Jansen, Maurice . . . . . . . . . . . . . . . . . 195 Jenhani, Ilyes . . . . . . . . . . . . . . . . . . . . 205 Jervell, Herman Ruge . . . . . . . . . . . . 215
Kahle, Reinhard . . . . . . . . . . . . . . . . . 224 Kar´adais, Basil . . . . . . . . . . . . . . . . . . 234 Kim, Han-Doo . . . . . . . . . . . . . . . 77, 165 Kim, Jin-Gyoung . . . . . . . . . . . . . . . . . .77 Kim, Seok-Tae . . . . . . . . . . . . . . . . . . . 367 Koepke, Peter . . . . . . . . . . . . . . . 496, 498 Kogabaev, Nurlan . . . . . . . . . . . . . . . .505 Korovina, Margarita . . . . . . . . . . . . . 246 Kothari, Sunil . . . . . . . . . . . . . . . . . . . 254 Koutras, Costas . . . . . . . . . . . . . . . . . . 117 Køber, Petter Kristian . . . . . . . . . . . 504
Laﬁtte, Gr´egory . . . . . . . . . . . . . . . . . . 264 Le Roux, St´ephane . . . . . . . . . . . . . . . 274 Le´on, Maikel . . . . . . . . . . . . . . . . . . . . . 284 Leigh, Graham . . . . . . . . . . . . . . . . . . . 506 Li, Chung-Chih . . . . . . . . . . . . . . . . . . 294 Loﬀ, Bruno . . . . . . . . . . . . . . . . . . . . . . 507

Eleftheriou, Pantelis . . . . . . . . . . . . . 117 Elouedi, Zied . . . . . . . . . . . . . . . . . . . . 205
Fischbach, Tim . . . . . . . . . . . . . . . . . . 498 Fokina, Ekaterina . . . . . . . . . . . . . . . . 127
Galdino, Andr´e Luiz . . . . . . . . . . . . . 137 Ganchev, Hristo . . . . . . . . . . . . . . . . . .499 Garc´ıa, Zenaida . . . . . . . . . . . . . . . . . . 284 Garc´ıa Chamizo, Juan Manuel . . . . 97 Gavryushkin, Alexander . . . . . . . . . 500 Gaßner, Christine . . . . . . . . . . . . . . . . 147 Gerber, Annelies . . . . . . . . . . . . . . . . . . 45 Gerdjikov, Stefan . . . . . . . . . . . . . . . . 501

Makowsky, Johann . . . . . . . . . . . . . . . 304 Manousaridis, Angelos . . . . . . . . . . . 324 Mora Mora, Higinio . . . . . . . . . . . . . . . 97 Moreira, Nelma . . . . . . . . . . . . . . . . . . . . 3 Morphett, Anthony . . . . . . . . . . . . . . 508 Moschovakis, Yiannis . . . . . . . . . . . . 509 Mostowski, Marcin . . . . . . . . . . . . . . . 332
Nagy, Benedek . . . . . . . . . . . . . . . . . . . 435 Nasﬁ, Miriam . . . . . . . . . . . . . . . . . . . . 498 Negrea, Romeo . . . . . . . . . . . . . . . . . . .503 Nigam, Vivek . . . . . . . . . . . . . . . . . . . . 344 Nomikos, Christos . . . . . . . . . . . . . . . 117

Oitavem, Isabel . . . . . . . . . . . . . . . . . . 507
Papakyriakou, Michalis . . . . . . . . . . 324 Papaspyrou, Nikolaos . . . . . . . . . . . . 324 Pelupessy, Florian . . . . . . . . . . . . . . . .354 Petrova, Katya . . . . . . . . . . . . . . . . . . . 361 Piao, Yongri . . . . . . . . . . . . . . . . . . . . . 367 Potgieter, Petrus . . . . . . . . . . . . . . . . . 377 Protti, F´abio . . . . . . . . . . . . . . . . . . . . . . 35 Prunescu, Mihai . . . . . . . . . . . . . . . . . 387
Qiu, Daowen . . . . . . . . . . . . . . . . . . . . . 397
Rathjen, Michael . . . . . . . . . . . . 493, 506 Ratiu, Diana . . . . . . . . . . . . . . . . . . . . . 510 Reis, Rog´erio . . . . . . . . . . . . . . . . . . . . . . . 3 Revenko, Alexandra . . . . . . . . . . . . . . 511 Rubin, Sasha . . . . . . . . . . . . . . . . . . . . 512
Sadowski, Zenon . . . . . . . . . . . . . . . . . 407 Seyﬀerth, Benjamin . . . . . . . . . . . . . . 175 Sihman, Marcelo . . . . . . . . . . . . . . . . . . 35 Solon, Boris . . . . . . . . . . . . . . . . . . . . . . 361 Soltys, Michael . . . . . . . . . . . . . . . . . . . 415 Soskov, Ivan N. . . . . . . . . . . . . . . . . . . 513 Souto, Andr´e . . . . . . . . . . . . . . . . . . . . . .25 Stavrinos, Yiorgos . . . . . . . . . . . . . . . 514 Stephan, Frank . . . . . . . . . . . . . . . . . . 185
Tadaki, Kohtaro . . . . . . . . . . . . . . . . . 425 Tajti, A´ kos . . . . . . . . . . . . . . . . . . . . . . 435 Takahashi, Hayato . . . . . . . . . . . . . . . 445 Trifonov, Trifon . . . . . . . . . . . . . . . . . . 510
Vardi, Moshe Y. . . . . . . . . . . . . . . . . . . . . 2 Vasilieva, Alina . . . . . . . . . . . . . . . . . . 453 Veneti, Anastasia . . . . . . . . . . . . . . . . 514 Vorobjov, Nicolai . . . . . . . . . . . . . . . . 246
Wallden, Petros . . . . . . . . . . . . . . . . . . 515 Weckbecker, Gregor . . . . . . . . . . . . . . 498 Weiermann, Andreas . . . . . . . . . . . . . 354 Weiss, Michael . . . . . . . . . . . . . . . . . . . 264 Wilson, Craig . . . . . . . . . . . . . . . . . . . . 415 Winter, Joost . . . . . . . . . . . . . . . . . . . . 463
Ye, Nan . . . . . . . . . . . . . . . . . . . . . . . . . . 185 Yokoyama, Keita . . . . . . . . . . . . 157, 473
Zwo´zniak, Graz˙yna . . . . . . . . . . . . . . 483

Author Index 517


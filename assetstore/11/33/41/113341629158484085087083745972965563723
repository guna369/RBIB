University of Innsbruck

Institute of Computer Science

Cumulative Habilitation Thesis
A Formalization of Termination Techniques in Isabelle/HOL
Ren´e Thiemann January 21, 2013

fu¨r Karin, Hannah und Jonas

Contents

I. Preface

3

1. Introduction

5

2. Major Problems

7

3. Related Work

11

4. Contributions

13

4.1. Certiﬁcation of Termination Proofs using CeTA (Chapter 6) . . . . . . . . . 13

4.2. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules (Chapter 7) . . . . 15

4.3. Signature Extensions Preserve Termination – An Alternative Proof via De-

pendency Pairs (Chapter 8) . . . . . . . . . . . . . . . . . . . . . . . . . . 16

4.4. Modular and Certiﬁed Semantic Labeling and Unlabeling (Chapter 9) . . . 18

4.5. Termination of Isabelle Functions via Termination of Rewriting (Chapter 10) 19

4.6. Generalized and Formalized Uncurrying (Chapter 11) . . . . . . . . . . . . 20

4.7. On the Formalization of Termination Techniques Based on Multiset Order-

ings (Chapter 12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

4.8. Certiﬁcation of Nontermination Proofs (Chapter 13) . . . . . . . . . . . . . 22

4.9. Further Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

4.10. Summary of Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

5. Future Research

27

II. Selected Papers

29

6. Certiﬁcation of Termination Proofs using CeTA

31

6.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

6.2. Formalizing Term Rewriting . . . . . . . . . . . . . . . . . . . . . . . . . . 33

6.3. Certifying Dependency Pairs . . . . . . . . . . . . . . . . . . . . . . . . . . 34

6.4. Certifying the Dependency Graph Processor . . . . . . . . . . . . . . . . . 36

6.4.1. Certifying Graph Decompositions . . . . . . . . . . . . . . . . . . . 37

6.4.2. Certifying Dependency Graph Estimations . . . . . . . . . . . . . . 38

6.4.3. Certifying Dependency Graph Decomposition . . . . . . . . . . . . 41

6.5. Certifying the Reduction Pair Processor . . . . . . . . . . . . . . . . . . . 41

6.6. Certifying the Whole Proof Tree . . . . . . . . . . . . . . . . . . . . . . . . 42

6.7. Error Messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

6.8. Experiments and Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . 44

VI Contents

7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules

47

7.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

7.2. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

7.3. The Subterm Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

7.4. Usable Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

7.5. Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

7.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62

8. Signature Extensions Preserve Termination

63

8.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

8.2. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

8.3. Dependency Pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

8.4. Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

8.5. Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

8.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

9. Modular and Certiﬁed Semantic Labeling and Unlabeling

77

9.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

9.2. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

9.2.1. Term Rewriting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

9.2.2. Semantic Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

9.3. Modular Semantic Labeling and Unlabeling . . . . . . . . . . . . . . . . . 80

9.4. Dependency Pair Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 86

9.5. Problems in Certiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90

9.6. Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

9.7. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

10.Termination of Isabelle Functions via Termination of Rewriting

95

10.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95

10.2. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

10.2.1. Higher-Order Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

10.2.2. Supported Fragment . . . . . . . . . . . . . . . . . . . . . . . . . . 97

10.2.3. Function Deﬁnitions by Well-Founded Recursion . . . . . . . . . . . 98

10.2.4. IsaFoR - Term Rewriting Formalized in Isabelle/HOL . . . . . . . . 99

10.2.5. Terminology and Notation . . . . . . . . . . . . . . . . . . . . . . . 99

10.3. The Reduction to Rewriting . . . . . . . . . . . . . . . . . . . . . . . . . . 100

10.3.1. Encoding Expressions and Deﬁning Equations . . . . . . . . . . . . 100

10.3.2. Embedding Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 101

10.3.3. Rewrite Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

10.3.4. The Simulation Property . . . . . . . . . . . . . . . . . . . . . . . . 102

10.3.5. Reduction of Termination Goals . . . . . . . . . . . . . . . . . . . . 103

10.3.6. Proof of the Simulation Property . . . . . . . . . . . . . . . . . . . 105

10.4. Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106

10.5. Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

10.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

11.Generalized and Formalized Uncurrying

111

11.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

11.2. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112

Contents

VII

11.3. Applicative Rewriting and Uncurrying . . . . . . . . . . . . . . . . . . . . 112 11.4. Uncurrying in the Dependency Pair Framework . . . . . . . . . . . . . . . 118 11.5. Heuristics and Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 123 11.6. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

12.Formalization of Termination Techniques Based on Multiset Orderings 127 12.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 12.2. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 12.3. Formalization of the Generalized Multiset Ordering . . . . . . . . . . . . . 130 12.4. Multiset and Recursive Path Ordering . . . . . . . . . . . . . . . . . . . . 132 12.5. SCNP Reduction Pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 12.6. Certiﬁcation Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 12.7. Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142

13.Certiﬁcation of Nontermination Proofs

143

13.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143

13.2. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

13.3. A Framework for Certifying Nontermination . . . . . . . . . . . . . . . . . 145

13.4. Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

13.5. Formalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

13.6. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156

Bibliography

159

Acknowledgements
Clearly, without an appropriate environment it would not have been possible to develop this thesis. Therefore, I want to express my gratitude to those people that made this thesis possible.
First of all, I want to thank Aart Middeldorp for giving me the freedom to freely let me pursue my research interests in the last years, for his support in providing me with the necessary equipment, and for many useful feedbacks.
Insbesondere mo¨chte ich auch Christian Sternagel danken. Es war eine wunderbare Zeit, jemanden vor Ort zu haben, mit dem man u¨ber Isabelle diskutieren konnte und gemeinsam Ideen in Formalisierungen und Paper umsetzen konnte.
Mein Dank gilt auch Georg Moser. Besonders wa¨hrend langen Isabelle Entwicklungen war es eine Freude, durch ihn die Gedanken wieder frei zu bekommen: sei es durch kurze Exkurse in die Welt der Komplexit¨at, durch politische Diskussionen in Steering Committees, oder besonders durch den Austausch kurzer Anekdoten u¨ber unsere Kleinen.
Of course, I’m also grateful to the whole Computational logic group for a pleasant and friendly research environment.
Natu¨rlich bin ich auch den Entwicklern der Terminierungs Tools AProVE und TTT2 dankbar, insbesondere Bertram Felgenhauer, Carsten Fuhs, Christian Kuknat, Christian Sternagel, Fabian Emmes, und Harald Zankl: ohne deren Entwicklung der Beweisausgabe wa¨re keine Zertiﬁzierung mo¨glich gewesen.
I would also like to express my gratitude to the Isabelle community. Since I started to work with Isabelle, I always got useful feedback on my questions. Moreover, several recent developments on Isabelle have been beneﬁcial for this thesis, like the code generator, the Isabelle collection framework, partial functions, regexp, parallelization, and jedit. Here, special thanks go to Alexander Krauss, Andreas Lochbihler, Florian Haftmann, Lukas Bulwahn, Makarius Wenzel, Peter Lammich, and Tobias Nipkow.
Many thanks also goes to the Austrian Science Fund (FWF). They supported all the selected papers (within the projects P18763 and P22767-N13).
Schließlich m¨ochte ich mich natu¨rlich auch bei meiner Familie bedanken, insbesondere bei Karin: in ihr fand ich immer den so wichtigen Ausgleich, der mich von der sehr technischen Arbeit zuru¨ck in die reale Welt fu¨hrte, und ich bekam immer Ru¨ckhalt, selbst in Zeiten, wo ich nur wenig Zeit fu¨r die Familie hatte.

Part I. Preface

1. Introduction
Termination is the important property of a program that all computation paths produce a result in ﬁnite time. Although undecidable in general, much work has been spent on automated termination analysis. As a result, today there are a variety of powerful tools for automatic termination analysis for various languages: e.g., there are AProVE [39], COSTA [1], Julia [93], Matchbox [106], Polytool [82], Terminator [24], and TTT2 [67]. Several of these tools are competing in the annual international termination competition1 where the aim is to automatically investigate termination and complexity of programs in diﬀerent languages, i.e., currently term rewriting, Prolog, Haskell, or Java.
Due to the complexity of the tools itself, it is obvious that there might be bugs in the implementations leading to wrong answers. And indeed, nearly every year in the competition some bugs were spotted since two tools gave contradicting answers, e.g., by providing a termination and a nontermination proof for the same program. Therefore, it has been identiﬁed as a key challenge to independently certify the correctness of the generated proofs. Due to the size of these proofs a manual inspection is infeasible and also error-prone. However, it can be done using interactive theorem provers like Coq [11], Isabelle [84], and PVS [88]. These theorem provers are used to model various aspects of mathematics, programming languages, etc. in a logical system and afterwards formally verify desired properties.
As a result, since 2007 also certiﬁers have entered the termination competition.
• In the A3PAT project2 [21, 25], the tool Ci ME was extended by a feature that transforms termination certiﬁcates into Coq scripts. These scripts can be checked with help of the underlying Coq library Coccinelle on rewriting.
• Similarly, in the CoLoR project3 [13], the tool Rainbow produces Coq-scripts for certiﬁed termination proofs. It is based on CoLoR, the Coq Library on Rewriting and termination.
• CeTA (Certiﬁed Termination Analysis) with its underlying formalization IsaFoR (Isabelle Formalization of Rewriting) is our own certiﬁer which entered the competition in 2009.
With the help of these three certiﬁers (which we abbreviate by CC for CiME + Coccinelle, RC for Rainbow + CoLoR, and CI for CeTA + IsaFoR), several implementation bugs have been revealed which previously remained undetected. Furthermore, the formalization of the termination techniques also discovered some ﬂaws in pen-and-paper proofs which in at least one case invalidated the main theorem of a termination technique [97].
In this thesis, we report on our work in this area, i.e., the development of CeTA and IsaFoR. It is structured as follows. In Chapter 2 we illustrate four main problems when
1http://www.termination-portal.org/wiki/Termination_Competition 2http://a3pat.ensiie.fr 3http://color.inria.fr

6 Chapter 1. Introduction
trying to develop a certiﬁer. Related work is addressed in Chapter 3. Our own contributions in this area are discussed in Chapter 4. Here, we also explain which part of each selected paper has been developed by the co-authors. Future work is addressed in Chapter 5 which ends Part I of this thesis. Afterwards, in Part II the selected papers are provided in chronological order.

2. Major Problems

In order to develop a certiﬁer for termination proofs three tasks have to be performed.
(i) The deﬁnition of termination has to be formally speciﬁed.
(ii) Termination methods have to be formalized in some library of a theorem prover.
(iii) Starting from a given certiﬁcate, one has to reconstruct a formal termination proof using the termination methods from the previous step.
Once, these tasks have been established, a possible workﬂow for getting certiﬁed termination proofs is as follows, cf. Figure 2.1.
First, an untrusted termination tool is invoked which has to deliver the answer (terminating, nonterminating, don’t know) in combination with a certiﬁcate which has to contain enough information to reconstruct a proof to validate the answer. As an example, the certiﬁcate might be an XML-document which states which termination techniques have been applied and how they have been parametrized.
Afterwards, one can start the reconstruction to obtain a formal proof script. In this step, one has to link the termination techniques in the certiﬁcate to those which have been formalized. Moreover, in the proof script several assertions have to be stated which ensure that the termination techniques are applied correctly.
Finally, the formal proof script must be checked within a proof assistant. In order to maximize the reliability of this certiﬁcation workﬂow, one should use proof assistants which are based on the LCF-approach [43, 45, 89], where all generated proofs have to be accepted by some small trusted kernel. In that way, one can be quite certain that during proof checking only correct proofs are accepted. Moreover, for the major properties and deﬁnitions in the speciﬁcation—which are termination and the rewrite relation in our case—one can try to prove that their formalization corresponds to various alternative deﬁnitions that are present in the literature [5,100]. For example, we deﬁne the rewrite relation as →R := {(C[ σ], C[rσ]) | → r ∈ R} where C ranges over all contexts and σ over all substitutions, and afterwards we prove that our deﬁnition is equivalent to another standard deﬁnition of the rewrite relation: →R = {(t, t[rσ]p) | t|p = σ, → r ∈ R, p ∈ Pos(t)} where Pos(t) is the set of positions in t.

input termination answer + certificate

proof

proof script

tool reconstruction

proof checking

accept/failure

Figure 2.1.: workﬂow for certiﬁed termination proofs

8 Chapter 2. Major Problems
Although the reliability is quite high, we want to mention that there still remain certain risks where we just have to trust the system: the logic of the proof assistant may be inconsistent, there may be bugs in the kernel of the proof assistant as well as in the compiler / interpreter that is used to run the proof assistant, and there may be errors in the operating-system and in the hardware, and the hardware may be damaged. To minimize these risks, several projects have been performed. For example, with Milawa and Jitawa there is a veriﬁed theorem prover running in a veriﬁed Lisp runtime, i.e., soundness of the theorem prover was proven down to x86 machine code [80]. And in another impressive project, an operating-system kernel was proven to be sound [60].
Despite these risks, we believe that formalized proofs contain far less errors than those which are checked by human—we leave it up to the reader to ﬁgure out a list of risks that arise when proofs are solely checked by humans. But even if we trust formalized proofs, we have to establish all three tasks that have been mentioned above.
Whereas the ﬁrst task is easily solved in our case—since the notions of termination and the rewrite relation of a term rewrite system (TRS) are easily deﬁned—the second task is by far more challenging because of the following two major problems.
Problem 1: Formalize many termination techniques Termination tools for term rewriting use several incomparable termination techniques. And for each individual termination technique, we require a fully formalized proof. Here, standard formalization problems may arise: proofs in papers can contain gaps and are often not suﬃciently detailed (“w.l.o.g.”, “easy to see”, “by induction”, “we consider only the most interesting case”, . . . ), they may be based on other nontrivial theorems (from Ramsey theory, from graph-theory, . . . ), they may use nontextual representations like diagrams, and they make use of intuitive arguments which are hard or tedious to describe formally (“we can easily reorder this sequence as follows such that . . . ”). Moreover, we will encounter the problem, that many termination techniques are using completely diﬀerent concepts in their soundness proofs: the proofs are based on syntactic criteria, graph theory, automata theory, algebra, combinatorics, etc.
Hence, to get formalized proofs, all gaps have to be ﬁlled, all background theories have to be formalized, and one has to ﬁnd a good formal model for all the concepts that are used in the proofs.
Problem 2: Combine termination techniques Even if individual termination techniques have been proven correct, for their combination we need a common semantics, such that all techniques are sound w.r.t. that semantics. For example, for the rewrite relation of a TRS luckily there is only one semantics. However, often termination proofs are performed using the notions of dependency pairs and chains, and here there are at least two diﬀerent versions of evaluation (chains and minimal chains) [3]. The problem is that some termination techniques are only sound for chains whereas other require minimal chains. Hence, checking proofs which utilize both kinds of termination techniques cannot be done without adaptation: one ﬁrst has to ﬁnd some notion of chain which is compatible with all termination techniques.
When considering the third task—proof reconstruction—two additional problems arise.
Problem 3: Check application of termination techniques In principle, the certiﬁcate contains all major proof steps, i.e., every termination technique that has been applied is

9
listed in the certiﬁcate in combination with its parameters. However, it must be checked that the technique has been applied correctly, which is a problem that may range from easy via complex to undecidable.
For example, some termination methods are based on simple syntactic criteria which can easily be checked, like the application of dependency pairs or the switch from full- to innermost-termination for orthogonal TRSs. However, for the latter technique one can weaken orthogonality to locally conﬂuent overlay TRSs [46]. In that case, checking the preconditions becomes undecidable as local conﬂuence is undecidable.
As another example, consider termination methods based on well-founded orders, like polynomial orders [73], the lexicographic path order (LPO) [58], or the recursive path order (RPO) [27]. For these methods one has to provide an algorithm to check whether two terms are in relation. This problem is in P (for LPO) [71], NP-complete (for RPO), and undecidable for polynomial orders. In the latter case, suﬃcient decidable criteria like absolute positiveness [56] are used which are again easy to check. Of course, one might enrich the certiﬁcates by providing more detailed evidence why two terms are in relation. Then checking can be done more eﬃciently, but the certiﬁcates easily become bulky and are harder to produce.
As a last example, we consider size-change termination which is a PSPACE-complete problem [74]. Here, there is no chance to enlarge the certiﬁcate such that checking the proof can be done in polynomial time, unless PSPACE = NP. We shortly illustrate why also in practice an enlarged certiﬁcate is not helpful for this technique. One standard algorithm to decide size-change termination works as follows. It computes a (possibly exponentially large) closure of size-change graphs where one has to test that no bad graphs appear in this closure. In principle, one can add the closure to the certiﬁcate, but this does not improve the situation, since checking that the provided closure is correct has the same asymptotic complexity as computing the closure.
To summarize, we require methods to guarantee the correct application of termination techniques which is not always easy.
Problem 4: Obtaining certiﬁcates It is obvious, that for checking a termination proof we require certiﬁcates from the tools. These certiﬁcates should be suﬃciently detailed and in some machine readable format. However, when we started our work, each of the certiﬁers had its own proof format with varying level of detail. Hence, for a termination tool to support many certiﬁer, the tool had to implement many proof printing procedures: one for each certiﬁer. Putting this into another perspective, we had to convince the tool authors to also support and integrate our proof format.
As a solution to this problem we developed a common proof format for certiﬁcates for termination problems: the certiﬁcation problem format (CPF).1 The CPF-format was designed as a combination of all previously existing formats by several groups: members from all three certiﬁers CC, RC, and CI contributed, as well as members from the termination tools AProVE and TTT2. As a result, today termination tools only have to support one output format (CPF) and then all three certiﬁers can be used to independently certify the answer. Moreover, we also wrote pretty printer which transform CPF proofs into human readable proofs, so that the termination tool authors can also drop their human readable export functionality for those parts of the proof that are covered by CPF.
1http://cl-informatik.uibk.ac.at/software/cpf/

3. Related Work
The alternative certiﬁers CC and RC clearly have the closest connection to our work. Moreover, there also is a PVS formalization on term rewriting, trs [34, 35]. It was not mentioned before, since as far as we know, it is currently not used for certiﬁcation of termination proofs.
Although all of the formalizations are on term rewriting there are several diﬀerences which we will list in the remainder of this chapter.
Theorem Prover CI is based on Isabelle/HOL, CC and RC use Coq, and trs utilizes PVS. As a consequence, the proof of some result in IsaFoR can look quite diﬀerent from a similar proof of the same result in one of the other formalizations. Moreover, there is no easy way to import or share theorems which have already been integrated in the other formalizations. As a matter of fact, we are only aware that some results are shared among the Coq based formalizations Coccinelle and CoLoR.
Representation In CI everything is developed using a deep embedding, whereas in CC and RC also shallow embeddings are used. To shortly illustrate the diﬀerence between deep and shallow embedding, consider the following TRS R for subtraction:
minus(x, 0) → x minus(s(x), s(y)) → minus(x, y)
For certiﬁcation, in both CC and RC ﬁrst an inductive set or type for the signature is created on the ﬂy within Coq (shallow), which contains exactly the three symbols minus, s, and 0. In CI however, there is no dedicated signature—minus, s, and 0 are just strings. Similarly, for solving arithmetic constraints which may arise from proving termination of R by a polynomial interpretation, in CC the Coq tactic omega is invoked (shallow), whereas for CI an algorithm for solving these constraints has been formalized (deep).
Both designs have their own advantages: using a shallow embedding sometimes allows for more elegant formalizations since the builtin methods from the theorem prover can be used to adequately model or solve a problem; and with a deep embedding, the certiﬁcation algorithms can also run outside the theorem prover and are less brittle to changes in the theorem prover.
Support of termination techniques As we have focussed our work to develop a certiﬁer with a high coverage of termination technique for ﬁrst order term rewriting, CI contains several techniques in this area which are not available in the other certiﬁer.1 For example, during the latest full run of termination tools in 2011, where every participating tool was tested on every problem of the termination problem database, 3675 termination and
1See http://cl-informatik.uibk.ac.at/software/ceta/#introduction for the current list of techniques which are supported by CeTA.

12 Chapter 3. Related Work nontermination proofs have been generated for TRSs.2 In this experiment, there have been dedicated termination tools to support both CI and CC, i.e., some termination tools delivered proofs which are known to be supported by CI and other tools delivered proofs especially for CC. Whereas CC had to refuse 1694 proofs as they contained unsupported techniques, CeTA only had to refuse 525 proofs. And in the meantime, all 3675 proofs can be certiﬁed using CeTA version 2.8. These numbers clearly demonstrate a high coverage of CeTA in this area.
However, in other areas the picture is inverted: for example, IsaFoR does not contain any result on higher-order rewriting, but CoLoR does [63].
2http://termcomp.uibk.ac.at/termcomp/competition/certificationResults.seam?cat= 10235&comp=260918

4. Contributions
In this chapter, we ﬁrst explain how the selected papers in the second part of this thesis contribute to solving the four mentioned problems. Here, the papers are listed in order of their publication date. Afterwards, we report on further work in the area of certiﬁcation, which has been performed in IsaFoR but is not covered in the selected papers.
4.1. Certiﬁcation of Termination Proofs using CeTA (Chapter 6)
Publication details
Ren´e Thiemann and Christian Sternagel. Certiﬁcation of Termination Proofs using CeTA. In Proceedings of the 22nd International Conference on Theorem Proving in Higher Order Logics, volume 5674 of LNCS, pages 452–468. Springer, 2009.
In this initial paper on IsaFoR and CeTA, we give details on our formalization of three important termination techniques—dependency pairs, dependency graph decomposition, and reduction pairs—a clear contribution to Problem 1.
Note that all of these techniques are also supported by the other certiﬁers. Therefore, the real novelty in this paper is the approach of dealing with Problem 3. Both RC and CC use a mixture of three possibilities to generate proof scripts for a certiﬁcate: invoke executable functions which have been proven correct, perform on-the-ﬂy generation of proofs by untrusted tools, and call built-in solvers of the proof assistant. In contrast, for every termination technique in IsaFoR we always provide an executable function that checks the correct application—even for those techniques that compose other termination techniques.
As a consequence, there is also one check-function check-trs-termination-proof which checks full proof trees. It takes a proof and a TRS as input and we have formally proven its correctness: if check-trs-termination-proof R results in “OK” then termination of R is guaranteed. Hence, our proof scripts are always identical, they just evaluate check-trs-termination-proof and demand that the result is “OK”.
This allows us to not generate a proof script at all. We can just invoke the code generator of Isabelle [49] to obtain check-trs-termination-proof as external program (Haskell, OCaml, SML, or Scala) which can then be compiled and executed. And this is exactly what CeTA is. It consists of a small hand written main function which reads a ﬁle, invokes check-trs-termination-proof on the input, and accepts the proof if the result is “OK”, and otherwise throws an error-message. Hence, for checking proofs one does not have to install and run Isabelle, but one can just compile CeTA and execute it like every other program.

14 Chapter 4. Contributions
This design has several consequences:
• Since proof checking via CeTA is execution of native code, and does not run within the proof assistant, CeTA is usually faster than the other two certiﬁers. Moreover, it allows us to also integrate more time intensive termination techniques. For example, the dependency graph estimation EDG∗∗∗ (a combination of the estimations in [41] and [50]), and the termination techniques of semantic labeling [110] (Chapter 9), size-change termination [74], and matchbounds [36] all have complex application criteria where the corresponding check-functions beneﬁt from the speedup that is obtained from running native code.
• If a proof script is rejected, then one can immediately see where the proof is rejected. Depending on the structure of the proof script, it is then more or less easy to ﬁgure out which part of the termination proof was rejected. In contrast, if the result of our check-trs-termination-proof function were just a Boolean, then it would not be visible which part of the proof was rejected. Therefore, the return type of our check-functions is a disjoint sum: either we return “OK”, or a string which is the error message. Since all our check-functions provide detailed human readable error messages, it is easy to ﬁgure out which part of the proof is not accepted and why it is not accepted, where it is not required to read any proof script at all. In the paper we further describe how readable error messages can be integrated into the check-functions without becoming an overhead when proving soundness of these functions.
• Since all our formal proofs are done statically (soundness of termination techniques and soundness of check-functions), we are quite robust against any changes in the proof assistant. With every new release of the proof assistant, we just have to replay all our proofs and immediately detect problems which are due to changes in the proof assistant—which can then be easily ﬁxed. However, CeTA will be the same program as before (unless there are severe changes in the code generator), so the changes in the proof assistant will have no impact on checking the certiﬁcates. In contrast, if we would have generated proof scripts on the ﬂy, then some problems may occur only during certiﬁcation which are not visible when replaying the proofs for the theorems in the library.
• As all our check-functions have to be fully executable, they cannot make use of internal tactics, solvers, etc., which might be available in the proof assistant. Hence, it is more tedious to develop these checks, as most parts have to be developed from scratch. Moreover, our design requires that the full formalization is deeply embedded, so we cannot model parts of term rewriting by the builtin constructs of the proof assistant as it is done in both RC and CC.
To summarize, in this paper we formalized well known termination criteria, and presented a new approach to certify termination proofs via code generation. Here, my contributions are the formalization of EDG∗∗∗, polynomial orders, and the idea of using code generation and error messages. My co-author formalized dependency pairs and the underlying theory on abstract reduction systems and term rewrite systems.

4.2. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules (Chapter 7)

15

4.2. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules (Chapter 7)
Publication details
Christian Sternagel and Ren´e Thiemann. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules. In Proceedings of the 21st International Conference on Rewriting Techniques and Applications, volume 6 of LIPIcs, pages 325–340. Schloss Dagstuhl – Leibniz-Zentrum fu¨r Informatik, 2010.

This paper is mainly devoted to Problem 1, the formalization of important termination techniques. In the following, we will concentrate on the formalization of usable rules [3, 41, 42] since my co-author developed the part about the subterm criterion [51].
Note that as for the estimation of the dependency graph, there are various deﬁnitions of usable rules. To be able to certify many termination proofs we integrated one of the most powerful versions of usable rules, namely the one in [41] which additionally is combined with argument ﬁlters as in [42]. Here, we tried to generalize as much as possible.
One of the main results is easily stated where the exact deﬁnitions can be seen in Chapter 7: if R is ﬁnite, (P, R) is a dependency pair problem, ( , ) is a reduction pair compatible with an argument ﬁlter π, U are the usable rules of (P, R) w.r.t. π, and P ∪ U ∪ Cε ⊆ , then for a termination proof it is allowed to delete all pairs s → t from P which are strictly decreasing, i.e., which satisfy s t.
Although this seems exactly like the statement that is given in literature, we want to illustrate that our formalization contains something more general:
• We do not require that R is a well-formed TRS, i.e., that for every rule → r ∈ R the left-hand side must not be a variable and that all variables of r also occur in . As a consequence during certiﬁcation we do not have to check for well-formedness when applying this techniques. However, it complicates the soundness proof of this termination technique, as it relies upon the fact that →R is ﬁnitely branching, but non-well-formed TRSs may be inﬁnitely branching.
• We do not make any assumption on the set of variables or function symbols, so that this technique can be applied for every type of variables and function symbols, even for degenerated cases. As an example consider the TRS Cε which in the literature is deﬁned as Cε = {c(x, y) → x, c(x, y) → y} which can be used to make nondeterministic choices. Now let the set of variables be a singleton set, i.e. x = y. Then there is no choice anymore and the proof would fail. Therefore, in the formalization we deﬁne Cε := s,t{c(s, t) → s, c(s, t) → t} where s and t range over all terms. With this deﬁnition we can prove the result, and moreover, whenever {c(x, y) → x, c(x, y) → y} ⊆ , then also Cε ⊆ since is required to be closed under substitutions. Hence, we do not impose any additional constraint on the reduction pairs that can be used.
Besides these generalizations, the paper illustrates a nice trick to avoid the development of a working list algorithm to compute the usable rules: The usable rules are usually deﬁned via some inductive deﬁnition, i.e., they are the least set U ⊆ R such that some property P on U is satisﬁed. Although a standard working list algorithm for computing the usable rules is easily written, it would require a tedious proof to show termination

16 Chapter 4. Contributions

of the algorithm, and of course, one would require proofs for soundness and correctness. In contrast, if the usable rules are provided in the certiﬁcate, then one just has to check whether they satisfy property P : it is not at all essential, that the given set of rules is indeed the least set satisfying P .
Note that requiring the usable rules in the certiﬁcate is not a severe burden for the termination tools,1 since they will have to compute the usable rules in any case. Moreover, providing the usable rules in the certiﬁcate has the advantage, that one can also show this information when pretty printing the certiﬁcate into a human readable format.
To summarize, in this paper we formalized two important termination techniques and generalized them, such that less preconditions have to checked during certiﬁcation. Moreover, we illustrated how to completely avoid the formalization of a working list algorithm to compute an inductively deﬁned set.

4.3. Signature Extensions Preserve Termination – An Alternative Proof via Dependency Pairs (Chapter 8)
Publication details
Christian Sternagel and Ren´e Thiemann. Signature Extensions Preserve Termination – An Alternative Proof via Dependency Pairs. In Proceedings of the 19th Annual Conference of the EACSL on Computer Science Logic, volume 6247 of LNCS, pages 514–528. Springer, 2010.

Assume that some property P can be proven for all terms over the signature F where F are all those symbols that appear in the TRS R. Now consider an extended signature F ⊇ F. The question of signature extensions is the question for which properties P we can conclude that P also holds for all terms over F .
It is a well-known result that signature extensions are sound for termination and this fact is also used in termination tools which is illustrated in the following example.

Example 4.1. Let R consist of the following rules, and let F = {a, b, f}.

a(b(b(x))) → a(b(a(a(a(a(x)))))) f(x, y) → x f(x, y) → y

(1) (2) (3)

Using an polynomial order it is possible to remove rules (2) and (3), and it remains to prove termination of R = {(1)} for all terms over the signature F. Using the result of signature extensions it suﬃces to prove termination of R w.r.t. the signature F = {a, b}. Since in F every symbol is unary, one can interpret the terms as strings and apply string reversal which results in

b(b(a(x))) → a(a(a(a(b(a(x)))))).

Afterwards there are no dependency pairs and termination is trivially proven. Without the result on signature extensions, one would not be able to apply string
reversal, and a more complex proof would be required to prove termination of R over F.

1In the meantime, IsaFoR also contains an algorithm to compute the usable rules. It was developed for other termination techniques where the computation of usable rules occurs as a side-problem, and where adding the usable rules to the certiﬁcates would make the certiﬁcates too bulky.

4.3. Signature Extensions Preserve Termination – An Alternative Proof via Dependency Pairs (Chapter 8)
In the paper there are essentially three contributions:

17

(i) There is a new formalized proof that signature extensions are sound for termination, which is simpler than existing proofs. It just requires that signature extensions are sound when considering chains of dependency pairs, which can be proven in a straight-forward way. And using this result in combination with soundness and completeness of dependency pairs already suﬃces to establish the signature extension result.

(ii) A counter-example is provided which proves that signature extensions are unsound when considering minimal chains of dependency pairs.

(iii) It is shown that under the additional assumption of left-linearity signature extensions are sound for minimal chains.

Especially Contributions (ii) and (iii) are important: In at least two papers [53, 95] signature extensions are used in combination with minimal chains—taking this unsound result for granted. Luckily, in case of uncurrying [53], the major result is still valid (cf. Chapter 11). In contrast, for root-labeling [95] we were able to also construct a counter-example showing that without further restrictions, root-labeling is unsound when considering minimal chains. Hence, Contribution (iii) was important since it allows to use root-labeling at least for left-linear TRSs.
As a consequence, termination tools like AProVE and TTT2 now require left-linearity if they want to apply root-labeling when using dependency pairs.
Clearly this paper contributes to Problem 1, as now termination techniques such as string reversal can be certiﬁed, however, it shows the overall relevance of formalization, as we have refuted the soundness of an existing termination technique in theory and practice.
Whereas most of the formalization for this paper was mainly done by my co-author, I developed the counter-examples for both signature extensions and root-labeling. Both of us were involved in the process of ﬁnding the alternative proof for signature extensions via dependency pairs and in the process of developing a ﬁx for minimal chains—in the form of the additional requirement of left-linearity. Note that in the meantime, I extended the work on signature extensions by integrating the following facts in IsaFoR.

• Signature extensions are sound for innermost rewriting, no matter whether one considers termination of TRSs, chains, or minimal chains.

• Signature extensions are sound for relative termination of R modulo S2 if S is well-formed.

• In general, signature extensions are unsound for relative termination. For the TRSs R = {a → b} and S = {a → x}, we have relative termination of R modulo S if we consider terms over the signature F = {a, b}, but for the extended signature F ∪ {f} where f is a binary symbol, relative termination does no longer hold: a →S f(a, a) →R f(a, b) = C[a] →S C[f(a, a)] →R . . . .
2Relative termination of R modulo S is deﬁned as strong normalization of →R ◦ →∗S .

18 Chapter 4. Contributions
4.4. Modular and Certiﬁed Semantic Labeling and
Unlabeling (Chapter 9)
Publication details
Christian Sternagel and Ren´e Thiemann. Modular and Certiﬁed Semantic Labeling and Unlabeling. In Proceedings of the 22nd International Conference on Rewriting Techniques and Applications, volume 10 of LIPIcs, pages 329–344. Schloss Dagstuhl – LeibnizZentrum fu¨r Informatik, 2011.
This paper addresses Problem 1 by formalizing the important termination technique of semantic labeling [110]. Even more important is the development of a solution to Problem 2, and it also addresses Problem 3.
When applying semantic labeling, one has to ﬁnd an interpretation such that the rules are a model (or quasi-model) of this interpretation, cf. Chapter 9 for more details. Afterwards, each rule is replaced by several new labeled variants where the function symbols are labeled by their semantics, and where one rule is created for every possible assignment.
The advantage is that in this way arbitrary semantic information can be annotated as labels in the symbols which can then be exploited afterwards, e.g., the precedence for some RPO can be based on the labels. The disadvantage is that the labeled TRS is much larger than the original TRS. To this end the following approach is often used: after labeling, one tries to remove all labeled variants of at least one rule using some termination techniques, and afterwards removes all labels again, in order to avoid the size-increase from the labeling. Whereas this approach is sound for rewriting and for chains, i.e., when working directly on TRSs or on chains of dependency pairs, unfortunately, it is unsound when considering minimal chains.
Hence, with the standard semantics of dependency pairs, one can either use semantic labeling and unlabeling (if one considers chains), or one can use the subterm criterion and usable rules (if one considers minimal chains), but not both at the same time. Since both techniques contribute signiﬁcantly to the overall power of a termination tool, it would be nice to ﬁnd a new notion of chain—including a new semantics—where all these techniques are sound. And this is exactly what we developed in this paper.
We proved that semantic labeling and unlabeling are sound w.r.t. to the new semantics, and we also showed that a whole class of termination techniques can be lifted from the old to the new semantics, which includes the subterm criterion and usable rules. As a matter of fact, all but one technique in IsaFoR could be lifted to the new semantics without requiring a new proof, only for string reversal the soundness proof had to be manually adapted.
Concerning certiﬁcation of semantic labeling, we shortly want to mention that certiﬁcation is not just checking syntactic criteria or term-order constraints, since another kind of problem arises. When using semantic labeling with quasi-models, one has to add the decreasing rules Dec in addition to the labeled rules. In fact, some termination tools do not add Dec but just use a subset Dec ⊆ Dec such that →Dec ⊆ →+Dec . To accept these termination proofs, ﬁrst we have formally proven that it suﬃces to add such a set Dec instead of Dec itself. As a consequence, for certiﬁcation one now requires an algorithm to check that →Dec ⊆ →+Dec is satisﬁed, which we developed in a second step.
For this paper, my co-author developed the formalizations of root-labeling (an instance of semantic labeling which often requires preprocessing using ﬂat-context closures),

4.5. Termination of Isabelle Functions via Termination of Rewriting (Chapter 10) 19
whereas the new semantics, the theory on semantic labeling, and the checks for semantic labeling with arbitrary ﬁnite models was done by myself.
4.5. Termination of Isabelle Functions via Termination of Rewriting (Chapter 10)
Publication details
Alexander Krauss, Christian Sternagel, Ren´e Thiemann, Carsten Fuhs, and Ju¨rgen Giesl. Termination of Isabelle Functions via Termination of Rewriting. In Proceedings of the 2nd International Conference on Interactive Theorem Proving, volume 6898 of LNCS, pages 152–167. Springer, 2011.
This paper is not related to one of the four mentioned problems. Instead, it describes an application of our certiﬁer. Note that in Isabelle/HOL there are three major alternatives to deﬁne recursive functions.
• primrec: functions in primitive recursive form
• fun or function: functions with no syntactic restriction on the recursive calls
• partial-function: functions with no syntactic restriction on the recursive calls
Of course, whenever a function can be conveniently written in primitive recursive form, one can use primrec. If this is not the case, then one has to use one of the alternatives. For partial-function there also is a syntactic restriction, namely that the function must be tail-recursive, or the result of the function must be an option type. Moreover, partial-function currently does not yield an induction scheme for tail-recursive functions, and it only provides an inconvenient induction scheme if the result is an option type. Therefore, fun and function are the most frequently used commands to deﬁne functions, where a nice induction scheme is provided, but where termination of the function has to be proven. If one uses function, this proof has to be done manually, whereas fun ﬁrst invokes function, and afterwards tries to ﬁnd a termination proof using some heuristics.
To give some statistics, in the archive of formal proofs, primrec is used 705 times, partial-function is only invoked 17 times, fun is applied 1906 times, and function is utilized 98 times. In over words, for at least 98 functions, the heuristic of fun is not good enough to prove termination of the function. Notice that even in those cases where fun was successful, sometimes manual interaction was required—in the form of auxiliary lemmas that can be used by fun.
The aim of this paper was simple, we wanted to use existing termination tools to reduce the number of manual termination proofs. To this end, we encode a function f as TRS Rf , use an external termination tool for getting a termination proof for Rf , execute CeTA (within Isabelle/HOL) to formally prove termination of Rf , and prove that from termination of Rf we can conclude termination of f . To this end, we developed a tactic which essentially shows that the computation of f can be simulated by Rf , where every Isabelle/HOL term is encoded into a term over the signature of Rf .
Unfortunately, so far the experimental results are a bit disappointing, since for most of the 98 functions, we have not been able to successfully apply our approach and there are two major reasons for this.

20 Chapter 4. Contributions
First, our encoding as described in the paper is restricted to ﬁrst-order functions, whereas many of the 98 examples are higher-order examples. To this end, we later started to extend our work by also providing an encoding for higher-order functions. Using these extensions, we are able to treat functions like “map” and “fold”.
Second, for proving termination of f one has to regard side-conditions. Sometimes these are easy to encode, for example if they arise from an if-then-else as in
f n = if n > 0 then n * f (n - 1) else 1
where one has to ﬁnd a well-founded order > such that n > n - 1 whenever n > 0. Here, the test n > 0 over naturals is converted into the test whether the term n > 0 rewrites to true via some TRS which encodes comparison of natural numbers.
However, when using higher-order functions these conditions may become more complex. As an example consider the following function which computes the size of varyadic trees.
size (Node ts) = 1 + listsum (map size ts)
Here, to prove termination one has to ﬁnd a well-founded order > such that Node ts > t whenever t ∈ set ts (where set converts a list into a set). Using the same solution as before, one would demand that the term t ∈ set ts rewrites to true, but the problem is that t is a free variable, which would immediately result in nontermination of the TRS. Instead, one can use a function which is dual to membership, namely a selection function, to encode a conditional rewrite rule for size: if select ts →∗ t, then size (Node ts) → size t.
It remains as interesting future work to also develop suitable encodings for these conditions, and we believe that once this has been established, indeed our approach can be useful to reduce the number of manual termination proofs.
Whereas most of the tactics that are described in the paper have been implemented solely by the ﬁrst author, I contributed to both the development and the presentation of the tactics on a higher level. Moreover, the extensions to higher-order are completely done by myself, and they have already been presented in an invited tool demonstration at the International Workshop on Higher-Order Rewriting in 2012.
4.6. Generalized and Formalized Uncurrying (Chapter 11)
Publication details
Christian Sternagel and Ren´e Thiemann. Generalized and Formalized Uncurrying. In Proceedings of the 8th International Symposium on the Frontiers of Combining Systems, volume 6989 of LNCS, pages 243–258. Springer, 2011.
This paper is clearly devoted to partially solve Problem 1, since one important termination technique has been formalized: Uncurrying. Uncurrying as described in [53] works over applicative signatures, where there is one binary application symbol ◦, and where all other symbols are constants. It is used to turn applicative TRSs—TRSs where the signature is applicative—into functional form. If rules contain head variables, i.e., subterms of the form x ◦ t1 ◦ . . . ◦ tn, uncurrying is performed as far as possible. For example, the applicative rewrite rule
map ◦ f ◦ (cons ◦ x ◦ y) → cons ◦ (f ◦ x) ◦ (map ◦ f ◦ y)

4.7. On the Formalization of Termination Techniques Based on Multiset Orderings

(Chapter 12)

21

is uncurried into
map(f, cons(x, y)) → cons(f ◦ x, map(f, y))
where the subterm f ◦ x could not be uncurried. For the approach to be sound, also the uncurrying rules have to be added, i.e., rules like cons ◦ x → cons(x) and cons(x) ◦ y → cons(x, y).
Note that uncurrying is extremely important to successfully treat applicative TRSs, since several other termination techniques perform badly on applicative signatures.
During the formalization, we immediately spotted one problematic step in the original soundness proof due to our knowledge on signature extensions (cf. Section 4.3 and Chapter 8): in [53], w.l.o.g. it is assumed that signature extensions are sound when regarding minimal chains.
However, in contrast to root-labeling, where we have been able to refute the whole theorem due to this wrong assumption, for uncurrying we could provide an alternative proof which does not rely upon signature extensions: we just dropped the condition that uncurrying is only applied on TRSs over applicative signatures, and generalized the whole proof for arbitrary signatures.
This generalization led to a strictly stronger theorem. For example, using our generalization, one may uncurry the map-rule above, even if the remaining TRS contains some functional rule like plus(s(x), y) → s(plus(x, y)). This is not possible using the original technique.
My own part in this work was the full formalization and adaptation of the proofs, whereas my co-author integrated the generalized uncurrying technique in the TTT2 termination tool, which was essential to obtain empirical results for the adapted termination technique.

4.7. On the Formalization of Termination Techniques Based on Multiset Orderings (Chapter 12)
Publication details
Ren´e Thiemann, Guillaume Allais, and Julian Nagele. On the Formalization of Termination Techniques Based on Multiset Orderings. In Proceedings of the 23rd International Conference on Rewriting Techniques and Applications, volume 15 of LIPIcs, pages 339– 354. Schloss Dagstuhl – Leibniz-Zentrum fu¨r Informatik, 2012.
In this paper we consider two termination techniques which are based on multiset orders: RPO and an approximation of size-change termination (SCNP reduction pairs), hence the paper contributes to solve Problem 1.
To support these techniques, of course we required a formalization of the multiset extension of an order. Although this has already been done in the Isabelle-distribution, we could not use these results, as we require the multiset extension of two orders and
where is well-founded and is a compatible nonstrict order. As a motivation why we need this extension, consider polynomials over the naturals: we know that 2x x and y + 1 y. And for SCNP reduction pairs, we have to be able to show that the multiset {{2x, y + 1}} is strictly larger than the multiset {{y, x}}, which is easily possible using our deﬁnition, but which is not possible if one only deﬁnes the

22 Chapter 4. Contributions
multiset extension of the strict order and then takes equality as nonstrict order. The reason is that neither 2x x nor 2x = x is valid.
Also for RPO we require the multiset extension of two orders, since termination provers like AProVE use a variant of RPO, where f(g(x, y), s(z)) f(g(0, y), z). This orientation is possible, if 0 has least precedence among all symbols: then x 0 and thus, g(x, y) g(0, y), which shows that the multiset {{g(x, y), s(z)}} is strictly larger than {{g(0, y), z}}.
In addition to the formalization eﬀort of the multiset extension of two orders, we also investigated the diﬀerence to the standard multiset extension of one order. Whereas most properties are similar for both variants, we proved one important diﬀerence: the decision procedure for checking whether two multisets are in relation becomes NP-complete when using two orders, whereas it is known to be in P for one order. This is clearly an interesting result w.r.t. Problem 3.
After having integrated the required multiset extension, we formalized RPO and SCNP reduction pairs. For RPO we used a deﬁnition that contains many extensions which is required for checking proofs that are generated by AProVE. For example, both quasiprecedences and inference rules like x 0 are integrated. For strong normalization we did not use Kruskal’s tree theorem [72], but adapted a direct strong normalization proof of the lexicographic path order from [17] which uses computability predicates `a la Tait and Girard (a similar and extended proof has also been performed in [57] for the higher-order recursive path order.) During the formalization we detected that the RPO as deﬁned in AProVE is not a reduction order, since it is not closed under substitutions. However, we were able to repair the deﬁnition by adding two more inference rules.
Regarding SCNP reduction pairs, there was an even more severe problem. In [20] a nonstandard deﬁnition of reduction pair is used, which turned out to be inconsistent. Therefore, we ﬁrst had to ﬁnd an alternative deﬁnition of reduction pair which could be used to prove the results of the paper. Here, the notion of reduction triples of [51] was helpful, and eventually we could adapt reduction triples to achieve the soundness result for SCNP reduction pairs. Moreover, for SCNP reduction pairs we also had to formalize other multiset extensions of two orders, like the dual-multiset-extension [7].
To summarize, we formalized the multiset extension of two orders, proved that the decision problem for this order is NP-complete, and formalized and corrected both SCNP reduction pairs and a powerful variant of RPO.
My work was the full development of LPO in IsaFoR (which was later extended to RPO by my co-authors), the full formalization of SCNP reduction pairs, and the NPcompleteness proof for the multiset extension of two orders. My co-authors integrated the multiset extension for two orders in IsaFoR, and implemented RPO in a memoized version within IsaFoR, since the naive recursive algorithm would immediately result in exponential runtime, even without performing multiset comparisons.
4.8. Certiﬁcation of Nontermination Proofs (Chapter 13)
Publication details
Christian Sternagel and Ren´e Thiemann. Certiﬁcation of Nontermination Proofs. In Proceedings of the 3rd International Conference on Interactive Theorem Proving, volume 7406 of LNCS, pages 266–282. Springer, 2012.

4.8. Certiﬁcation of Nontermination Proofs (Chapter 13)

23

In the last selected paper, we mainly consider Problem 1. However, this time we want to certify nontermination proofs. So where in the previous papers, one was mainly concerned about soundness of termination techniques, here we are looking at completeness of termination techniques, where we not only consider termination but also innermost termination.
Luckily, for many termination techniques, completeness is easy to prove. For example all techniques that just remove rules are immediately sound: whenever the reduced TRS is nonterminating, then so is the full one. Hence, rule removal with any order, dependency graph decomposition, and various reduction pair processors are all complete techniques as they just remove rules and dependency pairs.
However, there is at least one technique which is not trivial, namely the technique which directly proves nontermination of a TRS. In the paper we consider an easy suﬃcient criterion of nontermination, namely loops, i.e., derivations of the form t →+R C[tσ] for some context C and substitution σ. Of course, given such a derivation in the certiﬁcate, one can easily check whether all steps in the derivation are indeed correct rewrite steps, and afterwards can guarantee nontermination, since →R is closed under substitutions and contexts.
But for innermost termination—where rewriting is performed using the innermost evaluation strategy, i.e., in an innermost rewrite step C[ σ] →R C[rσ], all arguments of σ must be normal forms—this is not necessarily the case: in general, the innermost rewrite relation is not closed under substitutions. For example, the TRS {f(s(x)) → f(s(s(x))), s(s(s(s(x)))) → overﬂow} is innermost termination, but it has a loop: t = f(s(x)) → f(s(s(x))) = C[tσ] for the empty context C and the substitution which replaces x by s(x).
In [103] it was shown, that the question whether a loop is an innermost loop, i.e., whether we can conclude innermost nontermination, is decidable. To this end, a complex algorithm was provided, which works in three phases. Especially the last phase utilizes some complex algorithm, where one has to decide for given s, t, and σ whether there is some n such sσn = tσn. Termination of this algorithm is based on Kruskal’s tree theorem and in the original proof it is argued via inﬁnite terms which are obtained as limit of the terms s, sσ, sσ2, . . . . To ensure that these limit terms are well deﬁned, some preprocessing on σ is done before starting the real algorithm.
In contrast, in the formalization we were able to provide a simpler algorithm with simpler proofs: no preprocessing is required, and termination is easy to prove. Moreover, from our algorithm one can easily extract a precise bound b on n which results in a trivial algorithm: there exists an n such that sσn = tσn if and only if sσb = tσb where b = (|s| · |t| · |σ|)2. This result is then used to prove that the whole decision procedure for innermost loops is in P.
The paper also contains some contributions w.r.t. Problem 3, namely it shows how partial-function can be used to develop eﬃcient algorithms in Isabelle which cannot be deﬁned via function.
For this paper, my co-author implemented a framework for nontermination proofs and integrated completeness results for several techniques. My own work was the complete development and implementation of the new decision procedure for innermost loops in combination with its soundness proof.
In addition to what is written in the paper, I also formalized a result on nonlooping nontermination [30] and integrated suﬃcient syntactic criteria where nontermination implies innermost nontermination which are based on conﬂuence [40, 41, 46].

24 Chapter 4. Contributions
4.9. Further Contributions
In the area of term rewriting, we integrated several further important results in IsaFoR that are not addressed in the selected papers.
• on conﬂuence: the critical pair lemma and the result that weak orthogonality implies conﬂuence
• on termination: Knuth-Bendix orders, matchbounds, bounded increase, switching between termination and innermost termination, outermost loops, unravelings for conditional rewriting
• on the word problem: Birkhoﬀ’s theorem and Knuth-Bendix completion
• on complexity: strongly linear interpretations, triangular matrix interpretations, modular complexity proofs
All of these techniques are complemented with executable functions for certiﬁcation. As a result, with CeTA we can also check innermost (non)termination proofs, (non)conﬂuence proofs, completion proofs, and equational reasoning (dis)proofs. Clearly, these developments are contributions w.r.t. Problems 1 and 3—if one replaces termination by other properties like completion, conﬂuence, or complexity. Concerning Problem 4, we extended CPF for these kinds of proofs as well, and currently CeTA can certify completion proofs from mkbTT [107] and KBCV [99], (non)conﬂuence proofs from CSI [108], and complexity proofs from TCT[4] and CaT [81]. For conﬂuence and complexity, CeTA already participated in the corresponding competitions.3
Besides term rewriting, we also developed several auxiliary libraries which became available to all Isabelle users in the archive of formal proofs (AFP).4 In the following list, we only mention those AFP-entries where the majority of the formalization was performed by ourselves: executable operations on nonlinear multivariate polynomials, executable operations on matrices, executable algorithms to compute (reﬂexive-)transitive closures, the automatic generation of linear orders for algebraic datatypes, and the formalization of the Babylonian method to compute square roots.
4.10. Summary of Contributions
We formalized a large amount of termination techniques for term rewrite systems (Problem 1). All of them can be freely combined using a uniﬁed semantics for termination of dependency pair problems (Problem 2). Moreover, for all techniques we developed executable functions which check the application criteria and can thus be used to certify termination proofs (Problem 3). These have to provided in the common CPF format which is supported by the certiﬁers CC, CI, and RC, as well as the termination tools AProVE, Matchbox, and TTT2 (Problem 4).
The size of our formalization IsaFoR is over 100,000 lines, and the generated certiﬁer CeTA is a Haskell program of over 24,000 lines. Both IsaFoR and CeTA are freely available at http://cl-informatik.uibk.ac.at/software/ceta/.
3http://coco.nue.riec.tohoku.ac.jp/2012/ and http://termcomp.uibk.ac.at/termcomp/ competition/competitionSummary.seam?comp=362062
4http://afp.sourceforge.net

4.10. Summary of Contributions

25

To evaluate the power of CeTA, we performed the following experiment: we took the most powerful tool from the recent termination competition (AProVE TC 2012) and let it run on all 2778 TRSs of the termination problem database.5 The results show, that AProVE can prove termination or nontermination of 2101 TRSs and it fails on the remaining 677 TRSs.6 If we restrict AProVE to only use techniques that are supported by CPF and CeTA (version 2.8), then AProVE can still prove termination or nontermination of 1850 TRSs, i.e., the step from untrusted to certiﬁable proofs reduces the power by only 12 %. And indeed, all these 1850 generated proofs are correct, as they have been certiﬁed by CeTA.
Concerning soundness of termination techniques, the most important observation was the fact that signature extensions are unsound when used in combination with dependency pair problems. As a consequence, we could prove that the termination technique of rootlabeling is unsound without additional restrictions. Hence, all tools that implement rootlabeling had to be adapted.
In addition to problems in theory, there were also problems in the implementations of the termination tools which became visible during certiﬁcation. In the following, we list rejected techniques, i.e., techniques whose application was rejected by CeTA at least once, and the underlying reason why these proofs have been rejected.

termination technique dependency pairs dependency pairs loop detection bounded increase reduction pair proc. reduction pair proc.

reason for rejection no well-formedness check bug in computation of dependency pairs evaluation strategy was ignored bug in implementation of calculus for conditional constraints bug in output of LPO bug in computation of usable rules

In the ﬁrst three problems, it was easy to exploit the bug to let the tool give a wrong answer, i.e., we could develop nonterminating TRSs where the tools provided termination “proofs”, or vice versa. In the forth problem, it is currently open, whether the bug can be exploited to obtain a wrong answer, however in experiments it was shown, that after ﬁxing the bug, at least for one TRS a termination proof can no longer be detected by that tool, and it is unknown whether the TRS is terminating or not. Finally, in the ﬁfth and sixth problem, the bug just had impact on the output, i.e., the principle answer of termination or nontermination was correct, but the corresponding generated proof was incorrect. For example, in the sixth problem, one termination tool utilizes two methods to compute the usable rules. One, which is used in the search engine and has impact on the internal state (without the bug); and a bogus one for pretty printing the proof.
Several of these bugs have remained undetected for quite some while: they were already present in versions of the tools, that have participated in competitions and have been used for getting empirical data for refereed papers, where no one spotted the mistakes in the generated proofs.
All of these problems have been ﬁxed in the meantime, so that these termination tools became more reliable due to certiﬁcation.

5Version 8.0.6, available at http://termcomp.uibk.ac.at/status/downloads/. 6The details of the experiments are available at http://termcomp.uibk.ac.at/termcomp/termexec/
categoryList.seam?competitionId=411173. For technical reasons, the experiments are split into
string rewrite systems (SRS standard) and term rewrite systems (TRS standard).

5. Future Research
As already mentioned, CeTA can currently handle around 88 % of the termination and nontermination proofs for term rewrite systems since dozens of termination techniques have already been formalized.
This is in stark contrast to other properties of term rewrite systems. For example, in the conﬂuence competition 2012, the tools ACP [2], CSI, and Saigawa [52] were successfully on at least 88 TRSs. In contrast, with the conﬂuence techniques in IsaFoR, only 27 TRSs can be handled, i.e., the power is reduced to only 30 %. Similarly, if one restricts complexity tools like AProVE, CaT, Matchbox, or TCT to only use techniques supported by CeTA, then the power is again signiﬁcantly reduced, where the most severe loss is for runtime complexity: in the competition 2012, the restricted version of TCT achieved a score of 27 whereas AProVE and TCT reached a score of at least 92.
For both conﬂuence and complexity, the problem is that there are many techniques in these areas which require several nontrivial formalizations. It will be an interesting task to extend IsaFoR towards complexity and conﬂuence to obtain a similar coverage as for termination techniques.
Even more severe is the situation when regarding complexity or termination techniques for other programming languages than term rewriting which are integrated in tools like AProVE, COSTA [1], Julia [93], Polytool, RAML [55], SPEED [47], and Terminator: we are not aware of any formalizations of these important techniques. Here, it looks promising to continue our work by formalizing those methods which prove termination via a transformation to TRSs as in [38] (for Haskell) or in [15, 16, 87] (for Java): we can reuse our existing formalization on rewriting, and there is already an Isabelle/HOL formalization of a Java like language [61, 75].

Part II. Selected Papers

6. Certiﬁcation of Termination Proofs using CeTA
Publication details
Ren´e Thiemann and Christian Sternagel. Certiﬁcation of Termination Proofs using CeTA. In Proceedings of the 22nd International Conference on Theorem Proving in Higher Order Logics, volume 5674 of LNCS, pages 452–468. Springer, 2009.
Abstract
There are many automatic tools to prove termination of term rewrite systems, nowadays. Most of these tools use a combination of many complex termination criteria. Hence generated proofs may be of tremendous size, which makes it very tedious (if not impossible) for humans to check those proofs for correctness.
In this paper we use the theorem prover Isabelle/HOL to automatically certify termination proofs. To this end, we ﬁrst formalized the required theory of term rewriting including three major termination criteria: dependency pairs, dependency graphs, and reduction pairs. Second, for each of these techniques we developed an executable check which guarantees the correct application of that technique as it occurs in the generated proofs. Moreover, if a proof is not accepted, a readable error message is displayed. Finally, we used Isabelle’s code generation facilities to generate a highly eﬃcient and certiﬁed Haskell program, CeTA, which can be used to certify termination proofs without even having Isabelle installed.
6.1. Introduction
Termination provers for term rewrite systems (TRSs) became more and more powerful in the last years. One reason is that a proof of termination no longer is just some reduction order which contains the rewrite relation of the TRS. Currently, most provers construct a proof in the dependency pair framework which allows to combine basic termination techniques in a ﬂexible way. Then a termination proof is a tree where at each node a speciﬁc technique has been applied. So instead of stating the precedence of some lexicographic path order (LPO) or giving some polynomial interpretation, current termination provers return proof trees which reach sizes of several megabytes. Hence, it would be too much work to check by hand whether these trees really form a valid proof.
That we cannot blindly trust the output of termination provers is regularly demonstrated: Every now and then some tool delivers a faulty proof for some TRS. But most often this is only detected if there is some other prover giving the opposite answer on the same TRS, i.e., that it is nonterminating. To solve this problem, in the last years two systems have been developed which automatically certify or reject a generated termination proof: CiME/Coccinelle [21, 25] and Rainbow/CoLoR [12] where Coccinelle and CoLoR are

32 Chapter 6. Certiﬁcation of Termination Proofs using CeTA

libraries on rewriting for Coq (http://coq.inria.fr), and CiME and Rainbow are used to convert proof trees into Coq-proofs which heavily rely on the theorems within those libraries.

CiME/Rainbow Coq + Coccinelle/CoLoR

proof tree

proof.v

accept/failure

In this paper we present a new combination, CeTA/IsaFoR, to automatically certify termination proofs. Note that the system design has two major diﬀerences in comparison to the two existing ones. First, our library IsaFoR (Isabelle Formalization of Rewriting, containing 173 deﬁnitions, 863 theorems, and 269 functions) is written for the theorem prover Isabelle/HOL1 [84] and not for Coq.
Second, and more important, instead of generating for each proof tree a new Coq-proof using the auxiliary tools CiME/Rainbow, our library IsaFoR contains several executable “check”-functions (within Isabelle) for each termination technique we formalized. We have formally proven that whenever such a check is accepted, then the termination technique is applied correctly. Hence, we do not need to create an individual Isabelle-proof for each proof tree, but just call the “check”-function for checking the whole tree (which does nothing else but calling the separate checks for each termination technique occurring in the tree). This second diﬀerence has several advantages:
• In the other two systems, whenever a proof is not accepted, the user just gets a Coq-error message that some step in the generated Coq-proof failed. In contrast, our functions deliver error messages using notions of term rewriting.
• Since the analysis of the proof trees in IsaFoR is performed by executable functions, we can just apply Isabelle’s code-generator [48] to create a certiﬁed Haskell program [90], CeTA, leading to the following workﬂow.

IsaFoR

Isabelle

Haskell compiler

Haskell program

CeTA

proof tree CeTA accept/error message

Hence, to use our certiﬁer CeTA (Certiﬁed Termination Analysis) you do not have to install any theorem prover, but just execute some binary. Moreover, the runtime of certiﬁcation is reduced signiﬁcantly. Whereas the other two approaches take more than one hour to certify all (≤ 580) proofs during the last certiﬁed termination competition, CeTA needs less than two minutes for all (786) proofs that it can handle. Note that CeTA can also be used for modular certiﬁcation. Each single application of a termination technique can be certiﬁed—just call the corresponding Haskellfunction.
Concerning the techniques that have been formalized, the other two systems oﬀer techniques that are not present in IsaFoR, e.g., LPO or matrix interpretations. Nevertheless, we also feature one new technique that has not been certiﬁed so far. Whereas currently only the initial dependency graph estimation of [3] has been certiﬁed, we integrated the most powerful estimation which does not require tree automata techniques and is based
1In the remainder of this paper we just write Isabelle instead of Isabelle/HOL.

6.2. Formalizing Term Rewriting

33

on a combination of [41, 50] where the function tcap is required. Initial problems in the formalization of tcap led to the development of etcap, an equivalent but more eﬃcient version of tcap which is also beneﬁcial for termination provers. Replacing tcap by etcap within the termination prover TTT2 [67] reduced the time to estimate the dependency graph by a factor of 2. We will also explain, how to reduce the number of edges that have to be inspected when checking graph decompositions.
Another beneﬁt of our system is its robustness. Every proof which uses weaker techniques than those formalized in IsaFoR is accepted. For example, termination provers can use the graph estimation of [3], as it is subsumed by our estimation.
The paper is structured as follows. In Sect. 6.2 we recapitulate the required notions and notations of term rewriting and the dependency pair framework (DP framework). Here, we also introduce our formalization of term rewriting within IsaFoR. In Sect. 6.3– 6.6 we explain our certiﬁcation of the four termination techniques we currently support: dependency pairs (Sect. 6.3), dependency graph (Sect. 6.4), reduction pairs (Sect. 6.5), and combination of proofs in the dependency pair framework (Sect. 6.6). However, to increase readability we abstract from our concrete Isabelle code and present the checks for the techniques on a higher level. How we achieved readable error-messages while at the same time having maintainable Isabelle proofs is the topic of Sect. 6.7. We conclude in Sect. 6.8 where we show how CeTA is created from IsaFoR and where we give experimental data.
IsaFoR, CeTA, and all details about our experiments are available at CeTA’s website http: //cl-informatik.uibk.ac.at/software/ceta.

6.2. Formalizing Term Rewriting
We assume some basic knowledge of term rewriting [5]. Variables are denoted by x, y, z, etc., function symbols by f , g, h, etc., terms by s, t, u, etc., and substitutions by σ, µ, etc. Instead of f (t1, . . . , tn) we write f (tn). The set of all variables occurring in term t is denoted by Var(t). By T (F, V) we denote the set of terms over function symbols from F and variables from V.
In the following we give an overview of our formalization of term rewriting in IsaFoR. Our main concern is termination of rewriting. This property—also known as strong normalization—can be stated without considering the structure of terms. Therefore it is part of our Isabelle theory AbstractRewriting. An abstract rewrite system (ARS) is represented by the type (’a×’a)set in Isabelle. Strong normalization (SN) of a given ARS A is equivalent to the absence of an inﬁnite sequence of A-steps. On the lowest level we have to link our notion of strong normalization to the notion of well-foundedness as deﬁned in Isabelle. This is an easy lemma since the only diﬀerence is the orientation of the relation, i.e., SN(A) = wf(A−1). At this point we can be sure that our notion of strong normalization is valid.
Now we come to the level of ﬁrst-order terms (in theory Term):
datatype (’f,’v)"term" = Var ’v | Fun ’f "(’f,’v)term list"
Many concepts related to terms are formalized in Term, e.g., an induction scheme for terms (as used in textbooks), substitutions, contexts, the (proper) subterm relation etc.
By restricting the elements of some ARS to terms, we reach the level of TRSs (in theory Trs), which in our formalization are just binary relations over terms.

34 Chapter 6. Certiﬁcation of Termination Proofs using CeTA

Example 6.1. As an example, consider the following TRS, encoding rules for subtraction and division on natural numbers.

minus(x, 0) → x minus(s(x), s(y)) → minus(x, y)

div(0, s(y)) → 0 div(s(x), s(y)) → s(div(minus(x, y), s(y)))

Given a TRS R, ( , r) ∈ R means that is the lhs and r the rhs of a rule in R (usually written as → r ∈ R). The rewrite relation induced by a TRS R is denoted by →R and has the following deﬁnition in IsaFoR:

Deﬁnition 6.2. Term s rewrites to t by R, iﬀ there are a context C, a substitution σ, and a rule → r ∈ R such that s = C[ σ] and t = C[rσ].

Note that this section contains the only parts where you have to trust our formalization, i.e., you have to believe that SN(→R) as deﬁned in IsaFoR really describes “R is terminating.”

6.3. Certifying Dependency Pairs

Before we introduce dependency pairs [3] formally and give some details about our Isabelle formalization, we recapitulate the ideas that led to the ﬁnal deﬁnition (including a reﬁnement proposed by Dershowitz [28]).
For a TRS R, strong normalization means that there is no inﬁnite derivation t1 →R t2 →R t3 →R · · · . Additionally we can concentrate on derivations, where t1 is minimal in the sense that all its proper subterms are terminating. Such terms are called minimal nonterminating. The set of all minimally nonterminating terms with respect to a TRS R is denoted by TR∞. Observe that for every term t ∈ TR∞ there is an initial part of an inﬁnite derivation having a speciﬁc shape: A (possibly empty) derivation taking place below the root, followed by an application of some rule → r ∈ R at the root, i.e., t →>εR∗ σ →ε R rσ, for some substitution σ. Furthermore, since rσ is nonterminating, there is some subterm u of r, such that uσ ∈ TR∞, i.e., rσ = C[uσ]. Then the same reasoning can be used to get a root reduction of uσ, . . . , cf. [3].
To get rid of the additional contexts C a new TRS, DP(R), is built.
Deﬁnition 6.3. The set DP(R) of dependency pairs of R is deﬁned as follows: For every rule → r ∈ R, and every subterm u of r such that u is not a proper subterm of and such that the root of u is deﬁned,2 → u is contained in DP(R). Here t is the same as t except that the root of t is marked with the special symbol .
Example 6.4. The dependency pairs for the TRS from Ex. 6.1 consist of the rules

M(s(x), s(y)) → M(x, y) (MM) D(s(x), s(y)) → M(x, y)

(DM)

D(s(x), s(y)) → D(minus(x, y), s(y)) (DD)

where we write M instead of minus and D instead of div for brevity.

Note that after switching to ’ ’-terms, the derivation from above can be written as t →∗R σ →DP(R) u σ. Hence every nonterminating derivation starting at a term t ∈ TR∞
2A function symbol f is deﬁned (w.r.t. R) if there is some rule f (. . . ) → r ∈ R.

6.3. Certifying Dependency Pairs

35

can be transformed into an inﬁnite derivation of the following shape where all →DP(R)-steps are applied at the root.

t →∗R s1 →DP(R) t1 →∗R s2 →DP(R) t2 →∗R · · ·

(1)

Therefore, to prove termination of R it suﬃces to prove that there is no such derivation. To formalize DPs in Isabelle we modify the signature such that every function symbol now appears in a plain version and in a -version.
datatype ’f shp = Sharp ’f ("_ ") | Plain ’f ("_@")
Sharping a term is done via
fun plain :: "(’f,’v)term => (’f shp,’v)term" where "plain(Var x) = Var x"
| "plain(Fun f ss) = Fun f@ (map plain ss)"

fun sharp :: "(’f,’v)term => (’f shp,’v)term" where "sharp(Var x) = Var x"
| "sharp(Fun f ss) = Fun f (map plain ss)"
Thus t in Def. 6.3 is the same as sharp(t). Since the function symbols in DP(R) are of type ’f shp and the function symbols of R are of type ’f, it is not possible to use the same TRS R in combination with DP(R). Thus, in our formalization we use the lifting ID—that just applies plain to all lhss and rhss in R.
Considering this technicalities and omitting the initial derivation t →∗R s1 from the derivation (1), we obtain
s1 →DP(R) t1 →∗ID(R) s2 →DP(R) t2 →∗ID(R) · · ·
and hence a so called inﬁnite (DP(R), ID(R))-chain. Then the corresponding DP problem (DP(R), ID(R)) is called to be not ﬁnite, cf. [40]. Notice that in IsaFoR a DP problem is just a pair of two TRSs over arbitrary signatures—similar to [40].
In IsaFoR an inﬁnite chain3 and ﬁnite DP problems are deﬁned as follows.
fun ichain where "ichain(P, R) s t σ = (∀i.
(s i,t i) ∈ P ∧ (t i)·(σ i) →∗R (s(i+1))·(σ(i+1))"
fun finite_dpp where
"finite_dpp(P, R) = (¬(∃s t σ. ichain (P, R) s t σ))"
where ‘t · σ’ denotes the application of substitution σ to term t. We formally established the connection between strong normalization and ﬁniteness of
the initial DP problem (DP(R), ID(R)). Although this is a well-known theorem, formalizing it in Isabelle was a major eﬀort.
Theorem 6.5. wf_trs(R) ∧ finite_dpp(DP(R), ID(R)) −→ SN(→R).
The additional premise wf_trs(R) ensures two well-formedness properties for R, namely that for every → r ∈ R, is not a variable and that Var(r) ⊆ Var( ).
At this point we can obviously switch from the problem of proving SN(→R) for some TRS R, to the problem of proving finite_dpp(DP(R), ID(R)), and thus enter the realm
3We also formalized minimal chains, but here only present chains for simplicity.

36 Chapter 6. Certiﬁcation of Termination Proofs using CeTA
of the DP framework [40]. Here, the current technique is to apply so-called processors to a DP problem, in order to get a set of simpler DP problems. This is done recursively, until the leafs of the so built tree consist of DP problems with empty P-components (and therefore are trivially ﬁnite). For this to be correct, the applied processors need to be sound, i.e., every processor P roc has to satisfy the implication
(∀p ∈ P roc(P, R). finite_dpp(p)) −→ finite_dpp(P, R)
for every input. The termination techniques that will be introduced in the following sections are all such (sound) processors.
So much to the underlying formalization. Now we will present how the check in IsaFoR certiﬁes a set of DPs P that was generated by some termination tool for some TRS R. To this end, the function checkDPs is used.
checkDPs(P,R) = checkWfTRS(R) ∧ computeDPs(R) ⊆ P
Here checkWfTRS checks the two well-formedness properties mentioned above (the diﬀerence between wf_trs and checkWfTRS is that only the latter is executable) and computeDPs uses Def. 6.3, which is currently the strongest deﬁnition of DPs. To have a robust system, the check does not require that exactly the set of DPs w.r.t. to Def. 6.3 is provided, but any superset is accepted. Hence we are also able to accept proofs from termination tools that use a weaker deﬁnition of DP(R). The soundness result of checkDPs is formulated as follows in IsaFoR.
Theorem 6.6. If checkDPs(P, R) is accepted, then ﬁniteness of (P, ID(R)) implies SN(→R).
6.4. Certifying the Dependency Graph Processor
One important processor to prove ﬁniteness of a DP problem is based on the dependency graph [3, 40]. The dependency graph of a DP problem (P, R) is a directed graph G = (P, E) where (s → t, u → v) ∈ E iﬀ s → t, u → v is a (P, R)-chain. Hence, every inﬁnite (P, R)-chain corresponds to an inﬁnite path in G and thus, must end in some strongly connected component (SCC) S of G, provided that P contains only ﬁnitely many DPs. Dropping the initial DPs of the chain results in an inﬁnite (S, R)-chain.4 Hence, if for all SCCs S of G the DP problem (S, R) is ﬁnite, then (P, R) is ﬁnite. In practice, this processor allows to prove termination of each block of mutual recursive functions separately.
To certify an application of the dependency graph processor there are two main challenges. First of all, we have to certify that a valid SCC decomposition of G is used, a purely graph-theoretical problem. Second, we have to generate the edges of G. Since the dependency graph G is in general not computable, usually estimated graphs G are used which contain all edges of the real dependency graph G. Hence, for the second problem we have to implement and certify one estimation of the dependency graph.
Notice that there are various estimations around and that the result of an SCC decomposition depends on the estimation that is used. Hence, it is not a good idea to implement the strongest estimation and then match the result of our decomposition against some given decomposition: problems arise if the termination prover used a weaker estimation and thus obtained larger SCCs.
4We identify an SCC S with the set of nodes S within that SCC.

6.4. Certifying the Dependency Graph Processor

37

Therefore, in the upcoming Sect. 6.4.1 about graph algorithms we just speak of decompositions where the components do not have to be SCCs. Moreover, we will also elaborate on how to minimize the number of tests (s → t, u → v) ∈ E. The reason is that in Sect. 6.4.2 we implemented one of the strongest dependency graph estimations where the test for an edge can become expensive. In Sect. 6.4.3 we ﬁnally show how to combine the results of Sections 6.4.1 and 6.4.2.

6.4.1. Certifying Graph Decompositions
Instead of doing an SCC decomposition of a graph within IsaFoR we base our check on the decomposition that is provided by the termination prover. Essentially, we demand that the set of components is given as a list C1, . . . , Ck in topological order where the component with no incoming edges is listed ﬁrst. Then we aim at certifying that every inﬁnite path must end in some Ci. Note that the general idea of taking the topological sorted list as input was already publicly mentioned at the “Workshop on the certiﬁcation of termination proofs” in 2007. In the following we present how we ﬁlled the details of this general idea.
The main idea is to ensure that all edges (p, q) ∈ E correspond to a step forward in the list C1, . . . , Ck , i.e., (p, q) ∈ Ci × Cj where i ≤ j. However, iterating over all edges of G will be costly, because it requires to perform the test (p, q) ∈ E for all possible edges (p, q) ∈ P × P. To overcome this problem we do not iterate over the edges but over P. To be more precise, we check that

∀(p, q) ∈ P × P. (∃i ≤ j. (p, q) ∈ Pi × Pj) ∨ (p, q) ∈/ E

(2)

where the latter part of the disjunction is computed only on demand. Thus, only those edges have to be computed, which would contradict a valid decomposition.

Example 6.7. Consider the set of nodes P = {(DD), (DM), (MM)}. Suppose that we have to check a decomposition of P into L = {(DD)}, {(DM)}, {(MM)} for some graph G = (P, E). Then our check has to ensure that the dashed edges in the following illustration do not belong to E.

(DD) (DM) (MM)

It is easy to see that (2) is satisﬁed for every list of SCCs that is given in topological order. What is even more important, whenever there is a valid SCC decomposition of G, then (2) is also satisﬁed for every subgraph. Hence, regardless of the dependency graph estimation a termination prover might have used, we accept it, as long as our estimation delivers less edges.
However, the criterion is still too relaxed, since we might cheat in the input by listing nodes twice. Consider P = {p1, . . . , pm} where the corresponding graph is arbitrary and L = {p1}, . . . , {pm}, {p1}, . . . , {pm} . Then trivially (2) is satisﬁed, because we can always take the source of edge (pi, pj) from the ﬁrst part of L and the target from the second part of L. To prevent this kind of problem, our criterion demands that the sets Ci in L are pairwise disjoint.
Before we formally state our theorem, there is one last step to consider, namely the handling of singleton nodes which do not form an SCC on their own. Since we cannot easily infer at what position these nodes have to be inserted in the topological sorted

38 Chapter 6. Certiﬁcation of Termination Proofs using CeTA

list—this would amount to do an SCC decomposition on our own—we demand that they are contained in the list of components.5
To distinguish a singleton node without an edge to itself from a “real SCC”, we require that the latter ones are marked. Then condition (2) is extended in a way that unmarked components may have no edge to themselves. The advantage of not marking a component is that our IsaFoR-theorem about graph decomposition states that every inﬁnite path will end in some marked component, i.e., here the unmarked components can be ignored.

Theorem 6.8. Let L = C1, . . . , Ck be a list of sets of nodes, some of them marked, let G = (P, E) be a graph, let α be an inﬁnite path of G. If

• ∀(p, q) ∈ P × P. (∃i < j. (p, q) ∈ Ci × Cj) ∨ (∃i. (p, q) ∈ Ci × Ci ∧ Ci is marked) ∨

(p, q) ∈/ E

and

• ∀i = j. Ci ∩ Cj = ∅

then there is some suﬃx β of α and some marked Ci such that all nodes of β belong to Ci.
Example 6.9. If we continue with example Ex. 6.7 where only components {(MM)} and {(DD)} are marked, then our check also analyzes that G contains no edge from (DM) to itself. If it succeeds, every inﬁnite path will in the end only contain nodes from {(DD)} or only nodes from {(MM)}. In this way, only 4 edges of G have to be calculated instead of analyzing all 9 possible edges in P × P.

6.4.2. Certifying Dependency Graph Estimations

What is currently missing to certify an application of the dependency graph processor,

is to check, whether a singleton edge is in the dependency graph or not. Hence, we

have to estimate whether the sequence s → t, u → v is a chain, i.e., whether there are substitutions σ and µ such that tσ →∗R uµ. An obvious solution is to just look at the root symbols of t and u—if they are diﬀerent there is no way that the above condition is met (since all the steps in tσ →∗R uµ take place below the root, by construction of the dependency pairs). Although eﬃcient and often good enough, there are more advanced

estimations around.

The estimation EDG [3] ﬁrst replaces via an operation cap all variables and all subterms

of t which have a deﬁned root-symbol by distinct fresh variables. Then if cap(t) and u do

not unify, it is guaranteed that there is no edge.

The estimation EDG∗[50] does the same check and additionally uses the reversed TRS

R−1 = {r →

|

→

r

∈

R},

i.e.,

it

uses

the

fact

that

tσ

→∗R

uµ

implies

uµ

→∗ R−1

tσ

and

checks whether cap(u) does not unify with t. Of course in the application of cap(u) we

have to take the reversed rules into account (possibly changing the set of deﬁned symbols)

and it is not applicable if R contains a collapsing rule → x where x ∈ V.

The last estimation we consider is based on a better version of cap, called tcap [41]. It

only replaces subterms with deﬁned symbols by a fresh variable, if there is a rule that

uniﬁes with the corresponding subterm.

Deﬁnition 6.10. Let R be a TRS.
5Note that Tarjan’s SCC decomposition algorithm produces exactly this list.

6.4. Certifying the Dependency Graph Processor

39

• tcap(f (tn)) = f (tcap(t1), . . . , tcap(tn)) iﬀ f (tcap(t1), . . . , tcap(tn)) does not unify with any variable renamed left-hand side of a rule from R
• tcap(t) is a fresh variable, otherwise
To illustrate the diﬀerence between cap and tcap consider the TRS of Ex. 6.1 and t = div(0, 0). Then cap(t) = xfresh since div is a deﬁned symbol. However, tcap(t) = t since there is no division rule where the second argument is 0.
Apart from tree automata techniques, currently the most powerful estimation is the one based on tcap looking both forward as in EDG and backward as in EDG∗. Hence, we aimed to implement and certify this estimation in IsaFoR.
Unfortunately, when doing so, we had a problem with the domain of variables. The problem was that although we ﬁrst implemented and certiﬁed the standard uniﬁcation algorithm of [78], we could not directly apply it to compute tcap. The reason is that to generate fresh variables as well as to rename variables in rules apart, we need a type of variables with an inﬁnite domain. One solution would have been to constrain the type of variables where there is a function which delivers a fresh variable w.r.t. any given ﬁnite set of variables.
However, there is another and more eﬃcient approach to deal with this problem than the standard approach to rename and then do uniﬁcation. Our solution is to switch to another kind of terms where instead of variables there is just one special constructor “P” representing an arbitrary fresh variable. In essence, this data structure represents contexts which do not contain variables, but where multiple holes are allowed. Therefore in the following we speak of ground-contexts and use C, D, . . . to denote them.
Deﬁnition 6.11. Let C be the equivalence class of a ground-context C where the holes are ﬁlled with arbitrary terms: C = {C[t1, . . . , tn] | t1, . . . , tn ∈ T (F , V)}.
Obviously, every ground-context C can be turned into a term t which only contains distinct fresh variables and vice-versa. Moreover, every uniﬁcation problem between t and can be formulated as a ground-context matching problem between C and , which is satisﬁable iﬀ there is some µ such that µ ∈ C .
Since the result of tcap is always a term which only contains distinct fresh variables, we can do the computation of tcap using the data structure of ground-contexts; it only requires an algorithm for ground-context matching. To this end we ﬁrst generalize groundcontext matching problems to multiple pairs (Ci, i).
Deﬁnition 6.12. A ground-context matching problem is a set of pairs M = {(C1, 1), . . . , (Cn, n)}. It is solvable iﬀ there is some µ such that iµ ∈ Ci for all 1 ≤ i ≤ n. We sometimes abbreviate {(C, )} by (C, ).
To decide ground-context matching we devised a specialized algorithm which is similar to standard uniﬁcation algorithms, but which has some advantages: it does neither require occur-checks as the uniﬁcation algorithm, nor is it necessary to preprocess the left-hand sides of rules by renaming (as would be necessary for standard tcap). And instead of applying substitutions on variables, we just need a basic operation on ground-contexts called merge such that merge(C, D) = ⊥ implies C ∩ D = ∅, and merge(C, D) = E implies C ∩ D = E .

40 Chapter 6. Certiﬁcation of Termination Proofs using CeTA

Deﬁnition 6.13. The following rules simplify a ground-context matching problem into solved form (where all terms are distinct variables) or into ⊥.

(a) M ∪ {(P, )} ⇒match M

(b) M ∪ {(f (Dn), f (un))} ⇒match M ∪ {(D1, u1), . . . , (Dn, un)}

(c) M ∪ {(f (Dn), g(uk))} ⇒match ⊥

if f = g or n = k

(d) M ∪ {(C, x), (D, x)} ⇒match M ∪ {(E, x)} if merge(C, D) = E

(e) M ∪ {(C, x), (D, x)} ⇒match ⊥

if merge(C, D) = ⊥

Rules (a–c) obviously preserve solvability of M (where ⊥ represents an unsolvable matching problem). For Rules (d,e) we argue as follows:

{(C, x), (D, x)} ∪ . . . is solvable iﬀ

• there is some µ such that xµ ∈ C and xµ ∈ D and . . . iﬀ

• there is some µ such that xµ ∈ C ∩ D and . . . iﬀ

• there is some µ such that xµ ∈ merge(C, D) and . . . iﬀ

• {(merge(C, D), x)} ∪ . . . is solvable

Since every ground-context matching problem in solved form is solvable, we have devised a decision procedure. It can be implemented in two stages where the ﬁrst stage just normalizes by the Rules (a–c), and the second stage just applies the Rules (d,e). It remains to implement merge.

merge(P, C) ⇒merge C merge(C, P) ⇒merge C merge(f (Cn), g(Dk)) ⇒merge ⊥ if f = g or n = k merge(f (Cn), f (Dn)) ⇒merge f (merge(C1, D1), . . . , merge(Cn, Dn)) f (. . . , ⊥, . . .) ⇒merge ⊥

Note that our implementations of the matching algorithm and the merge function in IsaFoR are slightly diﬀerent due to diﬀerent data structures. For example matching problems are represented as lists of pairs, so it may occur that we have duplicates in M. The details of our implementation can be seen in IsaFoR (theory Edg) or in the source of CeTA.
Soundness and completeness of our algorithms are proven in IsaFoR.
Theorem 6.14. • If merge(C, D) ⇒∗merge ⊥ then C ∩ D = ∅.
• If merge(C, D) ⇒∗merge E then C ∩ D = E .
• If (C, ) ⇒∗match ⊥ then there is no µ such that µ ∈ C .
• If (C, ) ⇒∗match M where M is in solved form, then there exists some µ such that µ∈ C .

Using ⇒match, we can now easily reformulate tcap in terms of ground-context matching which results in the eﬃcient implementation etcap.

Deﬁnition 6.15. • etcap(f (tn)) = f (etcap(t1), . . . , etcap(tn)) iﬀ (f (etcap(t1), . . . , etcap(tn)), ) ⇒∗match ⊥ for all rules → r ∈ R.

6.5. Certifying the Reduction Pair Processor

41

• P, otherwise
One can also reformulate the desired check to estimate the dependency graph whether tcap(t) does not unify with u in terms of etcap. It is the same requirement as demanding (etcap(t), u) ⇒∗match ⊥. Again, the soundness of this estimation has been proven in IsaFoR where the second part of the theorem is a direct consequence of the ﬁrst part by using the soundness of the matching algorithm.
Theorem 6.16. (a) Whenever tσ →∗R s then s ∈ etcap(t) .
(b) Whenever tσ →∗R uτ then (etcap(t), u) ⇒∗match ⊥ and (etcap(u), t) ⇒∗match ⊥ where etcap(u) is computed w.r.t. the reversed TRS R−1.
6.4.3. Certifying Dependency Graph Decomposition
Eventually we can connect the results of the previous two subsections to obtain one function to check a valid application of the dependency graph processor.
checkDepGraphProc(P, L, R) = checkDecomposition(checkEdg(R), P, L)
where checkEdg just applies the criterion of Thm. 6.16 (b). In IsaFoR the soundness result of our check is proven.
Theorem 6.17. If checkDepGraphProc(P, L, R) is accepted and if for all P ∈ L where P is marked, the DP problem (P , R) is ﬁnite, then (P, R) is ﬁnite.
To summarize, we have implemented and certiﬁed the currently best dependency graph estimation which does not use tree automata techniques. Our check-function accepts any decomposition which is based on a weaker estimation, but requires that the components are given in topological order. Since our algorithm computes edges only on demand, the number of tests for an edge is reduced considerably. For example, the ﬁve largest graphs in our experiments contain 73,100 potential edges, but our algorithm only has to consider 31,266. This reduced the number of matchings from 13 millions down to 4 millions.
Furthermore, our problem of not being able to generate fresh variables or to rename variables in rules apart led to a more eﬃcient algorithm for tcap based on matching instead of uniﬁcation: simply replacing tcap by etcap in TTT2 reduced the time for estimating the dependency graph by a factor of two.

6.5. Certifying the Reduction Pair Processor
One important technique to prove ﬁniteness of a DP problem (P, R) is the so-called reduction pair processor. The general idea is to use a well-founded order where all rules of P ∪ R are weakly decreasing. Then we can delete all strictly decreasing rules from P and continue with the remaining dependency pairs.
We ﬁrst state a simpliﬁed version of the reduction pair processor as it is introduced in [40], where we ignore the usable rules reﬁnement.
Theorem 6.18. If all the following properties are satisﬁed, then ﬁniteness of (P \ , R) implies ﬁniteness of (P, R).
(a) is a well-founded and stable order

42 Chapter 6. Certiﬁcation of Termination Proofs using CeTA
(b) is a stable and monotone quasi-order
(c) ◦ ⊆ and ◦ ⊆
(d) P ⊆ ∪ and R ⊆
Of course, to instantiate the reduction pair processor with a new kind of reduction pair, e.g., LPO, polynomial orders,. . . , we ﬁrst have to prove the ﬁrst three properties for that kind of reduction pairs. Since we plan to integrate many reduction pairs, but only want to write the reduction pair processor once, we tried to minimize these basic requirements such that the reduction pair processor still remains sound in total. In the end, we replaced the ﬁrst three properties by:
(a) is a well-founded and stable relation
(b) is a stable and monotone relation
(c) ◦ ⊆
In this way, for every new class of reduction pairs, we do not have to prove transitivity of or anymore, as it would be required for Thm. 6.18. Currently, we just support reduction pairs based on polynomial interpretations with negative constants [51], but we plan to integrate other reduction pairs in the future.
For checking an application of a reduction pair processor we implemented a generic function checkRedPairProc in Isabelle, which works as follows. It takes as input two functions checkS and checkNS which have to approximate a reduction pair, i.e., whenever checkS(s, t) is accepted, then s t must hold in the corresponding reduction pair and similarly, checkNS has to guarantee s t.
Then checkRedPairProc(checkS, checkNS, P, P , R) works as follows:
• iterate once over P to divide P into P and P where the former set contains all pairs of P where checkS is accepted
• ensure for all s → t ∈ R ∪ P that checkNS(s, t) is accepted, otherwise reject
• accept if P ⊆ P , otherwise reject
The corresponding theorem in IsaFoR states that a successful application of checkRedPairProc(. . . , P, P , R) proves that (P, R) is ﬁnite whenever (P , R) is ﬁnite. Obviously, the ﬁrst two conditions of checkRedPairProc ensure condition (d) of Thm. 6.18. Note, that it is not required that all strictly decreasing pairs are removed, i.e., our checks may be stronger than the ones that have been used in the termination provers.
6.6. Certifying the Whole Proof Tree
From Sect. 6.3–6.5 we have basic checks for the three techniques of applying dependency pairs (checkDPs), the dependency graph processor (checkDepGraphProc), and the reduction pair processor (checkRedPairProc). For representing proof trees within the DP framework we used the following data structures in IsaFoR.

6.6. Certifying the Whole Proof Tree

43

datatype ’f RedPair = NegPolo "(’f × (cint × nat list))list"
datatype (’f,’v)DPProof = . . . 6
| PisEmpty | RedPairProc "’f RedPair" "(’f,’v)trsL" "(’f,’v)DPProof" | DepGraphProc "((’f,’v)DPProof option × (’f,’v)trsL)list"
datatype (’f,’v)TRSProof = . . . 6
| DPTrans "(’f shp,’v)trsL" "(’f shp,’v)DPProof"
The ﬁrst line ﬁxes the format for reduction pairs, i.e., currently of (linear) polynomial interpretations where for every symbol there is one corresponding entry. E.g., the list [(f, (−2, [0, 3]))] represents the interpretation where Pol(f )(x, y) = max(−2 + 3y, 0) and Pol(g)(x1, . . . , xn) = 1 + Σ1≤i≤nxi for all f = g.
The datatype DPProof represents proof trees for DP problems. Then the check for valid DPProofs gets as input a DP problem (P, R) and a proof tree and tries to certify that (P, R) is ﬁnite. The most basic technique is the one called PisEmpty, which demands that the set P is empty. Then (P, R) is trivially ﬁnite.
For an application of the reduction pair processor, three inputs are required. First, the reduction pair redp, i.e., some polynomial interpretation. Second, the dependency pairs P that remain after the application of the reduction pair processor. Here, the datatype trsL is an abbreviation for lists of rules. And third, a proof that the remaining DP problem (P , R) is ﬁnite. Then the checker just has to call createRedPairProc(redp, P, P , R) and additionally calls itself recursively on (P , R). Here, createRedPairProc invokes checkRedPairProc where checkS and checkNS are generated from redp.
The most complex structure is the one for decomposition of the (estimated) dependency graph. Here, the topological list for the decomposition has to be provided. Moreover, for each subproblem P , there is an optional proof tree. Subproblems where a proof is given are interpreted as “real SCCs” whereas the ones without proof remain unmarked for the function checkDepGraphProc.
The overall function for checking proof trees for DP problems looks as follows.
checkDPProof(P,R,PisEmpty) = (P = []) checkDPProof(P,R,(RedPairProc redp P prf)) =
createRedPairProc(redp,P,P ,R) ∧ checkDP(P ,R,prf) checkDPProof(P,R,DepGraphProc P s) =
checkDepGraphProc(P,map (λ(prfO,P ).(isSome prfO,P )) P s, R) ∧ (Some prf,P )∈P s checkDPProof(P ,R,prf)
Theorem 6.19. If checkDPProof(P,R,prf) is accepted then (P, R) is ﬁnite.
Using checkDPProof it is now easy to write the ﬁnal method checkTRSProof for proving termination of a TRS, where computeID is an implementation of ID.
checkTRSProof(R,DPTrans P prf) = checkDPs(R,P) ∧ checkDPProof(P,computeID(R),prf)
For the external usage of CeTA we developed a well documented XML-format, cf. CeTA’s website. Moreover, we implemented two XML parsers in Isabelle, one that transforms a given TRS into the internal format, and another that does the same for a given proof.
6CeTA supports even more techniques, cf. CeTA’s website for a complete list.

44 Chapter 6. Certiﬁcation of Termination Proofs using CeTA
The function certifyProof, ﬁnally, puts everything together. As input it takes two strings (a TRS and its proof). Then it applies the mentioned parsers and afterwards calls checkTRSProof on the result.
Theorem 6.20. If certifyProof(R,prf) is accepted then SN(→R).
To ensure that the parser produces the right TRS, after the parsing process it is checked that when converting the internal data-structures of the TRS back to XML, we get the same string as the input string for the TRS (modulo whitespace). This is a major beneﬁt in comparison to the two other approaches where it can and already has happened that the uncertiﬁed components Rainbow/CiME produced a wrong proof goal from the input TRS, i.e., they created a termination proof within Coq for a diﬀerent TRS than the input TRS.
6.7. Error Messages
To generate readable error messages, our checks do not have a Boolean return type, but a monadic one (isomorphic to ’e option). Here, None represents an accepted check whereas Some e represents a rejected check with error message e. The theory ErrorMonad contains several basic operations like >> for conjunction of checks, <- for changing the error message, and isOK for testing acceptance.
Using the error monad enables an easy integration of readable error messages. For example, the real implementation of checkTRSProof looks as follows:
fun checkTRSProof where "checkTRSProof R (DPTrans P prf) = (
checkDPs R P <- (λs. ’’error . . .’’ @ showTRS R @ ’’. . .’’ @ showTRS P @ s)
>> checkDPProof P (computeID R) prf <- (λs. ’’error below switch to dependency pairs’’ @ s))"
However, since we do not want to adapt the proofs every time the error messages are changed, we setup the Isabelle simpliﬁer such that it hides the details of the error monad, but directly removes all the error handling and turns monadic checks via isOK(...) into Boolean ones using the following lemmas.
lemma "isOK(m >> n) = isOK(m) ∧ isOK(n)" lemma "isOK(m <- s) = isOK(m)"
Then, for example, isOK(checkTRSProof R (DPTrans P prf)) directly simpliﬁes to isOK(checkDPs R P) ∧ isOK(checkDPProof P (computeID R) prf).
6.8. Experiments and Conclusion
Isabelle’s code-generator is invoked to create CeTA from IsaFoR. To compile CeTA one auxiliary hand-written Haskell ﬁle CeTA.hs is needed, which just reads two ﬁles (one for the TRS, one for the proof) and then invokes certifyProof.
We tested CeTA (version 1.03) using TTT2 as termination prover (TC and TC+). Here, TTT2 uses only the techniques of this paper in the combination TC, whereas in TC+ all supported techniques are tried, including usable rules and nontermination. We compare to CiME/Coccinelle using AProVE [39] or CiME [22] as provers (ACC,CCC), and to Rainbow/CoLoR using AProVE or Matchbox [106] (ARC,MRC) where we take the results of

6.8. Experiments and Conclusion

45

the latest certiﬁed termination competition in Nov 20087 involving 1391 TRSs from the
termination problem database.
We performed our experiments using a PC with a 2.0 GHz processor running Linux
where both TTT2 and CeTA where aborted after 60 seconds. The following table summarizes our experiments and the termination competition results.

proved / disproved certiﬁed rejected
cert. timeouts total cert. time

TC TC+ ACC CCC ARC MRC

401 / 0 572 / 214 532 / 0 531 / 0 580 / 0 458 / 0

391 / 0 572 / 214 437 / 0 485 / 0 558 / 0 456 / 0

10 0 3 0 0 2

0

0 92 46 22

0

33s 113s 6212s 6139s 7004s 3602s

The 10 proofs that CeTA rejected are all for nonterminating TRSs which do not satisfy the variable condition. Since TC supports only polynomial orders as reduction pairs, it can handle less TRSs than the other combinations. But, there are 44 TRSs which are only solved by TC (and TC+), the reason being the time-limit of 60 seconds (19 TRSs), the dependency graph estimation (8 TRSs), and the polynomial order allowing negative constants (17 TRSs).
The second line clearly shows that TC+ (with nontermination and usable rules support) currently is the most powerful combination with 786 certiﬁed proofs. Moreover, TC+ can handle 214 nonterminating and 102 terminating TRSs where none of ACC, CCC, ARC, and MRC were successful. The eﬃciency of CeTA is also clearly visible: the average certiﬁcation time in TC and TC+ for a single proof is by a factor of 50 faster than in the other combinations.8
For more details on the experiments we refer to CeTA’s website. To conclude, we presented a modular and competitive termination certiﬁer, CeTA, which is directly created from our Isabelle library on term rewriting, IsaFoR. Its main features are that CeTA is available as a stand-alone binary, the eﬃciency, the dependency graph estimation, nontermination and usable rules support, the error handling, and the robustness. As each sub-check for a termination technique can be called separately, and as our check to certify a whole termination proof just invokes these sub-checks, it seems possible to integrate other techniques (even if they are proved in a diﬀerent theorem prover) as long as they are available as executable code. However, we will need a common proof format and a compatible deﬁnition. As future work we plan to certify several other termination techniques where we already made progress in the formalization of semantic labeling and the subterm-criterion. We would further like to contribute to a common proof format.

7http://termcomp.uibk.ac.at/ 8Note that in the experiments above, for each TRS, each combination might have certiﬁed a diﬀerent
proof. In an experiment where the certiﬁers where run on the same proofs for each TRS (using only techniques that are supported by all certiﬁers, i.e., EDG and linear polynomials without negative constants), CeTA was even 190 times faster than the other approaches and could certify all 358 proofs, whereas each of the other two approaches failed on more than 30 proofs due to timeouts.

7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules
Publication details
Christian Sternagel and Ren´e Thiemann. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules. In Proceedings of the 21st International Conference on Rewriting Techniques and Applications, volume 6 of LIPIcs, pages 325–340. Schloss Dagstuhl – Leibniz-Zentrum fu¨r Informatik, 2010.
Abstract
In this paper we present our formalization of two important termination techniques for term rewrite systems: the subterm criterion and the reduction pair processor in combination with usable rules. For both techniques we developed executable check functions using the theorem prover Isabelle/HOL. These functions are able to certify the correct application of the formalized techniques in a given termination proof. As there are several variants of usable rules, we designed our check function in such a way that it accepts all known variants, even those which are not explicitly spelled out in previous papers.
We integrated our formalization in the publicly available IsaFoR-library. This led to a signiﬁcant increase in the power of CeTA, a certiﬁed termination proof checker that is extracted from IsaFoR.
7.1. Introduction
Termination provers for term rewrite systems (TRSs) became more and more powerful in the last years. One reason is that a proof of termination no longer is just some reduction order which contains the rewrite relation of the TRS. Currently, most provers construct a proof in the dependency pair framework (DP framework). This allows to combine basic termination techniques in a ﬂexible way. Hence, a termination proof is a tree where at each node a speciﬁc technique is applied. So instead of just stating the precedence of some lexicographic path order or giving some polynomial interpretation, current termination provers return proof trees consisting of many diﬀerent techniques and reaching sizes of several megabytes. Thus, it would be too much work to check by hand whether these trees really form a valid proof. (Additionally, checking by hand does not provide a very high degree of conﬁdence.)
It is regularly demonstrated that we cannot blindly trust in the output of termination provers. Every now and then, some termination prover delivers a faulty proof. Most often, this is only detected if there is another prover giving a contradicting answer on the same problem. To solve this problem, three systems have been developed over the last few years: CiME/Coccinelle [21, 23], Rainbow/CoLoR [12], and CeTA/IsaFoR [104]. These systems either certify or reject a given termination proof. Here, Coccinelle and CoLoR

⊇

⊇

48 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules
are libraries on rewriting for Coq (http://coq.inria.fr) and IsaFoR is our library on rewriting for Isabelle [84]. (Throughout this paper we just write Isabelle whenever we refer to Isabelle/HOL.)
All of these certiﬁers can automatically certify termination proofs that are performed within the DP framework. In this framework one tries to simplify so called DP problems (P, R) by processors until all pairs in P are removed.
The reduction pair processor [42, 50] is the major technique to remove pairs. Consequently, it has been formalized in all three libraries. One of the conditions of the processor demands that all rules in R must be weakly decreasing. If this and all other conditions are satisﬁed then one can remove all strictly decreasing pairs. In this paper, we present the details about the formalization of two important extensions of the reduction pair processor.
The ﬁrst extension is the subterm criterion [51]. By restricting the used “reduction pair” to the subterm relation in combination with simple projections, it is possible to ignore the R-component of a DP problem. Note that the subterm criterion has independently (and only recently) been formalized for the Coccinelle-library [23]. Here, we present the ﬁrst Isabelle formalization of this important technique.
The other extension is the integration of usable rules [40–42, 50, 105]. With this extension not all rules in R have to be weakly decreasing but only the usable rules which are most often a strict subset of R. However, there are several deﬁnitions of usable rules where the most powerful ones ([41] and [42]) are incomparable.
usable rules ([41]) all rules ⊇ usable rules ([105])⊇usable rules ([40, 50])
usable rules ([42])
Although it was often stated that a combination of the deﬁnitions of usable rules of [41] and [42] would be possible there never was a refereed paper which showed such a proof. (However, there have been unpublished soundness proofs of such a combined deﬁnition.) In this paper we not only present such a combined deﬁnition and the ﬁrst corresponding formalized soundness proof, but we also simpliﬁed and extended the existing proofs. For example, we never construct ﬁltered terms although we consider usable rules w.r.t. some argument ﬁlter. (An independent formalization of usable rules is present in Coccinelle. However, this formalization is unpublished and it only uses the variant of [50]: it does not feature the improvements from [41] and [42].) With these two extensions of the reduction pair processor we could increase the number of TRSs (from the Termination Problem Database) where a proof can be certiﬁed by our certiﬁer CeTA by over 50%.
Note that all the proofs that are presented (or omitted) in the following, have been formalized in our Isabelle library IsaFoR. Hence, in the paper we merely give sketches of our “real” proofs. Our goal is to show the general proof outlines and help to understand the full proofs. Our library IsaFoR with all formalized proofs, the executable certiﬁer CeTA, and all details about our experiments are available at CeTA’s website:
http://cl-informatik.uibk.ac.at/software/ceta
The paper is structured as follows: In Sec. 7.2, we recapitulate the required notions and notations of term rewriting and the DP framework. In Sec. 7.3, we describe our formalization of the subterm criterion. The reduction pair processor with usable rules and its formalization is presented in Sec. 7.4. Then, in Sec. 7.5, we shortly describe how CeTA is obtained from IsaFoR and give a summary about our experiments. We ﬁnally conclude in Sec. 7.6.

7.2. Preliminaries

49

7.2. Preliminaries

Term Rewriting. We assume familiarity with term rewriting [5]. Still, we recall the most important notions that are used later on. A (ﬁrst-order) term t over a set of variables V and a set of function symbols F is either a variable x ∈ V or an n-ary function symbol f ∈ F applied to n argument terms f (tn). A context C is a term containing exactly one occurrence of the special constant P (that is assumed to be distinct from symbols in F). Replacing P in a context C by a term t is denoted by C[t]. A term t is a (proper) subterm of a term s—written (s t) s t—whenever there exists a context C (= P), such that s = C[t]. We write s ≈ t iﬀ s and t are uniﬁable. An argument ﬁlter π is a mapping from symbols to integers or lists of integers. It induces a mapping from terms to terms where π(x) = x, π(f (tn)) = π(ti) if π(f ) = i, and π(f (tn)) = f (π(ti1), . . . , π(tik)) if π(f ) = [i1, . . . , ik]. Argument ﬁlters are also used to indicate which positions in a term are regarded. Then π maps symbols to sets of positions. It will be clear from the context which kind of argument ﬁlters is used.
A rewrite rule is a pair of terms → r and a TRS R is a set of rewrite rules. The rewrite relation (induced by R) →R is the closure under substitutions and under contexts of R, i.e., s →R t iﬀ there is a context C, a rewrite rule → r ∈ R, and a substitution σ such that s = C[ σ] and t = C[rσ]. Reductions at the root are denoted by →R, .
We say that an element t is terminating / strongly normalizing (w.r.t. some binary relation S), and write SNS(t), if it cannot start an inﬁnite sequence
t = t1 S t2 S t3 S · · · .
The whole relation is terminating, written SN(S), if all elements are terminating w.r.t. it. For a TRS R and a term t, we write SN(R) and SNR(t) instead of SN(→R) and SN→R(t). We write S+ for the transitive closure of S, and S∗ is the reﬂexive transitive closure.
Lemma 7.1 (Properties of Subterms).
(a) stability: s t =⇒ sσ tσ
(b) subterms preserve termination: SNR(s) ∧ s t =⇒ SNR(t).
Let →SN(R) denote the restriction of →R to terminating terms, i.e., {(s, t) | s →R t ∧ SNR(s)}. Let →SN(R) denote the same relation extended by the restriction of to terminating terms, i.e., →SN(R) ∪ {(s, t) | s t ∧ SNR(s)}.
Lemma 7.2 (Termination Properties). Let S be some binary relation, let R be a TRS.
(a) SN(S) ⇐⇒ SN(S+),
(b) SN(→SN(R)),
(c) SNS(s) ∧ (s, t) ∈ S =⇒ SNS(t).

Dependency Pair Framework. The DP framework [42] is a way to modularize termination proofs. Therefore, we switch from TRSs to so called DP problems, consisting of two TRSs. The initial DP problem for a TRS R is (DP(R), R) where DP(R) are the dependency pairs of R. A (P, R)-chain is a possibly inﬁnite derivation of the following form:

s1σ1 →P t1σ1 →∗R s2σ2 →P t2σ2 →∗R s3σ3 →P · · ·

()

50 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules

where si → ti ∈ P for all i > 0 (this implies that P-steps only occur at the root). If additionally every tiσi is terminating w.r.t. R, then the chain is minimal. A DP problem (P, R) is called ﬁnite [42], if there is no minimal (P, R)-chain. Proving ﬁniteness of a DP problem is done by simplifying (P, R) by so called processors recursively, until the P-components of all remaining DP problems are empty and therefore trivially ﬁnite. For this to be correct, the applied processors need to be sound. A processor Proc is sound whenever for all DP problems (P, R) we have that ﬁniteness of (P , R ) for all (P , R ) ∈ Proc(P, R) implies ﬁniteness of (P, R). The termination techniques that will be introduced in the following sections are all such sound processors.1
Example 7.3. In the following TRS R the term set(xs) evaluates to the list [x ∈ xs | 0 < x] where duplicates are removed:

x < 0 → ⊥, (1) del(x, nil) → nil,

(4)

0 < s(y) → ,

(2) del(x, y : z) → if(x < y, y < x, x, y, z), (5)

s(x) < s(y) → x < y,

(3) if(⊥, ⊥, x, y, z) → del(x, z),

(6)

set(nil) → nil,

if( , b, x, y, z) → y : del(x, z),

(7)

set(x : z) → if2(0 < x, x, z),

if(b, , x, y, z) → y : del(x, z),

(8)

if2( , x, z) → x : set(del(x, z)),

if2(⊥, x, z) → set(z).

After computing the initial DP problem (DP(R), R) we can split it into the three problems ({(9)}, R), ({(13)–(16)}, R), and ({(10)–(12)}, R). (This is done by applying the dependency graph processor [3,41,42,50], a well-known technique to perform separate termination proofs for each recursive function.)

s(x) < s(y) → x < y,

(9) del (x, y : z) → if (x < y, y < x, x, y, z), (13)

set (x : z) → if2 (0 < x, x, z), (10) if (⊥, ⊥, x, y, z) → del (x, z),

(14)

if2 ( , x, z) → set (del(x, z)), (11) if ( , b, x, y, z) → del (x, z),

(15)

if2 (⊥, x, z) → set (z),

(12) if (b, , x, y, z) → del (x, z).

(16)

7.3. The Subterm Criterion
The subterm criterion [51] is a termination technique that can be employed as a processor of the DP framework. It may be seen as a variant of the reduction pair processor with an attached argument ﬁltering [3]. The used orders ( and ) allow to ignore the R component of a DP problem (P, R). And the argument ﬁltering is restricted to be a so called simple projection. A simple projection π maps a term to one of its arguments, i.e., π(f (tn)) = ti for some 0 < i n. For convenience we use Rπ to denote the ’composition’ of the binary relation on terms R and π, i.e., (s, t) ∈ Rπ iﬀ (π(s), π(t)) ∈ R.
Theorem 7.4. Finiteness of (P \ π, R) implies ﬁniteness of (P, R), provided:
(a) all rules of P are oriented by π (i.e., P ⊆ π)
1To be more precise, in IsaFoR it is shown that all these processors are chain identifying (chain identifying proc) which is a slightly stronger requirement than soundness [101, Chapter 7]. The reason is that chain identifying processors can easily be combined with semantic labeling [110]. However, we omit the details here and just refer to theory DpFramework for the interested reader.

7.3. The Subterm Criterion

51

(b) all lhss and rhss of P are non-variable and non-constant terms where the roots of rhss are not deﬁned in R (i.e., s = f (sn) with n > 0 and t = g(tm) with m > 0 and g ∈/ DR for all s → t ∈ P)
Example 7.5. The DP problem ({(9)}, R) from Ex. 7.3 can be solved using the simple projection π(< ) = 1, since π(s(x) < s(y)) = s(x) x = π(x < y). Taking π(del ) = 2 and π(if ) = 5 we can remove Pair (13) from ({(13)–(16)}, R). The result ({(14)–(16)}, R) is then solved by the dependency graph processor. Removing a pair from ({(10)–(12)}, R) is impossible as there is no π such that Pair (11) is oriented. Note that < , del , if , . . . ∈/ DR whereas <, del, if, · · · ∈ DR.
Before we can prove Theorem 7.4, we need several lemmas. First, we prove that termination of some element w.r.t. some binary relation S is equivalent to termination of the same element w.r.t. S+. Note that this is a more general result than Lem. 7.2(a) and thus allows termination analysis of a single term, no matter if the whole TRS is terminating.

Lemma 7.6. SNS(t) ⇐⇒ SNS+(t).
Proof. The direction from right to left is trivial. For the other direction assume that t is not terminating w.r.t. S+. Hence t = t1 S+ t2 S+ t3 S+ · · · . Let S denote the restriction of R to terminating terms, i.e., S = {(s, t) | s S t ∧ SNS(s)}. By deﬁnition we have SN(S ) and with Lem. 7.2(a) also SN(S +). Using SNS(t) and Lem. 7.2(c) together with the inﬁnite sequence from above, we get SNS(ti) for all i > 0, and further t1 S + t2 S + t3 S + · · · . This contradicts SN(S +).
Next consider a general result on inﬁnite sequences conducted in the union of two binary relations N and S where often N is a non-strict relation and S a strongly normalizing relation. Intuitively it states the following: Assume that there is an inﬁnite sequence of steps, where each step is an N -step or an S-step. Further assume that whenever there is an N -step directly followed by an S-step, those two steps can be turned into a single S-step. Additionally, there is no inﬁnite S-sequence starting at the same point as the sequence we are reasoning about. Then, from some point in our sequence on, there are no more S-steps, i.e., it ends in N -steps. This is a versatile fact that is used at several places inside IsaFoR.

Lemma 7.7. Let N and S be two binary relations over some carrier and q an inﬁnite sequence of carrier elements. If

(a) (qi, qi+1) ∈ N ∪ S for all i > 0, (b) N ◦ S ⊆ S, and

(c) SNS(q1),
then there is some j such that for all i ≥ j we have (qi, qi+1) ∈ N \ S.
Proof. For the sake of a contradiction assume that the lemma does not hold. Then, together with (a), we obtain ∀i > 0. ∃j ≥ i. (qj, qj+1) ∈ S. Using the Axiom of Choice we get hold of a choice function f such that

∀i > 0. f (i) ≥ i ∧ (qf(i), qf(i)+1) ∈ S,

(†)

52 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules

i.e., f (i) produces some index of an S-step after position i in q. Using f we deﬁne a new sequence [·] of indices inductively

i if i = 1, [i] =
f ([i − 1]) + 1 otherwise.

With (†) we have f (i) ≥ i and (qf(i), qf(i)+1) ∈ S for all i > 0. Since f (i) ≥ i there is an N ∪ S sequence from every qi to the corresponding qf(i). Thus we obtain (qi, qf(i)+1) ∈S+ for all i > 0 using (b). This immediately implies (q[i], q[i+1]) ∈S+ for all i > 0 and thereby ¬SNS+(q[1]) which is equivalent to ¬SNS(q[1]) by Lem. 7.6. But q[1] = q1 and thus ¬SNS(q1). Together with (c), this provides the desired contradiction.
Lemma 7.8. SNR(t) =⇒ SN( ∪→R)(t).
Proof. Assume that t is not terminating w.r.t. ( ∪ →R). Hence, we obtain the inﬁnite sequence t = t1 ( ∪ →R) t2 ( ∪ →R) t3 ( ∪ →R) · · · . From the assumption we have SNR(t1) and by Lem. 7.1 and Lem. 7.2(c) we obtain SNR(ti) for all i > 0. Thus, ti →SN(R) ti+1 for all i and since SN(→SN(R)) by Lem. 7.2(b) we arrive at a contradiction.
Proof of Theorem 7.4. In order to show that ﬁniteness of (P \ π, R) implies ﬁniteness of (P, R) we prove its contraposition. Hence, we may assume (in addition to the premises of Theorem 7.4) that there is a minimal inﬁnite (P, R)-chain and have to transform it into a minimal inﬁnite (P \ π, R)-chain. Thus we may assume that for all i > 0:
(a) si → ti ∈ P,
(b) tiσi →∗R si+1σi+1, and
(c) SNR(tiσi).
We start by a case distinction on ∃j > 0. ∀i ≥ j. (si, ti) ∈ (P \ π). If there is such a j, we can combine this with (b) and obtain the desired minimal (P \ π, R)-chain by shifting the original chain j positions to the left. Hence, consider the second case and assume ∀i > 0. ∃j ≥ i. (sj, tj) ∈/ (P \ π). With (a) and the preconditions of the subterm criterion processor this results in

∀i > 0. ∃j ≥ i. π(sj)σj π(tj)σj.

(17)

From this point on, the proof mainly runs by instantiating the relations N and S of
Lem. 7.7 appropriately and showing the assumptions Lem. 7.7(a)–Lem. 7.7(c) in turn. For N we use the reﬂexive and transitive closure of the rewrite relation, i.e., →∗R. For S we use ( ∪ →R)+. Finally, we use the inﬁnite sequence q deﬁned by qi = π(si+1)σi+1 (the
index shift is needed to establish termination of q1 later on). From (a) and Thm. 7.4(a),
together with Lem. 7.1(a) we get

π(si)σi π(ti)σi.

(18)

Furthermore, we obtain

π(ti)σi →∗R π(si+1)σi+1,

(19)

since the roots of ti and si+1 are guaranteed to be non-constant symbols and the root of ti is not a deﬁned symbol by Thm. 7.4(b). In combination we get π(si)σi ◦ →∗R π(si+1)σi+1

7.4. Usable Rules

53

and in turn (qi, qi+1) ∈ N ∪S, thereby discharging assumption Lem. 7.7(a). For our speciﬁc relations assumption Lem. 7.7(b) trivially holds. This leaves us with showing termination of q1 with respect to the relation ( ∪ →R)+. From the minimality of the initial chain (c) we know SNR(t1σ1) and by Lem. 7.1 we get SNR(π(s2)σ2) and thus SNR(q1). By Lemmas 7.8 and 7.6 we then achieve SN( ∪→R)+(q1). At this point (by Lem. 7.7) we get grip of some j > 0 such that

∀i ≥ j. (qi, qi+1) ∈ N \ S.

(20)

Now we prove ∀i ≥ j. π(si+1)σi+1 = π(ti+1)σi+1 as follows. Assume i ≥ j and π(si+1)σi+1
= π(ti+1)σi+1. Then with (18) we get π(si+1)σi+1 π(ti+1)σi+1. By (19), this results in π(si+1)σi+1 ◦ →∗R π(si+2)σi+2 and consequently in (qi, qi+1) ∈ S (contradicting 20). Thus ∀i ≥ j. qi = π(ti+1)σi+1. However, this contradicts (17).

7.4. Usable Rules

One important technique to prove termination within the DP framework is the reduction pair processor. A reduction pair ( , ) consists of a well-founded and stable relation in combination with a monotone and stable relation . Further, has to be compatible with , i.e., ◦ ⊆ . Note that it is not required that and are partial orders [104]. Examples for reduction pairs are polynomial orders [51,73,76], matrix orders [32,64], and the lexicographic path order (LPO) [58]. (There are several other classes of reduction pairs. We listed those which have been formalized in IsaFoR.)
The basic version of the reduction pair processor [42, 50] requires that all rules of R are weakly decreasing w.r.t. (then →R ⊆ ) and all pairs of P are weakly or strictly decreasing. From ( ) on page 49 it is easy to see that this implies that every reduction in a (P, R)-chain corresponds to a weak or strict decrease. Thus, the strictly decreasing pairs cannot occur inﬁnitely often and can be removed from P. This technique is already present in IsaFoR and its formalization is described in [104].
Theorem 7.9. Finiteness of (P \ , R) implies ﬁniteness of (P, R), provided:
(a) ( , ) is a reduction pair,

(b) P ⊆ ∪ ,

(c) R ⊆ .
Starting with [105], there have been several papers [41, 42, 50] on how to improve the last requirement. Therefore, R in (c) is replaced by the usable rules.
The main idea of the usable rules is easy to explain: since in chains rewriting is only performed with instances of rhss of P, it should suﬃce to rewrite with rules of deﬁned symbols that occur in rhss of P. If these usable rules introduce new deﬁned symbols then the rules deﬁning them also have to be considered as usable. Hence, in the remaining DP problem ({(10)–(12)}, R) of Ex. 7.3 only rules (1)–(3) and (4)–(8) are usable. This idea is formally expressed in the following deﬁnition.
Deﬁnition 7.10 (Usable Rules). The function urClosedU,R(t) deﬁnes whether a term t is closed under usable rules U w.r.t. some TRS R.

urClosedU,R(x) = true,

urClosedU,R(f (tn)) = urClosedU,R(ti) ∧ (root( ) = f =⇒

1in

→r∈R

→ r ∈ U).

54 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules

A TRS Q is closed under usable rules whenever all rhss are closed under usable rules, i.e.,

urClosedU,R(Q) =

urClosedU ,R (r).

→r∈Q

A DP problem (P, R) is closed under usable rules iﬀ P and U are closed under usable rules.

urClosedU (P, R) = urClosedU,R(P) ∧ urClosedU,R(U ).
The usable rules of DP problem (P, R) are the least set U satisfying urClosedU (P, R).
Note that there are several other equivalent deﬁnitions of usable rules, e.g., one can ﬁnd deﬁnitions of usable rules via so called usable symbols (i.e., root symbols of lhss of usable rules). However, there are also two improvements that yield smaller sets.2
The ﬁrst improvement is to take the reduction pair into account. If certain positions of terms are disregarded then their usable rules do not have to be considered. For example, usually due to the rhs if2 (0 < x, x, z) of DP (10) all <-rules are usable. However, if we would use a reduction pair based on polynomial orders, where Pol(if2 (b, x, z)) = z then the call to < is ignored by the order. This can be exploited by excluding the <-rules from the set of usable rules. This improvement is called usable rules w.r.t. an argument ﬁlter [42]. Here, argument ﬁlters are used to describe which positions in a term are relevant: an argument ﬁlter π with π(f ) = {i1, . . . , ik} indicates that in a term f (tn) only arguments ti1, . . . , tik are regarded. This is formalized by the notion of π-compatibility.
Deﬁnition 7.11 (π-Compatibility). A relation is π-compatible iﬀ for all n-ary symbols f , all i with 1 i n, all t1, . . . , tn, and all s and s :
i ∈/ π(f ) =⇒ f (t1, . . . , ti−1, s, ti+1, . . . , tn) f (t1, . . . , ti−1, s , ti+1, . . . , tn).

To formally deﬁne the usable rules w.r.t. an argument ﬁlter, a minor modiﬁcation of Def. 7.10 suﬃces. Just

replace

urClosedU ,R (ti )
1in

by

urClosedU ,R (ti ).
i∈π(f )

The second improvement to reduce the set of usable rules is performed by taking the structure of terms into account. Observe that in the rhs if2 (0 < x, x, z) of DP (10), the ﬁrst argument of < is 0. Hence, only the rules (1) and (2) are possibly applicable, but not the remaining Rule (3). This is not captured by Def. 7.10. As there, all f -rules have to be usable whenever the symbol f occurs. On the contrary, in [41], an improved version of usable rules is described which can ﬁgure out that Rule (3) is not applicable. To this end, the condition root( ) = f in Def. 7.10 is replaced by a condition based on uniﬁcation. Demanding ≈ f (tn) would be unsound. First f (tn) has to be preprocessed by the function tcap of [41]. This function keeps only those parts of the input term which cannot be reduced, even if the term is instantiated. All other parts are replaced by fresh variables.
2There also is another extension of usable rules, the generalized usable rules. It allows a variant of the reduction pair processor where reduction pairs with non-monotone may be used, cf. [33, Thm. 10]. However, that reduction pair processor is incomparable to both Thm. 7.9 and the upcoming Thm. 7.14. It may be interesting to formalize the soundness of that processor, too, but that would be a diﬀerent proof.

7.4. Usable Rules

55

Deﬁnition 7.12. Let R be a TRS.3

tcap(t) = f (tcap(tn))

if t = f (tn) and ≈ f (tcap(tn)) for all lhss of R,

a fresh variable otherwise.

Here, tcap(tn) is the list of terms where tcap is applied to all arguments of tn.
Now the second improvement can be deﬁned formally. Again, it is a minor but crucial modiﬁcation of Def. 7.10. It suﬃces to

replace

(root( ) = f =⇒ → r ∈ U) by

( ≈ f (tcap(tn)) =⇒ → r ∈ U ).

→r∈R

→r∈R

Hence, incorporating both improvements results in the following deﬁnition which now contains a π in the superscript to distinguish it from Def. 7.10.

Deﬁnition 7.13 (Improved Closure Under Usable Rules).

urClosedπU,R(x) = true,

urClosedπU,R(f (tn)) = urClosedπU,R(ti) ∧ ( ≈ f (tcap(tn)) =⇒

i∈π(f )

→r∈R

urClosedπU,R(Q) =

urClosedπU ,R (r),

→r∈Q

urClosedπU (P, R) = urClosedπU,R(P) ∧ urClosedπU,R(U ).

→ r ∈ U),

Note that we do not deﬁne the improved usable rules of a DP problem (P, R) (as we would, by demanding that U is the least set satisfying urClosedπU (P, R)). Hence, every set U that satisﬁes the closure properties can be used later on. It is easy to see, that the
usable rules w.r.t. Def. 7.10 satisfy the closure properties as well as a version of usable
rules which only incorporates one of the two improvements. Thus, by this deﬁnition we
gain the advantage that we can handle several variants of usable rules.
Having deﬁned all necessary notions, we are ready to present the improved reduction
pair processor with usable rules where the second part also incorporates [40, Theorem 28]
which allows to delete rules by a syntactic criterion.

Theorem 7.14 (Reduction Pair Processors with Usable Rules). Let c be some binary symbol, let Cε = {c(x, y) → x, c(x, y) → y},4 and let U be some TRS (called the usable rules). For every signature F and TRS R, let R¬F = { → r ∈ R | ∈/ T (F , V)}. Finiteness of (P \ , R) implies ﬁniteness of (P, R), provided:

(a) ( , ) is a reduction pair,
3Note that in IsaFoR we do not use tcap, but the more eﬃcient and equivalent version etcap which is based on ground-contexts. Moreover, uniﬁcation is replaced by ground-context matching [104, Section 4.2]. But as the notions of tcap and uniﬁcation are more common, in the following we stick to these two notions.
4The real deﬁnition of Cε in IsaFoR is slightly diﬀerent due to technical reasons. Since there is no restriction on the type of variables, there might be only one variable. To this end x and y are replaced by all possible terms. And as the type of function symbols is also unrestricted there might be no fresh symbol c. However, in IsaFoR the signature is implicit where every arity is allowed. For example, the term c(c, c) contains one symbol (c, 1) and two symbols (c, 0) where (c, 1) = (c, 0). In this way, we can always get a fresh symbol (c, n) where n is larger than all arities that occur in R and we use a constant (d, 0) to obtain this high arity. Hence, we deﬁne Cε = s,t{c(s, t, d, . . . , d) → s, c(s, t, d, . . . , d) → t}.

56 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules
(b) P ⊆ ∪ ,
(c) U ∪ Cε ⊆ ,
(d) is π-compatible,
(e) urClosedπU (P, R).
If additionally Cε ⊆ , is monotone, U ⊆ R, and F is the set of all symbols occurring in rhss of P ∪ U, then R can be replaced by U without any strictly decreasing rules and one can remove all rules which contain symbols of F in their left-hand side. Formally, ﬁniteness of (P¬F \ , U¬F \ ) implies ﬁniteness of (P, R).
Regarding requirement (c), observe that most reduction pairs that are currently used in automated termination tools do satisfy Cε ⊆ . Therefore, requirement (c) of Thm. 7.9 can usually be replaced by U ⊆ . Requirement (d) is easy to satisfy by choosing an appropriate argument ﬁlter π which depends on the reduction pair. And if U is chosen as the usable rules w.r.t. any known deﬁnition of usable rules, then condition (e) is satisﬁed. Thus, for most reduction pairs one has only replaced requirement R ⊆ of Thm. 7.9 by the weaker condition U ⊆ in Thm. 7.14.
Example 7.15. To solve the remaining DP problem ({(10)–(12)}, R) of Ex. 7.5 we use a polynomial order [73] where Pol(set (x)) = Pol(del(y, x)) = x, Pol(x : y) = Pol(if(. . . , y)) = y + 1, Pol(if2 (x, y, z)) = x + z, Pol(x < y) = Pol(⊥) = Pol( ) = 0, and Pol(c(x, y)) = x + y. A corresponding compatible argument ﬁlter π is deﬁned by π(set ) = {1}, π(del) = π(:) = {2}, π(if) = {5}, π(if2 ) = {1, 3}, π(<) = π(⊥) = π( ) = ∅, and π(c) = {1, 2}. For this argument ﬁlter, the minimal set of usable rules is U = {(1), (2), (4)–(8)}. Note that it would also be accepted if, e.g., Rule (3) would be added.
Then all conditions of Thm. 7.14 are satisﬁed and one can remove Pair (10) as it is strictly decreasing. The remaining DP problem ({(11),(12)}, R) is then easily solved by another application of the reduction pair processor where one chooses Pol(set (x)) = 0, Pol(if2 (x, y, z)) = 1, Pol(c(x, y)) = x + y, and where U = ∅.
In theory UsableRules we have proven Thm. 7.14. Although a similar proof has already been performed by the authors of [41] and [42]—on paper, not formalized—this proof has never been published in some reviewed article, it is only available in [101]. Moreover, our proof is not just a formalization of the proof in [101], but there are some essential diﬀerences which are pointed out in the following.
The standard proof of Thm. 7.14 is by transforming a minimal chain t1σ1 →∗R s2σ2 →P t2σ2 . . . into a chain over ﬁltered terms π(t1)δ1 →∗π(U)∪Cε π(s2)δ2 →π(P) π(t2)δ2 . . . and then uses the preconditions of the theorem to show that certain pairs of π(P) (and therefore also pairs of P) cannot occur inﬁnitely often. Here, one uses a transformation Iπ which transforms σi into δi. For the second part of the theorem where is monotone, one requires another transformation I which does not apply any argument ﬁlter. Hence, there are two transformations I and Iπ where for both transformations similar results are shown.
In our formalization we were able to simplify the proofs considerably by not constructing ﬁltered terms. Moreover, we use the same transformation I for both parts of the theorem.
A problem in the standard proof of the reduction pair processor with usable rules is the implicit assumption that the TRS R meets the variable condition, i.e., V( ) ⊇ V(r)

7.4. Usable Rules

57

and ∈/ V for all rules → r ∈ R. Although in practice this condition is nearly always satisﬁed, we have to deal with this assumption, where there are three alternatives. First, one can check that the TRS R meets the variable condition whenever the theorem is applied on some concrete DP problem (P, R). This would clearly increase the runtime for certifying a given proof. Second, one can deﬁne ﬁniteness of DP problems or soundness of processors in a way that it incorporates the variable condition. However, this will make the development of other processors more complicated which do not care about the variable condition. And third, one can try to prove Thm. 7.14 without assuming the variable condition. This is the alternative we have ﬁnally formalized and which does not appear in the literature so far.
For the upcoming formal deﬁnition of I we essentially use a combination of the deﬁnitions of [40] and [41] where x # xs denotes the Isabelle list with head x and tail xs.

Deﬁnition 7.16. Let R and U be two TRSs, let F be some signature, and let c be the binary symbol of Cε. We deﬁne I as a function from terms to terms as follows:

comb([t]) = t,

comb(t # s # ts) = c(t, comb(s # ts)),

rewrite(R, t) = {C[rσ]p | → r ∈ R, t = C[u], match(u, ) = σ}, I(x) = x,

 f (tn)

if ¬SNR(f (tn)),



I(f (tn)) = f (I(tn)) if SNR(f (tn)), f ∈ F , and ∀ → r ∈ R \ U . ≈ f (tcap(tn)),

comb(f (I(tn)) # I(rewrite(R, f (tn))))

otherwise.

I and tcap are homeomorphically extended to operate on lists, i.e., I(tn) = (I(t1), . . . , I (tn )).
The function comb just combines a non-empty list of terms into one term. It is easy to prove that one can access all terms in the list by rewriting with Cε: comb([. . . , ti, . . . ]) →∗Cε ti.
The function rewrite computes the list of one-step reducts of a given term by using a sound and complete matching algorithm match. The major diﬀerence between {s | t →R s} and rewrite(R, t) is that the latter instantiates a rule by the matcher of the lhs and
the corresponding redex (as usual), but it never instantiates variables which only occur in the rhs of the rule. For example, if R = {a → x} then {s | a →R s} is the set of all terms, whereas rewrite(R, a) = [x]. Hence, rewrite is sound (rewrite(R, t) ⊆ {s | t →R s}) but in general not complete. However, under one condition completeness is achieved: whenever t →R s by a reduction with a rule that satisﬁes the variable condition, then s ∈ rewrite(R, t).
The main reason for introducing rewrite is that without the assumption of the variable condition on R the set {s | t →R s} may be inﬁnite. Then the deﬁnition of I as in [41]—where {I(s) | f (tn) →R s} is used instead of I(rewrite(R, f (tn)))—does not work in combination with comb, as comb expects a list (or a ﬁnite set) as input. Also note that
this input must be ﬁnite as one ﬁnally wants to obtain a single term containing all input
terms. The ﬁrst case of I(f (tn)) is mainly a technicality. Usually, I is only deﬁned on termi-
nating terms. To make I a total function on all terms we inserted the case distinction on SNR(f (tn)). Termination of I is then proven using well-founded induction on →SN(R) where in this proof the soundness result for rewrite is crucial.

58 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules

The transformation I is constructed in such a way that for every reduction t →R s one obtains a weak decrease, provided that the usable rules and Cε are weakly decreasing. Therefore, in the deﬁnition of I there are essentially two cases for a term f (tn). If only usable rules can be used to reduce f (tn) at the root position, then I(f (tn)) is obtained by applying I on the arguments, resulting in f (I(tn)). The corresponding reduction will then result in a weak decrease as one can also perform the reduction with the usable rules
on the transformed term. The condition that only usable rules are applicable is ensured by demanding that no lhs of a non-usable rule in R \ U can be uniﬁed with f (tcap(tn)).
Otherwise, all rules may have been used to rewrite f (tn). Then, in addition to f (I(tn)) we have to store all one-step reducts of t. This is done by encoding them in a single term using comb. Now every possible reduct can be accessed using Cε. And since Cε is weakly decreasing we obtain a weak decrease. This is proven formally in the upcoming lemma.

Lemma 7.17 (Properties of I). Let ( , ) be a reduction pair, let be π-compatible, let U ∪ Cε ⊆ , let urClosedπU,R(U ), and let the rhss of U be terms within T (F , V). Let
SNR(t), SNR(tσ), and SNR(f (tn)).

(i) If urClosedπU,R(t) and t ∈ T (F , V) then tI(σ) ∗ I(tσ).

(ii) I(tσ) →∗Cε tI(σ). And if t ∈/ T (F , V) then I(tσ) →+Cε tI(σ).

(iii) If f (tn) →R, s using a rule → r and I(f (tn)) = f (I(tn)) then → r ∈ U , I(f (tn)) ∗ ◦ →U ◦ ∗ I(s). And I(f (tn)) →+Cε ◦ →U ◦ ∗ I(s) if ∈/ T (F , V).

(iv) If f (tn) →R s and I(f (tn)) = comb(. . .) then I(f (tn)) →+Cε I(s).

(v) If t →R s then I(t) ∗ I(s). Moreover, t →U¬F s or I(t) →+Cε ◦ ∗ I(s).

(vi) If t →∗R s then I(t)

∗ I(s). Moreover, I(t)

∗ ◦ →Cε ◦

∗

I (s)

or

t

→∗ U¬F

s.

Here, I is homeomorphically extended to substitutions, i.e., I(σ)(x) = I(σ(x)).

In the following proof sketch of Lem. 7.17, all essential points are included, especially those where we deviate from the standard proofs.

Proof. The proof of (vi) is a straight-forward induction on the reduction length using (v).
We prove (v), by induction over t. First note that t is not a variable. Otherwise, there
would be some x → r ∈ R, contradicting SNR(t). Hence the base case is trivial. In the step-case, we make a case distinction on how t = f (tn) is transformed. The case I(t) = comb(. . .) is solved by (iv). Otherwise, I(t) = f (I(tn)). For a root reduction we use (iii). Otherwise, s = f (t1, . . . , si, . . . , tn) and ti →R si. Applying the induction hypothesis is easy, but some additional eﬀort is required to prove I(s) = f (I(t1), . . . , I(si), . . . , I(tn)).
Proving (iv) essentially requires completeness of rewrite. To this end, we ﬁrst prove
that if f (tn) →R s then the employed rule → r must satisfy V( ) ⊇ V(r), as otherwise SNR(f (tn)) would not hold. Under this condition, completeness of rewrite states that s ∈ rewrite(R, f (tn)). The remaining proof of (iv) can be done by simple inductions using the deﬁnitions of comb and Cε.
To prove (iii), we ﬁrst show that I(f (tn)) = f (I(tn)) ensures that the employed rule → r is usable. The reason is that f (tn) = σ implies f (tcap(tn)) ≈ and hence, → r ∈/ R \ U by the deﬁnition of I. By the requirement on the rhss of U we know that
r ∈ T (F, V). Hence, we can build the following steps using (ii) and (i) in combination

7.4. Usable Rules

59

with urClosedπU,R(U ): I(f (tn)) = I( σ) ∗ I(σ) →U rI(σ) ∗ I(rσ) = I(s). And if ∈/ T (F , V) then we additionally get I( σ) →+Cε I(σ) by (ii). Proving (ii) is a straight-forward induction on t.
And ﬁnally, for (i), we also use induction on t. In the step-case we ﬁrst prove that t ∈ T (F , V) in combination with urClosedπU,R(f (tn)) implies f (I(tnσ)) = I(f (tnσ)). Then, for all argument positions i ∈ π(f ), we apply the induction hypothesis to obtain tiI(σ) ∗ I(tiσ). Then, by monotonicity of , we obtain f (. . . , tiI(σ), . . . ) ∗ f (. . . , I(tiσ), . . . ).
For all other positions, π-compatibility of provides the same inequality.

With the help of Lem. 7.17 it is now possible to prove the main result of this section.

Proof of Thm. 7.14. Assume that there is a minimal inﬁnite chain where siσi →P tiσi →∗R si+1σi+1 and SNR(tiσi) for all i. Let F be the set of all symbols that occur in rhss of P ∪ U. Then by the conditions of the theorem and by using Lem. 7.17 (i), (vi), and (ii),
for all i we conclude

siI(σi) →P tiI(σi) ∗ I(tiσi) ∗ I(si+1σi+1) ∗ si+1I(σi+1).

(‡)

By using P ⊆ ∪ we obtain a strict or weak decrease between every two terms siI(σi) and si+1I(σi+1). Thus, by Lem. 7.7, the strictly decreasing pairs can only occur ﬁnitely often. This shows that there must be some n such that for all i, si+n → ti+n ∈ P \ .

Hence, there is an inﬁnite minimal (P \ , R)-chain.

If additionally, Cε ⊆ and is monotone, we ﬁrst prove that there is some n with

tn+iσn+i

→∗ U¬F

sn+i+1σn+i+1

and

sn+i

∈

T (F, V)

for

all

i.

If

this

would

not

be

the

case,

then

inﬁnitely

often

tiσ

→∗ U¬F

si+1σi+1

does

not

hold

or

inﬁnitely

often

si

∈/

T (F, V).

Hence, by Lem. 7.17(vi), for inﬁnitely many i, I(tiσi) ∗ ◦ →Cε ◦ ∗ I(si+1σi+1) or

by Lem. 7.17(ii), for inﬁnitely many i, I(siσi) →+Cε siI(σi). As contains Cε and is

monotone, we also have →Cε ⊆ , and hence in both cases we obtain inﬁnitely many

strict decreases. Using the same reasoning as for (‡), we have inﬁnitely many i with

siI(σi) ( ∪ ) ◦ ∗ ◦ ◦ ∗ si+1I(σi+1) and for the remaining is we can use the previous results showing siI(σ) ( ∪ ) ◦ ∗ si+1I(σi+1). Then, using Lem. 7.7 where N = ( ∪ )∗ and S = N ◦ ◦ N , yields a contradiction.

Hence,

for

all

i

we

obtain

sn+iσn+i

→P¬F

tn+1σn+i

→∗ U¬F

sn+i+1σn+i+1.

Since

P ∪U

⊆

∪ and since both and are monotone and stable, we conclude (again by Lem. 7.7)

that from some point onwards only rules from (P¬F ∪ U¬F ) \ are used. Hence, we

have constructed a (P¬F \ , U¬F \ )-chain which is also minimal since SNR(tiσi) implies SNU¬F \ (tiσi) as U¬F ⊆ R by the requirement U ⊆ R.

To obtain an executable function which checks for correct applications of Thm. 7.14 it
is only demanded that the input and output DP problem, the reduction pair (without the
details of the fresh symbol c), and the usable rules are given. The corresponding inter-
pretation / precedence / . . . for c is then added automatically. Moreover, the argument
ﬁlter is constructed from the reduction pair. For example, for polynomial interpretations one always deﬁnes π in a way that i ∈ π(f ) iﬀ xi occurs within Pol(f (xn)). In this way,
is always π-compatible and Cε ⊆ . Hence, for the automation of Thm. 7.14 where is not monotone, one only has to check P ⊆ ∪ , U ⊆ , and urClosedπU (P, R) since the remaining requirements are satisﬁed by construction.
For the other case, where also rules of R are deleted, it is additionally checked that is monotone and that U ⊆ R. To achieve the former for polynomials, it is ensured that
the coeﬃcients of all variables are larger than zero and that all remaining parts of the

60 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules
polynomial are non-negative. For path orders in combination with argument ﬁlters, it is ensured that no argument is dropped, i.e., the argument ﬁlter may only permute and duplicate arguments.
7.5. Experiments
In the end, what we want to have is a workﬂow which automatically certiﬁes or rejects a given termination proof in CPF-format (a common format for termination proofs that is supported by all certiﬁers).5 To this end, we have to parse the proof, detect which processors have been applied on which DP problems, and ensure that the preconditions of every processor are met. We achieved this goal by writing a CPF parser and for each processor an executable function which checks the preconditions. If a processor application cannot be certiﬁed, the function rejects, providing an informative error message. As the parser and the check-functions are written in Isabelle, we just invoke Isabelle’s codegenerator [49] to obtain the executable program CeTA from IsaFoR.
This is in contrast to the other two certiﬁers CiME/Coccinelle and Rainbow/CoLoR.6 Both of them provide a parser (as part of CiME and Rainbow) that takes a termination proof and produces a Coq-script as output. The resulting script refers to facts proven in Coccinelle and CoLoR, respectively, which can then be checked by Coq.
For more details on this diﬀerence and the architecture of the overall proof-checking function in CeTA we refer to [104].
To measure the impact of our results we used 5 strategies for the two termination tools AProVE [39] and TTT2 [67].
• In the basic strategy the termination tools only use the dependency graph processor and the reduction pair processor without usable rules. (These are the only techniques that have been described in [104].)
• The sc strategy is an extension of basic by the subterm criterion processor.
• Similarly, ur is like basic except that usable rules may be used.
• The fourth strategy, sc+ur, is a combination of the previous three.
• Finally, full, is a strategy where all CeTA-certiﬁable processors may be used. We refer to http://cl-informatik.uibk.ac.at/software/ceta/versions.php for the details where all techniques are listed. (Our experiments have been performed using CeTA version 1.10.)
Note that for basic, sc, ur, and sc+ur, we only take linear polynomial interpretations as reduction pairs, whereas in full also other reduction pairs are used.
For each of the 2132 standard TRSs in the Termination Problem Database (version 7.0.2)7 and for each strategy, we ﬁrst ran both termination tools for at most one minute and then tried to certify all successful proofs with CeTA. The experiments were performed on a machine with 8 Dual Core AMD Opteron 885 processors and 64 GB RAM running Linux. An overview of the results is given in the following table where the column labels A, C, and T, refer to AProVE, CeTA, and TTT2, respectively.
5http://cl-informatik.uibk.ac.at/software/cpf/ 6Note that for a restricted set of techniques, CoLoR also features code-generation. 7available at http://termcomp.uibk.ac.at/

7.5. Experiments

61

YES avg. time
YES avg. time

basic AC 453 453
0.063
TC 439 439
0.059

sc AC 566 566
0.051
TC 553 553
0.048

ur AC 681 681
0.064
TC 663 663
0.065

sc+ur AC 684 684
0.061
TC 669 669
0.062

full AC 1242 1242
0.051

T 1223

C 1223 0.074

The table rows show successful termination proofs / certiﬁcates for termination proofs
(YES), and the average time for certifying (in seconds). All details on the experiments are available on CeTA’s website.
When comparing basic with sc+ur one can observe that adding the subterm criterion
and usable rules helps to increase the number of certiﬁed termination proofs by over 50%
for both termination tools. Moreover, checking the additional application conditions of the new techniques—where urClosedπU (P, R) is the most expensive one—does not have any measurable impact on the certiﬁcation time. That checking AProVE’s proofs is slightly
faster is explained by the fact that TTT2 always produces proofs with polynomial orders over the rationals, even if all coeﬃcients are naturals. And thus, for TTT2’s proofs, CeTA always has to perform computations over the rationals.
Our results also helped to win the annual termination competition for certiﬁed termination of TRSs in 2009.8 First several termination tools were run to generate proofs on
a random subset of 365 TRSs from the TPDB. For this, the tools where usually con-
ﬁgured for a speciﬁc certiﬁer in mind by restricting the set of termination techniques
correspondingly. Then, all certiﬁers were run on all proofs. The following table summa-
rizes the results, where the bold entries correspond to those proofs which were constructed
speciﬁcally for that certiﬁer.

tool intended certiﬁer proofs CeTA Coccinelle CoLoR

AProVE TTT2 CeTA 259 264
259 264 12 2 67 53

AProVE CiME Coccinelle 165 56 94 50 104 55 92 9

AProVE CoLoR
220 107 30 178

total
964 774 203 399

We observe that many proofs generated for CeTA cannot be handled by the other certiﬁers—only between 1 % and 26 % of these proofs have been certiﬁed—where one major reason is that the other certiﬁers do not incorporate usable rules.
Looking at the other direction we see, that even if the termination tool produced proofs for another certiﬁer, CeTA (version 1.09) achieved between 60 % and 90 % of the score of the intended certiﬁer.
In total, only 190 proofs have been rejected by CeTAin the competition. Of these proofs, 65 are supported in the meantime (the competition version did not feature monotone matrix interpretations [32], which are supported by version 1.10), 117 contain unsupported techniques (non-linear polynomial orders and RPO [27]), 6 are obviously buggy (e.g., the subterm criterion is applied with a projection that maps a binary symbol to its third argument), and 2 are faulty (some LPO was wrongly applied and some argument ﬁlter
8http://termcomp.uibk.ac.at/termcomp/competition/certificationResults.seam?cat= 10235&comp=101722

62 Chapter 7. Certiﬁed Subterm Criterion and Certiﬁed Usable Rules
delivers an unsolvable constraint). (At least for one of these proofs we know that this was due to an output bug of the proof producing tool.)
7.6. Conclusion
We have presented the ﬁrst formalization of two important termination techniques within the theorem prover Isabelle/HOL: the subterm criterion and the reduction pair processor with usable rules, where we combined the improvements of [41] and [42]. The integration of these techniques into our termination proof certiﬁer CeTA allowed to certify signiﬁcantly more termination proofs.
However, there are several termination techniques that have not been certiﬁed by now. To change this, in the future we aim at certifying several techniques for innermost termination like narrowing, rewriting, and instantiation [3, 42], or estimations of innermost dependency graphs [3, 41, 50].

8. Signature Extensions Preserve Termination – An Alternative Proof via Dependency Pairs
Publication details
Christian Sternagel and Ren´e Thiemann. Signature Extensions Preserve Termination – An Alternative Proof via Dependency Pairs. In Proceedings of the 19th Annual Conference of the EACSL on Computer Science Logic, volume 6247 of LNCS, pages 514–528. Springer, 2010.
Abstract
We give the ﬁrst mechanized proof of the fact that for showing termination of a term rewrite system, we may restrict to well-formed terms using just the function symbols actually occurring in the rules of the system. Or equivalently, termination of a term rewrite system is preserved under signature extensions. We did not directly formalize the existing proofs for this well-known result, but developed a new and more elegant proof by reusing facts about dependency pairs.
We also investigate signature extensions for termination proofs that use dependency pairs. Here, we were able to develop counterexamples which demonstrate that signature extensions are unsound in general. We further give two conditions where signature extensions are still possible.
8.1. Introduction
Our main objective is to formally show that the termination behavior of (ﬁrst-order) term rewrite systems (TRSs) [5] does not change under signature extensions. This is an important part of a bigger development inside IsaFoR (an Isabelle Formalization of Rewriting) which is used to generate CeTA (a tool for Certiﬁed Termination Analysis) [104].1 All our results have been formalized and machine-checked in the interactive proof assistant Isabelle/HOL [84]. In the following, whenever we speak about formalizing something, we mean a machine-checked formalization using Isabelle/HOL.
In the literature, termination of R (denoted by SN(R)), is usually only deﬁned for terms that do exclusively incorporate function symbols from the signature F of R. Often, it is implicitly assumed that this is equivalent to termination for terms over arbitrary extensions F ⊇ F. This is legitimate, since it has been shown that termination is modular under certain conditions (see [79,85] for details) and signature extensions satisfy these conditions. A property P is called modular, whenever P R and P S, for TRSs R and S over disjoint signatures F and G, implies P (R ∪ S). (Note that P x is Isabelle/HOL’s
1http://cl-informatik.uibk.ac.at/software/ceta

64 Chapter 8. Signature Extensions Preserve Termination
way of writing a function or predicate P applied to an argument x.) Now, to use modularity of termination to achieve SN(R) over the signature F ⊇ F, we choose S = ∅ and G = F − F. Then, the above mentioned conditions are trivially satisﬁed and we obtain SN(R) =⇒ SN(R ∪ S), where the latter system has the same rules as R, but the signature F .
In this way, the two aforementioned proofs (which both use rather similar proof techniques), can be used to obtain termination preservation under signature extensions. However, the ﬁrst proof [79] is quite long and complicated even on paper (10 pages, neglecting preliminaries). Concerning the second proof [85]—although short on paper—there are two reasons for not going that way:
(i) This proof would require to formalize concepts that are currently not available in our library but are assumed as preliminaries in the paper proof (which is the only reason that the proof is short). This includes, e.g., multi-hole contexts, and functions like rank , top, etc. Furthermore, some of those concepts seem bulky to formalize, e.g., multi-hole contexts would require that a context having n holes is always applied to exactly n terms. This cannot be guaranteed on the type level without having dependent types and would lead to side-conditions that had to be added to all proofs using multi-hole contexts.
(ii) We do already have a formalization of many term rewriting related concepts. Thus, it seems only natural to build on top of those available results.
Hence, we take a diﬀerent road (that may seem as a detour in the beginning). We use F(R) to denote the signature just containing function symbols that do actually occur in some rule of R. By ¿R we denote the rewrite relation induced by R just using terms over F(R) and by →R the same relation but for terms over arbitrary extensions of F(R) (i.e., ¿R is a restriction of →R). Hence, our ﬁrst main result can be written as
Theorem 8.1. SN(¿R) ←→ SN(→R)
In the proof, we concentrate on the direction from left to right, since the converse trivially holds. Before we give our general proof idea, we want to show why “the direct approach” is diﬃcult. By “direct” we mean:
Assume that there is an inﬁnite sequence in →R and construct an inﬁnite sequence in ¿R out of it.
For this purpose we would have to provide a function f such that f s ¿R f t is implied by s →R t for arbitrary terms s and t. This requires that f somehow removes all function symbols that are not in F(R) from its argument but still preserves any redexes (i.e., subterms where rules are applicable). For example the simple idea to clean terms by replacing all subterms f (. . .) where f ∈/ F(R) by the same variable, does not work. The reason is that a given inﬁnite →R-derivation might take place strictly below a symbol f ∈/ F(R) and then, after turning f (. . .) into a variable, those reductions can no longer be simulated. To cut a long story short, we stopped at some point to investigate this direction further, since all our approaches became awfully complicated (especially for mechanizing the proof).
Our salvation appeared in the form of dependency pairs (DPs). By redirecting the course of our proof into the DP setting [3] and back again, we were able to give a short and (in our opinion) elegant proof of Theorem 8.1, using the simple technique of cleaning.

8.2. Preliminaries

65

The reason is that by using DPs we obtain a derivation which contains inﬁnitely many reductions at the root position. And all these root reductions are still possible after cleaning. Note that this also shows that signature extensions are sound for termination problems in the DP setting (Lemma 8.8)—our second main result.
However, after trying to extend our proof to the DP setting with minimal chains, we discovered a counterexample demonstrating that signature extensions are unsound for non-left-linear TRSs. A small modiﬁcation of this counterexample also shows that the technique of root-labeling [95] in the DP setting with minimal chains—which relies on signature extensions—is also only sound for left-linear TRSs. This refutes the corresponding result in [95] which does not demand left-linearity. (As the modularity results of [79, 85] do not consider minimal chains, these results are not aﬀected by our counterexample.)
In total, in this paper we show that signature extensions are possible for termination of TRSs and that they can be used in the DP setting for left-linear TRSs or for nonminimal chains. We also show that the soundness proofs of root-labeling can be repaired by additionally demanding left-linearity.
The structure of our discourse is as follows: In Section 8.2 we recall some necessary deﬁnitions of term rewriting (as used in our formalization). Afterwards, in Section 8.3, we give some results on DPs. Two of our main results are given in Section 8.4, where we also formally prove completeness of DPs. Then, in Section 8.5 we show some applications— including root-labeling—and limitations of our results. Here, we also discuss the problem of signature extensions in combination with minimal chains and show that there is no problem in the left-linear case. We ﬁnally conclude in Section 8.6.
Since all facts we are using have been machine-checked, we do not give any proofs for results from Sections 8.2 and 8.3 and refer the interested reader to the IsaFoR sources (freely available from its website). Our formalization of Theorem 8.1 can be found under the name SN wfrstep SN rstep conv in the theory DpFramework. Also in Section 8.4 we try to skip technical details and give a high-level overview of our proofs.

8.2. Preliminaries
In IsaFoR we are concerned with ﬁrst-order terms deﬁned by the data type:
datatype (α, β) term = Var β | Fun α ((α, β) term list)
Hence, a term is either a variable, or a function symbol applied to a list of argument terms. Note that this deﬁnition does not incorporate any well-formedness conditions. In particular, there is no signature that terms are restricted to. We identify a function symbol by its representation together with its arity. Hence, the function symbol f in the term Fun f [] is diﬀerent from the function symbol f in the term Fun f [Var x ] (the former has arity 0 and the latter arity 1). To increase readability we write terms like the previous two as f (a constant without arguments) and f (x), respectively. A (rewrite) rule is a pair of terms and a TRS is a set of rules. A TRS is well-formed iﬀ all left-hand sides of rules are non-variable terms and for each rule every variable occurring in the right-hand side also occurs in the left-hand side. We write wf trs R to indicate that the TRS R is well-formed.
Example 8.2. The TRS { add(0, y) → y, add(s(x), y) → s(add(x, y)) }, encoding addition on Peano numbers, is well-formed.

66 Chapter 8. Signature Extensions Preserve Termination

The rewrite relation induced by a TRS R is obtained by closing R under substitutions and contexts, i.e., →R is deﬁned inductively by the rules:

(l , r ) ∈ R l →R r

s →R t sσ →R t σ

s →R t C [s] →R C [t]

Here, tσ denotes the application of a substitution σ to a term t and C [t] denotes substituting the hole in the context C by the term t. Whenever s →R t, we say that s rewrites (in one step) to t.
A TRS is terminating/strongly normalizing iﬀ the rewrite relation →R induced by R is well-founded—denoted by SN(→R). (We sometimes write SN(R) instead of SN(→R) to stress that termination is a property depending merely on R.) Termination of a speciﬁc term is written as SNR(t), i.e., there is no inﬁnite →R-derivation starting from t.
Using the deﬁnition of →R, termination is formalized as SN(→R) ≡ t. ∀ i . ti →R ti + 1. Here, we use functions from natural numbers to some type τ , to encode inﬁnite sequences over elements of type τ , which are written by t in contrast to terms t. We use
subscripts to indicate positions in such inﬁnite sequences, i.e., we write ti to denote the i-th element in the inﬁnite sequence t.
Remember that by F(R) we denote the signature of function symbols actually occurring in some rule of R. Using the function

F(x ) = ∅, F(f (ts)) = {(f , |ts|)} ∪ {F(t) | t ∈ ts}.

F(R) is obtained by extending F(·) to TRSs in the obvious way.

Example 8.3. The signature of the TRS from Example 8.2 is {(add, 2), (s, 1), (0, 0)}.

8.3. Dependency Pairs
To get hold of the (recursive) function calls in a TRS, the so called dependency pairs are used [3, 28].
Deﬁnition 8.4. The DPs of a TRS R are deﬁned by
DP(R) = {(l , f (ts)) | ∃ r . (l , r ) ∈ R ∧ f ∈ D(R) ∧ r ¤ f (ts) ∧ l f (ts)}
where ( £) ¤ denotes the (proper) subterm relation on terms and D(R) is the set of deﬁned function symbols in R.2 By · we denote the operation of marking the root symbol of a term with the special marker . In examples we use capitalization and hence write F instead of f .
Example 8.5. Since the TRS of Example 8.2 contains just one “recursive call,” we get the single DP ADD(s(x), y) → ADD(x, y).
Note how DPs get rid of context information. This is exactly what makes them so useful in our proof.
Having DPs, we can use an alternative characterization of nontermination using DP problems and chains. A DP problem (P, R) just consists of two TRSs P and R. Then a (P, R)-chain is an inﬁnite sequence of the following shape:
2A function symbol is deﬁned in a TRS, if it occurs as the root of a left-hand side.

8.4. Main Results

67

∀ i . (si, ti) ∈ P ∧ tiσi →∗R si + 1σi + 1.
We use the abbreviation ichain (P, R) s t σ for such a sequence. The soundness result of DPs then states that a (well-formed) TRS R is terminating if there is no inﬁnite (DP(R), R)-chain where the formalization was described in [104].
Lemma 8.6. wf trs R =⇒ ¬ SN(→R) =⇒ ∃ s t σ. ichain (DP(R), R) s t σ
Sometimes, we are interested in minimal (P, R)-chains. The only diﬀerence between min ichain (P, R) s t σ and ichain (P, R) s t σ, is the additional requirement in minimal chains that SNR(tiσi) for all i.

8.4. Main Results

Since our term data type does not take care of building only terms corresponding to a speciﬁc signature, by default any rewrite relation →R in our formalization is deﬁned over terms containing arbitrary function symbols. Our ﬁrst goal is to show that once we have shown termination for terms using only function symbols from F(R), this implies that →R does terminate for arbitrary terms. Before doing that, we need means to identify well-formed terms. To this end we use the inductively deﬁned set T (F), containing all terms that are well-formed with respect to the signature F.
Deﬁnition 8.7 (Well-Formed Terms).

x ∈ T (F)

(f , |ts|) ∈ F ∀ t∈ts. t ∈ T (F) f (ts) ∈ T (F)

Using this deﬁnition we can deﬁne the well-formed rewrite relation induced by a TRS R:

¿R ≡ {(s, t) | s →R t ∧ s ∈ T (F (R)) ∧ t ∈ T (F (R))}.
Further, let C(F) denote the set of well-formed contexts with respect to the signature F. What we want to show is SN(¿R) =⇒ SN(→R). For the proof we need a way to remove unwanted function symbols from terms. This is the purpose of the following cleaning function:
yF =y f (ts) F = if (f , |ts|) ∈ F then f (map · F ts) else z
where z denotes an arbitrary but ﬁxed variable. Intuitively, every subterm of a term whose root is not in the given signature, is replaced by z. Having this, the proof of SN(¿R) =⇒ SN(→R) (actually we prove its contrapositive) is done in three stages:
(i) First, we assume ¬ SN(→R). Then by the soundness of DPs (Lemma 8.6) we obtain an inﬁnite (DP(R), R)-chain.

(ii) Next, we show that every inﬁnite chain can be transformed into an inﬁnite clean chain.

(iii) And ﬁnally, we show completeness of the DP-transformation for well-formed terms, i.e., that an inﬁnite clean (DP(R), R)-chain can be transformed into an inﬁnite derivation w.r.t. ¿R. Hence, ¬ SN(¿R), concluding the proof.

68 Chapter 8. Signature Extensions Preserve Termination

In total we get wf trs R =⇒ SN(¿R) =⇒ SN(→R) and since every non-well-formed TRS is nonterminating, we ﬁnally have a proof of Theorem 8.1. Note that the second step also shows the second main result: signature extensions are valid when performing termination proofs using DPs (without minimality).
It remains to prove the following two lemmas where we use the abbreviations F(P, R) ≡ F(P) ∪ F(R) and (R) ≡ F(R) ∪ F (R) with F (R) ≡ {(f , n) | (f , n) ∈ F(R)}, and the cleaning function is extended to sequences of substitutions in the obvious way.
Lemma 8.8 (Signature Restrictions for Chains). F (P, R) ⊆ F =⇒ ichain (P, R) s t σ =⇒ ichain (P, R) s t σ F
Lemma 8.9 (Completeness of DPs for ¿R). ichain (DP(R), R) s t σ (R) =⇒ ¬ SN(¿R)
Note that by applying ﬁrst Lemma 8.8 and then Lemma 8.9, we also obtain the classical completeness result of DPs.
Lemma 8.10 (Completeness of DPs). ichain (DP(R), R) s t σ =⇒ ¬ SN(→R)
Proof. Obviously, we have F(DP(R), R) ⊆ (R). Together with the assumption ichain (DP(R), R) s t σ, this yields ichain (DP(R), R) s t σ (R), using Lemma 8.8. Then, from Lemma 8.9, we obtain ¬ SN(¿R) and thus ¬ SN(→R).
of Lemma 8.8. From the assumptions of Lemma 8.8 we obtain

∀i. si ∈ T (F ) ∧ ti ∈ T (F ), ∀i. tiσi →∗R si+1σi+1, ∀i. (si, ti) ∈ P.

(1) (2) (3)

Further note that whenever there is an R-step from s to t, then either this step is also possible in the cleaned versions of s and t, or the cleaned versions are equal, i.e.,

s →R t =⇒ s F(R) →=R t F(R).

From this and (2) we may conclude

∀i. tiσi F →∗R si+1σi+1 F

by induction over the length of the rewrite sequence (remember that F(R) ⊆ F). Using (1) we may push the applications of the clean function inside, resulting in

∀i. ti F σi F →∗R si+1 F σi+1 F .

Together with (3) we obtain the desired clean inﬁnite chain as (1) shows si F = si and ti F = ti for all i.
of Lemma 8.9. Again, we show the lemma in its contrapositive form. Thus, we assume SN(¿R). Now, let F denote the signature of R and u(·) the operation of ‘unsharping,’ i.e., removing s from terms:

f (map u(·) ts) if t = f (ts) or t = f (ts), and u(t) =
t otherwise.

8.5. Applications

69

The extension of u to substitutions is deﬁned as u(σ)(x) = u(σ(x)). For the sake of a
contradiction, assume that there is an inﬁnite (DP(R), R)-chain over s, t, and σ (R). Since cleaning does not aﬀect s and t, this implies an inﬁnite (DP(R), R)-chain over
s (R), t (R), and σ (R), i.e.,

∀i. ( si (R), ti (R)) ∈ DP(R) ∀i. ti (R) σi (R) →∗R si+1 (R) σi+1 (R)

(4) (5)

Then from (4) we obtain

∀i. ∃C. C ∈ C(F ) ∧ u(si) F →R C[ u(ti) F ]

by construction of DP(R). Using the Axiom of Choice we hence obtain a sequence of contexts C, such that Ci is the context employed in the i-th step of (4), i.e.,

∀i. Ci ∈ C(F ) ∧ u(si) F →R Ci[ u(ti) F ]

(6)

Let D denote the following sequence:

P if i = 0, Di = Di−1 ◦ (Ci u(σi) F ) otherwise.
Where ◦ denotes the composition of contexts, i.e., the right context replaces the hole of the left one. This function gives for the i-th DP-step in the inﬁnite chain, all the contexts that have been lost due to using DP(R) instead of R and additionally applies all the necessary substitutions. For the sake of brevity we deﬁne:

si = Di[ u(si) F u(σi) F ] ti = Di+1[ u(ti) F u(σi) F ]
Then by (6) we have si →R ti, since rewriting is closed under contexts and substitutions. From (5) we conclude u(ti) F u(σi) F →∗R u(si+1) F u(σi+1) F , since removing s does not destroy any redexes of R. By wrapping this derivation in the context Di+1 we obtain ti →∗R si+1. Combining this with si →R ti yields
si →+R si+1
From our assumption SN(¿R) we conclude that R is well-formed. Moreover, it is apparent from the deﬁnitions of · F and Di, together with (6) that all the sis are well-formed, i.e., si ∈ T (F). Together with the well-formedness of R one can prove that also all intermediate terms in all derivations si →+R si+1 are in T (F ). Thus we have an inﬁnite ¿R-sequence which contradicts our assumption SN(¿R).

8.5. Applications
In most termination tools, termination techniques are freely combined within a complex termination proof. For example, it is a standard procedure to ﬁrst remove some rules from R, resulting in R , and then prove SN(R ) without caring about any changes in the signature. I.e., proving termination of SN(R ) is performed as if the signature were

70 Chapter 8. Signature Extensions Preserve Termination
F(R ) and not the original signature F(R). The soundness of this approach relies upon Theorem 8.1.
At ﬁrst view, Theorem 8.1 might not seem important, as there are several termination techniques which do not rely upon the signature. For example, when using polynomial interpretations, it always suﬃces to give the interpretations for the function symbols occurring in the TRS, no matter if the signature contains other symbols. The reason is that the interpretation of any other symbol has no impact when computing the polynomials for the left-hand sides and right-hand sides of the rules. Similar situations occur for other reduction orders and other termination techniques, like semantic labeling [110].
However, we are aware of at least two termination techniques where the signature is essential.
String Reversal. If we restrict terms in rewriting to employ only unary function symbols, we are in the setting of string rewriting. For notational convenience we write abc instead of a(b(c(x))), where the variable x is implicit. There are several termination techniques that work only/better for strings. One of them is string reversal. This technique uses the fact that a string rewrite system (SRS) S is terminating iﬀ rev(S) is terminating. Here, rev(S) denotes the mapping of the function
rev(t )a if t = at , rev(t) =
t otherwise,
over all left-hand sides and right-hand sides of S. In practice, this often helps to automatically ﬁnd a termination proof.
Example 8.11. Consider the following TRS
a(b(b(x))) → a(b(a(a(a(a(x)))))) f(x, y) → x
which is not an SRS. One can remove the second rule by a polynomial order which maps a(x) and b(x) to x, and f(x, y) to x + y + 1. Then the SRS
abb → abaaaa
remains, where the signature still contains the binary symbol f. As string reversal is only deﬁned for unary symbols, the presence of f forbids the application of string reversal. But after applying the signature restriction to a and b we are allowed to forget about f and apply string reversal to obtain the following SRS:
bba → aaaaba
Note that in this reversed SRS there are no dependency pairs as ba is a proper subterm of bba. Therefore, termination is now trivially proven.
Root-Labeling. Root-labeling [95] is a special version of semantic labeling [110]. We start with a short description of semantic labeling. We interpret a TRS R by an Falgebra M = (M, {fM}f∈F ). That is, we interpret every function symbol f of arity n, by a function fM : M n → M , over the carrier M . Then, the interpretation of a term with respect to a given variable assignment µ, is given by: [µ]M(x) = µ(x) and

8.5. Applications

71

[µ]M(f (t1, . . . , tn)) = fM([µ]M(t1), . . . , [µ]M(tn)). We say that M is a model of R iﬀ for all assignments µ and all rules l → r ∈ R, we have [µ]M(l) = [µ]M(r).
Now, we can label the function symbols of R according to the interpretation of their arguments. For every n-ary function symbol f , we choose a set of labels Lf = ∅ in combination with a mapping πf : M n → Lf . The labeling is extended to terms as follows: labµ(x) = x and labµ(f (t1, . . . , tn)) = fm(labµ(t1), . . . , labµ(tn)) with m = πf ([µ]M(t1), . . . , [µ]M(tn)). Then, the labeled TRS Rlab consists of the rules labµ(l) → labµ(r) for all l → r ∈ R and assignments µ. Zantema [110] has shown that for every model of R, the TRS R is terminating iﬀ the TRS Rlab is terminating.
The diﬃcult part of applying semantic labeling for proving termination, is to ﬁnd a proper model. This is solved in the special case of root-labeling by ﬁxing the interpretation. Every function symbol is interpreted by itself (i.e., fM(x1, . . . , xn) = f ) and the labeling is ﬁxed to tuples of function symbols (i.e., πf (x1, . . . , xn) = (x1, . . . , xn)). Now, to satisfy the necessary model condition, we close the rules of a TRS under the so called ﬂat contexts before labeling. This makes sure that for every resulting rule l → r, the root symbol of l is the same as the root symbol of r and thus, [µ]M(l) = [µ]M(r) for every assignment µ. Here, the ﬂat contexts are determined solely by the signature. Again, Theorem 8.1 shows that one can reduce the possibly inﬁnite set of ﬂat contexts (if the signature F is inﬁnite) to a ﬁnite set of ﬂat contexts (if F(R) is ﬁnite).
Example 8.12. Consider the TRS {a(b(x)) → b(a(a(x)))}. This yields the set of ﬂat contexts {a(P), b(P)}. After closing the TRS under ﬂat contexts we obtain the two rules {a(a(b(x))) → a(b(a(a(x)))), b(a(b(x))) → b(b(a(a(x))))}.3 Now, root-labeling results in the following labeled TRS:
aa(ab(ba(x))) → ab(ba(aa(aa(x)))) aa(ab(bb(x))) → ab(ba(aa(ab(x)))) ba(ab(ba(x))) → bb(ba(aa(aa(x)))) ba(ab(bb(x))) → bb(ba(aa(ab(x))))
The advantage of root-labeling or semantic labeling is that afterwards one can distinguish diﬀerent occurrences of symbols as they might have diﬀerent labels. For example, the last but one symbols of the left- and right-hand sides are ab and aa, respectively, whereas in the original TRS these symbols were just a and could not be distinguished. That such a distinction can be helpful is demonstrated in several examples [95, 110].
Note that root-labeling is also applied in the DP setting. Here, Lemma 8.8 can be used to show that it suﬃces to build the ﬂat contexts w.r.t. the signature of the given DP problem.
However, many termination tools base their termination analysis on DPs where always minimal chains are considered. The reason to work with minimal chains is that many powerful termination techniques are only sound when regarding minimal chains [42, 101].
Unfortunately, when trying to lift Lemma 8.8 to minimal chains, we ﬁgured out that this is impossible. It is easy to show that cleaning terms might introduce nontermination if non-left-linear rules are present. For example if a and b are not in the signature and
3If the signature would be larger, e.g., if there would be an additional ternary symbol c, then the ﬂat contexts would include {c(P, x2, x3), c(x1, P, x3), c(x1, x2, P)} and for each of these contexts one would get another rule. Hence, the signature restriction is essential to get few ﬂat contexts and therefore small systems.

72 Chapter 8. Signature Extensions Preserve Termination
there is a rule f(x, x) → f(x, x), then this rule cannot be applied on f(a, b). However, it is applicable on the cleaned term f(z, z).
Moreover, we even found a counter-example where there is an inﬁnite minimal (P, R)chain, but no inﬁnite minimal (P, R)-chain if only terms over F(P ∪ R) may be used. Hence, there cannot be any function that transforms an inﬁnite minimal (P, R)-chain over an arbitrary signature into an inﬁnite minimal chain which only contains terms over F(P ∪ R). In other words, Lemma 8.8 does not hold if one would replace inﬁnite chains by minimal inﬁnite chains.
Example 8.13 (Restricting the signature to F(P ∪ R) is unsound for minimal chains). To present a counter-example we give a “termination proof” for a nonterminating TRS where the only unsound step is the signature restriction to the signature of the current DP problem. Here, we make use of the DP-framework [42] in which one proves termination by simplifying the initial DP problems by termination techniques until one obtains a DP problem that does not admit an inﬁnite minimal chain. For soundness it is only required that whenever (P, R) is simpliﬁed to (P , R ) then an inﬁnite minimal (P, R)-chain must imply the existence of an inﬁnite minimal (P , R )-chain.
So, let R be the following nonterminating TRS.
g(f(x, y, x , z, z, u)) → g(f(x, y, x, x, y, h(y, x )))
a→b
a→c
h(x, x) → h(x, x)
h(a, x) → h(x, x)
h(b, x) → h(x, x)
h(c, x) → h(x, x)
h(h(x1, x2), x) → h(x, x) h(f(x1, . . . , x6), x) → h(x, x)
The initial DP problem (DP(R), R) can be simpliﬁed to (P, R) where P = {G(f(x, y, x , z, z, u)) → G(f(x, y, x, x, y, h(y, x )))}. The reason is that there is a minimal inﬁnite (P, R)-chain: choose every si and ti to be the left-hand side and right-hand side of the only rule in P, respectively. Further, choose σi = σ for σ(x) = g(a), σ(y) = σ(z) = g(b), σ(x ) = g(c), and σ(u) = h(g(b), g(c)).
Note that this chain is also a minimal (P, R )-chain where R is like R but without the g(. . . ) → g(. . . )-rule. Thus, (P, R) can be simpliﬁed to (P, R ).
Using the argument ﬁlter processor [101, Theorem 4.37], it is shown that there also is an inﬁnite minimal chain when collapsing G to its ﬁrst argument. Hence, the same substitution σ can be used to obtain an inﬁnite minimal chain for the DP problem (P , R ) where P = {f(x, y, x , z, z, u) → f(x, y, x, x, y, h(y, x ))}.
Now, if it would be sound to restrict the signature of (P , R ) to F(P ∪ R ) = {a, b, c, f, h} then we can conclude termination. The reason is that over this signature there are no inﬁnite minimal (P , R )-chains anymore.
We prove this last statement by contradiction. Suppose there is an inﬁnite minimal (P , R )-chain over s, t, and δ, where δi instantiates all variables by terms over F(P ∪R ), si = f(x, y, x , z, z, u), and ti = f(x, y, x, x, y, h(y, x )), for all i. δi. Then all tiδi are terminating w.r.t. R . Hence, δ1(y) must be a variable (otherwise, h(y, x )δ1 would not be terminating due to the six h-rules of R ). Moreover, by using that δ1(y) is a variable,

8.5. Applications

73

the derivation t1δ1 →∗R s2δ2 shows that δ1(y) = δ2(y) and δ1(y) = δ2(z). Note that since R is not collapsing, whenever a term rewrites to a variable then the term must be identical to the variable. Hence, since δ2(z) is a variable and δ1(x) →∗R δ2(z) we obtain δ1(x) = δ2(z) and for a similar reason we obtain δ1(x) = δ2(x ). In total, we can conclude δ2(y) = δ2(x ). This ﬁnally yields a contradiction as there is the nonterminating subterm h(y, x )δ2 = h(δ2(y), δ2(y)).
The consequences are severe: termination proofs relying upon techniques that require minimal chains and also use signature restrictions are unsound without further restrictions.
And indeed, for the technique of root-labeling—which performs a signature restriction within the soundness proof—we were able to construct a counter-example which refutes the main theorem for root-labeling with DPs.
Example 8.14 (Root-labeling is unsound for minimal chains). We use a similar TRS as in Example 8.13 to show the problem of root-labeling with minimal chains. Let R consist of the following rules.
g(f(x, y, x , z, z, u)) → g(f(x, y, x, x, y, h(y, x )))
a→b
a→c
h(x, x) → h(x, x)
h(a, x) → h(x, x)
h(x, a) → h(x, x)
f(x1, . . . , a, . . . , x5) → f(x1, . . . , a, . . . , x5)
Here, the last rule represents 6 rules where the a can be at any position. We again can simplify the initial DP-problem (DP(R), R) to (P, R) for the same P =
{G(f(x, y, x , z, z, u)) → G(f(x, y, x, x, y, h(y, x )))} that we had in the previous example. The reason is again that there is an inﬁnite minimal chain by choosing σi = σ for σ(x) = g(a), σ(y) = σ(z) = g(b), σ(x ) = g(c), and σ(u) = h(g(b), g(c)).
Note that by using this substitution we also get an inﬁnite minimal (P, R )-chain where R = R \ {g(. . . ) → g(. . . )}. Hence, it is sound to simplify (P, R) to (P, R ).
Now, in [95, proofs of Lemmas 13 and 17] it is wrongly stated4 that for this example, w.l.o.g. one can restrict to substitutions over the signature {a, b, c, f, h}: With a similar reasoning as in Example 8.13 one can prove that there is no inﬁnite minimal (P, R )-chain using this restricted class of substitutions. We further show in detail that Lemma 17 of [95] itself is wrong, not only its proof.
The result of Lemma 17 states that if there is an inﬁnite minimal (P, R )-chain then there also is an inﬁnite minimal chain for the DP problem (P , R ) that is obtained by the ﬂat context closure. In our example we obtain P = P ∪ {G(a) → G(b), G(a) → G(c)} and R = (R \ {a → b, a → c}) ∪ R where R consists of the following rules:
h(a, x) → h(b, x)
h(a, x) → h(c, x)
h(x, a) → h(x, b)
h(x, a) → h(x, c)
4In detail, in [95] our upcoming Lemma 8.17 is used without the requirement of left-linearity.

74 Chapter 8. Signature Extensions Preserve Termination
f(x1, . . . , a, . . . , x5) → f(x1, . . . , b, . . . , x5) f(x1, . . . , a, . . . , x5) → f(x1, . . . , c, . . . , x5)
We show that for this DP problem (P , R ) there are no inﬁnite minimal chains anymore. So, if Lemma 17 of [95] would be sound, we could wrongly “prove” termination of R. Again, we assume there is an inﬁnite minimal (P , R )-chain where δi are the corresponding substitutions and where we do not even restrict the signature of any δi. Obviously, all (si, ti) are taken from P and not from one of the additional rules in P . Since every lefthand side of R also is a left-hand side of a nonterminating rule in R , we know that every terminating term w.r.t. R is also a normal form w.r.t. R . Hence, from t1δ1 →∗R s2δ2 we conclude t1δ1 = s2δ2. Thus, δ2(x ) = δ1(x) = δ2(z) = δ1(y) = δ2(y). Therefore, we obtain the nonterminating subterm h(y, x )δ2 = h(δ2(y), δ2(y)) which is a contradiction to the minimality of the chain.
To conclude, the current applications of root-labeling in termination tools which rely upon DPs with minimal chains are wrong for two reasons: ﬁrst, one cannot restrict the signature to the implicit signature of the given DP-problem, and second, root-labeling is unsound in the DP setting with minimal chains.
However, for signature restrictions in combination with minimal chains we were able to prove soundness, provided that the TRS R of a DP problem (P, R) is left-linear.
Lemma 8.15 (Signature Restrictions for Minimal Chains). left linear R =⇒ F (P, R) ⊆ F =⇒ min ichain (P, R) s t σ =⇒ min ichain (P, R) s t σ F
The proof of Lemma 8.15 is similar to the proof of Lemma 8.8. The only missing step is to prove that left-linearity ensures that cleaning does not introduce nontermination.
Lemma 8.16 (Cleaning of Left-Linear TRSs Preservers SN). (i) left linear R =⇒
F (R) ⊆ F =⇒ SNR(s) =⇒ s F →R t =⇒ ∃ u. u F = t ∧ s →R u
(ii) left linear R =⇒ F (R) ⊆ F =⇒ SNR(s) =⇒ SNR( s F )
Proof. (i) We prove this fact via induction over s. In the base case, s is a variable. Then we have the rewrite step s →R t, since cleaning does not change variables. But then, there is a variable left-hand side, implying that R is not terminating and thus contradicting SNR(s).
In the step case we have s = f (ss). Now, we proceed by a case distinction. If (f, |ss|) ∈/ F then cleaning will transform s into the variable z. Again, there would be a variable left-hand side, contradicting strong normalization of s. Thus, (f, |ss|) ∈ F . Hence, f (map · F ss) →R t. If this is a non-root step, the result follows from the induction hypothesis. Otherwise, this is a root rewrite step. Thus we obtain a rule (l, r) ∈ R and a substitution σ, such that, f (ss) F = lσ and rσ = t. Additionally, we know that this rule is left-linear and that its left-hand side is wellformed. It can be shown that this implies the existence of a substitution τ , such that, τVar(l) F = σ|Var(l) and f (ss) = lτ (we omit the rather technical proof). Here, σ|V denotes the restriction of a substitution σ to a set of variables V, i.e., all variables that are not in V, are no longer modiﬁed by the restricted substitution. Then rτ F = r F τ F = r τVar(l) F = rσ|Var(l) = rσ = t and s = f (ss) = lτ →R rτ . Here, we needed to use the property Var(r) ⊆ Var(l), which must be valid since otherwise SNR(s) does not hold.

8.6. Conclusion

75

(ii) Assume that s F is not terminating. Thus, there is an inﬁnite sequence of R-steps, starting from s F . By iteratively applying the previous result, we obtain an inﬁnite R-sequence starting at s.
We were also able to formally show that the signature restriction that is done in rootlabeling (which is exactly the upcoming Lemma 8.17 without the requirement of leftlinearity) is sound for minimal chains with the requirement of left-linearity. Hence, with the following lemma one can repair the paper proofs of [95, Lemmas 13 and 17] by demanding left-linearity. Essentially, the lemma states that one can restrict to the symbols that occur below the root in P (F> (P)), together with the symbols of R, under the additional assumption that neither left-hand sides nor right-hand sides of P are variables and the roots of P are not deﬁned in R.
Lemma 8.17 (Signature Restrictions Ignoring Roots). left linear R =⇒ F> (P) ∪ F (R) ⊆ F =⇒ ∀ s t. (s, t) ∈ P −→ s ∈/ Var ∧ t ∈/ Var ∧ ¬ root(t) ∈ D(R) =⇒ min ichain (P, R) s t σ =⇒ min ichain (P, R) s t σ F
The lemma is proven in the same way as Lemma 8.15, except that one only applies cleaning strictly below the root. By cleaning below the root one can also proof a variant of Lemma 8.17 where minimal chains are replaced by arbitrary chains, and where leftlinearity is no longer required.5
Using Lemma 8.17 and the original proofs of [95] it is shown that root-labeling is sound in combination with minimal chains if we restrict to left-linear R-components. Hence, the main example of [95, Touzet’s SRS] is still working, since it applies root-labeling on a DP problem with left-linear R.

8.6. Conclusion
We presented an alternative, and more importantly, the ﬁrst mechanized proof of the fact that termination is preserved under signature extensions. We have also shown that signature extensions are possible when using DPs, but only if one considers arbitrary chains or left-linear TRSs. For minimal chains we have given a counterexample which shows that for non-left-linear TRSs one cannot restrict to the signature of the current DP problem.
We believe these results to be interesting in their own. However, we developed these results with a certain goal in mind. In the end we want to apply our main positive results to be able to certify termination proofs which rely upon techniques where the signature is essential: string reversal and root-labeling. If one applies these techniques directly on a TRS, then both techniques can now be certiﬁed in the way they are used in current termination tools. For root-labeling in the DP setting with minimal chains, we have shown that it is unsound for arbitrary DP problems. We have further shown how to repair the existing proofs by demanding left-linearity. It remains as future work, to also formalize the remaining proof for root-labeling in the DP setting.

5However, one needs the additional requirement that left-hand sides of R are not variables, which in Lemma 8.17 follows from the minimality of the chain.

9. Modular and Certiﬁed Semantic Labeling and Unlabeling
Publication details
Christian Sternagel and Ren´e Thiemann. Modular and Certiﬁed Semantic Labeling and Unlabeling. In Proceedings of the 22nd International Conference on Rewriting Techniques and Applications, volume 10 of LIPIcs, pages 329–344. Schloss Dagstuhl – LeibnizZentrum fu¨r Informatik, 2011.
Abstract
Semantic labeling is a powerful transformation technique to prove termination of term rewrite systems. The dual technique is unlabeling. For unlabeling it is essential to drop the so called decreasing rules which sometimes have to be added when applying semantic labeling. We indicate two problems concerning unlabeling and present our solutions.
The ﬁrst problem is that currently unlabeling cannot be applied as a modular step, since the decreasing rules are determined by a semantic labeling step which may have taken place much earlier. To this end, we give an implicit deﬁnition of decreasing rules that does not depend on any knowledge about preceding labelings.
The second problem is that unlabeling is in general unsound. To solve this issue, we introduce the notion of extended termination problems. Moreover, we show how existing termination techniques can be lifted to operate on extended termination problems.
All our proofs have been formalized in Isabelle/HOL as part of the IsaFoR/CeTA project.
9.1. Introduction
In recent years, termination provers for term rewrite systems (TRSs) became more and more powerful. Nowadays, we do no longer have to prove termination by embedding all rules of a TRS into a single reduction order. Instead, most provers construct multi-step proofs by combining diﬀerent termination techniques1 resulting in tree-like termination proofs. As a result, termination provers became more complex and thus, more error-prone. It is regularly demonstrated that we cannot blindly trust termination provers. Every now and then, some prover delivers a faulty proof. Most of the time, this is only detected if there is another prover giving a contradictory answer. Furthermore, it just is too much work to check a generated proof by hand. (Besides, checking by hand is not very reliable.)
To solve this issue, recent interest is in the automatic certiﬁcation of termination proofs [12, 23, 104]. To this end, we formalized many termination techniques in our Isabelle/HOL [84] library IsaFoR [104] (in the remainder we just write Isabelle, instead of Isabelle/HOL). Using IsaFoR, we obtain CeTA, an automatic certiﬁer for termination proofs.
1Several termination techniques are based upon reduction orders, but there are also techniques which do not generate orders. Hence, the multi-step proofs are not just a lexicographic combination of orders.

78 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling
In this paper, we present our formalization of semantic labeling and unlabeling [110], two important termination techniques. Semantic labeling introduces diﬀerently labeled variants of the same symbol, allowing a distinction in orders, etc. Semantic labeling typically produces large TRSs. Hence, unlabeling is important to keep the number of symbols and rules small.
Example 9.1. Consider the small TRS Secret_05/teparla3 from the termination problem database (TPDB) which only consists of two rules, has two diﬀerent symbols, and two variables. We just describe the structure of the proof that has been generated by the termination prover AProVE [39] in 21 seconds during the 2008 termination competition.2
After applying the dependency pair transformation [3] and some standard techniques, a termination problem containing three rules and three diﬀerent symbols is obtained. Then, semantic labeling is applied. The result after simpliﬁcation, is a system with ﬁve rules and seven diﬀerent symbols. Unlabeling yields a problem with three rules and three symbols. Another labeling produces a new termination problem with 12 rules. This is ﬁnally proven to be terminating using a matrix interpretation [32] of dimension two.
Note that without the unlabeling step, the second labeling would have returned a system with 5025 rules instead of 12—for this huge termination problem no suitable matrix interpretation of dimension two is detected.
Whereas the previous example shows that unlabeling is essential to keep systems small, we also found examples where unlabeling was the key to get a successful termination proof at all, cf. Example 9.24 for details.
Unfortunately, unlabeling is not sound in general. In order to allow nested labeling and unlabeling and turn unlabeling into a sound and modular technique (not relying on context information), we have designed a new framework. All existing termination techniques are easily integrated in this framework. In fact, CeTA uses the new framework for certiﬁcation.
Note that all the proofs that are presented (or omitted) in the following, have been formalized as part of IsaFoR. Hence, we merely give sketches of our “real” proofs. Our goal is to show the general proof outlines and help to understand the full proofs. The library IsaFoR with all formalized proofs, the executable certiﬁer CeTA, and all details about our experiments are available at CeTA’s website:
http://cl-informatik.uibk.ac.at/software/ceta
The paper is structured as follows. In Section 9.2, we recapitulate some required notions of term rewriting as well as the basic deﬁnitions of semantic labeling. Afterwards, in Section 9.3, we give some challenges for modular labeling and unlabeling. Then, in Section 9.4, we extend the previous results to the dependency pair framework. We discuss challenges for the certiﬁcation in Section 9.5. Our experiments are presented in Section 9.6 before we conclude in Section 9.7.
9.2. Preliminaries
9.2.1. Term Rewriting
We assume familiarity with term rewriting [5]. Still, we recall the most important notions that are used later on. A (ﬁrst-order) term t over a set of variables V and a set of function
2See http://termcomp.uibk.ac.at/termcomp/competition/resultDetail.seam?resultId=35708

9.2. Preliminaries

79

symbols F is either a variable x ∈ V or an n-ary function symbol f ∈ F applied to n argument terms f (tn). For brevity we write tn to denote a sequence of n elements t1, . . . , tn and (h(tn)) (note the additional pair of parentheses) for (h(t1), . . . , h(tn)), i.e., mapping a function h over the elements of a sequence tn. A context C is a term containing exactly one hole (P). Replacing P in a context C by a term t is denoted by C[t]. A (rewrite) rule is a pair of terms → r and a TRS R is a set of such rules. The rewrite relation (induced by R) →R is the closure under substitutions and contexts of R, i.e., s →R t iﬀ there is a context C, a rule → r ∈ R, and a substitution σ such that s = C[ σ] and t = C[rσ].
We say that an element t is terminating / strongly normalizing (w.r.t. some binary relation S), and write SNS(t), if it cannot start an inﬁnite sequence t = t1 S t2 S t3 S · · · . The whole relation is terminating, written SN(S), if all elements are terminating w.r.t. it. For a TRS R and a term t, we write SN(R) and SNR(t) instead of SN(→R) and SN→R(t). We write S+ (S∗) for the (reﬂexive and) transitive closure of S.
Deﬁnition 9.2 (Termination Technique). A termination technique is a mapping TT from TRSs to TRSs. It is sound if termination of TT(R) implies termination of R.
Using sound termination techniques one tries to modify a given TRS R until the empty TRS is reached. If this succeeds, one obtains a proof tree showing termination of R.

9.2.2. Semantic Labeling
An algebra A over F is a pair (A, {fA}f∈F ) consisting of a non-empty carrier A and an interpretation function fA : An → A for every n-ary function symbol f ∈ F . Given an assignment α : V → A, we write [α]A(t) for the interpretation of the term t. An algebra A is a model of a rewrite system R, if [α]A( ) = [α]A(r) for all rules → r ∈ R and all assignments α. If the carrier A is equipped with a well-founded order >A such that [α]A( ) A [α]A(r) for all → r ∈ R and all assignments α, then A is a quasi-model of R.
For each function symbol f there also is a corresponding non-empty set Lf of labels for f and a labeling function f : An → Lf . The labeled signature Flab consists of n-ary function symbols fa for every n-ary function symbol f ∈ F and label a ∈ Lf . The labeling function f determines the label of the root symbol f of a term f (tn) based on the values of the arguments tn. For every assignment α : V → A the mapping labα : T (F , V) → T (Flab, V) is inductively deﬁned by

labα(t) =

f f ([α]A(tn))(labα(tn)) t

if t = f (tn), otherwise.

The labeled TRS lab(R) over the signature Flab consists of the rules labα( ) → labα(r) for all → r ∈ R and α : V → A.
For quasi-models, every set of labels Lf needs to be equipped with a well-founded order >Lf , giving rise to the set Dec of decreasing rules:
Dec = {fa(xn) → fb(xn) | a, b ∈ Lf , a >Lf b, n-ary f ∈ F }
Furthermore, every interpretation function fA and every labeling function f has to be weakly monotone, i.e., if a A a then fA(a1, . . . , a, . . . , an) A fA(a1, . . . , a , . . . , an) and f (a1, . . . , a, . . . , an) Lf f (a1, . . . , a , . . . , an).

80 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling
Unlabeling a symbol is deﬁned via the following function, removing one layer of labels. Then, the function is extended homomorphically to terms, rules, and TRSs.
unlab(f ) = g if f = ga, f if f is not labeled.
In [110], Zantema showed that labeled TRSs can simulate their unlabeled counterparts (corresponding to i and ii in the following lemma; iii and iv are obvious).
Lemma 9.3. Let R be a TRS,A an algebra, and α an arbitrary assignment.
(i) If A is a model of R then t →R u implies labα(t) →lab(R) labα(u).
(ii) If A is a quasi-model of R then t →R u implies labα(t) →+lab(R)∪Dec labα(u).
(iii) t →lab(R) u implies unlab(t) →R unlab(u).
(iv) t →Dec u implies unlab(t) = unlab(u).
From Lemma 9.3 we obtain that R is terminating if and only if lab(R) (∪ Dec) is terminating when A is a (quasi-)model of R. Completeness is achieved by unlabeling all terms in a possible inﬁnite rewrite sequence of the labeled TRS. Soundness is proved by transforming a presupposed inﬁnite rewrite sequence in R into an inﬁnite rewrite sequence in lab(R) (∪ Dec). This is done by applying the labeling function labα(·) (for an arbitrary assignment α) to all terms in the inﬁnite rewrite sequence of R. Hence, semantic labeling is sound and complete for termination (using models and quasi-models, respectively).
9.3. Modular Semantic Labeling and Unlabeling
One problem with semantic labeling is that the labeled system is usually large. Hence, termination provers such as AProVE [39], Jambox [31], Torpa [111], and TPA [62] perform labeling, then try to simplify the resulting TRS by sound termination techniques, and afterwards unlabel the TRS again, to continue on a small system. This poses two challenges:
(i) If labeling was performed using a quasi-model, then the decreasing rules are added. However, unlabeling a decreasing rule fa(xn) → fb(xn) leads to the nonterminating rule f (xn) → f (xn). Hence, one has to remove the decreasing rules before unlabeling.
(ii) Between labeling and unlabeling, arbitrary (sound) termination techniques may be applied. However, for unlabeling we want to remove the decreasing rules that are determined by the corresponding labeling step. Hence, unlabeling is not a modular technique that only takes a TRS as input. Instead, it relies on context information, namely the decreasing rules that have been used in the corresponding labeling step (which may occur several steps upwards in the termination proof).
Solving the ﬁrst challenge is technically easy: just remove the decreasing rules before unlabeling. The only question is, whether it is always sound to remove the decreasing rules.
To handle the second challenge, we propose an implicit deﬁnition of decreasing rules.

9.3. Modular Semantic Labeling and Unlabeling

81

Deﬁnition 9.4 (Decreasing rules of a TRS). We deﬁne the decreasing rules of a TRS R as D(R) = { → r ∈ R | unlab( ) = unlab(r) ∧ = r}. We further deﬁne the unlabeled version of a TRS as U(R) = unlab(R \ D(R)).
The condition = r ensures that a labeled variant of an original rule is never decreasing. For example, if f (xn) → f (xn) is a rule (and hence the original TRS is not terminating), then each labeled variant has the form fa(xn) → fa(xn) for some a ∈ Lf . If we would consider such a rule as decreasing, we could transform a nonterminating TRS into a terminating one, using labeling and unlabeling.
Lemma 9.5. Let Lf and >Lf be given for each symbol f to determine Dec. Then D(Dec) = Dec, D(lab(R)) = ∅, D(lab(R) ∪ Dec) = Dec, and U(lab(R) ∪ Dec) = R.
Now it is easy to deﬁne a modular version of unlabeling which does not require external knowledge about what the decreasing rules are.
Deﬁnition 9.6 (Unlabeling as modular termination technique). The unlabeling termination technique replaces a TRS R by U(R).
Hence, we solved the second challenge and made unlabeling into an independent technique which does not need any knowledge on the previous application of semantic labeling that introduced the decreasing rules. Thus, termination proofs can now use the following structure where no global information has to be passed around:
(i) Switch from R to lab(R) ( ∪ Dec).
(ii) Modify lab(R) ( ∪ Dec) by sound termination techniques resulting in R .
(iii) Unlabel R resulting in U(R ).
Although this approach is used in termination provers, it is unsound in general as not every sound termination technique may be used between labeling and unlabeling. This is illustrated by the following example.
Example 9.7. We start with the nonterminating TRS R = {f(a) → f(b), b → a}. Then, we apply semantic labeling using the algebra A with A = {0, 1}, interpretations fA(x) = 0, aA = 0, bA = 1, Lf = A, f(x) = x, and the standard order on the naturals. Note that A is a quasi-model of R. The resulting labeled TRS is lab(R) ∪ Dec = {f0(a) → f1(b), b → a, f1(x) → f0(x)}. It is sound to replace lab(R) ∪ Dec by the (nonterminating) TRS R = {f1(x) → f0(x), f0(x) → f1(x)}. However, unlabeling R yields U (R ) = ∅ as both rules in R are decreasing according to Deﬁnition 9.4. Hence, some of the performed deductions were not sound. Since semantic labeling and the switch from lab(R) ∪ Dec to R are sound, we obtain that unlabeling via U is unsound.
The problematic step when unlabeling, i.e., when switching from R to U(R) = unlab(R\ D(R)), is the removal of the decreasing rules. If the decreasing rules are the only source of nontermination, then this removal is unsound. However, the decreasing rules Dec that are obtained from semantic labeling are always terminating. Thus, after labeling we have to prove termination of the labeled system including the decreasing rules, but we may assume that the decreasing rules are terminating. If we know that the decreasing rules are terminating, then unlabeling by U is sound. We obtain the following structure of termination proofs:

82 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling
(i) Initially we have to prove SN(R).
(ii) After labeling, we have to prove SN(D(R )) =⇒ SN(R ) for R = lab(R) ∪ Dec.
(iii) Then, we modify R to R with SN(D(R )) =⇒ SN(R ) implies SN(D(R )) =⇒ SN(R ).
(iv) Finally, we unlabel R resulting in U(R ) and have to prove SN(U(R )).
This approach works ﬁne for termination proofs where semantic labeling is not nested. However, we are aware of termination proofs where labeling is applied in a nested way.
Example 9.8. Consider the TRS Gebhardt_06/16 from the TPDB. During the 2008 termination competition, Jambox proved termination of this TRS, applying the following steps: labeling - labeling - labeling - polynomial order - unlabeling - four applications of polynomial orders - unlabeling - unlabeling.3
To support this kind of proof we deﬁne the following variant of strong normalization.
Deﬁnition 9.9. An extended termination problem is a pair (R, n) consisting of a TRS R and a number n ∈ N. An extended problem (R, n) is strongly normalizing (SN(R, n)) iﬀ
(∀m < n. SN(D(U m(R)))) =⇒ SN(R).
An extended termination technique is a mapping xTT from extended termination problems to extended termination problems. It is sound iﬀ SN(xTT(R, n)) implies SN(R, n).
The number n in an extended termination problem (R, n) describes how often we can assume that the decreasing rules are terminating, and hence, it tells us how often we can delete the decreasing rules during unlabeling. The following lemma provides the link between both variants of strong normalization.
Lemma 9.10. (i) SN(R) iﬀ SN(R, 0).
(ii) If SN(R) then SN(R, n).
Lemma 9.11 (Extended Unlabeling). Extended unlabeling is sound where
U(R, n) = (U(R), n − 1) if n > 0, (unlab(R), 0) otherwise.
Proof. We only consider the interesting case where n > 0. So, we have to show SN(R, n) under the ﬁrst assumption SN(U(R), n − 1). To prove SN(R, n), we have to prove SN(R) under the second assumption ∀m < n. SN(D(Um(R))). Since n > 0 we can choose m = 0 and obtain SN(D(R)).
To show SN(R) we assume that there is an inﬁnite →R-derivation t1 →R t2 →R · · · and obtain a contradiction. The inﬁnite derivation is also an inﬁnite →R\D(R) ∪ →D(R)derivation. Since D(R) is terminating, we know that there are inﬁnitely many i with ti→R\D(R)ti+1. Hence unlab(ti) →U(R) unlab(ti+1) for all these i as U (R) = unlab(R \ D(R)). Moreover, for all i where ti →D(R) ti+1, we know that unlab(ti) →unlab(D(R)) unlab(ti+1) and hence, unlab(ti) = unlab(ti+1) since every rule in unlab(D(R)) has the
3See http://termcomp.uibk.ac.at/termcomp/competition/resultDetail.seam?resultId=27220

9.3. Modular Semantic Labeling and Unlabeling

83

same left- and right-hand side. Thus, we have constructed an inﬁnite derivation for U(R) proving that SN(U(R)) does not hold. Together with the assumption SN(U(R), n − 1), we obtain that ∀m < n − 1. SN(D(Um(U(R)))) does not hold (by Deﬁnition 9.9). Hence, there is some m < n − 1 such that SN(D(U m+1(R))) does not hold. Now, using the second assumption and m + 1 < n we obtain SN(D(U m+1(R))), providing the required contradiction.
Lemma 9.12 (Extended Semantic Labeling). Semantic labeling is sound as extended termination technique: Whenever we can switch from R to lab(R) ( ∪ Dec) via semantic labeling, then it is sound to switch from (R, n) to (lab(R) ( ∪ Dec), n + 1).
Proof. Note that models are just a special case of quasi-models as already observed in [110]. Hence, we only consider quasi-models in the proof. So, assuming SN(lab(R) ∪ Dec, n+ 1) we have to prove SN(R, n). To show the latter, we may assume ∀m < n. SN(D(Um(R))) and have to prove SN(R). We do so by assuming that there is an inﬁnite R-derivation t1 →R t2 →R · · · and deriving a contradiction. As we have a quasi-model we know that labα(t1) →+lab(R)∪Dec labα(t2) →+lab(R)∪Dec · · · is an inﬁnite lab(R) ∪ Dec-derivation, showing that SN(lab(R) ∪ Dec) does not hold. By the conditions of semantic labeling, we further know SN(Dec). Using SN(lab(R) ∪ Dec, n + 1) we conclude that ∀m < n + 1. SN(D(Um(lab(R) ∪ Dec))) does not hold. Hence there is some m < n + 1 such that SN(D(Um(lab(R) ∪ Dec))) does not hold. If m = 0 then by Lemma 9.5 we know that D(Um(lab(R) ∪ Dec)) = D(lab(R) ∪ Dec) = Dec, and thus SN(Dec) does not hold, a contradiction. Otherwise, m = m + 1 for some m where m < n. Together with ∀m < n. SN(D(U m(R))), we obtain SN(D(U m (R))). On the other hand, we know that SN(D(U m +1(lab(R) ∪ Dec))) does not hold. This again leads to a contradiction since D(U m +1(lab(R) ∪ Dec)) = D(U m (U (lab(R) ∪ Dec))) = D(U m (R)) by Lemma 9.5.
The previous two lemmas show that labeling and unlabeling can be performed as independent techniques on extended termination problems.
The question remains how to integrate other existing termination techniques, i.e., which techniques may be applied between labeling and unlabeling. Here, we consider two variants.
Deﬁnition 9.13 (Lift). Let TT be some termination technique. Then lift(TT) and lift0(TT) are extended termination techniques where lift(TT)(R, n) = (TT(R), n) and lift0(TT)(R, n) = (TT(R), 0).
In principle lift(TT) is preferable, since it does not change n, allowing to remove the decreasing rules when unlabeling (which is not possible using lift0(TT)). However, in general the fact that TT is sound does not imply that lift(TT) is sound. This can easily be seen by reusing Example 9.7 where the extended termination problem (R, 0) is transformed to (lab(R) ∪ Dec, 1) by semantic labeling, then to (R , 1) using lift(TT) for the unnamed sound termination technique TT in Example 9.7, and then to (∅, 0) by unlabeling. Since this establishes a complete termination proof for the nonterminating TRS R, and since labeling and unlabeling are sound, we know that lift(TT) is unsound.
Since we cannot always use lift(TT), we give three diﬀerent approaches to use termination techniques as extended termination techniques (in order of preference):
(i) Identify a (hopefully large) class of termination techniques TT for which soundness of TT implies soundness of lift(TT).

84 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling

(ii) Perform a direct proof that lift(TT) is sound as extended termination technique.

(iii) Use lift0(TT) for any sound termination technique TT. We ﬁrst prove soundness of approach iii.

Lemma 9.14. If TT is sound then lift0(TT) is sound.
Proof. We have to prove that SN(TT(R), 0) implies SN(R, n). So, assume SN(TT(R), 0). Hence, SN(TT(R)) using Lemma 9.10(i). As TT is sound, we conclude SN(R) and this implies SN(R, n) by Lemma 9.10(ii).

We start to prove soundness of lift(TT) for some sound termination technique TT
in order to detect where the problem is. To prove soundness, we have to show that SN(TT(R), n) implies SN(R, n). Thus, assume SN(TT(R), n). To prove SN(R, n) we may assume that ∀m < n. SN(D(Um(R))) and have to prove SN(R). Since TT is sound, it suﬃces to prove SN(TT(R)). To this end, it suﬃces to show ∀m < n. SN(D(Um(TT(R)))) by using SN(TT(R), n). Hence, the only missing step is to conclude

SN(D(U m(R))) =⇒ SN(D(U m(TT(R)))).

()

Lemma 9.15. If TT is sound and if ( ) is satisﬁed for all m, then lift(TT) is sound.
A suﬃcient condition to ensure ( ) is to demand that TT(R) ⊆ R as unlab, D, and U are monotone w.r.t. set inclusion. Hence, all techniques that remove rules like rule removal via reduction pairs, or (RFC) matchbounds [36, 66] can safely be used between labeling and unlabeling. However, this excludes techniques like the ﬂat context closure which is required for root-labeling.
Deﬁnition 9.16 (Root-Labeling). Let R be a TRS over the signature F. Let AF be an algebra with carrier F. Moreover, for every n-ary f ∈ F, we ﬁx the interpretation function fAF (xn) = f , the set of labels Lf = F n, and the labeling function f (xn) = (xn).
Note that root-labeling is just a speciﬁc instantiation of general semantic labeling with models. Hence, it is sound whenever AF is a model of R. However, in general AF does not constitute a model of R. Hence, a transformation technique was introduced that modiﬁes R in a way that AF always is a model of the result: the closure under ﬂat contexts.
Deﬁnition 9.17 (Flat Context Closure). For an n-ary symbol f , the ﬂat context for the i-th argument is F Ci(f ) = f (x1, . . . , xi−1, P, xi+1, . . . , xn), where all the xj are fresh variables. The set of ﬂat contexts over F is deﬁned by FC(F) = {FCi(f ) | n-ary f ∈ F, 1 i n}. The closure under ﬂat contexts of a TRS R w.r.t. the signature F is given by
F CF (R) = {C[ ] → C[r] | C ∈ F C(F ), → r ∈ Ra} ∪ (R \ Ra)
where Ra denotes those rules of R, for which the root of the left-hand side and the root of the right-hand side diﬀer.

Since Jambox applies root-labeling recursively (the labeling in Example 9.8 is root-
labeling), we deﬁnitely would like to aim at a larger class of termination techniques than those which satisfy TT(R) ⊆ R. A natural extension would be to use the weaker condition →TT(R) ⊆ →R. Then, also root-labeling together with the closure under ﬂat contexts would be supported. Unfortunately, →TT(R) ⊆ →R does not imply →D(Um(TT(R))) ⊆ →D(Um(R)) and thus, does not imply ( ). Moreover, in the following example we show that even if TT is sound and →TT(R) ⊆ →R then soundness of lift(TT) cannot be guaranteed.

9.3. Modular Semantic Labeling and Unlabeling

85

Example 9.18. Consider the TRS R = {f1(x) → f0(a), f0(x) → f1(x)}. Let TT be the termination technique that replaces R by R = {f1(a) → f0(a), f0(x) → f1(x)}. Then, TT is sound as R is not terminating. Moreover, →R ⊆ →R. Nevertheless, lift(TT) is unsound, since it would replace (R, 1) by (R , 1). That this replacement is unsound can be seen as follows: SN(R, 1) does not hold since R is not terminating but the decreasing rules of R (i.e., D(R) = {f0(x) → f1(x)}) are terminating. However, SN(R , 1) is satisﬁed as D(R ) = R and hence termination of D(R ) implies termination of R .
We have seen that requiring TT(R) ⊆ R is too restrictive to allow root-labeling. But only requiring →TT(R) ⊆ →R is unsound. However, there is another condition which is weaker than set inclusion, implies soundness, and allows the application of ﬂat context closures.
Deﬁnition 9.19. The context subset relation ⊆c is deﬁned as
R ⊆c S iﬀ ∀ → r ∈ R. ∃C, → r ∈ S. = C[ ] ∧ r = C[r ].
Lemma 9.20. (i) R ⊆ S implies R ⊆c S
(ii) R ⊆c S implies →R ⊆ →S
(iii) R ⊆c S implies D(R) ⊆c D(S) and U (R) ⊆c U (S)
(iv) If TT is sound and ∀R. TT(R) ⊆c R then lift(TT) is sound
Proof. (i) To show R ⊆c S, let → r ∈ R. Using R ⊆ S we know that → r ∈ S. Hence, ∃C, → r ∈ S. = C[ ]∧r = C[r ] by choosing C = P and → r = → r.
(ii) Assume t = D[ σ] →R D[rσ] = s using some rule → r ∈ R. As R ⊆c S, we obtain C and → r ∈ S such that = C[ ] and r = C[r ]. Hence, t = D[ σ] = D[C[ ]σ] = D[Cσ[ σ]] →S D[Cσ[r σ]] = D[C[r ]σ] = D[rσ] = s.
(iii) We ﬁrst show D(R) ⊆c D(S). So, let → r ∈ D(R). Hence, → r ∈ R, unlab( ) = unlab(r) and = r. Using R ⊆c S we obtain C and → r ∈ S such that = C[ ] and r = C[r ]. Thus, unlab(C)[unlab( )] = unlab(C[ ]) = unlab( ) = unlab(r) = unlab(C[r ]) = unlab(C)[unlab(r )] shows that unlab( ) = unlab(r ). Similarly, C[ ] = = r = C[r ] implies = r . So, → r ∈ D(S) and thus, ∃C, → r ∈ D(S). = C[ ] ∧ r = C[r ].
Now let us show U(R) = unlab(R \ D(R)) ⊆c unlab(S \ D(S)) = U(S). This property is the crucial part, since potentially we remove less rules from R than from S. Assume unlab( ) → unlab(r) ∈ U(R), i.e., → r ∈ R and unlab( ) = unlab(r) ∨ = r. As R ⊆c S we obtain C and → r ∈ S such that = C[ ] and r = C[r ]. Hence, unlab( ) = unlab(C[ ]) = unlab(C)[unlab( )] and unlab(r) = unlab(C[r ]) = unlab(C)[unlab(r )]. Thus, we can simplify unlab( ) = unlab(r)∨ = r to unlab(C)[unlab( )] = unlab(C)[unlab(r )]∨C[ ] = C[r ] and further to unlab( ) = unlab(r ) ∨ = r . Using → r ∈ S this shows that → r ∈ S \ D(S) and thus, unlab( ) → unlab(r ) ∈ U(S). By choosing the context unlab(C) and the rule unlab( ) → unlab(r ) we have ﬁnally shown that ∃C, → r ∈ U(S). unlab( ) = C[ ] ∧ unlab(r) = C[r ].

86 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling

(iv) By Lemma 9.15 we only have to prove ( ). Using TT(R) ⊆c R and iii one can show that U m(TT(R)) ⊆c U m(R) by induction on m. Using iii again, we conclude D(U m(TT(R))) ⊆c D(U m(R)) and thus, →D(Um(TT(R))) ⊆ →D(Um(R)) by ii. Then ( ) immediately follows.
Corollary 9.21. Let R be a TRS over the signature F. Then lift(FCF ) is sound.
Proof. It was shown in [95] that F CF is sound for TRSs. Furthermore, F CF (R) ⊆c R by deﬁnition of FC(F) and thus, by Lemma 9.20(iv), lift(FCF ) is sound, too.
Note that several termination techniques TT satisfy TT(R) ⊆c R and hence, can be used between labeling and unlabeling. However, there are still some techniques which do not satisfy this requirement. Examples would be string reversal and uncurrying [53].
Of course, it is possible to use lift0(TT), however, for string reversal also a direct soundness proof can be performed to show that lifting string reversal is sound.
Theorem 9.22. Let TT be the technique of string reversal where TT(R) = rev(R), if R is a string rewrite system, and TT(R) = R, otherwise. Then lift(TT) is sound.
Proof. By Lemma 9.15 we just have to prove ( ), i.e., we have to show for all m that SN(D(U m(R))) implies SN(D(U m(rev(R)))). To this end, we have proven that reversing can be commuted with both D and U: rev(D(R)) = D(rev(R)) and rev(U(R)) = U (rev(R). Hence, rev(D(U m(R))) = D(U m(rev(R))). This completes the proof: since string reversal is complete, we know that termination of D(Um(R)) implies termination of rev(D(U m(R))) and therefore, also of D(U m(rev(R))).
To summarize, we can now certify termination proofs where labeling and unlabeling are modular techniques (and hence, can be applied recursively), and where all supported techniques of CeTA (except uncurrying) can be used between labeling and unlabeling.
An easy alternative to our extended termination techniques would be the use of relative rewriting. The obvious idea is to add the decreasing rules as relative rules when performing semantic labeling. In this way, unlabeling would directly be modular and sound, since one can always remove relative rules where both sides of the rule are identical. This alternative is used in the independent and unpublished formalization of semantic labeling in the CoLoR library. The main problem with this alternative is that some techniques like RFC matchbounds can be used in our framework, but not in combination with relative rewriting in general (during the termination competition in 2010 a tool has been disqualiﬁed for giving a wrong answer for a relative termination problem; the reason was the use of RFC matchbounds). For a further discussion on matchbounds and relative rewriting we refer to [54].

9.4. Dependency Pair Framework

The DP framework [42] is a way to modularize termination proofs. Instead of TRSs one investigates so called DP problems, consisting of two TRSs. The initial DP problem for a TRS R is (DP(R), R) where DP(R) denotes the dependency pairs of R [3]. A (P, R)-chain is a possibly inﬁnite derivation of the form:

s1σ1 →P t1σ1 →∗R s2σ2 →P t2σ2 →∗R s3σ3 →P · · ·

()

9.4. Dependency Pair Framework

87

where si → ti ∈ P for all i > 0. If additionally every tiσi is terminating w.r.t. R, then the chain is minimal. A DP problem (P, R) is called ﬁnite [42], if there is no minimal inﬁnite (P, R)-chain. Proving ﬁniteness of a DP problem is done by simplifying (P, R) using so called processors recursively. A processor transforms a DP problem into a new DP problem. The aim is to reach a DP problem where the P-component is empty (such DP problems are trivially ﬁnite). To conclude ﬁniteness of the initial DP problem, the applied processors need to be sound. A processor Proc is sound whenever for all DP problems (P, R) we have that ﬁniteness of Proc(P, R) implies ﬁniteness of (P, R).
Semantic labeling can easily be lifted to DP problems. Soundness of the following processor is an immediate consequence of [110].
Theorem 9.23. Let (P, R) be a DP problem and A be an algebra. If A is a quasi-model of R, then it is sound to return the DP problem (lab(P), lab(R) ∪ Dec).
The following example shows that unlabeling is not only necessary for eﬃciency, but that unlabeling is required to apply other techniques.
Example 9.24. We consider the TRS Secret_07/4 from the TPDB.

1: g(c, g(c, x)) → g(e, g(d, x)) 4: g(x, g(y, g(x, y))) → g(a, g(x, g(y, b)))

2: g(d, g(d, x)) → g(c, g(e, x)) 5:

f(g(x, y)) → g(y, g(f(f(x)), a))

3: g(e, g(e, x)) → g(d, g(c, x))

In the 2008 termination competition AProVE found a termination proof of the following
structure (we present a simpliﬁed version, missing some unnecessary steps that have been applied in the original proof).4 First, the initial DP problem is transformed into (P, {1–4}) where P consists of the pairs G(c, g(c, x)) → G(e, g(d, x)), G(d, g(d, x)) → G(c, g(e, x)), and G(e, g(e, x)) → G(d, g(c, x)). Then, labeling and further processing yields the DP problem (P , R ) where P contains the pairs

G00(c, g00(c, x)) → G00(e, g00(d, x)) G00(d, g00(d, x)) → G00(c, g00(e, x))
and R is the following TRS.

G00(e, g00(e, x)) → G00(d, g00(c, x))

g00(c, g00(c, x)) → g00(e, g00(d, x)) g00(d, g00(d, x)) → g00(c, g00(e, x)) g00(e, g00(e, x)) → g00(d, g00(c, x))

g00(c, g01(c, x)) → g00(e, g01(d, x)) g00(d, g01(d, x)) → g00(c, g01(e, x)) g00(e, g01(e, x)) → g00(d, g01(c, x))

Hence, all labeled versions of Rule 4 have been deleted, and unlabeling yields the DP problem (P, {1–3}). This DP problem is applicative. Hence, we may apply the Atransformation [41] to obtain the DP problem having the pairs

C(c(x)) → E(d(x))

D(d(x)) → C(e(x))

E(e(x)) → D(c(x))

and the rules

c(c(x)) → e(d(x))

d(d(x)) → c(e(x))

e(e(x)) → d(c(x))

This DP problem is solved using standard techniques. Note that for the A-transformation it was essential that unlabeling was performed, as the DP problem (P , R ) is not applicative.
4See http://termcomp.uibk.ac.at/termcomp/competition/resultDetail.seam?resultId=35909

88 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling
Unfortunately, unlabeling as processor is in general unsound. In contrast to unlabeling on TRSs, here a problem already arises when using the model-version of semantic labeling without decreasing rules. The main reason is that unlabeling might introduce nontermination. Hence, minimality of an unlabeled inﬁnite chain cannot be guaranteed.5
Example 9.25. Consider the DP problem (P, ∅) where P = {F(x) → F(g(a))}. This DP problem is obviously not ﬁnite. Applying semantic labeling is trivially possible since there are no rules which have to satisfy the (quasi-)model condition. We choose A = {0, 1}, and for each f we deﬁne fA(. . .) = 0 and f (xn) = (xn). We obtain the labeled pairs lab(P) = {F0(x) → F0(g0(a)), F1(x) → F0(g0(a))} and by Theorem 9.23 we know that the DP problem (lab(P), ∅) is again not ﬁnite. We can further modify the DP problem by replacing it with (lab(P), R) where R = {g1(x) → g1(x)}. Note that this modiﬁcation is sound since (lab(P), R) still allows a minimal inﬁnite chain and is therefore not ﬁnite.
However, applying unlabeling we obtain the DP problem (P, unlab(R)) which is ﬁnite as now the right-hand side F(g(a)) of the only pair in P is not terminating w.r.t. U(R) = {g(x) → g(x)}. Hence, unlabeling is unsound in general. The main problem is again that the notion of soundness is too weak. It allows the application of processors between labeling and unlabeling which may replace (lab(P), ∅) by (lab(P), R).
To solve this problem, we again add a counter n which tells us how often we may unlabel.
Deﬁnition 9.26. An extended DP problem is a triple (P, R, n) where (P, R) is a DP problem and n ∈ N. An extended DP problem (P, R, n) is ﬁnite iﬀ there is no inﬁnite chain
s1σ1 →P t1σ1 →∗R s2σ2 →P t2σ2 →∗R s3σ3 →P t3σ3 →∗R · · · such that for all i: ∀m n. SNUm(R)(unlabm(tiσi)).
Hence, the only diﬀerence between ﬁniteness of DP problems and extended DP problems is the minimality condition (SNR(tiσi) versus ∀m n. SNUm(R)(unlabm(tiσi))). We therefore obtain a similar lemma to Lemma 9.10, but now for DP problems.
Lemma 9.27. (i) (P, R) is ﬁnite iﬀ (P, R, 0) is ﬁnite.
(ii) If (P, R) is ﬁnite then (P, R, n) is ﬁnite.
As for termination techniques we can lift every processor to an extended processor.
Deﬁnition 9.28 (Lift). Let Proc be a processor with Proc(P, R) = (P , R ). Then lift(Proc) and lift0(Proc) are extended processors where lift(Proc)(P, R, n) = (P , R , n) and lift0(Proc)(P, R, n) = (P , R , 0).
We obtain similar results for lift0 as for termination techniques: whenever Proc is sound then lift0(Proc) is sound. However, additionally demanding that R ⊆c R or even P ⊆ P ∧ R = R where Proc(P, R) = (P , R ) does not suﬃce to ensure soundness of lift(Proc). This is demonstrated in the upcoming example.
5There is no problem in the formalization of semantic labeling in CoLoR at this point, as it does not feature minimal chains.

9.4. Dependency Pair Framework

89

Example 9.29. Let P = {F0(x) → F0(b)}, P = {F0(x) → F0(g0(b))}, and R = {g1(x) → g0(h1(x))}. Then (P, R, 1) is not ﬁnite as obviously there is an inﬁnite (P, R)-chain where all terms in the chain are F0(b) and moreover, F0(b) is terminating w.r.t. R and unlab(F0(b)) = F(b) is terminating w.r.t. U(R) = {g(x) → g(h(x))}. Hence, also (P ∪ P , R, 1) is not ﬁnite by constructing the same chain.
Note that the processor Proc which replaces (P ∪ P , R) by (P , R) is sound, since (P , R) is not ﬁnite: again, there is an inﬁnite (P , R)-chain, and every chain is also minimal since R is terminating. However, lift(Proc) is unsound as (P , R, 1) is ﬁnite: otherwise, there would be an inﬁnite chain where F0(g0(b)) is terminating w.r.t. R and unlab(F0(g0(b))) = F(g(b)) is terminating w.r.t. U (R). But it is easy to see that F(g(b)) is not terminating w.r.t. U(R).
Since requiring just R ⊆c R (or even P ⊆ P ∧ R = R) does not suﬃce to ensure soundness of lift(Proc) we demand a slightly stronger property than soundness.
Deﬁnition 9.30. A processor Proc is chain-identifying iﬀ whenever Proc(P, R) = (P , R ) and there is some minimal inﬁnite (P, R)-chain
s1σ1 →P t1σ1 →∗R s2σ2 →P t2σ2 →∗R s3σ3 →P t3σ3 →∗R · · ·
then R ⊆c R and there is some k such that
skσk →P tkσk →∗R sk+1σk+1 →P tk+1σk+1 →∗R sk+2σk+2 →P tk+2σk+2 →∗R · · ·
is an inﬁnite (P , R )-chain.
Chain-identifying processors ensure that every minimal inﬁnite chain of (P, R) has an inﬁnite tail where R∗-steps can be replaced by R ∗-steps and all pairs are from P . Note that every chain-identifying processor is sound. Moreover, several processors are indeed chain-identifying. Some examples are the reduction pair processor, the dependency graph processor, and all standard processors which just remove pairs and rules. The following lemma shows that chain-identifying processors can be used as extended processors via lift.
Lemma 9.31. (i) If Proc is sound, then lift0(Proc) is sound.
(ii) If Proc is chain-identifying then lift(Proc) is sound.
Proof. Let P, R, P , and R be given such that Proc(P, R) = (P , R ).
(i) We assume that (P , R , 0) is ﬁnite and have to show that (P, R, n) is ﬁnite. By Lemma 9.27(i) and the assumption we know that (P , R ) is ﬁnite. Thus, also (P, R) is ﬁnite using the soundness of Proc. By Lemma 9.27(ii) we conclude ﬁniteness of (P, R, n).
(ii) Here, we may assume that (P , R , n) is ﬁnite and have to show that (P, R, n) is ﬁnite. We show ﬁniteness of (P, R, n) via contraposition. So, assume (P, R, n) is not ﬁnite. This shows that there is an inﬁnite (P, R)-chain
s1σ1 →P t1σ1 →∗R s2σ2 →P t2σ2 →∗R s3σ3 →P t3σ3 →∗R · · ·
such that for all i we have ∀m n. SNUm(R)(unlabm(tiσi)). By choosing m = 0 we also have SNR(tiσi) for all i. Hence, the chain is also a minimal inﬁnite (P, R)chain. Since Proc is chain-identifying we know that R ⊆c R and there is some k such that
skσk →P tkσk →∗R sk+1σk+1 →P tk+1σk+1 →∗R sk+2σk+2 →P tk+2σk+2 →∗R · · ·

90 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling
is an inﬁnite (P , R )-chain. We continue to prove that for every i and every m n we have SNUm(R )(unlabm(tiσi)). This leads to the desired contradiction, since then we have shown that (P , R , n) is not ﬁnite.
To prove SNUm(R )(unlabm(tiσi)) we ﬁrst use minimality of the (P, R)-chain to conclude SNUm(R)(unlabm(tiσi)). Then the result immediately follows since the rewrite relation of U m(R ) is a subset of the rewrite relation of U m(R) by Lemma 9.20, ii and iii.
Using these results allowed us to develop the ﬁrst certiﬁed proof of the TRS in Example 9.24. We only had to change the given proof such that uncurrying [53] is used instead of the A-transformation, since we have only formalized the former technique. The detailed proof is provided in the IsaFoR/CeTA repository.6
However, unlike for TRSs, root-labeling is not directly supported as root-labeling on DP problems [95, 97] is not a chain-identifying processor. Here again, root-labeling itself is not the problem, but making sure that the ﬁxed algebra is a model of R, which is again done by closing under ﬂat contexts. In the DP framework we need the auxiliary function block , given by the equations block (f (tn)) = f ( (tn)) and block (x) = x.
Deﬁnition 9.32 (Flat Context Closure). Let (P, R) be a DP problem such that R is left-linear and F is a superset of the signature of R combined with the non-root symbols of P. Furthermore, let be a function symbol not in F. Then the closure under ﬂat contexts of (P, R) is given by F CF (P, R) = (block (P), F C{ }∪F (R)).
As the pairs of a DP problem are modiﬁed, we do not get soundness of lift(FCF ) via Lemma 9.31. Nevertheless, by using the deﬁnition of ﬁniteness of extended DP problems and providing a manual proof one can show that lift(FCF ) is indeed sound.
9.5. Problems in Certiﬁcation
We present three problems that arose when trying to certify proofs with semantic labeling. The ﬁrst problem for the certiﬁer is that internally it only works on extended termi-
nation/DP problems, whereas in the provided proofs just TRSs and DP problems are given without the additional numbers. However, this problem is ﬁxed by computing the number during certiﬁcation. This is easy and seems to be a safe solution: the format for termination proofs remains unchanged, and so far no termination proof was refused with the reason that the internal computation of the number was wrong.
The second and third problem are concerned with how semantic labeling is applied, since usually variations of Lemma 9.12 and Theorem 9.23 are used in termination provers.
The second problem occurs for TRSs as well as DP problems. The theory about semantic labeling demands that Dec is added to the new TRS when using quasi-models. However, termination provers typically reduce the set of rules and “optimize” semantic labeling by only adding rules Dec such that →Dec ⊆ →+Dec .
For example, if Lf = {0, 1, 2} and the order is the standard order on the naturals, then Dec = {f2(x) → f1(x), f1(x) → f0(x), f2(x) → f0(x)}. However, the last rule is often omitted since it can be simulated by the previous two rules. To certify these termination proofs, we ﬁst need to show that we may safely replace Dec by any Dec
6See http://cl2-informatik.uibk.ac.at/rewriting/mercurial.cgi/IsaFoR/raw-file/v1.16/ examples/secret_07_trs_4_top.proof.xml

9.5. Problems in Certiﬁcation

91

where →Dec ⊆ →+Dec . Moreover, we have to provide a certiﬁed algorithm which for a given TRS Dec and a given order can ensure that the condition →Dec ⊆ →+Dec is satisﬁed. Furthermore, the algorithm should accept all Dec where the condition is satisﬁed.
The third problem only occurs when dealing with quasi-models in the DP framework. Note that in standard DP problems the roots of P are special symbols (tuple symbols) which do not occur in the remaining DP problem. However, when applying Theorem 9.23 as it is, this invariant is destroyed since the decreasing rules for tuple symbols are added as new rules. We illustrate the problem and two possible solutions in the following example.

Example 9.33. Consider a DP problem (P, R) where F(s(x), a) → F(x, b) ∈ P and b is not deﬁned in R. Then, the dependency graph estimation EDG [3] can detect that there is no connection from the mentioned pair to itself. However, when performing labeling with a quasi-model where s(x) is interpreted as min(x + 1, 2) and where F(x, y) = x then for the mentioned pair we get all three rules {6, 8, 10} in the labeled pairs P and the decreasing rules for F are DecF = {7, 9, 11}.

6: F2(s(x), a) → F2(x, b) 8: F2(s(x), a) → F1(x, b) 7: F2(x, y) → F1(x, y) 9: F2(x, y) → F0(x, y)

10: F1(s(x), a) → F0(x, b) 11: F1(x, y) → F0(x, y)

Note that when adding DecF as new rules, then the EDG contains an edge from F2(s(x), a) → F1(x, b) to all other pairs since F1 is deﬁned in DecF. Hence, this is not the preferred way to add decreasing rules: not even the decrease in the labels is recognized.
One solution is to add DecF as new pairs. Then one obtains a standard DP problem and the decrease in the labels is reﬂected in the EDG. But there still is a path from F2(s(x), a) → F2(x, b) to F1(s(x), a) → F0(x, b) via the pair F2(x, y) → F1(x, y), since the information that the second argument of Fn is b is lost when passing the pair F2(x, y) → F1(x, y).
To encounter this problem, there is another solution where DecF is not produced at all, but where the labels of all tuple-symbols in right-hand sides of P are decreased. In this example, one would have to add the additional pair F2(s(x), a) → F0(x, b) to P .
Hence, termination proofs might have used one of the two variants instead of Theorem 9.23. Here, the ﬁrst variant returns the problem (lab(P) ∪ DecF , lab(R) ∪ DecF ) and the second variant returns (lab(P)≥, lab(R) ∪ DecF ) where DecF are the decreasing rules for all tuple symbols, DecF are the decreasing rules for the remaining symbols, and lab(P)≥ = {s → f (t) | s → f (t) ∈ lab(P), ≥Lf }.
To certify these termination proofs the problem was mainly in formalizing that these
variants of Theorem 9.23 are indeed sound.

Theorem 9.34. Both variants of Theorem 9.23 are sound, provided that they are applied on DP problems (P, R) where neither left- nor right-hand sides of P are variables and the roots of P are distinguished tuple symbols which do not occur in the remaining DP problem.

We shortly describe the proof idea. The main problem is that we cannot w.l.o.g. restrict
the substitutions in a chain such that they do not contain tuple symbols [97]. Thus, we may have to apply rules in DecF also below the root, in order to simulate a reduction tiσi →∗R si+1σi+1. The trick is to introduce a second set of labels and labeling functions for the tuple symbols. The new labeling functions label all tuple symbols by the same
element. Hence, no decreasing rules are required for them (w.r.t. the second set of labeling
functions) and on all other symbols the labeling functions coincide.

92 Chapter 9. Modular and Certiﬁed Semantic Labeling and Unlabeling

Afterwards, we use a combined labeling of terms: The root of the term is labeled according to the original function, and below the root it is labeled w.r.t. the second labeling function. In this way no decreasing rules for the tuple symbols have to be applied below the root and moreover, on all terms in the DP problem, the original and the combined labeling produce the same result. Thus, we can transform a given (P, R)-chain into a (lab(P) ∪ DecF , lab(R) ∪ DecF )-chain. Theorem 9.34 easily follows.
To summarize, we discussed some problems which occurred when trying to certify existing proofs which are mainly due to optimizations of the basic semantic labeling theorems. Of course, we also need to check the model condition, whether the orders are weakly monotone when using quasi-orders, etc. Whereas the general theorems about soundness of semantic labeling have been formalized for arbitrary carriers, for the certiﬁcation we currently only support ﬁnite carriers. Then checking the required conditions is performed via enumerating all possible assignments.
In total, our formalization of pure semantic labeling consists of 3300 lines of Isabelle, where roughly half of it is about semantic labeling on generic algebras, and the other half contains executable functions for the certiﬁer using algebras over ﬁnite carriers and soundness proofs for these functions. Moreover, the theory about the semantic labeling framework with extended termination techniques, extended DP problems, etc., consists of another 1000 lines.

9.6. Experiments

To test the impact of our formalization we ran AProVE on the TPDB (version 8.0), considering all 2795 TRSs. We used two diﬀerent strategies which are similar to the strategy CERT that was used during the 2010 termination competition in the certiﬁed termination category: –SL is like CERT but with semantic labeling removed, and +SL is like CERT including all three variants of semantic labeling that are supported by AProVE (root-labeling, semantic-labeling on ﬁnite carriers with models and quasi-models).
We performed all our experiments on a machine with two 2.8 GHz Quad-Core Intel Xeon processors and 6 GB of main memory. The following results where obtained using a 60 seconds timeout.

termination proofs nontermination proofs total time (in minutes) certiﬁcation time (in minutes)

–SL +SL total

1137 225 1186
1

1207 218 1219
3

1227 227

CeTA (version 1.17) certiﬁed all but two proofs. On one TRS, both –SL and +SL delivered a faulty proof, caused by a bug in the LPO output of AProVE (which will be ﬁxed soonish).
The results show that by using semantic labeling we obtain 90 new certiﬁed termination proofs. This is an increase of nearly 8 %. Note that +SL has not solved all TRSs where –SL was successful. This is due to timing issues in the strategy.

9.7. Conclusion
During our formalization of semantic labeling we have detected that unlabeling is unsound when using the current semantics of termination problems. We solved the problem

9.7. Conclusion

93

by extending termination problems and the DP framework such that recursive labeling and unlabeling are supported, as well as all other existing termination techniques. This framework forms the semantic basis of our certiﬁer CeTA which now fully supports semantic labeling.
Acknowledgments We thank Christian Kuknat and Carsten Fuhs for their support in providing certiﬁable proofs with semantic labeling generated by AProVE.

10. Termination of Isabelle Functions via Termination of Rewriting
Publication details
Alexander Krauss, Christian Sternagel, Ren´e Thiemann, Carsten Fuhs, and Ju¨rgen Giesl. Termination of Isabelle Functions via Termination of Rewriting. In Proceedings of the 2nd International Conference on Interactive Theorem Proving, volume 6898 of LNCS, pages 152–167. Springer, 2011.
Abstract
We show how to automate termination proofs for recursive functions in (a ﬁrst-order subset of) Isabelle/HOL by encoding them as term rewrite systems and invoking an external termination prover. Our link to the external prover includes full proof reconstruction, where all necessary properties are derived inside Isabelle/HOL without oracles. Apart from the certiﬁcation of the imported proof, the main challenge is the formal reduction of the proof obligation produced by Isabelle/HOL to the termination of the corresponding term rewrite system. We automate this reduction via suitable tactics which we added to the IsaFoR library.
10.1. Introduction
In a proof assistant based on higher-order logic (HOL), such as Isabelle/HOL [84], recursive function deﬁnitions typically require a termination proof. To release the user from ﬁnding suitable termination arguments manually, it is desirable to automate these termination proofs as much as possible.
There have already been successful approaches to port and adapt existing termination techniques from term rewriting and other areas to Isabelle [18, 68]. They indeed increase the degree of automation for termination proofs of HOL functions. However, these approaches do not cover all powerful techniques that have been developed in term rewriting, e.g., [32, 110]. These techniques are implemented in a number of termination tools (e.g., AProVE [39], TTT2 [67] and many others) that can show termination of (ﬁrst-order) term rewrite systems (TRSs) automatically. (In the remainder we use ‘termination tool’ exclusively to refer to such fully automatic and external provers.) Instead of porting further proof techniques to Isabelle, we prefer to use the existing termination tools, giving direct access to an abundance of methods and their eﬃcient implementations.
Using termination tools inside proof assistants has been an open problem for some time and is often mentioned as future work when discussing certiﬁcation of termination proofs [13, 21]. However, this requires more than a communication interface between two programs. In LCF-style proof assistants [44] such as Isabelle, all proofs must be checked by a small trusted kernel. Thus, integrating external tools as unveriﬁed oracles is

96 Chapter 10. Termination of Isabelle Functions via Termination of Rewriting
unsatisfactory: any error in the external tool or in the integration code would compromise the overall soundness. Instead, the external tool must provide a certiﬁcate that can be checked by the proof assistant.
Our approach involves the following steps.
(i) Generate the deﬁnition of a TRS Rf which corresponds to the function f .
(ii) Prove that termination of Rf indeed implies the termination goal for f .
(iii) Run the termination tool on Rf and obtain a certiﬁcate.
(iv) Replay the certiﬁcate using a formally veriﬁed checker.
While steps 1 and 3 are not hard, and the ground work for step 4 is already available in the IsaFoR library [94, 104], which formalizes term rewriting and several termination techniques,1 this paper is concerned with the missing piece, the reduction of termination proof obligations for HOL functions to the termination of a TRS. This is non-trivial, as the languages diﬀer considerably. Termination of a TRS expresses the well-foundedness of a relation over terms, i.e., of type (term × term) set, where terms are ﬁrst-order terms. In contrast, the termination proof obligation for a HOL function states the well-foundedness of its call relation, which has the type (α × α) set, where α is the argument type of the function. In essence, we must move from a shallow embedding (the functional programming fragment of Isabelle/HOL) to a deep embedding (the formalization of term rewriting in IsaFoR).
The goal of this paper is to provide this formal relationship between termination of ﬁrst-order HOL functions and termination of TRSs. More precisely, we develop a tactic that automatically reduces the termination proof obligation of a HOL function to the termination problem of a TRS. This allows us to use arbitrary termination tools for fully automated termination proofs inside Isabelle. Thus, powerful termination tools become available to the Isabelle user, while retaining the strong soundness guarantees of an LCFstyle proof assistant. Since our approach is generic, it automatically beneﬁts from future improvements to termination tools and the termination techniques within IsaFoR. Our implementation is available as part of IsaFoR.
Outline of this paper. We give a short introduction on term rewriting, HOL and HOL functions in §10.2. Then we show our main result in §10.3 on how to systematically discharge the termination proof obligation of a HOL function via proving termination of a TRS. In §10.4 we present some examples which show the strengths and limitations of our technique. How to extend our approach to support more HOL functions is discussed in §10.5. We conclude in §10.6.
10.2. Preliminaries
10.2.1. Higher-Order Logic
We consider classical HOL, which is based on simply-typed lambda-calculus, enriched with a simple form of ML-like polymorphism. Among its basic types are a type bool of truth values and a function space type constructor ⇒ (where α ⇒ β denotes the type of
1See http://cl-informatik.uibk.ac.at/software/ceta for a list of supported techniques.

10.2. Preliminaries

97

total functions mapping values of type α to values of type β). Sets are modeled by a type α set, which just abbreviates α ⇒ bool.
By an add-on tool, HOL supports algebraic datatypes, which includes the types nat (with constructors 0 and Suc) and list (with constructors [ ] and #).
Another add-on tool, the function package [69], completes the functional programming layer by allowing recursive function deﬁnitions, which are not covered by the primitives of the logic. Since it internally employs a well-founded recursion principle, it requires the user to prove well-foundedness of a certain relation, extracted automatically from the function deﬁnition (cf. §10.2.3). This proof obligation, by its construction, directly corresponds to the termination of the function being deﬁned. It is the proof of this goal that we want to automate.
As opposed to functional programming languages, there is no operational semantics for HOL; the meaning of its expressions is instead given by a set-theoretic denotational semantics. As a consequence, there is no direct notion of evaluation or termination of an expression. Thus, when we informally say that we prove “termination of a HOL function,” this simply means that we discharge the proof obligation produced by the function package.
10.2.2. Supported Fragment
Isabelle supports a wide spectrum of speciﬁcations, using various forms of inductive, coinductive and recursive deﬁnitions, as well as quantiﬁers and Hilbert’s choice operator. Clearly, not all of them can be easily expressed using TRSs. Thus, we must limit ourselves to a subset which is suﬃciently close to rewriting, and consider only algebraic datatypes, given by a set of constructors together with their types, and recursive functions, given by their deﬁning equations with pattern matching. Additionally, we impose the following restrictions:
(i) Functions and constructors must be ﬁrst-order (no functions as arguments).
(ii) Patterns are constructor terms and must be linear and non-overlapping.
(iii) Patterns must be complete.
(iv) Expressions consist of variables, function applications, and case-expressions only. In particular, partial applications and λ-abstractions are excluded.
Linearity is always satisﬁed by function deﬁnitions that are accepted by Isabelle’s function package, and pattern overlaps are eliminated automatically. For ease of presentation, we assume that there is no mutual recursion (f calls g and g calls f ) and no nested recursion (arguments of a recursive call contain other recursive calls; they may of course contain calls to other deﬁned functions).
Most of the above restrictions are not fundamental, and we discuss in §10.5 how some of them can be removed. Our chosen fragment of HOL rather represents a compromise between expressive power and a reasonably simple presentation and implementation of our reduction technique. Note that case-expressions encompass the simpler if-expressions, which can be seen as case-expressions on type bool. Isabelle’s (non-recursive and monomorphic) let-expressions can simply be inlined or replaced by case-expressions if patterns are involved.
The functions half and log below (log computes the logarithm) illustrate our supported fragment and will be used as running examples throughout this paper.

98 Chapter 10. Termination of Isabelle Functions via Termination of Rewriting
half 0 = 0
half (Suc 0) = 0
half (Suc (Suc n)) = Suc (half n) log n = (case half n of 0 ⇒ 0 | Suc m ⇒ Suc (log (Suc m)))
10.2.3. Function Deﬁnitions by Well-Founded Recursion
When the user writes a recursive deﬁnition, the function package analyzes the equations and extracts the recursive calls. This information is then used to synthesize the termination proof obligation.
Formally, we deﬁne the operation callsf that computes the set of calls to f inside an expression, each together with a condition under which it occurs.
• callsf (g e1 . . . ek) ≡ callsf (e1)∪. . .∪callsf (ek) if g is a constructor or a deﬁned function other than f ,
• callsf (f e1 . . . en) ≡ callsf (e1) ∪ . . . ∪ callsf (en) ∪ {(e1, . . . , en, True)},
• callsf (x) ≡ ∅ for all variables x, and
• callsf (case e of p1 ⇒ e1 | . . . | pk ⇒ ek) ≡ callsf (e) ∪ (callsf (e1) ∧ e = p1) ∪ . . . ∪ (callsf (ek) ∧ e = pk) where callsf (ei) ∧ e = pi is like callsf (ei), but every (t1, . . . , tm, ϕ) ∈ callsf (ei) is replaced by (t1, . . . , tm, ϕ ∧ e = pi).
The termination proof obligation requires us to exhibit a strongly normalizing relation such that for each deﬁning equation f p1 . . . pn = e and each (r1, . . . , rn, φ) ∈ callsf (e) we can prove φ =⇒ (p1, . . . , pn) (r1, . . . , rn). Consider for example the deﬁnition of half, where we have callshalf(0) ≡ ∅ and callshalf(Suc (half n)) ≡ {(n, True)}. We obtain the following obligation.
1. SN ?R 2. ∀n. (Suc (Suc n), n) ∈ ?R
The variable ?R :: (nat×nat) set is a schematic variable, which can be instantiated during the proof, i.e., it can be seen as existentially quantiﬁed.
For log, we have callslog(case half n of 0 ⇒ 0 | Suc m ⇒ Suc (log (Suc m))) ≡ {(Suc m, half n = Suc m)}, and the following proof obligation is produced.
1. SN ?R 2. ∀n m. half n = Suc m =⇒ (n, Suc m) ∈ ?R
Two things should be noted here. First, the fact that the recursive call is guarded by a case-expression is reﬂected by a condition in the corresponding subgoal. Without this condition, which models the usual evaluation behavior of case, the goal would be unprovable. Second, the goal may refer to previously deﬁned functions. To prove it, we must refer to properties of these functions, either through their deﬁnitions, or through other lemmas about them.
When the proof obligation is discharged, the function package can use the result to derive the recursive equations as theorems (previously, they were just conjectures—consider the recursive equation f x = Suc (f x), which is inconsistent). Additionally, an induction rule is provided, which expresses “induction along the computation.” The induction rules for half and log are shown below.

10.2. Preliminaries

99

P 0 =⇒ P (Suc 0) =⇒ (∀n. P n =⇒ P (Suc (Suc n))) =⇒ ∀n. P n (∀n. (∀m. half n = Suc m =⇒ P (Suc m)) =⇒ P n) =⇒ ∀n. P n
10.2.4. IsaFoR - Term Rewriting Formalized in Isabelle/HOL
In the following, we assume that the reader is familiar with the basics of term rewriting [5]. Many notions and facts from rewriting have been formalized in the Isabelle library IsaFoR [104]. Before we can give the reduction from termination of HOL functions to termination of corresponding TRSs in §10.3, we need some more details on IsaFoR. Terms are represented straightforwardly by the datatype:
datatype (α, β) term = Var β | Fun α ((α, β) term list)
The type variables α and β, which represent function and variable symbols, respectively, are always instantiated with the type string in our setting. Hence, we abbreviate (string, string) term by term in the following. For example, the term f(x, y) is represented by Fun “f ” [Var “x”, Var “y”]. A TRS is represented by a value of type (term × term) set.
The semantics of a TRS is given by its rewrite relation →R, deﬁned by closing R under contexts and substitutions. Termination of R is formalized as SN (→R).
IsaFoR formalizes many criteria commonly used in automated termination proofs. Ultimately, it contains an executable and terminating function
check-proof :: (term × term) list ⇒ proof ⇒ bool
and a proof of the following soundness theorem:
Theorem 10.1 (Soundness of Check). check-proof R prf =⇒ SN (→R)
Here, prf is a certiﬁcate (i.e., a termination proof of R) from some external source, encoded as a value of a suitable datatype, and R is the TRS under consideration.2 Whenever check-proof returns True for some given TRS R and certiﬁcate prf, we have established (inside Isabelle) that prf is a valid termination proof for R. Thus, we can prove termination of concrete TRSs inside Isabelle.
The technical details on the supported termination techniques and the structure of the certiﬁcate (i.e., the type proof ) are orthogonal to our use of the check function, which only relies on Theorem 10.1.
10.2.5. Terminology and Notation
The layered nature of our setting requires that we carefully distinguish three levels of discourse. Primarily, there is higher-order logic (implemented in Isabelle/HOL), in which all mechanized reasoning takes place. The termination goals we ultimately want to solve are formulated on this level. Of course, the syntax of HOL consists of terms, but to distinguish them from the embedded term language of term rewriting, we refer to them as expressions. They are uniformly written in italics and follow the conventions of the lambda-calculus (in particular, function application is denoted by juxtaposition). HOL equality is denoted by =. For example, the deﬁnition of half above is a HOL expression.
2To be executable, check-proof demands that R is given as a list of rules and not as a set. We ignore this diﬀerence, since it is irrelevant for this paper.

100 Chapter 10. Termination of Isabelle Functions via Termination of Rewriting
The second level is the “sub-language” of ﬁrst-order terms, which is deeply embedded into HOL by the datatype term. When we speak of a term, we always refer to a value of that type, not an arbitrary HOL expression. While this embedding is simple and adequate, the concrete syntax with the Fun and Var constructors and string literals is rather unwieldy. Hence, for readability, we use sans-serif font to abbreviate the constructors and the quotes: Instead of Var “v” we write v, and instead of Fun “f ” [. . .] we write f(. . .), omitting the parentheses () for nullary functions. This recovers the well-known concrete syntax of term rewriting, but we must keep in mind that the constructors and strings are still present, although they are not written as such.
Finally, we must relate the two languages with each other, and describe the proof procedures that derive the relevant properties. While the properties themselves can be stated in HOL for each concrete instance, the general schema cannot, as it must talk about “all HOL expressions.” Thus, we use a meta-language as another level above HOL, in which we express the transformations and tactics. This level corresponds to our implementation (in ML). Functions of the meta-language are written in small capitals (e.g., callsf ), and variables of the meta-language, which typically range over arbitrary HOL expressions or patterns, are written e or p, possibly with annotations. For HOL expressions that are arguments of recursive calls we also use r. Equality of the metalanguage is written ≡ and denotes syntactic equality of HOL expressions. In particular, e ≡ e implies e = e , since HOL’s equality is reﬂexive.
Both embeddings are deep, that is, each level can talk about the syntax of the lower levels. As a simple example, the concept of a ground term can be deﬁned as a recursive HOL function ground :: term ⇒ bool:
ground (Var x ) = False ground (Fun f ts) = (∀t∈set(ts). ground t)
Then we can immediately deduce that ground (f(x)) = False, due to the presence of x. Note however that the similar-looking statement ground (f(x)) = False is not uniformly true. More precisely, its universal closure ∀x. ground (f(x)) = False does not hold, since we could instantiate x with the term c (i.e., Fun “c” [ ]). Thus, we must not confuse variables of the diﬀerent levels. Obviously, we cannot quantify over a variable x, which is just the Var constructor applied to a string.
Similarly, the meta-language can talk about the syntax of HOL, as in the deﬁnition of callsf , which is recursive over the structure of HOL expressions.
10.3. The Reduction to Rewriting
10.3.1. Encoding Expressions and Deﬁning Equations
We deﬁne a straightforward encoding of HOL expressions as terms, denoted by the metalevel operation enc. For case-free expressions, we turn variables into term variables and (curried) applications into applications on the term level:
enc(x ) ≡ x enc(f e1 . . . en) ≡ f(enc(e1), . . . , enc(en))
Each case-expression is replaced by a new function symbol, for which we will include additional rules below. To simplify bookkeeping, we assume that each occurrence of a

10.3. The Reduction to Rewriting

101

case-expression is annotated with a unique integer j.

enc(casej e of p1 ⇒ e1 | . . . | pk ⇒ ek) ≡ casej(enc(e), enc(y1), . . . , enc(ym))

where y1, . . . , ym are all variables that occur free in some ei but not in pi. The operation rules yields the rewrite rules for a function or case-expression. For a
function f with deﬁning equations 1 = r1, . . . , k = rk, they are
rules(f ) ≡ { enc( 1) → enc(r1), . . . , enc( k) → enc(rk) } .

For the case-expression casej e of p1 ⇒ e1 | . . . | pk ⇒ ek we have

rules(casej) ≡ { casej(enc(p1), enc(y1), . . . , enc(ym)) → enc(e1),
...,
casej(enc(pk)), enc(y1), . . . , enc(ym)) → enc(ek) } .
We deﬁne the TRS for f as Rf = rules(f ) ∪ g∈Sf rules(g) where Sf is the set of all functions that are used (directly or indirectly) by f . Our encoding is similar to the well known technique of unraveling which transforms conditional into unconditional TRSs [77, 86].3
For example, Rlog is deﬁned as follows and completely contains Rhalf.

half( ) → half(Suc( )) → half(Suc(Suc(n))) → Suc(half(n))

log(n) → case0(half(n)) case0( ) → case0(Suc(m)) → Suc(log(Suc(m)))

10.3.2. Embedding Functions
At this point, we have deﬁned a translation, but we cannot reason about it in Isabelle, since enc is only an extra-logical concept, deﬁned on the meta-level. In fact, it is easy to see that it cannot be deﬁned in HOL: If we had a HOL function enc satisfying enc 0 = and enc (half 0) = half( ), we would immediately have a contradiction, since half 0 = 0, and half( ) = , but a function must always yield the same result on the same input.
In a typical reﬂection scenario, we would proceed by deﬁning an interpretation for term. For example, if we were modeling the syntax of integer arithmetic expressions, then we could deﬁne a function eval :: term ⇒ int (possibly also depending on a variable assignment) which interprets terms as integers. However, in our setting, the result type of such a function is not ﬁxed, as our terms represent HOL expressions of arbitrary types. Thus, the result type of eval would depend on the actual term it is applied to. This cannot be expressed in a logic without dependent types, which means we cannot use this approach here.
Instead, we take the opposite route: For all relevant types σ, we deﬁne a function embσ :: σ ⇒ term, mapping values of type σ to their canonical term representation.
Using Isabelle’s type classes, we use a single overloaded function emb, which belongs to a type class embeddable. Concrete datatypes can be declared to be instances of this class by deﬁning emb, usually by structural recursion w.r.t. the datatype. For example, here are the deﬁnitions for the types nat and list:
3It would be possible to directly generate dependency pair problems. However, techniques like [96] and several termination tools rely on the notion of “minimal chains,” which is not ensured by our approach.

102 Chapter 10. Termination of Isabelle Functions via Termination of Rewriting

emb 0 =

emb [ ] = Nil

emb (Suc n) = Suc(emb n) emb (x # xs) = Cons(emb x , emb xs)

This form of deﬁnition is canonical for all algebraic datatypes, and suitable deﬁnitions of emb can be automatically generated for all user-deﬁned datatypes, turning them into instances of the class embeddable. This is analogous to the instances generated automatically by Haskell’s “deriving” statement. It is also possible to manually provide the deﬁnition of emb for other types if they behave like datatypes like the predeﬁned type bool for the Booleans.
Note that by construction, the result of emb is always a constructor ground term. For a HOL expression e that consists only of datatype constructors, (e.g., Suc (Suc 0)), we have emb e = enc(e). For other expressions this is not the case, e.g., emb (half 0) = emb 0 = , but enc(half 0) ≡ half( ).
To formulate our proofs, we need another encoding of expressions as terms: The operation genc is a slight variant of enc, which treats variables diﬀerently, mapping them to their embeddings instead of term variables.

genc(x ) ≡ emb x genc(f e1 . . . en) ≡ f(genc(e1), . . . , genc(en)) genc(casej e of p1 ⇒ e1 | . . . | pk ⇒ ek)
≡ casej(genc(e), genc(y1), . . . , genc(ym))

where y1, . . . , ym are all variables that occur free in some ei but not in pi. Hence, genc(e) never contains term variables. However, it contains the same HOL
variables as e. For example, genc(half (Suc n)) ≡ half(Suc(emb n)).

10.3.3. Rewrite Lemmas

The deﬁnitions of Rhalf and Rlog above are straightforward, but reasoning with them is clumsy and low-level: To establish a single rewrite step, we must extract the correct rule (that is, prove that it is in the set Rhalf or Rlog), invoke closure under substitution, and construct the correct substitution explicitly as a function of type string ⇒ term.
To avoid such repetitive reasoning, we automatically derive an individual lemma for each rewrite rule. From the deﬁnition of Rhalf, we obtain the following rules, which we call rewrite lemmas:

half( ) →Rhalf ∀t . half(Suc(Suc(t ))) →Rhalf Suc(half(t ))

half(Suc( )) →Rhalf

Note that the term variable n in the last rule has been turned into a universally-quantiﬁed HOL variable by applying the “generic substitution” {n → t}. The advantage of this format is that applying a rewrite rule merely involves instantiating a universal quantiﬁer, for which we can use the matching facilities of Isabelle. In particular, we can instantiate t with emb n, which in general results in a rewrite lemma of the form genc(f p1 . . . pn) →R genc(e) for a deﬁning equation f p1 . . . pn = e.

10.3.4. The Simulation Property
The following property connects our generated TRSs with HOL expressions.

10.3. The Reduction to Rewriting

103

Deﬁnition 10.2 (Simulation Property). For every expression e and R = {Rf | f occurs in e}, the simulation property for e is the statement
genc(e) →∗R emb e.

As we cannot quantify over all HOL expressions within HOL itself, we cannot formalize that the simulation property holds for all e.
However, we will devise a tactic that derives this property for any given concrete expression. The basic building blocks of such proofs are lemmas of the following form, which are derived for each function symbol and can be composed to show the simulation property for a given expression.

Deﬁnition 10.3 (Simulation Lemma). The simulation lemma for a function f of arity n is the statement

∀x1 . . . xn. f(emb x1, . . . , emb xn) →∗Rf emb (f x1 . . . xn) .

E.g., the simulation lemma for half

is

∀n .

half(emb

n)

→∗ Rhalf

emb

(half n).

The lemma claims that the rules that we produced for f can indeed be used to reduce

a function application to the (embedding of) the value of the function. Of course, this

way of saying “Rf computes f ” admits the possibility that there are other Rf -reductions

that lead to diﬀerent normal forms or that do not terminate, since we are not requiring

conﬂuence or strong normalization. But this form of simulation lemma is suﬃcient for

our purpose.

We show in §10.3.6 how simulation lemmas are proved automatically.

10.3.5. Reduction of Termination Goals
After having proved termination of Rf using a termination tool in combination with IsaFoR and Theorem 10.1, we now show how to use this result to solve the termination goal for the HOL function f . Recall from §10.2.3 that we must exhibit a strongly normalizing relation such that φ =⇒ (p1, . . . , pn) (r1, . . . , rn) for all (r1, . . . , rn, φ) ∈ callsf (e) for each deﬁning equation f p1 . . . pn = e.
To this end, we ﬁrst deﬁne Y as →Rf ∪ £ where £ is the strict subterm relation. The addition of £ is required to strip oﬀ constructors and non-recursive function applications that are wrapped around recursive calls in right-hand sides of Rf . Since →Rf is strongly normalizing and closed under contexts, also Y is strongly normalizing. This allows us to ﬁnally choose as the following relation.
(x1, . . . , xn) (y1, . . . , yn) iﬀ f(emb x1, . . . , emb xn) Y+ f(emb y1, . . . , emb yn)
It remains to show that the arguments of recursive calls decrease w.r.t. . That is, for each recursive call we have a goal of the form
φ =⇒ f(emb p1, . . . , emb pn) Y+ f(emb r1, . . . , emb rn)
where f p1 . . . pn = e is a deﬁning equation of f and (r1, . . . , rn, φ) ∈ callsf (e). In the following, we illustrate the automated proof of this goal.

104 Chapter 10. Termination of Isabelle Functions via Termination of Rewriting

Note that since the pi’s are patterns, we have emb pi = genc(pi), and hence

f(emb p1, . . . , emb pn) = f(genc(p1), . . . , genc(pn)) ≡ genc(f p1 . . . pn) →Rf genc(e)

(pi are patterns) (deﬁnition of genc) (rewrite lemma)

Thus, it remains to construct a sequence genc(e) Y∗ f(emb r1, . . . , emb rn), which reduces the right-hand side of the deﬁnition to a particular recursive call, eliminating any surrounding context. We proceed recursively over e.

• If e ≡ g e1 . . . em for a constructor g or a deﬁned function symbol g ≡ f , then (r1, . . . , rn, φ) ∈ calls(ei) for some particular i. Hence, we have

genc(e) ≡ g(genc(e1), . . . , genc(em)) £ genc(ei) Y∗ f(emb r1, . . . , emb rn)

(deﬁnition of genc) (deﬁnition of £) (apply tactic recursively)

• If e ≡ f e1 . . . en then (since we excluded nested recursion) we have ei = ri for all i. Hence, we have

genc(e)
≡ f(genc(r1), . . . , genc(rn)) →∗Rf f(emb r1, . . . , emb rn)

(deﬁnition of genc) (simulation property)

• If e ≡ casej e0 of p1 ⇒ e1 | . . . | pk ⇒ ek then we distinguish where the recursive call is located. If (r1, . . . , rn, φ) ∈ callsf (e0), then we have

genc(e) ≡ casej(genc(e0), genc(y1), . . . , genc(ym)) £ genc(e0) Y∗ f(emb r1, . . . , emb rn)

(deﬁnition of genc) (deﬁnition of £) (apply tactic recursively)

Otherwise, φ ≡ (χ ∧ e0 = pi) for some χ and 1 i k, and (r1, . . . , rn, χ) ∈ calls(ei). We may therefore use the assumption e0 = pi and proceed with

genc(e)
≡ casej(genc(e0), genc(y1), . . . , genc(ym)) →∗Rf casej(emb e0, genc(y1), . . . , genc(ym))
= casej(emb pi, genc(y1), . . . , genc(ym))
= casej(genc(pi), genc(y1), . . . , genc(ym)) →Rf genc(ei)
Y∗ f(emb r1, . . . , emb rn)

(deﬁnition of genc) (simulation property) (assumption e0 = pi) (since pi is a pattern) (rewrite lemma) (apply tactic recursively)

10.3. The Reduction to Rewriting

105

10.3.6. Proof of the Simulation Property
We have seen that for the reduction of termination goals it is essential to use the simulation property genc(e) →∗Rf emb e for expressions e that occur below recursive calls or within conditions that guard a recursive call. Below, we show how this property is derived for an individual expression, assuming that we already have simulation lemmas for all functions that occur in it. We again proceed recursively over e.

• If e is a HOL variable x then genc(e) ≡ genc(x) ≡ emb x ≡ emb e and thus, the result follows by reﬂexivity of →∗Rf .
• If e ≡ g e1 . . . ek for a function symbol g then

genc(e)
≡ g(genc(e1), . . . , genc(ek)) →∗Rf g(emb e1, . . . , emb ek) →∗Rf emb (g e1 . . . ek)
≡ emb e

(deﬁnition of genc) (apply tactic recursively) (simulation lemma for g)

• If e ≡ casej e0 of p1 ⇒ e1 | . . . | pk ⇒ ek then we construct the following rewrite sequence:

genc(e)
≡ casej(genc(e0), genc(y1), . . . , genc(ym)) →∗Rf casej(emb e0, genc(y1), . . . , genc(ym))

(deﬁnition of genc) (apply tactic recursively)

Now we apply a case analysis on e0, which must be equal (in HOL, not syntactically) to one of the patterns. In each particular case we may assume e0 = pi. Then we continue:

casej(emb e0, genc(y1), . . . , genc(ym))
= casej(emb pi, genc(y1), . . . , genc(ym))
= casej(genc(pi), genc(y1), . . . , genc(ym)) →Rf genc(ei) →∗Rf emb ei
= emb e

(assumption e0 = pi) (since pi is a pattern) (rewrite lemma) (apply tactic recursively) (assumption e0 = pi)

The tactic above assumes that simulation lemmas for all functions in e are already present. Note the simulation lemma is trivial to prove if f is a constructor, since f(emb x1, . . . , emb xn) = emb (f x1 . . . xn) by deﬁnition of emb.
For deﬁned symbols of non-recursive functions the simulation lemmas are derived by unfolding the deﬁnition of the function and applying the tactic above. Thus, simulation lemmas are proved bottom-up in the order of function dependencies. When a function is recursive, the proof of its simulation lemma proceeds by induction using the induction principle from the function deﬁnition.
Example 10.4. We show how the simulation lemma for log is proved, assuming that the simulation lemmas for 0, Suc, and half are already available.

106 Chapter 10. Termination of Isabelle Functions via Termination of Rewriting

So

our

goal

is

to

show

log(emb

n)

→∗ Rlog

emb (log n) for any n :: nat.

We apply the

induction rule of log and obtain the following induction hypothesis.

∀m. half n = Suc m

=⇒

log(emb

(Suc

m))

→∗ Rlog

emb

(log

(Suc

m))

Let c abbreviate case half n of 0 ⇒ 0 | Suc m ⇒ Suc (log (Suc m)). Then

log(emb n)

→Rlog case0(half(emb n))

→∗ Rlog

case0(emb

(half

n))

(rewrite lemma) (simulation lemma of half )

We continue by case analysis on half n. We only present the more interesting case half n = Suc m (the other case half n = 0 is similar):

case0(emb (half n))

= case0(emb (Suc m))

= case0(Suc(emb m))

→Rlog Suc(log(Suc(emb m)))

→∗ Rlog

Suc(log(emb

(Suc

m)))

→∗ Rlog

Suc(emb

(log

(Suc

m)))

→∗ Rlog

emb

(Suc

(log

(Suc

m)))

= emb c

= emb (log n)

(assumption half n = Suc m) (def. of emb) (rewrite lemma) (simulation lemma of Suc) (induction hypothesis) (simulation lemma of Suc) (assumption half n = Suc m) (def. of log)

10.4. Examples
We show some characteristic examples that illustrate the strengths and weaknesses of our approach. Each example is representative for several similar ones that occur in the Isabelle distribution.
Example 10.5. Consider binary trees deﬁned by the type
datatype tree = E | N tree nat tree
and a function remdups that removes duplicates from a tree. The function is deﬁned by the following equations (the auxiliary function del removes all occurrences of an element from a tree; we omit its straightforward deﬁnition here):
remdups E = E remdups (N l x r ) = N (remdups (del x l )) x (remdups (del x r ))
The termination argument for remdups relies on a property of del : the result of del is smaller than its argument. In Isabelle, the user must manually state and prove (by induction) the lemma size (del x t) ≤ size t, before termination can be shown. Here, size is an overloaded function generated automatically for every algebraic datatype.
For a termination tool, termination of the related TRS is easily proved using standard techniques, eliminating the need for ﬁnding and proving the lemma.

10.5. Extensions

107

Example 10.6. The following function (originally due to Boyer and Moore [14]) normalizes conditional expressions consisting of atoms (AT ) and if-expressions (IF ).
norm (AT a) = AT a norm (IF (AT a) y z ) = IF (AT a) (norm y) (norm z ) norm (IF (IF u v w ) y z ) = norm (IF u (IF v y z ) (IF w y z ))
Isabelle’s standard size measure is not suﬃcient to prove termination of norm, and a custom measure function must be speciﬁed by the user. Using a termination tool, the proof is fully automatic and no measure function is required.
Example 10.7. The Isabelle distribution contains the following implementation of the merge sort algorithm (transformed into non-overlapping rules internally):
msort [ ] = [ ] msort [x ] = [x ] msort xs = merge (msort (take (length xs div 2) xs)) (msort (drop (length xs div 2) xs))
The situation is similar to Example 10.5, as we must know how take and drop aﬀect the length of the list. However, in this case, Isabelle’s list theory already provides the necessary lemmas, e.g., length (take n xs) = min n (length xs). Together with the built-in arithmetic decision procedures (which know about div and min), the termination proof works fully automatically.
For termination tools, the proof is a bit more challenging and requires techniques that are not yet formalized in IsaFoR (in particular, the technique of rewriting dependency pairs [37]). Thus, our connection to termination tools cannot handle msort yet. However, when this technique is added to IsaFoR in the future, no change will be required in our implementation to beneﬁt from it.
These examples show the main strength of our reduction to rewriting: absolutely no user input in the form of lemmas or measure functions is required. On the other hand, Isabelle’s ability to pick up previously established results can make the built-in termination prover surprisingly strong in the presence of a good library, as the msort example shows. Even though that example can be solved by termination tools (and only the formalization lags behind), it shows an intrinsic weakness of the approach, since existing facts are not used and must be rediscovered by the termination tool if necessary.

10.5. Extensions
In this section, we reconsider the restrictions imposed in §10.2.2.
Nested Recursion. So far, we excluded nested recursion like f (Suc n) = f (f n). The problem is that to prove termination of f we need its simulation lemma to reduce the inner call in the proof of the outer call, cf. §10.3.5. But proving the simulation lemma uses the induction rule of f , which in turn requires termination.
To solve this problem, we can use the partial induction rule that is generated by the function package even before a termination proof [69]. This rule, which is similar to the one used previously, contains extra domain conditions of the form domf x. It allows us to derive the restricted simulation lemma domf n =⇒ f(emb n) →∗Rf emb (f n). In

108 Chapter 10. Termination of Isabelle Functions via Termination of Rewriting
the termination proof obligation for the outer recursive call, we may assume this domain condition for the inner call (a convenience provided by the function package), so that this restricted form of simulation lemma suﬃces. Hence, dealing with nested recursion simply requires a certain amount of additional bookkeeping.
Underspeciﬁcation. So far, we require functions to be completely deﬁned, i.e., no cases are missing in left-hand sides or case-expressions. However, head (x # xs) = x is a common deﬁnition. It is internally completed by head [ ] = undeﬁned in Isabelle, where undeﬁned :: α is an arbitrary but unknown value of type α.
For such functions, we cannot derive the simulation lemma, since this would require head(Nil) to be equal to emb undeﬁned, which is an unknown term of the form Suck( ). The obvious idea of adding the rule head(Nil) → undeﬁned to the TRS does not work, since undeﬁned cannot be equal to emb undeﬁned.
We can solve the problem by using fresh variables for unspeciﬁed cases, e.g., by adding the rule head(Nil) → x. Then, the simulation lemma holds. However, the resulting TRS is no longer terminating. This new problem can be solved by using a variant of innermost rewriting, which would require support by IsaFoR as well as the termination tool.
Non-Representable Types and Polymorphism. Clearly, our embedding is limited to types that admit a term representation. This excludes uncountable types such as real numbers and most function types. However, even if such types occur in HOL functions, they may not be relevant for termination. Then, we can simply map all such values to a ﬁxed constant by deﬁning, e.g., emb (r :: real) = real. Hence, the simulation lemmas for functions returning real numbers are trivial to prove. Furthermore, a termination proof that does not rely on these values works without problems. Like for underspeciﬁed functions, the generated TRS no longer models the original function completely, but is only an abstraction that is suﬃcient to prove termination.
A similar issue arises with polymorphic functions: To handle a function of type α list ⇒ α list we need a deﬁnition of emb on type α. Mapping values of type α to a constant is unproblematic, since the deﬁnition is irrelevant for the proof. However, a class instance α :: embeddable would violate the type class discipline. This can be solved by either replacing the use of type classes by explicit dictionary constructions (where emblist would take the embedding function to use for the list elements as an argument), or by restricting α to class embeddable. Since the type class does not carry any axioms, the system allows us to remove the class constraint from the ﬁnal theorem, so no generality is lost.
Higher-Order Functions. Higher-order functions pose new diﬃculties. First, we cannot hope to deﬁne emb on function types. In particular, this means that we cannot even state the simulation lemma for a function like map. Second, the termination conditions for functions with higher-order recursion depend on user-provided congruence rules of a certain format [69]. These congruence rules then inﬂuence the form of the premise φ in the termination condition.
A partial solution could be to create a ﬁrst-order function mapf for each invocation of map on a concrete function f . Commonly used combinators like map, ﬁlter and fold could be supported in this way.

10.6. Conclusion

109

10.6. Conclusion
We have presented a generic approach to discharge termination goals of HOL functions by proving termination of a corresponding generated TRS. Hence, where before a manual termination proof might have been required, now external termination tools can be used. Since our approach is not tied to any particular termination proof technique, its power scales up as the capabilities of termination tools increase and more techniques are formalized in IsaFoR.
A complete prototype of our implementation is available in the IsaFoR/CeTA distribution (version 1.18, http://cl-informatik.uibk.ac.at/software/ceta), which also includes usage examples. It remains as future work to extend our approach to a larger class of HOL functions. Moreover, the implementation has to be more smoothly embedded into the Isabelle system such that a user can easily access the provided functionality. The general approach is not limited to Isabelle, and could be ported to other theorem provers like Coq, which has similar recursive deﬁnition facilities (e.g., [6]) and rewriting libraries similar to IsaFoR [13, 21].
Acknowledgment. Jasmin Blanchette gave helpful feedback on a draft of this paper.

11. Generalized and Formalized Uncurrying
Publication details
Christian Sternagel and Ren´e Thiemann. Generalized and Formalized Uncurrying. In Proceedings of the 8th International Symposium on the Frontiers of Combining Systems, volume 6989 of LNCS, pages 243–258. Springer, 2011.
Abstract
Uncurrying is a termination technique for applicative term rewrite systems. During our formalization of uncurrying in the theorem prover Isabelle, we detected a gap in the original pen-and-paper proof which cannot directly be ﬁlled without further preconditions. Our ﬁnal formalization does not demand additional preconditions, and generalizes the existing techniques since it allows to uncurry non-applicative term rewrite systems. Furthermore, we provide new results on uncurrying for relative termination and for dependency pairs.
11.1. Introduction
In recent years, the way to prove termination of term rewrite systems (TRSs) has changed. Current termination tools no longer search for a single reduction order containing the rewrite relation. Instead, they combine various termination techniques in a modular way, resulting in large and tree-like termination proofs, where at each node a speciﬁc technique is applied.
On the one hand, this combination makes termination tools more powerful. On the other hand, it makes them more complex and error-prone. It is regularly demonstrated that we cannot blindly trust the output of termination provers. Every now and then, some prover delivers a faulty proof. Often, this is only detected if there is another prover giving a contradictory answer for the same input, as a manual inspection of proofs is infeasible due to their size.
The problem is solved by combining two systems. For a given TRS, we ﬁrst use a termination tool to automatically detect a termination proof (which may contain errors). Then, we use a highly trusted certiﬁer which checks whether the detected proof is indeed correct. In total, the combination yields a powerful and trustable workﬂow to prove termination.
To obtain a highly trustable certiﬁer, a common approach is to ﬁrst formalize the desired termination techniques once and for all (thereby ensuring their soundness) and then, for a given proof, check that the used techniques are applied correctly [12, 23, 104]. We formalized the dependency pair framework (DP framework) [42] and many termination techniques in our Isabelle/HOL [84] library IsaFoR [104] (in the remainder

112 Chapter 11. Generalized and Formalized Uncurrying
we just write Isabelle, instead of Isabelle/HOL). From IsaFoR, we code-extract CeTA, an automatic certiﬁer for termination proofs.
In this paper, we present one of the latest additions to IsaFoR: the formalization of uncurrying, as described in [53]. However, we did not only formalize uncurrying, but also generalized it. Furthermore, we found a gap in one of the original proofs, which we could fortunately close.
Note that all the proofs that are presented (or omitted) in the following, have been formalized as part of IsaFoR. Hence, in this paper, we merely give sketches of our “real” proofs. Our goal is to show the general proof outlines and help to understand the full proofs. The library IsaFoR with all formalized proofs, the executable certiﬁer CeTA, and all details about our experiments are available at CeTA’s website: http://cl-informatik. uibk.ac.at/software/ceta.
The paper is structured as follows. In Sect. 11.2, we shortly recapitulate some required notions of term rewriting. Afterwards, in Sect. 11.3, we describe applicative rewriting, give an overview of approaches using uncurrying for proving termination, and present our generalization of uncurrying for TRSs. Then, in Sect. 11.4, we show how to lift uncurrying to the DP framework. We present heuristics and our experiments in Sect. 11.5, before we conclude in Sect. 11.6.
11.2. Preliminaries
We assume familiarity with term rewriting [5]. Still, we recall the most important notions that are used later on. A (ﬁrst-order) term t over a set of variables V and a set of (function) symbols F is either a variable x ∈ V or a function symbol f ∈ F applied to argument terms f (t1, . . . , tn) where the arity of f is ar(f ) = n. A context C is a term containing exactly one hole P. Replacing P in a context C by a term t is denoted by C[t].
A rewrite rule is a pair of terms → r and a TRS R is a set of rewrite rules. The set of deﬁned symbols (of R) is DR = {f | f (. . .) → r ∈ R}. The rewrite relation (induced by R) →R is the closure under substitutions and contexts of R, i.e., s →R t iﬀ there is a context C, a rewrite rule → r ∈ R, and a substitution σ such that s = C[ σ] and t = C[rσ]. A term t is root-stable w.r.t. R iﬀ there is no derivation t →∗R σ for some
→ r ∈ R and substitution σ. We say that a term t is terminating w.r.t. R (SNR(t)) if it cannot start an inﬁnite derivation t = t1 →R t2 →R t3 →R · · · . A TRS is terminating (SN(R)) iﬀ all terms are terminating w.r.t. R. A TRS R is terminating relative to a TRS S iﬀ there is no inﬁnite R ∪ S-derivation with inﬁnitely many R-steps.
11.3. Applicative Rewriting and Uncurrying
An applicative term rewrite system (ATRS) is a TRS over an applicative signature F = {◦} ∪ C, where ◦ is a unique binary symbol (the application symbol) and all symbols in C are constants. ATRSs can be used to encode many higher-order functions without explicit abstraction as ﬁrst-order TRSs. In the remainder we use ◦ as an inﬁx-symbol which associates to the left (s ◦ t ◦ u = (s ◦ t) ◦ u). In examples we omit ◦ whenever this increases readability.

11.3. Applicative Rewriting and Uncurrying

113

Example 11.1. The following ATRS R (a variant of [53, Example 7], replacing addition by subtraction) contains the map function (which applies a function to all arguments of a list) and subtraction on Peano numbers in applicative form.

1: sub 0 → K 0 2: sub x 0 → x 3: sub x x → 0 4: sub (s x) (s y) → sub x y

5: K x y → x 6: map z nil → nil 7: map z (cons x xs) → cons (z x) (map z xs)

Proving termination of ATRSs is challenging without dedicated termination techniques (e.g., for reduction orders, we cannot interpret sub ◦ x ◦ y as x, since sub is a constant and not binary).
Until now, there have at least been three approaches to tackle this problem. All of them try to uncurry a TRS such that, for example, Rule 4 from above becomes sub(s(x), s(y)) → sub(x, y).
To distinguish the three approaches, we need the following deﬁnitions:

Deﬁnition 11.2. A term t is head variable free iﬀ t does not contain a subterm of the form x ◦ s where x is a variable. The applicative arity of a constant f in an ATRS R (aaR(f )) is the maximal number n, such that f ◦ t1 ◦ · · · ◦ tn occurs as a subterm in R. Uncurrying an application f ◦ t1 ◦ · · · ◦ tn with aaR(f ) = n yields the term f (t1, . . . , tn). A term t is proper w.r.t. aaR iﬀ t is a variable or t = f ◦ t1 ◦ · · · ◦ tn where aaR(f ) = n and each ti is proper.
The oldest of the three approaches is from [59]. It requires that all terms in a TRS are proper w.r.t. aaR, and shows that then termination of R is equivalent to termination of the TRS obtained by uncurrying all terms of R. Since proper terms do not contain any partial applications, the application symbol is completely eliminated by uncurrying. However, requiring proper terms is rather restrictive: Essentially, it is demanded that the TRS under consideration is a standard ﬁrst-order TRS which is just written in applicative form. For example, the approach is not applicable to Example 11.1, since there is a head variable in the right-hand side of Rule 7 (z ◦ x) and sub as well as K are applied to a single argument in Rule 1, even though aaR(sub) = 2 and aaR(K) = 2.
The second approach was given in [41, 101]. Here, the same preconditions as in [59] apply, but the results are extended to innermost rewriting and to the DP framework. The latter has the advantage, that only the current subproblem has to satisfy the preconditions. For example, when treating the dependency pair

map ◦ z ◦ (cons ◦ x ◦ xs) → map ◦ z ◦ xs

(1)

for the recursive call of map, we can perform uncurrying (since there are no usable rules and (1) satisﬁes the preconditions). Moreover, in [101] uncurrying is combined with the reduction pair processor to further relax the preconditions.
The third approach is given in [53]. Here, the preconditions for uncurrying have been reduced drastically as only the left-hand sides of the TRS R must be head variable free. In return, we have to η-saturate R and add uncurrying rules. Moreover, for each constant f with aaR(f ) = n we obtain n new function symbols f1, . . . , fn of arities 1, 2, . . . , n which handle partial applications.
Example 11.3. When η-saturating the TRS R of Example 11.1, we have to add the rule sub ◦ 0 ◦ y → K ◦ 0 ◦ y. The uncurried TRS consists of the following rules:

114 Chapter 11. Generalized and Formalized Uncurrying

8: sub1(0) → K1(0) 9: sub2(0, y) → K2(0, y) 10: sub2(x, 0) → x 11: sub2(x, x) → 0

12: sub2(s1(x), s1(y)) → sub2(x, y) 13: K2(x, y) → x 14: map2(z, nil) → nil 15: map2(z, cons2(x, xs)) → cons2(z ◦ x, map2(z, xs))

Moreover, we have to add the following uncurrying rules:

16: s ◦ x → s1(x) 17: K ◦ x → K1(x) 18: K1(x) ◦ y → K2(x, y) 19: sub ◦ x → sub1(x) 20: sub1(x) ◦ y → sub2(x, y)

21: cons ◦ x → cons1(x) 22: cons1(x) ◦ y → cons2(x, y) 23: map ◦ x → map1(x) 24: map1(x) ◦ y → map2(x, y)

Also [53] gives an extension to the DP framework. To summarize, the traditional technique of uncurrying of [59] is completely subsumed by [41, 101], but [41, 101] and [53] are incomparable. The advantage of [41, 101] is that the generated TRSs and DP problems are smaller, and that uncurrying is also available in a combination with the reduction pair processor, whereas [53] supports head variables (see [101, Chap. 6] for a more detailed comparison). Since [53] is used in more termination tools (it is used in at least Jambox [31] and TTT2 [67] whereas we only know of AProVE [39] that implements all uncurrying techniques of [41, 101]), we incorporated the techniques of [53] in our certiﬁer CeTA. During our formalization we have

• detected a gap in a proof of [53] which could not directly be closed without adding further preconditions to one of the main results,

• generalized the technique of uncurrying which now entails the result of [53] even without adding any additional precondition, and

• generalized the technique of freezing from [53].

The structure in [53] is as follows. First, uncurrying is developed for TRSs over applicative signatures {◦} ∪ C. Then, it is extended to DP problems, introducing a second application symbol ◦ that may only occur at root-positions of P and is not uncurried at all. Finally, freezing is applied to uncurry applications of ◦ .
Following this structure, we ﬁrst fully formalized uncurrying on TRSs. However, in the extension to DP problems there is a missing step which is illustrated in more detail in Example 11.14 on page 119. The main problem is that signature restrictions on DP problems are in general unsound.
To ﬁll the gap, one option is to use the results of [97] about signature restrictions, which can however only be applied if R is left-linear. This clearly weakens the applicability of uncurrying, e.g., Example 11.1 is not left-linear.
Alternatively, one can try to perform uncurrying without restricting to applicative signatures. This is what we did. All uncurrying techniques that we formalized work on terms and TRSs over arbitrary signatures.
The major complication is the increase of complexity in the cases that have to be considered. For example, using an applicative signature, we can assume that every term is of the form x ◦ t1 ◦ · · · ◦ tn or f ◦ t1 ◦ · · · ◦ tn where n ∈ N, x is a variable, and f is a constant. Generalizing this to arbitrary signatures we have to consider the two cases x ◦ t1 ◦ · · · ◦ tn and f (s1, . . . , sm) ◦ t1 ◦ · · · ◦ tn instead, where f is an m-ary symbol. Hence,

11.3. Applicative Rewriting and Uncurrying

115

when considering a possible rewrite step, we also have to consider the new case that the step is performed in some si.
We not only generalized uncurrying to work for arbitrary signatures and relative rewriting, but also to a free choice of the applicative arity aa(f ). This is in contrast to [53], where the applicative arity is ﬁxed by Deﬁnition 11.2. We will elaborate on this diﬀerence after presenting our main theorem.

Deﬁnition 11.4 (Symbol maps and applicative arity). Let F be a signature. A symbol

map is a mapping π : F → [F] from symbols to non-empty lists of symbols. It is injective

if for all f and g, π(f ) contains no duplicates, π(f ) does not contain ◦, and whenever

f = g then π(f ) and π(g) do not share symbols. If π(f ) = [f0, . . . , fn], then the applicative

arity aaπ (f

of f w.r.t. π is ) −. n, where x −.

aaπ(f ) = n. y = max(x −

The applicative arity y, 0), for t = f (s1, . . .

of a , sm)

term ◦ t1 ◦ ·

is deﬁned · · ◦ tn and

by aaπ(t) = is undeﬁned

otherwise.

Intuitively, if π(f ) = [f0, . . . , fn] then every application of f on i n arguments t1, . . . , ti will be fully uncurried to fi(t1, . . . , ti). If more than n arguments are applied, then we obtain fn(t1, . . . , tn) ◦ tn+1 ◦ . . . ◦ ti. A symbol map containing an entry for f , uniquely determines the applicative arity n as well as the names of the (partial) applica-
tions f0, . . . , fn of f . In the following we assume a ﬁxed symbol map π and just write aa(f ) and aa(t) instead
of aaπ(f ) and aaπ(t), respectively. Additionally, we assume that π(f ) = [f0, . . . , faa(f)] where in examples we write f instead of f0. Now we can deﬁne the uncurrying TRS w.r.t. π.

Deﬁnition 11.5. The uncurrying TRS U contains the rule

fk(x1, . . . , xm, y1, . . . , yk) ◦ yk+1 → fk+1(x1, . . . , xm, y1, . . . , yk+1)

for every f ∈ F with ar(f ) = m and aa(f ) = n, and every k < n. The variables x1, . . . , xm, y1, . . . , yk+1 are pairwise distinct.
In [53], terms are uncurried by computing the unique normal form w.r.t. U. For our formalization we instead used the upcoming uncurrying function for the following two reasons: First, we do not have any results about conﬂuence of TRSs. Hence, to even deﬁne the normal form w.r.t. U would require to formalize several additional lemmas which show that every term has exactly one normal form. This would be quite some eﬀort which we prefer to avoid. The second reason is eﬃciency. When certifying the application of uncurrying in large termination proofs, we have to compute the uncurried version of a term. It is just more eﬃcient to use a function which performs uncurrying directly, than to compute a normal form w.r.t. a TRS where possible redexes have to be searched, etc.

Deﬁnition 11.6. The uncurrying function · on terms is deﬁned as

• x ◦ t1 ◦ · · · ◦ tn = x ◦ t1 ◦ · · · ◦ tn
• f (s1, . . . , sm) ◦ t1 ◦ · · · ◦ tn = fk( s1 , . . . , sm , t1 , . . . , tk ) ◦ tk+1 ◦ · · · ◦ tn where k = min(n, aa(f ))

It is homomorphically extended to operate on rules, TRSs, and substitutions.

116 Chapter 11. Generalized and Formalized Uncurrying
We establish the link between U and · in the following lemma which generalizes the corresponding results in [53].
Lemma 11.7.
• if k < aa(f ) and ar(f ) = m then fk(s1, . . . , sm+k) ◦ t →U fk+1(s1, . . . , sm+k, t)
• if k + n aa(f ) and ar(f ) = m then fk(s1, . . . , sm+k) ◦ t1 ◦ . . . ◦ tn →∗U fk+n(s1, . . . , sm+k, t1, . . . , tn)
• s ◦ t1 ◦ · · · ◦ tn →∗U s ◦ t1 ◦ · · · ◦ tn
• if aa(s) = 0 or aa(s) is undeﬁned then s ◦ t1 ◦ · · · ◦ tn = s ◦ t1 ◦ · · · ◦ tn
• s · σ →∗U s · σ
• if t is head variable free then s · σ = s · σ
The last two results show how uncurrying can be exchanged with applying substitutions. As we will often need the equality · σ = · σ for left-hand sides , it is naturally to demand that left-hand sides are head variable free.
Deﬁnition 11.8. A TRS is left head variable free if all left-hand sides are head variable free.
Termination of R ∪ U does not suﬃce to conclude termination of R, cf. [53, Example 13]. The reason is that ﬁrst we have to η-saturate R.
Deﬁnition 11.9. A TRS R is η-closed iﬀ for every rule → r with aa( ) > 0 there is a rule ◦ x → r ◦ x ∈ R where x is fresh w.r.t. → r. The η-saturation Rη of R is obtained by adding new rules ◦ x → r ◦ x until the result is η-closed.
The upcoming theorem is the key to use uncurrying for termination proofs. It allows to simulate one R-step by many steps in the uncurried system.
Theorem 11.10. Let R be η-closed and left head variable free. Let there be no left-hand side of R which is a variable. If s →R t then s →+R ∪U t . Proof. Let s = C[ σ] →R C[rσ] = t where → r ∈ R. We show s →+R ∪U t by induction on the size of C.
• If C = f (s1, . . . , D, . . . , sm) ◦ t1 ◦ · · · ◦ tn for some f = ◦ then by the induction hypothesis we know that D[ σ] →+R ∪U D[rσ] . Moreover,
s = f (s1, . . . , D[ σ], . . . , sm) ◦ t1 ◦ · · · ◦ tn = fk( s1 , . . . , D[ σ] , . . . , sm , t1 , . . . , tk ) ◦ tk+1 ◦ · · · ◦ tn →+R ∪U fk(. . . , D[rσ] , . . . , sm , t1 , . . . , tk ) ◦ tk+1 ◦ · · · ◦ tn = f (s1, . . . , D[rσ], . . . , sm) ◦ t1 ◦ · · · ◦ tn =t
where k = min(n, aa(f )).

11.3. Applicative Rewriting and Uncurrying

117

• If C = t0 ◦ D ◦ t1 ◦ · · · ◦ tn then by the induction hypothesis we know D[ σ] →+R ∪U D[rσ] . If aa(t0) = 0 or aa(t0) is undeﬁned then
s = t0 ◦ D[ σ] ◦ t1 ◦ · · · ◦ tn = t0 ◦ D[ σ] ◦ t1 ◦ · · · ◦ tn →+R ∪U t0 ◦ D[rσ] ◦ t1 ◦ · · · ◦ tn = t0 ◦ D[rσ] ◦ t1 ◦ · · · ◦ tn =t
Otherwise, aa(t0) > 0 and hence, t0 = f (s1, . . . , sm) ◦ sm+1 ◦ · · · ◦ sm+k where k < aa(f ). It follows that
s = f (s1, . . . , sm) ◦ sm+1 ◦ · · · ◦ sm+k ◦ D[ σ] ◦ t1 ◦ · · · ◦ tn = fk+1+n (. . . , sm+k , D[ σ] , t1 , . . . , tn ) ◦ tn +1 ◦ · · · ◦ tn →+R ∪U fk+1+n (. . . , sm+k , D[rσ] , t1 , . . . , tn ) ◦ tn +1 ◦ · · · = f (s1, . . . , sm) ◦ sm+1 ◦ · · · ◦ sm+k ◦ D[rσ] ◦ t1 ◦ · · · ◦ tn =t
where n = min(aa(f ) − k − 1, n).
• If C = P ◦ t1 ◦ · · · ◦ tn and n = 0 ∨ aa( ) = 0 then
s = · σ ◦ t1 ◦ · · · ◦ tn = · σ ◦ t1 ◦ · · · ◦ tn = · σ ◦ t1 ◦ · · · ◦ tn → R r · σ ◦ t1 ◦ · · · ◦ tn →∗U r · σ ◦ t1 ◦ · · · ◦ tn →∗U r · σ ◦ t1 ◦ · · · ◦ tn =t
since is head variable free and if n = 0 then aa( σ) = aa( ) = 0.
• If C = P ◦ t1 ◦ · · · ◦ tn with n > 0 and aa( ) > 0 then → r = ◦ x → r ◦ x ∈ R since R is η-closed. Moreover, by changing σ to δ = σ {x/t1} we achieve s = · σ ◦ t1 ◦ · · · ◦ tn = δ ◦ t2 ◦ · · · ◦ tn = D[ δ] and r = r · σ ◦ t1 ◦ · · · ◦ tn = r δ ◦ t2 ◦ · · · ◦ tn = D[r δ] for the context D = P ◦ t2 ◦ · · · tn which is strictly smaller than C. Hence, the result follows by the induction hypothesis.
• If C = P ◦ t1 ◦ · · · ◦ tn with n > 0 and aa( ) is undeﬁned then = x ◦ 1 ◦ · · · ◦ k with k ≥ 0. But if k > 0 then is not head variable free and if k = 0 then R contains a variable as left-hand side. In both cases we get a contradiction to the preconditions in the theorem.
Note that the condition that the left-hand sides of R are not variables is new in comparison to [53]. Nevertheless, in the following corollary we can drop this condition, since otherwise R is not terminating anyway.
Corollary 11.11. If Rη is left head variable free then termination of Rη ∪ U implies termination of R.

118 Chapter 11. Generalized and Formalized Uncurrying

When using uncurrying for relative termination of R/S, it turns out that the new condition on the left-hand sides can only be ignored for R – since otherwise relative termination of R / S ∪ U does not hold – but not for S.
Corollary 11.12. If Rη ∪ Sη is left head variable free and the left-hand sides of S are not variables, then relative termination of Rη / Sη ∪ U implies relative termination of R/S .
Example 11.13. Let R = {f ◦ f ◦ x → f ◦ x} and S = {x → f ◦ x}. Then R/S is not relative terminating since f ◦f ◦x →R f ◦x →S f ◦f ◦x →R . . . is an inﬁnite R∪S-derivation with inﬁnitely many R-steps.
For π(f) = [f, f1, f2] we have Rη = R, Sη = S, Rη = {f2(f, x) → f1(x)}, Sη = {x → f1(x)}, and U = {f ◦ x → f1(x), f1(x) ◦ y → f2(x, y)}. It is easy to see that Rη / Sη ∪ U is relative terminating by counting the number of f-symbols. Since both Rη and Sη are head variable free, we have shown that Corollary 11.12 does not hold if one would drop the new variable condition on S.
As already mentioned, Corollary 11.11 generalizes the similar result of [53, Theorem 16] in two ways: ﬁrst, there is no restriction to applicative signatures, and second, one can freely choose the applicative arities. Since in principle the choice of π does not matter (uncurrying preserves termination for every choice of π), we can only heuristically determine whether the additional freedom increases termination proving power and therefore refer to our experiments in Sect. 11.5.

11.4. Uncurrying in the Dependency Pair Framework

The DP framework [42] facilitates modular termination proofs. Instead of single TRSs, we consider DP problems (P, R), consisting of two TRSs P and R where elements of P are often called pairs to distinguish them from the rules of R. The initial DP problem for a TRS R is (DP(R), R), where DP(R) = {f (s1, . . . , sn) → g (t1, . . . , tm) | f (s1, . . . , sn) → C[g(t1, . . . , tm)] ∈ R, g ∈ DR} is the set of dependency pairs of R, incorporating a fresh tuple symbol f for each deﬁned symbol f . The initial DP problem is also a standard DP problem, i.e., root symbols of pairs do not occur elsewhere in P or R.1 A (P, R)-chain
is a possibly inﬁnite derivation of the form:

s1σ1 →P t1σ1 →∗R s2σ2 →P t2σ2 →∗R s3σ3 →P · · ·

()

where si → ti ∈ P for all i > 0. If additionally every tiσi is terminating w.r.t. R, then the chain is minimal. A DP problem (P, R) is called ﬁnite [42], if there is no minimal inﬁnite (P, R)-chain. Proving ﬁniteness of a DP problem is done by simplifying (P, R) using so called processors recursively. A processor transforms a DP problem into a new DP problem. The aim is to reach a DP problem with empty P-component (such DP problems are trivially ﬁnite). To conclude ﬁniteness of the initial DP problem, the applied processors need to be sound. A processor Proc is sound whenever for all DP problems (P, R) we have that ﬁniteness of Proc(P, R) implies ﬁniteness of (P, R).
In the following we explain how uncurrying is used as processor in the DP framework. In Sect. 11.3 it was already mentioned that in [53] the notion of applicative TRS was lifted to applicative DP problem by allowing a new binary application symbol ◦ (where

1Several termination provers only work on standard DP problems.

11.4. Uncurrying in the Dependency Pair Framework

119

we sometimes just write in examples). The symbol ◦ naturally occurs as tuple symbol of ◦.
To prove soundness of the uncurrying processor, in [53] it is assumed that there is a minimal (P, R)-chain s1σ1 →P t1σ1 →∗R s2σ2 →P · · · , which is converted into a minimal ( P , Rη ∪ U)-chain by reusing the results for TRSs. However, there is a gap in this reasoning. Right in the beginning it is silently assumed that all terms siσi and tiσi have tuple symbols as roots and that their arguments are applicative terms, i.e., terms over an applicative signature {◦} ∪ C. Without this assumption it is not possible to apply the
results of uncurrying for TRSs, since those are only available for applicative terms in [53].
The following variant of [97, Example 14] shows that in general restricting substitutions in chains to an applicative signature {◦} ∪ C is unsound.

Example 11.14. Consider the applicative and standard DP problem (P, R) where P = {g (f x y z z u v) → g (f x y x y x (h y u))} and R contains the rules:

a→b a→c hxx→hxx

f a x2 x3 x4 x5 → f a x2 x3 x4 x5 f x1 a x3 x4 x5 → f x1 a x3 x4 x5 f (y z) x2 x3 x4 x5 → f (y z) x2 x3 x4 x5 f x1 (y z) x3 x4 x5 → f x1 (y z) x3 x4 x5

There is a minimal (P, R)-chain taking si = g (f x y z z u v), ti = g (f x y x y x (h y u)), and σi = {x/k(a), y/k(b), z/k(b), u/k(c), v/h (k(b)) (k(c))} where k is a unary symbol. However, there is no minimal (P, R)-chain using substitutions over the signature {◦} ∪ C,
regardless of the choice of constants C.

Since our generalizations in Sect. 11.3 do not have any restrictions on the signature, we
were able to correct the corresponding proofs in [53] such that the major theorems are still valid.2 It follows the generalization of [53, Theorem 33].

Theorem 11.15. The uncurrying processor U1 is sound where U1(P, R) =

( P , Rη ∪ U ) if P ∪ Rη is left head variable free and π is injective,

(P, R)

otherwise.

Proof. The proof mainly uses the results from the previous section. We assume an inﬁnite minimal (P, R)-chain s1σ1 →P t1σ1 →∗R s2σ2 →P t2σ2 →∗R · · · and construct an inﬁnite minimal ( P , Rη ∪ U )-chain as follows.
We achieve siσi = si · σi and ti · σi →∗U tiσi since P is left head variable free. Moreover, using tiσi →∗R si+1σi+1 and Theorem 11.10 we conclude tiσi →∗Rη ∪U
si+1σi+1 . Here, the condition that Rη must not contain variables as left-hand sides is ensured by the minimality of the chain: if x → r ∈ Rη then SNR(tiσ) does not hold. Hence, we constructed a ( P , Rη ∪ U )-chain as
siσi = si · σi → P ti · σi →∗U tiσi →∗Rη ∪U si+1σi+1
for all i. To ensure that the chain is minimal it is demanded that π is injective. Otherwise, two diﬀerent symbols can be mapped to the same new symbol which clearly can introduce nontermination. The structure of the proof that minimality is preserved is similar to the one in [53] and we just refer to IsaFoR for details.
2After the authors of [53] where informed of the gap, they independently developed an alternative ﬁx, which is part of an extended, but not yet published version of [53].

120 Chapter 11. Generalized and Formalized Uncurrying

The uncurrying processor of Theorem 11.15 generalizes [53, Theorem 33] in three ways: the signature does not have to be applicative, we can freely choose the applicative arity via π, and we can freely choose the application symbol. The last generalization lets Theorem 11.15 almost subsume the technique of freezing [53, Corollary 40] which is used to uncurry ◦ .
Deﬁnition 11.16 (Freezing [53]). A simple freeze A is a subset of F.3 Freezing is applied on non-variable terms as follows

A(f (t1, . . . , tn)) =

f (t1, . . . , tn) f g(s1, . . . , sm, t2, . . . , tm)

if n = 0 or f ∈/ A if t1 = g(s1, . . . , sm) and f ∈ A

where each f g is a new symbol. It is homomorphically extended to rules and TRSs. The freezing DP processor is deﬁned as A(P, R) =

(A(P), R)  (P, R)

if (P, R) is a standard DP problem where for all s → f (t1, . . . , tn) ∈ P with f ∈ A, both t1 ∈/ V and all instances of t1 are root-stable, otherwise.

In [53, Theorem 39], it is shown that freezing is sound.

Example 11.17. In the following we use numbers to refer to rules from previous examples. We consider the DP problem (P, R) where P = {sub (s x) (s y) → sub x y} and R = {2–4}. Uncurrying ◦ with π(s) = [s, s1], π(sub) = [sub, sub1, sub2], π(0) = [0], and π( ) = [ ] yields the DP problem ( P , Rη ∪ U ) where P = {sub1(s1(x)) s1(y) → sub1(x) y} and Rη ∪ U consists of {10–12, 16, 19, 20}.
Afterwards we uncurry the resulting DP problem using ◦ as application symbol and π where π(sub1) = [sub1, − ] and π(f ) = [f ] for all other symbols. We obtain (P , R ) where P = {s1(x) − s1(x)) → x − y} and R = Rη ∪ U ∪ {sub1(x) y → − (x, y)}. Note that freezing returns nearly the same DP problem. The only diﬀerence is that uncurrying produces the additional rule sub1(x) y → x − y which we do not obtain via freezing. However, since this rule is not usable it also does not harm that much.
Moreover, uncurrying sometimes is applicable where freezing is not. If we would have started with the DP problem (P, R ) where R = {1–5} then uncurrying would result in ( P , Rη ∪ U ) where Rη ∪ U = {8–13, 16–20}. On this DP problem freezing is not applicable (the instances of sub1(x) in the right-hand side of the only pair in
P are not root-stable due to Rule 8). Nevertheless, one can uncurry ◦ , resulting in (P , Rη ∪ U ∪ Rnew) where Rnew = {sub1(x) y → x − y, 0 − y → K1(0) y}. Note that the uncurrying of ◦ transformed a standard DP problem into a non-standard one, as − occurs as root of a term in P , but also within Rnew.
Whenever freezing with A = {◦ } is applicable, then also uncurrying of ◦ is possible: the condition t1 ∈/ V in Deﬁnition 11.16 implies that P ∪ Rη is left head variable free. The only diﬀerence is that uncurrying produces more rules than freezing, namely the uncurrying rules and the uncurried rules of those rules which have to be added for the η-saturation. However, if freezing is applicable then none of these additional rules are
3In [53] one can also specify an argument position for each symbol. This can be simulated by permuting the arguments accordingly.

11.4. Uncurrying in the Dependency Pair Framework

121

usable.4 Hence, all techniques which only consider the usable rules (like the reduction pair processor) perform equally well, no matter whether one has applied freezing or uncurrying. Still, one wants to get rid of the additional rules, especially since they are also the reason why standard DP problems are transformed into non-standard ones.
In Example 11.17 we have seen that sometimes uncurrying of tuple symbols is applicable where freezing is not. Thus, to have the best of both techniques we devised a special uncurrying technique for tuple symbols which fully subsumes freezing without the disadvantage of U1: if freezing is applicable then standard DP problems are transformed into standard DP problems by the new technique.
Before we describe the new uncurrying processor formally, we shortly list the diﬀerences to the uncurrying processor of Theorem 11.15:
• Since the task is to uncurry tuple symbols, we restrict the applicative arities to be at most one. Moreover, uncurrying is performed only on the top-level. Finally, the application symbol may be of arbitrary non-zero arity.

• Rules that have to be added for the η-saturation and the uncurrying rules are added as pairs (to the P-component), and not as rules (to the R-component).

• If freezing is applicable, we do neither add the uncurrying rules nor do we apply η-saturation.
Example 11.18. We continue with the DP problems of Example 11.17. If one applies the special uncurrying processor on ( P , Rη ∪ U) then one obtains
(P , Rη ∪ U ) which is the same as A( P , Rη ∪ U ) for A = {◦ }. And if one applies the special uncurrying processor on ( P , Rη ∪U ) then one obtains
the standard DP problem (P ∪ Rnew, Rη ∪ U ).
Deﬁnition 11.19. Let ◦ be an n-ary application symbol where n > 0. Let π be an injective symbol map where π(f ) ∈ {[f ], [f, f ]} for all f . The top-uncurrying function
· maps terms to terms. It is deﬁned as t =

f (s1, . . . , sm, t2, . . . , tn) if t = ◦ (f (s1, . . . , sm), t2, . . . , tn) and π(f ) = [f, f ] t otherwise

and is homomorphically extended to pairs, rules, and substitutions. The top-uncurrying rules are deﬁned as

U t = {◦ (f (x1, . . . , xm), y2, . . . , yn) → f (x1, . . . , xm, y2, . . . , yn) | π(f ) = [f, f ]}

Then the top-uncurrying processor is deﬁned as top(P, R) =

 ( Pη 

∪ U?t, R)

(P, R)

if ◦ is not deﬁned w.r.t. R and for all s → t ∈ Pη : s, t ∈/ V, s = ◦ (x, s2, . . . , sn), and the root of t is not deﬁned w.r.t. R otherwise

where U?t = ∅ and Pη = P if for all s → ◦ (t1, . . . , tn) ∈ P and σ the term t1σ is root-stable, and U?t = U t and Pη = P ∪ {◦ ( , x2, . . . , xn) → ◦ (r, x2, . . . , xn) | → r ∈ R, root( ) = g, π(g) = [g, g ]}, otherwise. Here, x2, . . . , xn are distinct fresh variables that do not occur in → r.
4In detail: a technique that can detect that instances of a subterm of a right-hand side of P are rootstable can also detect that the additional rules are not usable.

122 Chapter 11. Generalized and Formalized Uncurrying

Theorem 11.20. The top-uncurrying processor top is sound.

Proof. The crucial part is to prove that whenever t = f (t1, . . . , tm) →∗R s, f ∈/ DR, t

is an instance of a right-hand side of P, and SNR(t), then

t

→∗ Pη ∪U?t∪R

s

where

Pη ∪ U?t-steps are root steps and all terms in this derivation are terminating w.r.t. R.

Using this result, the main result is established as follows. Assume there is an inﬁnite

minimal (P, R)-chain. Then every step sσ →P tσ →∗R s σ in the chain is transformed

as follows. Since s → t ∈ P, we conclude that tσ = f (t1σ, . . . , tmσ) where f ∈/ DR and

SNR(tσ). Hence, using the crucial step we know that

by case analysis on t one can show that

t

σ

→∗ U?t

tσ

tσ

→∗ Pη ∪U?t∪R

sσ

. Moreover,

via root reductions, and similarly,

by additionally using the restrictions on s one derives sσ = s σ. Hence,

sσ = s σ → P

t

σ

→∗ U?t

tσ

→∗ Pη ∪U?t∪R

sσ

where all terms in this derivation right of → P are terminating w.r.t. R and where all

Pη ∪ U?t-steps are root reductions. Thus, we can turn the root reductions into pairs, resulting in an inﬁnite minimal ( Pη ∪ U?t, R)-chain.

To prove the crucial part we perform induction on the number of steps where the base

case – no reductions – is trivial. Otherwise, t = f (t1, . . . , tm) →∗R u →R s. Using SNR(t) we also know SNR(u) and since f ∈/ DR we know that u = f (u1, . . . , um) and ti →∗R ui

for all 1 i m. Moreover, s = f (s1, . . . , sm) and s is obtained from u by a reduction

ui →R si for some 1 i m. Hence, we may apply the induction hypothesis and

conclude

t

→∗ Pη ∪U?t∪R

u.

It remains to simulate the reduction u →R s. The simulation is easy if f = ◦ , since

then u = u →R s = s . Otherwise, f = ◦ and m = n. We again ﬁrst consider

the easy case where ui →R si for some i > 1. Then an easy case analysis on u1 yields

u →R s since u and s are uncurried in the same way (since u1 = s1). Otherwise,

u = ◦ (u1, . . . , um), s = ◦ (s1, u2, . . . , um) and u1 →R s1. If u1 →R s1 is a reduction below

the root then both s and t are uncurried in the same way and again u →R s is easily

established. If however u1 = σ → rσ = s1 for some rule → r ∈ R then we know that u1

is not root-stable and hence also t1 is not root-stable. As t = ◦ (t1, . . . , tn) is an instance

of a right-hand side of P we further know that there is a pair s → ◦ (t1, . . . , tn) ∈ P where t1 = t1σ. Since t1σ is not root-stable U?t = U t and Pη ⊇ {◦ ( , x2, . . . , xn) →

◦ (r, x2, . . . , xn) | → r ∈ R, root( ) = g, π(g) = [g, g ]}. Let = g( 1, . . . , k). If

π(g) = [g] then

u = ◦ (g( 1, . . . , k)σ, u2, . . . , un)

= ◦ (g( 1, . . . , k)σ, u2, . . . , un)

→R ◦ (rσ, u2, . . . , un)

→∗ U?t

◦ (rσ, u2, . . . , un)

= s.

And otherwise, π(g) = [g, g ]. Hence, ◦ ( , x2, . . . , xn) → ◦ (r, x2, . . . , xn) ∈ Pη. We deﬁne δ = σ {x2/u2, . . . , xn/un} and achieve

u = ◦ (g( 1, . . . , k)σ, u2, . . . , un) = g ( 1σ, . . . , kσ, u2, . . . , un) = g ( 1, . . . , k, x2, . . . , xn)δ = ◦ (g( 1, . . . , k), x2, . . . , xn) δ

11.5. Heuristics and Experiments

123

= ◦ ( , x2, . . . , xn) δ

→ Pη ◦ (r, x2, . . . , xn) δ

→∗ U?t

◦ (r, x2, . . . , xn)δ

= ◦ (rσ, u2, . . . , un)

= s.

Using that π is injective one can also show that termination of all terms in the derivation is guaranteed where we refer to our library IsaFoR for details.

Note that the top-uncurrying processor fully subsumes freezing since the step from
(P, R) to (A(P), R) using A = {f1, . . . , fn} can be simulated by n applications of top where in each iteration one chooses fi as application symbol and deﬁnes π(g) = [g, fig] for all g = fi. The following example shows that top is also useful where freezing is not applicable.

Example 11.21. Consider the TRS R where x ÷ y computes

x 2y

.

s(x) − s(y) → x − y 0−y →0 x−0 →x 0+y →y
s(x) + y → s(x + y)

double(x) → x + x double(0) → 0 double(s(x)) → s(s(double(x)))
0 ÷ s(y) → 0 s(x) ÷ s(y) → s((s(x) − double(s(y))) ÷ s(y))

Proving termination is hard for current termination provers. Let us consider the interesting DP problem (P, R) where P = {s(x) ÷ s(y) → (s(x) − double(s(y))) ÷ s(y)}. The problem is that one cannot use standard reduction pairs with argument ﬁlters since one has to keep the ﬁrst argument of −, and then the ﬁltered term of s(x) is embedded in the ﬁltered term of s(x) − double(s(y)). Consequently, powerful termination provers such as AProVE and TTT2 fail on this TRS.
However, one can uncurry the tuple symbol ÷ where π(−) = [−, − ], π(s) = [s, s ], and π(f ) = [f ], otherwise. Then the new DP problem (P , R) is created where P consists of the following pairs

(x − y) ÷ z → − (x, y, z)

− (s(x), s(y), z) → − (x, y, z)

s(x) ÷ y → s (x, y)

− (0, y, z) → 0 ÷ z

s (x, s(y)) → − (s(x), double(s(y)), s(y))

− (x, 0, z) → x ÷ z

where the subtraction is computed via the new pairs, and not via the rules anymore. The right column consists of the uncurried and η-saturated −-rules, and the left column contains the two uncurrying rules followed by the uncurried pair of P. Proving ﬁniteness
of this DP problem is possible using standard techniques: linear 0/1-polynomial interpre-
tations and the dependency graph suﬃce. Therefore, termination of the whole example
can be proven fully automatically by using a new version of TTT2 where top-uncurrying is integrated.

11.5. Heuristics and Experiments
The generalizations for uncurrying described in this paper are implemented in TTT2 [67]. To ﬁx the symbol map we used the following three heuristics:

124 Chapter 11. Generalized and Formalized Uncurrying
• π+ corresponds to the deﬁnition of applicative arity of [53]. More formally, π+(f ) = [f0, . . . , fn] where n is maximal w.r.t. all f (. . .) ◦ t1 ◦ · · · ◦ tn occurring in R. The advantage of π+ is that all uncurryings are performed.
• π± is like π+, except that the applicative arity is reduced whenever we would have to add a rule during η-saturation. Formally, π±(f ) = [f0, . . . , fn] where n = min(aaπ+(f ), min{k | f (. . .) ◦ t1 ◦ · · · ◦ tk → r ∈ R}).
• π− is almost dual to π+. Formally, π−(f ) = [f0, . . . , fn] where n is minimal w.r.t. all maximal subterms of the shape f (. . .) ◦ t1 ◦ · · · ◦ tn occurring in R. The idea is to reduce the number of uncurrying rules.
We conducted two sets of experiments to evaluate our work. Note that all proofs generated during our experiments are certiﬁed by CeTA (version 1.18). Our experiments were performed on a server with eight dual-core AMD Opteron® processors 885, running at a clock rate of 2.6 GHz and on 64 GB of main memory. The time limit for the ﬁrst set of experiments was 10 s (as in [53]), whereas the time limit for the second set was 5 s (TTT2’s time limit in the termination competition).
The ﬁrst set of experiments was run with a setup similar to [53]. Accordingly, as input we took the same 195 ATRSs from the termination problem database (TPDB). For proving termination, we switch from the input TRS to the initial DP problem and then repeat the following as often as possible: compute the estimated dependency graph, split it into its strongly connected components and apply the “main processor.” Here, as “main processor” we incorporated the subterm criterion and matrix interpretations (of dimensions one and two). Concerning uncurrying, the following approaches were tested: no uncurrying (none), uncurry the given TRS before computing the initial DP problem (trs), apply U1/U2 as soon as all other processors fail (where U2 is the composition of U1 and top). The results can be found in Table 11.1. Since on ATRSs, our generalization of uncurrying corresponds to standard uncurrying, it is not surprising that the numbers of the ﬁrst three columns coincide with those of [53] (modulo mirroring and a slight diﬀerence in the used strategy for trs). They are merely included to see the relative gain when using uncurrying on ATRSs.
With the second set of experiments, we tried to evaluate the total gain in certiﬁed termination proofs. Therefore, we took a restricted version of TTT2’s competition strategy that was used in the July 2010 issue of the international termination competition5 (called base strategy in the following). The restriction was to use only those termination techniques that where certiﬁable by CeTA before our formalization of uncurrying. Then, we used this base strategy to ﬁlter the TRSs (we did ignore all SRSs) of the TPDB (version 8.0). The result were 511 TRSs for which TTT2 did neither generate a termination proof nor a nontermination proof using the base strategy. For our experiments we extended the base strategy by the generalized uncurrying techniques using diﬀerent heuristics for the applicative arity. The results can be found in Table 11.2. It turned out, that the π− heuristic is rather weak. Concerning π±, there is at least one TRS that could not be proven using π+, but with π±. The total of 35 in the ﬁrst row of Table 11.2 is already reached without taking U1 into account. This indicates that in practice a combination of uncurrying as initial step (trs) and the processor U2, gives the best results. Finally, note that in comparison to the July 2010 termination competition (where TTT2 could generate 262 certiﬁable proofs), the number of certiﬁable proofs of TTT2 is increased by over
5http://termcomp.uibk.ac.at

11.6. Conclusions

125

Table 11.1.: Experiments as in [53]

subterm criterion matrix (dimension 1) matrix (dimension 2)

direct none trs
41 53 66 98 108 137

processor U1 U2
41 66 95 114 133 138

Table 11.2.: Newly certiﬁed proofs

π+ π± π−
total

direct trs
26 28 24
28

processor U1 U2
16 22 15 17 14 14
16 24

total
35 29 24 36

10 % using the new techniques. In these experiments, termination has been proven for 10 non-applicative TRSs where our generalizations of uncurrying have been the key to success.
11.6. Conclusions
This paper describes the ﬁrst formalization of uncurrying, an important technique to prove termination of higher-order functions which are encoded as ﬁrst-order TRSs. The formalization revealed a gap in the original proof which is now ﬁxed. Adding the newly developed generalization of uncurrying to our certiﬁer CeTA, increased the number of certiﬁable proofs on the TPDB by 10 %.

12. On the Formalization of Termination Techniques Based on Multiset Orderings
Publication details
Ren´e Thiemann, Guillaume Allais, and Julian Nagele. On the Formalization of Termination Techniques Based on Multiset Orderings. In Proceedings of the 23rd International Conference on Rewriting Techniques and Applications, volume 15 of LIPIcs, pages 339– 354. Schloss Dagstuhl – Leibniz-Zentrum fu¨r Informatik, 2012.
Abstract
Multiset orderings are a key ingredient in certain termination techniques like the recursive path ordering and a variant of size-change termination. In order to integrate these techniques in a certiﬁer for termination proofs, we have added them to the Isabelle Formalization of Rewriting. To this end, it was required to extend the existing formalization on multiset orderings towards a generalized multiset ordering. Afterwards, the soundness proofs of both techniques have been established, although only after ﬁxing some deﬁnitions.
Concerning eﬃciency, it is known that the search for suitable parameters for both techniques is NP-hard. We show that checking the correct application of the techniques— where all parameters are provided—is also NP-hard, since the problem of deciding the generalized multiset ordering is NP-hard.
12.1. Introduction
The multiset ordering has been invented in the ’70s to prove termination of programs [29]. It is an ingredient for important termination techniques like the multiset path ordering (MPO) [26], the recursive path ordering (RPO) [27], or recently [7,20] it has been used in combination with the size-change principle of [74], in the form of SCNP reduction pairs.
The original version of the multiset ordering w.r.t. some base ordering can be deﬁned as M ms N iﬀ it is possible to obtain N from M by replacing at least one element of M by several strictly smaller elements in N .
In other papers—like [7, 20, 92]—a generalization of the multiset ordering is used (denoted by gms). To deﬁne gms one assumes that in addition to there is a compatible non-strict ordering . Then gms is like ms, but in the multiset comparison it is additionally allowed to replace each element by a smaller one (w.r.t. ). To illustrate the diﬀerence between ms and gms, we take and as the standard orderings on polynomials over the naturals. Then gms is strictly more powerful than ms: for example,

128 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings
{{x + 1, 2y}} gms {{y, x}} since x + 1 x and 2y y, but {{x + 1, 2y}} ms {{y, x}} does not hold, since 2y = y and also 2y y as y can be instantiated by 0.
Note that gms is indeed used in powerful termination tools like AProVE [39]. Hence, for the certiﬁcation of termination proofs which make use of SCNP reduction pairs or RPOs which are deﬁned via gms, we need a formalization of this multiset ordering.
However, in the literature and in formalizations, often only ms is considered. And even those papers that use gms only shortly list the diﬀerences between ms and gms—if at all—and afterwards just assume that both multiset orderings have similar properties.
To change this situation, as one new contribution of this paper, we give the ﬁrst formalization of gms, and could indeed show that ms and gms behave quite similar. However, we also found one essential diﬀerence between both orderings. It is well known that deciding ms is easy, one just removes identical elements and afterwards has to ﬁnd for each element in the one set a larger element in the other. In contrast, we detected and proved that deciding gms is an NP-complete problem.
Note that the original deﬁnition of RPO misses a feature that is present in other orderings like polynomial interpretations where it is possible to compare variables against a least element: x 0. This feature was already integrated in other orderings like KBO [65], and it can also be integrated into RPO where one allows to compare x c if c is a constant of least precedence. For example, the internal deﬁnition of RPO in AProVE is the one of [92]—which is based on gms—with the additional inference rule of “x c”. As a second new contribution we formalized this RPO variant and ﬁxed its deﬁnition, as it turned out that it is not stable. Moreover, we show that the change from ms to gms increases the complexity of RPO: From [71] it is known that deciding whether two terms are in relation w.r.t. a given RPO or MPO is in P. However, if one uses the deﬁnitions of MPO and RPO in this paper which are based on gms, then the same decision problem becomes NP-complete.
As third new contribution we also give the ﬁrst formalization of SCNP reduction pairs where we could establish the main soundness theorem, although several deﬁnitions had to be ﬁxed. Again, we have shown that the usage of gms makes certiﬁcation for SCNP reduction pairs an NP-complete problem. As a consequence, the search for suitable SCNP reduction pairs and the problem whether two terms are in relation for a given SCNP reduction pair belong to the same complexity class, as they are both NP-complete.
All our formalizations are using the proof assistant Isabelle/HOL [84]. They are available within the IsaFoR library (Isabelle Formalization of Rewriting, [104]). The new parts of the corresponding proof checker CeTA—which is just obtained by applying the code generator of Isabelle [49] on IsaFoR—have been tested on the examples of the experiments performed in [92]. After ﬁxing some output bugs, eventually all proofs could be certiﬁed. Both IsaFoR and CeTA are freely available at:
http://cl-informatik.uibk.ac.at/software/ceta
The paper is structured as follows. We give preliminaries and the exact deﬁnitions for ms and gms in Sec. 12.2. Afterwards we discuss the formalization of gms in Sec. 12.3. Diﬀerent variants of RPO are discussed in Sec. 12.4. Here, we also show that certifying constraints for the RPO variant used in AProVE is NP-complete. In Sec. 12.5 we discuss the formalization of SCNP reduction pairs. Finally, in Sec. 12.6 we elaborate on our certiﬁed algorithms for checking whether two terms are in relation w.r.t. RPO or the orderings from an SCNP reduction pair, and we report on our experimental results.

12.2. Preliminaries

129

12.2. Preliminaries
We refer to [5] for basic notions and notations of rewriting. A signature is a set of symbols F = {f, g, F, G, . . . }, each associated with an arity. We write T (F, V) for the set of terms over signature F and set of variables V. We write V(t) for the set of variables occurring in t. A relation on T (F, V) is stable iﬀ it is closed under substitutions, and it is monotone, iﬀ it is closed under contexts. A term rewrite system (TRS) is a set of rules → r. The rewrite relation →R of a TRS R is the smallest stable and monotone relation containing R.
We write Id for the identity relation, and for each relation , let be its reﬂexive closure.
Deﬁnition 12.1 (Ordering pair, reduction pair). The pair ( , ) is an ordering pair (over carrier A) iﬀ is a quasi-ordering over A, is a transitive and well-founded relation over A, and and are compatible, i.e., ◦ ⊆ and ◦ ⊆ .
The pair ( , ) is a non-monotone reduction pair iﬀ ( , ) is an ordering pair over T (F, V) where both and are stable. If additionally is monotone, then ( , ) is a reduction pair, and if both and are monotone, then ( , ) is a monotone reduction pair.
Throughout this paper we only consider ﬁnite multisets {{x1, . . . , xn}} and we write P(A) for the set of all multisets with elements from A. For every function f : A → A and every multiset M ∈ P(A) we deﬁne the image of f on M as f [M ] = {{f (x) | x ∈ M }}.
Deﬁnition 12.2 (Multiset orderings). Let and be relations over A. We deﬁne the multiset ordering ( ms), the generalized multiset ordering ( gms), and the corresponding non-strict ordering ( gms ) over P(A) as follows: M1 ms / gms / gms M2 iﬀ there are Si and Ei such that M1 = S1 ∪ E1, M2 = S2 ∪ E2, and
• conditions i, ii, and iv are satisﬁed: M ms N
• conditions i, iii, and iv are satisﬁed: M gms N
• conditions i and iii are satisﬁed: M gms N
where conditions i–iv are deﬁned by:
(i) for each y ∈ S2 there is some x ∈ S1 with x y
(ii) E1 = E2
(iii) E1 = {{x1, . . . , xn}}, E2 = {{y1, . . . , yn}}, and xi yi for all 1 ≤ i ≤ n
(iv) S1 = ∅
Whenever Mi is split into Si ∪ Ei we call Si the strict part and Ei the non-strict part.
Note that gms indeed generalizes ms, since gms = ms if = Id .

130 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings
12.3. Formalization of the Generalized Multiset Ordering
Replacing condition ii by iii makes the formalization of gms a bit more involved than the one of ms: the simple operation of (multiset) equality E1 = E2 is replaced by demanding that both E1 and E2 can be enumerated in such a way that xi yi for all 1 ≤ i ≤ n. Note that instead of enumerations one can equivalently demand that there is a bijection f : E1 → E2 such that for all x ∈ E1 we have x f (x).
There is one main advantage of formalizing condition iii via enumerations instead of bijections. It will be rather easy to develop an algorithm deciding gms. However, using enumerations we also observe a drawback: the proof that gms and gms are compatible and transitive orderings will be harder than using bijections since composing bijections is easier than combining enumerations.
The formalization of the following (expected) properties for gms and gms has been rather simple.
Lemma 12.3. gms and gms have the following properties:
(i) the empty set is the unique minimum of gms and gms
(ii) if is reﬂexive then so is gms
(iii) if is irreﬂexive and compatible with then gms is irreﬂexive
(iv) if and are closed under an operation op then gms and gms are closed under op[·].
In contrast, the formalization of transitivity and compatibility was more tedious.
Lemma 12.4. If and are compatible and transitive then so are gms and gms.
Proof. For the sake of simplicity, we will work here with the deﬁnition of ( gms, gms) using bijections rather than enumerations: all technical details when using the representation with enumerations are available in IsaFoR, theory Multiset-Extension. Given that this lemma states four results that are pretty similar, we will only prove transitivity of
gms and let the reader see how the proof could be slightly modiﬁed in the other cases. Let M, N and P be three multisets such that M gms N (1) and N gms P (2). From (1) we get the partitions M = M ∪ E and N = N ∪ F and the bijection fMN : E → F . From (2) we get the partitions N = N ∪ F and P = P ∪ G and the bijection fNP : F → G. We deﬁne the multiset I as I = F ∩ F and claim that the partitions M = (M \ fM−1N [I]) ∪ fM−1N [I] and P = (P \ fNP [I]) ∪ fNP [I] and the bijection fNP ◦ fMN are the ones needed to prove M gms P . If p ∈ P \ fNP [I] then we have to ﬁnd some m ∈ M \ m ∈/ fM−1N [I] satisfying m p. We distinguish two cases. In the ﬁrst case, p ∈ P and thus there is an n ∈ N such that n p and n ∈/ I. If n ∈ N then by (1) there is some m ∈ M with m n and hence, m p by transitivity of . Moreover, since M ⊆ M \ fM−1N [I] we found the desired element m. Otherwise, n ∈ F and hence, for m = fM−1N (n) we know m n using (1). By compatibility, also m p. Again, m ∈ M \ fM−1N [I] since n ∈/ I. In the second case, p ∈ G and thus for n = fN−P1 (p) we conclude n ∈ F \ I and n p, and hence n ∈/ F . Thus, there is an element m ∈ M which satisﬁes m n. By compatibility we again achieve m p. If p ∈ fNP [I], we have an element n ∈ F such that n p; n is also in F and therefore we have an element m ∈ fM−1N [I] such that m n p and hence, m p.

12.3. Formalization of the Generalized Multiset Ordering

131

Here lies the main diﬀerence to the formalization of ms in [63]. While just transitivity needs to be shown for ms, we had to show transitivity of both gms and gms as well as compatibility from both sides. Moreover the formalized proofs of these facts get more complicated since we cannot simply use bijections, set intersections, and diﬀerences, but have to deal with enumerations.
After having established Lem. 12.3 and Lem. 12.4 it remains to prove the most interesting and also most complicated property of gms, namely strong normalization. In the remainder of this section we assume that and are compatible and transitive, and that is strongly normalizing. Our proof is closely related to the one for ms [83] which is due to Buchholz: we ﬁrst introduce a more atomic relation gms-step which has gms as its transitive closure. Then it suﬃces to prove that gms-step is well-founded.
Deﬁnition 12.5. We deﬁne gms-step as M gms-step N iﬀ M gms N where in the split M = S ∪ E the size of S is exactly one.

Lemma 12.6. gms is the transitive closure of gms-step.
For strong normalization we perform an accessibility-style proof, i.e., we deﬁne A as the set of strongly normalizing elements w.r.t. gms-step and show that A contains all multisets. Note that to show M ∈ A it suﬃces to prove strong normalization for all N with M gms-step N . Moreover, since gms-step is strongly normalizing on A, one can use the following induction principle to prove some property P for all elements in A.

(∀M.(∀N ∈ A.M gms-step N → P (N )) → P (M )) → (∀M ∈ A.P (M ))

()

We will later on apply this induction scheme using the ﬁrst of the following two predicates:
• P (x) is deﬁned as ∀M.M ∈ A → M ∪ {{x}} ∈ A

• Q(M, x) is deﬁned as ∀b.x b → M ∪ {{b}} ∈ A

For showing that all multisets belong to A, we will require the following technical lemma.
Lemma 12.7. For all multisets M ∈ A and for all elements a, if ∀N.M gms-step N → Q(N, a) and ∀b.a b → P (b) then Q(M, a) holds.

Proof. To prove Q(M, a), let b be an element such that a b where we have to show M ∪{{b}} ∈ A. To prove the latter, we consider an arbitrary N with M ∪{{b}} gms-step N and have to show N ∈ A. From M ∪ {{b}} gms-step N we obtain suitable m1, . . . , mk, n1, . . . , nk, m, and N to perform the splits M ∪ {{b}} = {{m}} ∪ {{m1, . . . , mk}} and N = N ∪ {{n1, . . . , nk}}. We distinguish two cases: either b is part of {{m1, . . . , mk}} or equal to m.
If b = mi for some i then M gms-step N \ {{ni}}. Thanks to the ﬁrst hypothesis and the fact that a ni (because a b = mi ni), we can conclude that N ∈ A.
If b = m then one can prove {{n1, . . . , nk}} ∈ A using the fact that {{m1, . . . , mk}} = M ∈ A and ∀i.mi ni. By induction on the size of N and thanks to the second hypothesis and the fact that for any p ∈ N , a b p, we deduce that {{n1, . . . , nk}} ∪ N ∈ A which concludes the proof.
Lemma 12.8. ∀M.M ∈ A.

132 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings

Proof. The proof of the lemma is done by induction on the size of the multiset M . If M is the empty multiset, then obviously, it is strongly normalizing (hence in A). Otherwise we have to prove ∀a.∀M ∈ A.M ∪ {{a}} ∈ A which is the same as ∀a.P (a).
We perform a well-founded induction on a (w.r.t. ) using the property P and are left to prove P (a) assuming that ∀b.a b → P (b) holds. We pick a multiset M ∈ A and perform an induction on M using ( ) to prove the property Q(M, a) (which entails P (a) because
is reﬂexive): for any M ∈ A we have to prove Q(M, a) given the induction hypothesis ∀N.M gms-step N → Q(N, a). But this result is a trivial application of Lem. 12.7 using the two induction hypotheses that we just generated.
Using Lem. 12.6 and Lem. 12.8, strong normalization of gms immediately follows.
Theorem 12.9. gms is strongly normalizing.

12.4. Multiset and Recursive Path Ordering

In this section we study variations of MPO and RPO. To this end, throughout this section we assume that is some precedence for the signature F. We write > = \ for the strict part of the precedence, and ≈ = ∩ for its equivalence relation.
A standard version of MPO allowing quasi-precedences can be deﬁned by the following inference rules.

si mpo t f (s) mpo t

{{s}} f (s)

mpo ms
mpo

{{t}} f
g(t)

≈

g

f (s) mpo ti for all f (s) mpo g(t)

i f

>

g

For example for the precedence where g ≈ h we conclude that f(y, s(x)) mpo f(x, y) and

g(s(x))

mpo h(x), but f(g(y), s(x))

mpo f(x, h(y)) since {{g(y), s(x)}}

mpo ms

{{x,

h(y)}}

as

g(y) mpo h(y).

To increase the power of MPO, the following inference rules are sometimes used which

generalize MPO by deﬁning two orderings gmpo and gmpo where the multiset extension

is done via gms. This variant of MPO is the one that is internally used within AProVE, and if one removes the last inference rule (x gmpo c), then it is equivalent to the MPO

deﬁnition of [92].

(1)

si f (s)

gmpo t gmpo t

s gmpo t (4) s gmpo t

{{s}} (2)
f (s)

gmpo gms
gmpo

{{t}} f
g(t)

≈

g

{{s}} (5)
f (s)

gmpo gms
gmpo

{{t}} f
g(t)

≈

g

(3)

f (s) gmpo ti for all f (s) gmpo g(t)

i f

>

g

(6) x gmpo c if ∀f ∈ F : f c

Hence, there are two diﬀerences between gmpo (the reﬂexive closure of gmpo) and gmpo: only in gmpo one can compare multisets using the non-strict multiset ordering, and one
can compare variables against constants of least precedence. This latter feature is similar
to polynomial orderings where x 0, and it was also added to the Knuth-Bendix-Ordering
[65]. The increase of power of gmpo in comparison to mpo is due to both new features in the
non-strict relation. For example, using the same precedence as before, f(g(y), s(x)) gmpo f(x, h(y)) as g(y) gmpo h(y) and s(x) gmpo x. Moreover, f(f(y, z), s(x)) gmpo f(x, f(a, z))
if a has least precedence, where this decrease is only possible due to the comparison y gmpo a.

12.4. Multiset and Recursive Path Ordering

133

Note that gmpo is a strict superset of the equivalence relation where equality is deﬁned modulo ≈ and modulo permutations. For this inclusion, indeed all three inference rules (4-6) of gmpo are essential. With (4) and (5) we completely subsume the equivalence relation, and (6) exceeds the equivalence relation. However, then also (5) adds additional power by using gms instead of multiset equality w.r.t. the equivalence relation. As an example, consider g(x) gmpo g(a).
Finally note that our deﬁnition does not require any explicit deﬁnition of an equivalence relation, one can just use gmpo ∩ gmpo. This is in contrast to the RPO in related formalizations of CoLoR [13] and Coccinelle [21]. In Coccinelle ﬁrst an equivalence relation for RPO is deﬁned explicitly, before deﬁning the strict ordering,1 and the RPO of CoLoR currently just supports syntactic equality.2
The reason that AProVE uses gmpo for its MPO implementation is easily understood, as gmpo is more powerful than mpo, and the SAT/SMT-encodings of mpo and gmpo to ﬁnd a suitable precedence are quite similar, so there is not much overhead. Hence, in order to be able to certify AProVE’s MPO proofs—which then also allows to certify weaker variants of MPO—we have formally proved that ( gmpo, gmpo) is a monotone reduction pair. Concerning strong normalization of gmpo, we did not use Kruskal’s tree theorem, but we performed a proof similar to the strong normalization proof of the (higher-order) recursive path ordering as in [13, 21, 57, 63] (which is based on reducibility predicates of Tait and Girard.) By following this proof and by using the results of Sec. 12.3, it was an easy—but tedious—task to formalize the following main result. Here—as for the generalized multiset ordering—the transitivity proof became more complex as one has to prove transitivity and compatibility of gmpo and gmpo at the same time, i.e., within one large inductive proof.
Theorem 12.10. The pair ( gmpo, gmpo) is a monotone reduction pair.
Whereas Thm. 12.10 was to be expected— gmpo is just an extension of mpo, and it is well known that ( mpo, mpo) is a monotone reduction pair—we detected a major diﬀerence when trying to certify existing proofs where one has to compute for a given precedence whether two terms are in relation. This problem is in P for mpo but it turns out to be NP-complete for gmpo.
Theorem 12.11. Let there be some ﬁxed precedence. The problem of deciding gmpo r for two terms and r is NP-complete.
Proof. Membership in NP is easily proved. Since the size of every proof-tree for mpo r is bounded by 2 · | | · |r|, one can just guess how the inference rules for mpo have to be applied; and for the multiset comparisons one can also just guess the splitting.
To show NP-hardness we perform a reduction from SAT. So let φ be some Boolean formula over variables {x1, . . . , xn} represented as a set of clauses {C1, . . . , Cm} where each clause is a set of literals, and each literal l is variable xi or a negated variable xi. W.l.o.g. we assume n ≥ 2.
In the following we will construct one constraint gmpo r for terms , r ∈ T (F , V) where F = {a, f, g, h} and V = {x1, . . . , xn, y1, . . . , ym}. To this end, we deﬁne s(l, Cj) = yj, if l ∈ Cj, and s(l, Cj) = a, otherwise. Moreover, t+x = f(x, s(x, C1), . . . , s(x, Cm)), t−x = f(x, s(x, C1), . . . , s(x, Cm)), tx = f(x, a, . . . , a). We deﬁne L = {{t+x1, t−x1, . . . , t+xn, t−xn}}
1See http://www.lri.fr/~contejea/Coccinelle/doc/term_orderings.rpo.html. 2See http://color.inria.fr/doc/CoLoR.RPO.VRPO_Type.html.

134 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings

and R = {{tx1, . . . , txn, y1, . . . , ym}}. Finally, we deﬁne = g(L) and r = h(R)—where

here we abuse notation and interpret L and R as lists of terms.

We prove that φ is satisﬁable iﬀ gmpo r for the precedence where a ≈ f ≈ g ≈ h.

For this precedence,

gmpo r iﬀ L

gmpo gms

R

since

there

is

only

one

inference

rule

that

can successfully be applied. The reason is that f > g and for each term t±xi of L we have

t±xi gmpo r where t±xi represents one of the terms t+xi or t−xi. To see the latter, assume

t±xi gmpo r would hold. Then {xi, y1, . . . , ym} ⊇ V(t±xi) ⊇ V(r) ⊇ {x1, . . . , xn} being a

contradiction to n ≥ 2.

To examine whether L

gmpo gms

R

can

hold,

let

us

consider

an

arbitrary

splitting

of

R

into a strict part S and a non-strict part E . Notice that t+x gmpo tx and t−x gmpo tx,

but neither t+x gmpo tx nor t−x gmpo tx: the reason is that for each l and Cj we get

s(l, Cj) gmpo a but not s(l, Cj) gmpo a. Hence, each tx of R must be contained in E .

As moreover, t±xi gmpo yj iﬀ t±xi gmpo yj we can w.l.o.g. assume that each yj ∈ S . In

total, if L

gmpo gms

R,

then

R

must

be

split

into

S

∪E

where S

= {{y1, . . . , ym}} and

E = {{tx1, . . . , txn}} and L must be split into S ∪ E such that all conditions of

gmpo gms

are

satisﬁed.

At this point we consider both directions to show that φ is satisﬁable iﬀ L

gmpo gms

R.

First assume that φ is satisﬁable, so let α be some satisfying assignment. Then we

choose S = {{t+x | α(x) = }} ∪ {{t−x | α(x) = ⊥}} and E = L \ S. Notice that for each x exactly one of t+x and t−x is in S, and the other is in E. Hence, for each tx ∈ E there is a corresponding t±x ∈ E with t±x gmpo tx. Next, we have to ﬁnd for each yj ∈ S some term

in S which is larger than yj. To this end, notice that α is a satisfying assignment, thus

there is some literal xi or xi in Cj which evaluates to true. If xi ∈ Cj then α(xi) = and
hence, t+xi ∈ S where t+xi = f(. . . , yj, . . . ) gmpo yj. Otherwise, xi ∈ Cj and α(xi) = ⊥ and hence, t−xi ∈ S where t−xi = f(. . . , yj, . . . ) gmpo yj. Thus, in both cases there is some term in S being larger than yj. Moreover, S = ∅ since |S| = n ≥ 2.

For the other direction assume that S and E could be found such that L = S ∪ E and

all conditions of

gmpo gms

are

satisﬁed.

Hence for each txi ∈ E

there is some term t ∈ E

satisfying t gmpo txi. Then, t can only be t+xi or t−xi since xi ∈ V(tx) and each other term t±xj with i = j does not contain the variable xi. Thus, for each i exactly one of the terms t+xi and t−xi is contained in E and the other is contained in S. We deﬁne the assignment α where α(xi) = iﬀ t+xi ∈ S. It remains to show that α is a satisfying assignment for

φ. So let Cj be some clause of φ. Since yj ∈ S we know that there is some t ∈ S with

t gmpo yj, i.e., yj ∈ V(t). There are two cases. First, if t = t+xi for some i, then by the deﬁnition of t+ we know that yj ∈ V(t+xi) implies xi ∈ Cj, and hence Cj is evaluated to true, since α(xi) = by deﬁnition of α. Otherwise, t = t−xi for some i where now xi ∈ Cj. Moreover, as t−xi ∈ S, we know that t+xi ∈/ S and hence, α(xi) = ⊥. Together with xi ∈ Cj

this again shows that Cj is evaluated to true. Hence, all clauses evaluate to true using α

which proves that φ is satisﬁable.

The following corollary states that NP-completeness is essentially due to fact that gms and gms are hard to compute, even if all comparisons of the elements in the multiset are given. It can be seen within the previous proof, where the important part of the reduction

from SAT was to deﬁne for each formula φ the multisets L and R such that φ is satisﬁable

iﬀ L

gmpo gms

R

(and

also

iﬀ

L

gmpo gms

R).

Corollary 12.12. Given two orderings and , two multisets M and N , and the set {(x, y, x y, x y) | x ∈ M, y ∈ N }, deciding M gms N and M gms N is NPcomplete.

12.4. Multiset and Recursive Path Ordering

135

Of course, if the splitting for the multiset comparison is given, then deciding gms and gms becomes polynomial.
All our results have also been generalized to RPO where for every function symbol there is a status function τ which determines whether the arguments of each function f should be compared lexicographically (τ (f ) = lex ) or via multisets (τ (f ) = mul ).3 Here, the existing inference rules of gmpo are modiﬁed that instead of f ≈ g it is additionally demanded that τ (f ) = τ (g) = mul . Moreover, there are two additional inference rules for f ≈ g and τ (f ) = τ (g) = lex , one for the strict ordering grpo and one for the non-strict ordering grpo.
Again, grpo is used in AProVE instead of the standard deﬁnition of RPO ( rpo,[27]). However, during our formalization we have detected that in contrast to ( gmpo, gmpo), the pair ( grpo, grpo) is not a reduction pair, as the orderings are not stable. To see this, consider a precedence where a and b have least precedence, and τ (a) = τ (b). Then x grpo a, but b grpo a.
Our solution to this problem is to add the following further inference rules which allow comparisons of terms f (s) with g(t) where τ (f ) = τ (g). In detail, we require that t is empty and for a strict decrease, additionally s must be non-empty.

|s| > 0 |t| = 0 f (s) grpo g(t) f ≈ g, τ (f ) = τ (g)

|t| = 0

f (s)

grpo

f g(t)

≈

g, τ (f )

=

τ (g)

If these inference rules are added, then indeed ( grpo, grpo) is a monotone reduction
pair. As a consequence, one can interpret AProVE’s version of RPO as a sound, but non-stable under-approximation of grpo.

We also tried to to relax the preconditions further, e.g. by allowing vectors t of length at most one. But no matter whether we also restrict the length of s in some way or not, and no matter whether we compared the arguments s and t lexicographically or via multisets, the outcome was always that transitivity or strong normalization are lost.

For example, if we add the inference rule that {{s}}

grpo gms

{{t}},

|t|

≤

1,

f

≈

g,

and τ (f ) = τ (g) implies f (s) grpo g(t), then strong normalization is lost: assume the

precedence is deﬁned by f ≈ g ≈ h and 3 > 2 > 1 > 0, and the status is deﬁned by

τ (f) = τ (h) = lex and τ (g) = mul . Then f(0, 3) grpo g(2) grpo h(1) grpo f(0, 3) clearly

shows that the resulting ordering is not strongly normalizing anymore.

To summarize, gmpo and grpo are strictly more powerful reduction orderings than the standard deﬁnitions of MPO and RPO ( mpo and rpo). The price for the increased power is that checking constraints for gmpo and grpo becomes NP-complete whereas it is in P for mpo and rpo.

Note that if one would provide all required splittings for the multiset comparisons in gmpo and grpo, then constraint checking again becomes polynomial. However, this would make certiﬁcates more bulky, and since in practice the arities of function symbols are rather small, certiﬁcation can eﬃciently be done even without additional splitting information.

3We do not consider permutations for lexicographic comparisons in the deﬁnition of RPO as this feature can be simulated by generating reduction pairs using an RPO (without permutations) in combination with argument ﬁlterings as deﬁned in [3]. In this way, we only have to formalize permutations once and we can reuse them for other orderings like KBO.

136 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings
12.5. SCNP Reduction Pairs
The size-change criterion of [74] to prove termination of programs can be seen as a graphtheoretical problem: given a set of graphs—encoding for each recursive call the decrease in size of each argument—one tries to decide the size-change termination condition (SCT condition) on the graphs, namely that in every inﬁnite sequence of graphs one can ﬁnd an argument whose size is strictly decreased inﬁnitely often. If the condition is satisﬁed, then termination is proved.
Concerning automation of the size-change principle, there are two problems: ﬁrst, the base ordering (or size-measure or ranking function) must be provided to construct the graphs, and second, even for a given base ordering, deciding the SCT condition is PSPACE-complete.
To overcome these problems, in [7] and [20] suﬃcient criteria have been developed. They approximate the SCT condition in a way that can be encoded into SAT.
Another beneﬁt of [20] is an integration of the approximated SCT condition as a reduction pair in the dependency pair framework (DP framework) [42], called SCNP reduction pair.
Although IsaFoR contains already a full formalization of size-change termination as it is used in [102], an integration of SCNP reduction pairs in the certiﬁcation process might be beneﬁcial for two reasons:
• since deciding the SCT condition is PSPACE hard, whereas the approximated condition can be encoded into SAT, the certiﬁcates might be easier to check
• the approximated SCT condition is not fully subsumed by the SCT condition as only the former allows incremental termination proofs in the DP framework
Before we describe our formalization of SCNP reduction pairs, we shortly recall some notions of the DP framework, a popular framework to perform modular termination proofs for TRSs. The main data structure are DP problems (P, R) consisting of two TRSs where all rules in P are of the form F (. . . ) → G(. . . ) where F, G are symbols that do not occur in R. The main task is to prove ﬁniteness of the a given DP problem (P, R), i.e., absence of inﬁnite minimal chains s1σ → t1σ →∗R s2σ → t2σ . . . where all si → ti ∈ P and all tiσ are terminating w.r.t. R. To this end, one uses various processors to simplify the initial DP problem for a TRS until the P-component is empty.
One of the most important processors is the reduction pair processor [42, 51]—where here we only present its basic version without other reﬁnements like usable rules w.r.t. an argument ﬁltering [42]. It can remove strictly decreasing rules from P, provided that both P and R are at least weakly decreasing.
Theorem 12.13 (Reduction pair processor). Let ( , ) be a reduction pair. If P ⊆ ∪ and R ⊆ then (P, R) is ﬁnite if (P \ , R) is ﬁnite.
Hence, to prove termination it suﬃces to ﬁnd diﬀerent reduction pairs to iteratively remove rules from P until all rules of P have been removed. Thus, with SCNP reduction pairs it is possible to choose diﬀerent base orderings to remove diﬀerent rules of P.
In the following we report on details of SCNP reduction pairs as deﬁned in [20] and on their formalization. Essentially, SCNP reduction pairs are generated from reduction pairs ( , ) via a multiset extension of a lexicographic combination of with the standard ordering on the naturals.

12.5. SCNP Reduction Pairs

137

Deﬁnition 12.14 (Multiset extension). We deﬁne that µ is a multiset extension iﬀ for

each ordering pair ( , ) over A, the pair ( µ, µ) is an ordering pair over P(A). More-

over, whenever and are closed under an operator op (x y implies op(x) op(y)

()

()

for all x, y), then µ and µ must also be closed under the image of op on multisets

(M N implies op[M ] op[N ]).

( )µ

( )µ

We also call ( µ, µ) the multiset extension of ( , ) w.r.t. µ.

For example, gms is a multiset extension and in [7] and [20] in total four extensions
are listed to compare multisets (gms, min, max , and dms). The extension dms is the dual multiset extension [8] where our formulation is equivalent to the deﬁnition in [20].4 The strict relation is deﬁned as M dms N iﬀ M = {{x1, . . . , xm}} ∪ {{z1, . . . , zk}}, N = {{y1, . . . , yn}}∪{{z1, . . . , zk}}, n > 0, ∀i.zi zi, and ∀xi.∃yj.xi yj; the non-strict relation
dms is deﬁned like dms except that the condition n > 0 is omitted. For SCNP reduction pairs, multisets are compared via one of the four multiset exten-
sions. And to generate multisets from terms, the notion of a level mapping is used.

Deﬁnition 12.15 (Level mapping [20]). For each f ∈ F with arity n, let π(f ) ∈ P({1, . . . , n} × N).5 We deﬁne the level-mapping L : T (F, V) → P(T (F, V) × N) where L(f (s1, . . . , sn)) = {{ si, k | (i, k) ∈ π(f )}}.6

Deﬁnition 12.16. Let ( , ) be a reduction pair for terms in T (F, V). Let µ be a multiset

extension. The ordering pair ( N, N) over T (F , V) × N is deﬁned as the lexicographic

combination of ( , ) with the >-ordering on the naturals: s, n N t, m iﬀ s t∨(s

t ∧ n ≥ m), and s, n

N t, m iﬀ s

t ∨ (s

t ∧ n > m). The ordering pair (

Nµ ,

N µ

)

is deﬁned as the multiset extension of ( N, N) w.r.t. µ.

In principle,

N µ

∪

N µ

is

the

part

of

a

SCNP

reduction

pair

that

should

be

used

to

compare left- and right-hand sides of P within the reduction pair processor. However,

one also needs to orient the rules in R via . To this end, two types are introduced in

[20] so that the ﬁnal ordering incorporates both

N µ

∪

N µ

for

P

and

for R. In detail, the

signature F is partitioned into F b F consisting of base symbols F b and tuple-symbols

F . The set of base terms is T (F b, V), and a tuple term is a term of the form F (t1, . . . , tn) where F ∈ F and each ti is a base term.
Notice that for a DP problem (P, R) all terms in P are tuple terms and all terms in R

are base terms if one chooses F to be the set of root symbols of terms in P. Therefore,

SCNP reduction pairs are deﬁned in a way that the ordering depends on whether tuple

terms or base terms are compared.

Deﬁnition 12.17 (SCNP reduction pair [20]). Let µ be a multiset extension and L be a

level mapping. Let ( , ) be a reduction pair over T (F, V). The SCNP reduction pair

is the pair ( L,µ, L,µ) where the relations L,µ and L,µ over T (F , V) are deﬁned as

follows. If t and s are tuple terms, then t

L,µ s iﬀ L(t)

N µ

L(s),

and

t

L,µ s iﬀ

L(t)

N µ

L(s).

Otherwise,

if

t

and

s

are

base

terms,

then

t

L,µ s iﬀ t

s, and t L,µ s

iﬀ t s.

4There is a diﬀerence in the deﬁnition of the dual multiset extension in [7, 8] to the deﬁnition in [20]
which is similar to the diﬀerence between ms and gms . 5In [20] there was an additional condition that π(f ) may not contain two entries j, k1 and j, k2 . It
turned out that this condition is not required for soundness. 6We use the notation L instead of for level mappings as in this paper, are left-hand sides of rules.

138 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings

Before stating the major theorem of [20] that SCNP reduction pairs are reduction pairs, we ﬁrst have to clarify the notion of reduction pair: In [20, Sec. 2] a non standard deﬁnition of a reduction pair is used which diﬀers from Def. 12.1. To distinguish between both kinds of reduction pairs we call the ones of [20] typed reduction pairs.

Deﬁnition 12.18 (Typed reduction pair [20]). A typed reduction pair is a reduction pair ( , ) over T (F, V) with the additional condition that compares either two tuple terms or two base terms.

So, the major theorem of [20] states that whenever ( , ) is a typed reduction pair,

then so is ( L,µ, L,µ) where µ is one of the four mentioned multiset extensions.

The major problem in the formalization of exactly this theorem required a link between

the two notions of reduction pairs since all other theorems working with reduction pairs

in IsaFoR are using Def. 12.1.

At this point, it turned out that there are problems with Def. 12.18. Of course, to use

the major theorem of [20] one needs a typed reduction pair to start from. Unfortunately,

common reduction pairs like RPO are not typed reduction pairs w.r.t. Def. 12.18. For

example, if F ∈ F then F (F (x)) grpo F (F (x)), but then does not satisfy the addi-

tional condition of Def. 12.18, since here two terms are in relation which are neither base

terms nor tuple terms.

A possible solution might be to require that ( , ) is just a reduction pair and then

show that ( L,µ, L,µ) is a typed reduction pair. However, even with this adaptation the

problems remain, since Def. 12.18 itself is ﬂawed: assume ( , ) is a typed reduction pair

and F is a non-constant tuple symbol. Since is a quasi-ordering (on tuple-terms) it is

reﬂexive, and thus F (t) F (t) for every list t of base terms. By monotonicity of also

F (. . . , F (t), . . . ) F (. . . , F (t), . . . ) must hold, in contradiction to the condition that

only compares base terms or tuple terms. So, there is a severe problem in demanding

both monotonicity of and the additional condition of Def. 12.18.

Repairing Def. 12.18 by only requiring monotonicity w.r.t. Fb is also no solution, since

for using reduction pairs in the reduction pair processor, it is essential that is also

closed under F -contexts.

For a proper ﬁx of SCNP reduction pairs, note that the distinction between tuple terms

and base terms in [20] is solely performed, to have two kinds of orderings: for orienting

rules in R, and

N µ

and

N µ

for

orienting

rules

of

P.

However, there is already a notion which allows the usage of diﬀerent orderings for

orientation of P and R, namely reduction triples. The advantage of this notion is the fact

that it does not require any distinction between base and tuple terms.

Deﬁnition 12.19 (Reduction triple, [51]). A reduction triple is a triple ( , , ) such that ( , ) is a reduction pair and ( , ) is a non-monotone reduction pair.

Reduction triples can be used instead of reduction pairs in Thm. 12.13: in [51] it is shown that whenever P ⊆ ∪ and R ⊆ then P can be replaced by P \ in the DP problem (P, R). The proof is similar to the one of Thm. 12.13 and also in IsaFoR it was easy to switch from reduction pairs to reduction triples. Hence, the obvious attempt is to deﬁne SCNP reduction pairs as reduction triples. As there is no distinction of base terms and tuple terms, we will not run into problems that are caused by the required monotonicity of .

Deﬁnition 12.20 (SCNP reduction triple). Let µ be a multiset extension and L be a level mapping. Let ( , ) be a reduction pair.

12.5. SCNP Reduction Pairs

139

We deﬁne

and

as t

s iﬀ L(t)

N µ

L(s),

and

t

the SCNP reduction triple is deﬁned as ( , , ).

s iﬀ L(t)

N µ

L(s).

Then

If we are able to prove that every SCNP reduction triple is indeed a reduction triple,

then we are done. Unfortunately, it turns out that an SCNP reduction triple is not a

reduction triple, where the new problem is that compatibility between and cannot

be ensured. As an example, consider a reduction pair ( , ) which is deﬁned via an RPO

with precedence b > a and status τ (F ) = lex . Moreover, let the level-mapping be deﬁned

via π(F ) = {{ 2, 0 }} and take µ = gms. Then F (a, b) F (b, a) since L(F (a, b)) =

{{ b, 0 }}

N gms

{{ a, 0 }}

=

L(F (b, a)).

Furthermore, F (b, a)

F (a, b). However, if

and were compatible, then we would be able to conclude F (a, b) F (a, b), a

contradiction.

To this end, we ﬁnally have deﬁned a weaker notion than reduction triples which can

still be used like reduction triples.

Deﬁnition 12.21 (Root reduction triple). A root reduction triple is a triple ( , , )

such that ( , ) is a non-monotone reduction pair, is a stable and monotone quasi-

ordering, and whenever s vi+1, . . . , vn).

t then f (v1, . . . , vi−1, s, vi+1, . . . , vn)

f (v1, . . . , vi−1, t,

Note that every reduction triple ( , , ) is also a root reduction triple provided that ⊆ —and as far as we know this condition is satisﬁed for all reduction triples that are currently used in termination tools. Moreover, root reduction triples can be used in the same way as reduction triples to remove pairs which has been proved in IsaFoR.

Theorem 12.22 (Root reduction triple processor). Let ( , , ) be a root reduction triple. Whenever P ⊆ ∪ , R ⊆ , and (P \ , R) is ﬁnite, then (P, R) is ﬁnite.

Hence, the notion of root reduction triple seems useful for termination proving. And
indeed, it turns out that each SCNP reduction triple is a root reduction triple which ﬁnally shows that SCNP reduction triples can be used to remove pairs from DP problems.7

Theorem 12.23. Every SCNP reduction triple is a root reduction triple.

Thm. 12.23 is formally proved within IsaFoR, theory SCNP. Note that this formalization was straightforward, once the notion of root reduction triple was available: the whole formalization takes only 310 lines. It also includes the feature of -arguments—an extension of size-change graphs which is mentioned in both [102] and [20]—and it contains results on Ce-compatibility and compatibility w.r.t. to argument ﬁlterings. The latter results are important when dealing with usable rules, cf. [42] for further details.
Regarding the formalization of multiset extensions—which is orthogonal to the formalization of SCNP reduction triple—we were able to integrate three of the four mentioned multiset extensions. However, for the multiset extension dms it is essential that the signature F is ﬁnite as otherwise strong normalization is not necessarily preserved, cf. Ex. 12.24. Here, the essential issue is that without an explicit bound on the sizes of the multiset, strong normalization is lost (such a bound is explicitly demanded in [7], but not in [20]).
7An alternative—but unpublished—ﬁx to properly deﬁne SCNP reduction pairs has been developed by Carsten Fuhs. It is based on typed term rewriting. (private communication)

140 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings

Example 12.24. In [20] it is assumed that the initial TRS is ﬁnite. Hence, also the initial signature is ﬁnite which in turn gives a bound on the sizes of the constructed multisets. However, for the soundness of SCNP reduction pairs it is essential that the signature of the current system is ﬁnite, since otherwise the following steps would be a valid termination proof for the TRS R = {a → a} as there is no bound on the sizes of multisets.

• Build the initial DP problem ({A → A}, R).

• Replace this DP problem by the DP problem D = ({Ai(x, . . . , x) → Ai+1(x, . . . , x) | i ∈ N}, ∅) where the arity of each Ai is i. This step is sound, as D is not ﬁnite.

• Replace D by (∅, ∅). This can be done using the SCNP reduction pair where µ =
dms, π(Ai) = { j, 0 | 1 ≤ j ≤ i}, and ( , ) is any reduction pair. The reason is that
{{ x, 0 , . . . , x, 0 }} dms {{ x, 0 , . . . , x, 0 }}.

i times

i + 1 times

The demand for a bound on the signature for dms resulted in a small problem in our

formalization, since in IsaFoR we never have ﬁnite signatures: for each symbol f we directly

include inﬁnitely many f ’s, one for each possible arity in N. So, in principle there might

not be any bound on the size of the multisets that are constructed by the level mapping.

Hence, we use a slightly diﬀerent version of dms which includes some ﬁxed bound n on the

size of the multisets: we deﬁne M

N iﬀ M

N and |N | ≤ n ∨ |N | = |M |.

( ) dms-n

( ) dms

Hence, dms-n is a restriction of dms where either the sizes of the multisets are bounded

by n or where multisets of the same sizes are compared.

Note that the additional explicit restriction of |N | ≤ n ensures strong normalization

even for inﬁnite F or unbounded multisets. The other alternative |N | = |M | is added, as

otherwise reﬂexivity of dms-n is lost, for example {{x, . . . , x}} dms-n {{x, . . . , x}} would

no longer hold.

n+1

n+1

In practice, the diﬀerence between dms and dms-n can be neglected. As for the certiﬁ-

cation we only consider ﬁnite TRSs, we just precompute a suitable large enough number

n such that for the resulting constraints dms and dms-n coincide.

We conclude this section by showing that certiﬁcation of SCNP reduction triples is

NP-hard in the case of µ ∈ {gms, dms} which also implies that deciding dms is NP-hard.

Theorem 12.25. Given a SCNP reduction triple ( , , ) and µ ∈ {gms, dms}, the

problem of deciding

r for two terms and r is NP-hard.

Proof. For µ = gms we use nearly the identical reduction from SAT as we used in the

proof of Thm. 12.11. To be more precise, for each formula φ we use the exactly the same

terms and r and the same multisets L and R as before. Moreover, we deﬁne the level-

mapping by choosing π(g) = { i, 0 | 1 ≤ i ≤ 2n} and π(h) = { i, 0 | 1 ≤ i ≤ n + m}.

Then for L = {{ s, 0 | s ∈ L}} and R = {{ s, 0 | s ∈ R}} we obtain

r iﬀ L

N gms

R

iﬀ L gms R. It remains to choose as the MPO within the proof of Thm. 12.11. Then

L gms R iﬀ φ is satisﬁable, so in total,

r iﬀ φ is satisﬁable.

Furthermore, it can easily be argued that NP-hardness is not a result of our extended

deﬁnition of MPO: we also achieve L gms R iﬀ φ is satisﬁable if we deﬁne by a polynomial interpretation Pol with Pol(f(x0, . . . , xm)) = 1+x0 +· · ·+xm and Pol(a) = 0.
For µ = dms one can use a similar reduction from SAT: satisﬁability of φ is equivalent

to r using the same level-mapping as before, but where now = h(x1 ∨ x1, . . . , xn ∨

12.6. Certiﬁcation Algorithms

141

xn, s(C1), . . . , s(Cm)), r = g(x1, x1, . . . , xn, xn), and is deﬁned as the polynomial interpretation Pol where Pol(s(x)) = 1 + x and Pol(x ∨ y) = x + y. Here, each Ci is a representation of clause Ci as disjunction.
12.6. Certiﬁcation Algorithms
For the certiﬁcation of existing proofs using RPO and SCNP reduction pairs there are in principle two possibilities, which we will explain using RPO.
The ﬁrst solution for certifying s grpo t is to use a shallow embedding. In this approach, some untrusted tool ﬁgures out how the inference rules of RPO have to be applied and generates a proof script that can then be checked by the proof assistant. This solution cannot be used in our case, since CeTA is obtained from IsaFoR via code generation.
The second solution is to use a deep embedding where an algorithm for deciding RPO is developed within the proof assistant in combination with a soundness proof. Then this algorithm is amenable for code generation, and such an algorithm is also used in related certiﬁers [13, 21] where it is accessible via reﬂection. Hence, we had to develop a function for deciding RPO constraints within Isabelle. In our case, we have written a function grpoτ> for RPO, which is parametrized by a precedence > and a status τ . It takes two terms s and t as input and returns the pair (s grpo t, s grpo t). In fact, we even deﬁned RPO via grpoτ> and only later on derived the inference rules that have been presented in Sec. 12.4.
Since we proved several properties of RPO directly via grpoτ> (theory RPO), we implemented grpoτ> in a straightforward way as recursive function. As a result, grpoτ> has exponential runtime, since no sharing of identical subcalls is performed. To this end, we developed a second function for RPO, that has been proved to be equivalent to grpoτ>, but where memoization is integrated—a well known technique where intermediate results are stored to avoid duplicate computations. In principle, this is an easy programming task, but since we use a deep embedding, we had to formally prove correctness of this optimization.
To stay as general as possible, the memoized function was implemented independent of the actual data structure used as memory. The interface we use for a memory is as follows. We call a memory valid w.r.t. to a function f if all entries in the memory are results of f . Moreover we require functionality for looking up a result in the memory and for storing a new result with the obvious soundness properties: storing a correct entry in a valid memory yields a valid memory and looking up an entry in a valid memory returns a correct result, i.e., the same result that would have been computed by f .
Assuming we have such a memory at our disposal the idea of memoizing is straightforward: before computing a result we do a lookup in the memory and if an entry is found we return it and leave the memory unchanged. Otherwise we compute as usual, store the result in the memory and return both.
The main diﬃculty was that all recursive calls of grpoτ> are indirect via higher-order functions like the computation of the multiset- and lexicographic extension of a relation. Consequently results of recursive calls to RPO are not available directly, but only in these higher-order functions. Thus, they have to take care of storing results in the memory and of passing it on to the next RPO call. Hence, new memoizing versions of all these higherorder functions had to be implemented and their soundness had to be proved. Soundness meaning that, when given equivalent functions as arguments, they compute the same results as their counterparts without memory handling, and that given a valid memory as

142 Chapter 12. Formalization of Termination Techniques Based on Multiset Orderings
input they return a valid memory in addition to their result. For further details we refer to theory Eﬃcient-RPO.
Concerning the required decision procedures for gms and dms which have to ﬁnd suitable splittings, we used a branch-and-bound approach.
We tested our certiﬁcation algorithms by rerunning all experiments that have been performed in [20]. Here, AProVE tries to prove termination of 1,381 TRSs from the termination problem database (TPDB version 7.0) using 20 diﬀerent strategies where in 12 cases SCNP reduction pairs are used and full size-change termination is tried 4 times. Here, in 16 strategies RPO or weaker orderings have been tried. As in [20] we used a timeout of 60 seconds and although we used a diﬀerent computer than in [20], on the 20 × 1, 381 termination problems, there are only 8 diﬀerences in both experiments—all due to a timeout.
By performing the experiments we were able to detect a bug in the proof output of AProVE—the usable rules have not been computed correctly. After a corresponding ﬁx indeed all 9,025 generated proofs could be certiﬁed by CeTA (version 2.2).
The experiments contain 432 proofs where the usage of gms and gms was essential, i.e., where the constraints could not be oriented by ms and ms. If one allows equality modulo the equivalence relation induced by the ordering, then still 39 proofs require gms and gms .
Regarding the time required for certiﬁcation, although it is NP-complete, in our experiments, certiﬁcation is much faster than proof search. The reason is that our certiﬁcation algorithms are only exponential in the arity of the function symbols, and in the experiments the maximal arity was 7. In numbers: AProVE required more than 31 hours (≈ 4 seconds per example) whereas CeTA was done in below 6 minutes (≈ 0.04 seconds per example).
The experiments also show that proofs using SCNP reduction pairs can indeed be certiﬁed faster than proofs using full size-change termination. In average, the latter proofs require 50 % more time for certiﬁcation than the former.
All details of our experiments are available at http://cl-informatik.uibk.ac.at/ software/ceta/experiments/multisets/.
12.7. Summary
We have studied the generalization of the multiset ordering which is generated by two orderings: a strict one and a compatible non-strict one. Indeed this generalization preserves most properties of the standard multiset ordering, where we only detected one diﬀerence: the decision problem becomes NP-complete.
Concerning termination techniques that depend on multiset orderings, we formalized and corrected an extended variant of RPO that is used within AProVE, and we formalized and corrected SCNP reduction pairs. Certiﬁcation of both techniques is NP-hard.
Acknowledgments
We thank Carsten Fuhs for helpful discussions and his support in the experiments, and we thank the anonymous referees for their helpful comments.

13. Certiﬁcation of Nontermination Proofs
Publication details
Christian Sternagel and Ren´e Thiemann. Certiﬁcation of Nontermination Proofs. In Proceedings of the 3rd International Conference on Interactive Theorem Proving, volume 7406 of LNCS, pages 266–282. Springer, 2012.
Abstract
Automatic tools for proving (non)termination of term rewrite systems, if successful, deliver proofs as justiﬁcation. In this work, we focus on how to certify nontermination proofs. Besides some techniques that allow to reduce the number of rules, the main way of showing nontermination is to ﬁnd a loop, a ﬁnite derivation of a special shape that implies nontermination. For standard termination, certifying loops is easy. However, it is not at all trivial to certify whether a given loop also implies innermost nontermination. To this end, a complex decision procedure has been developed in [103]. We formalized this decision procedure in Isabelle/HOL and were able to simplify some parts considerably. Furthermore, from our formalized proofs it is easy to obtain a low complexity bound. Along the way of presenting our formalization, we report on generally applicable ideas that allow to reduce the formalization eﬀort and improve the eﬃciency of our certiﬁer.
13.1. Introduction
In program veriﬁcation the focus is on proving that a function satisﬁes some property, e.g., termination. However, in presence of a bug it is more important to ﬁnd a counterexample indicating the problem. In this way, we can save a lot of time by abandoning a veriﬁcation attempt as soon as a counterexample is found. In term rewriting, a well known counterexample for termination is a loop, essentially giving some “input” on which a “program” does not terminate. As soon as speciﬁc evaluation strategies are considered it might not be easy to verify whether a given loop constitutes a proper counterexample. However, since many programming languages employ an eager evaluation strategy, methods for proving innermost nontermination are important. What is more, some very natural functions are not even expressible without evaluation strategy. Take for example equality on terms. There is no (ﬁnite) term rewrite system (TRS) that encodes equality on arbitrary terms (the problem is the case where the two given terms are diﬀerent). Using innermost rewriting, encoding equality is possible by the following four rules, as shown by Daron Vroon (personal communication; he used this encoding to properly model the built-in equality of ACL2).

144 Chapter 13. Certiﬁcation of Nontermination Proofs

x == y → chk(eq(x, y)) eq(x, x) → true

(1) (2)

chk(true) → true chk(eq(x, y)) → false

(3) (4)

Current techniques for proving innermost nontermination of TRSs consist of preprocessing techniques (narrowing the search space by removing rules) followed by ﬁnding a loop, for which the complex decision procedure of [103] allows to decide whether it implies innermost nontermination. We formalized this decision procedure as part of our Isabelle Formalization of Rewriting (IsaFoR). The corresponding certiﬁer CeTA can be obtained by Isabelle/HOL’s code generator [49, 104]. Both IsaFoR and CeTA are freely available at http://cl-informatik.uibk.ac.at/software/ceta/ (the relevant theories for this paper are Innermost_Loops and Nontermination, together with their respective implementation theories, indicated by the suﬃx _Impl).
During our formalization we were able to simplify some parts of the decision procedure considerably. Mostly, due to a new proof which, in contrast to the original proof, does not depend on Kruskal’s tree theorem. As a result, we can replace the most complicated algorithm of [103] by a single line. Moreover, we report on how we managed to obtain eﬃcient versions of other algorithms from [103] within Isabelle/HOL [84].
The remainder is structured as follows. In Sect. 13.2 we give preliminaries. Then, in Sect. 13.3, we describe the preprocessing techniques (narrowing the search space for ﬁnding a loop) that are supported by our certiﬁer. Afterwards, we present details on loops w.r.t. the innermost strategy in Sect. 13.4. The main part of this paper is on our formalization of the decision procedure for innermost loops in Sect. 13.5, before we conclude in Sect. 13.6.

13.2. Preliminaries
We assume basic familiarity with term rewriting [5]. Nevertheless, we shortly recapitulate what is used later on. A term t ( , r, s, u, v) is either a variable x (y, z) from the set V, or a function symbol f (g) from the disjoint set F applied to some argument terms f (t1, . . . , tn). The root of a term is deﬁned by root(x) = x and root(f (t1, . . . , tn)) = f . The set args(t) of arguments of t is deﬁned by the equations args(x) = ∅ and args(f (t1, . . . , tn)) = {t1, . . . , tn}. The set of variables occurring in a term t is denoted by V(t). A context C (D) is a term containing exactly one occurrence of the special hole symbol P. Replacing the hole in a context C by a term t is written C[t]. The term t is a (proper) subterm of the term s, written (s t) s t, iﬀ there is a (non-hole) context C such that s = C[t], iﬀ there is a (non-empty) position p such that s|p = t. We write s F t iﬀ s t and t ∈/ V. A substitution σ (µ) is a mapping from variables to terms whose domain dom(σ) = {x | σ(x) = x} is ﬁnite. The range of a substitution is ran(σ) = {σ(x) | x ∈ dom(σ)}. We represent concrete substitutions using the notation {x1/t1, . . . , xn/tn}. We use σ interchangeably with its homomorphic extension to terms, writing, e.g., tσ to denote the application of the substitution σ to the term t. A (rewrite) rule is a pair of terms → r and a term rewrite system (TRS) R is a set of such rules. The rewrite relation (induced by R) →R is deﬁned by s →R t iﬀ there is a context C, a rewrite rule → r ∈ R, and a substitution σ such that s = C[ σ] and t = C[rσ]. Here, we call σ a redex (short for reducible expression) and sometimes write s →R, σ t to make it explicit. A normal form is a term that does not contain any redexes. When a rewrite step s →R, σ t additionally satisﬁes that all arguments of σ are normal forms, it is called an innermost (rewrite)

13.3. A Framework for Certifying Nontermination

145

step, written s →i R t. We freely drop R from s →R t if it is clear from the context. A term t is (innermost) nonterminating w.r.t. R, iﬀ there is an inﬁnite (innermost)
rewrite sequence starting at t, i.e., a derivation of the form
t = t1 →(i) R t2 →(i) R t3 →(i) R · · ·
A TRS R is (innermost) nonterminating iﬀ there is a term t that is (innermost) nonterminating w.r.t. R.

13.3. A Framework for Certifying Nontermination
As for termination, there are several techniques that may be combined in order to prove nontermination. On the one hand, there are basic techniques, i.e., those that immediately prove nontermination; and on the other hand, there are transformations, i.e., mappings that turn a given TRS R into a transformed TRS R (for which, proving nontermination is hopefully easier). Such transformations are complete iﬀ (innermost) nontermination of R implies (innermost) nontermination of R. In order to prove nontermination, arbitrary complete transformations can be applied, before ﬁnishing the proof by a basic technique.
In our development we formalized the following basic techniques and complete transformations. Except for innermost loops and string reversal, none of these techniques posed any diﬃculties in the formalization.
Well-Formedness Check. A TRS R is (weakly) well-formed iﬀ no left-hand side is a variable and all (applicable) rules → r satisfy V(r) ⊆ V( ). Where a rule is applicable iﬀ the arguments of its left-hand side are normal forms (otherwise the rule could never be used in the innermost case).
Lemma 13.1. If R is not (weakly) well-formed, it is (innermost) nonterminating.
Thus a basic technique is to check whether a TRS is (weakly) well-formed and conclude (innermost) nontermination, if it is not.
Finding Loops. The second basic technique is to ﬁnd a loop and it is treated in more detail in Sect. 13.4.
Rule Removal. One way to narrow the search space when trying to prove nontermination, is to get rid of rules that cannot contribute to any inﬁnite derivation. This can be done by employing the same techniques that are already known from termination, namely monotone reduction pairs [9, 73].
String Reversal. A special variant of TRSs are string rewrite systems, where all function symbols are ﬁxed to be unary. For this special case, string reversal (see, e.g., [111] and [97] for its formalization) can be applied.
Dependency Pair Transformation. As for termination, also for nontermination, it is possible to switch from TRSs to dependency pair problems (DPPs) [3]. This is done by the so called dependency pair transformation, which intuitively, identiﬁes the mutually

146 Chapter 13. Certiﬁcation of Nontermination Proofs

recursive dependencies of rewrite rules and makes them explicit in a second set of rewrite rules, the dependency pairs.
For nontermination of (innermost) DPPs, we support the following techniques:
Finding Loops. For DPPs (P, R) the search space for ﬁnding loops is further restricted by the fact that pairs from P are only applied at the root position.
Rule Removal. Also for DPPs it is possible to narrow the search space by employing reduction pairs to remove pairs and rules that do not contribute to any inﬁnite derivation. Note that for nontermination analysis, also the dependency graph processor and the usable rules processor do just remove pairs and rules.
Note. Since R is (innermost) nonterminating (by the well-formedness check) whenever R contains a rule x → r for some x ∈ V, we only consider TRSs where all left-hand sides of rules are not variables in the remainder.

13.4. Loops

Loops are derivations of the shape t →+R C[tµ]. They always imply nontermination where the corresponding inﬁnite reduction is

t →+R C[tµ] →+R C[Cµ[tµ2]] →+R C[Cµ[Cµ2[tµ3]]] →+R · · ·

(5)

A TRS which admits a loop is called looping. Note that for innermost rewriting, loopingness does not necessarily imply nontermina-
tion, since the innermost rewrite relation is not closed under substitutions. More precisely, it is not enough to have an “innermost loop” of the form t →i +R C[tµ], since this does not necessarily imply an inﬁnite sequence (5) when restricting to innermost rewriting. Therefore, in [103], the notion of an innermost loop was introduced. To facilitate the certiﬁcation of innermost loops (i.e., to decide for a given loop, whether it is innermost or not), we need its constituting steps, i.e., a derivation of length m > 0 with redexes iσi:

t = t1 →R, 1σ1 t2 →R, 2σ2 · · · →R, mσm tm+1 = C[tµ]
Deﬁnition 13.2 (Innermost Loops). A loop (6) is an innermost loop iﬀ for all 1 and n ∈ N, the term iσiµn is an innermost redex.
That is, no matter how often µ is applied, all steps should be innermost.

(6) im

Lemma 13.3. A loop (6) is an innermost loop iﬀ (5) is an innermost derivation.

Corollary 13.4. An innermost loop implies innermost nontermination.
Note that for every loop (6) and all n ∈ N, the term iσiµn is a redex. Hence, to make sure that those redexes are innermost, it suﬃces to check whether all arguments of iσiµn are normal forms for all n ∈ N. Since iσi is not a variable (we ruled out variables as left-hand sides of R) this is equivalent to checking that for all arguments t of iσi, the term tµn is a normal form for all n ∈ N. Thus, to decide whether a loop is innermost, we can use the following characterization.

13.5. Formalization

147

Lemma 13.5. Let R be a TRS, (6) a loop, and A = 1 i m args( iσi) the set of arguments of redexes in (6). Then, (6) is an innermost loop, iﬀ for all t ∈ A and n ∈ N the term tµn is a normal form, iﬀ for all t ∈ A and → r ∈ R the term tµn does not contain a redex σ for any n ∈ N and σ.
Hence, we can easily check, whether a loop is innermost, whenever for two terms t and , and a substitution µ, we can solve the problem whether there exist n and σ, such that tµn contains a redex σ. Such problems are called redex problems and a large part of [103] is devoted to develop a corresponding decision procedure.
Example 13.6. Consider a loop t →+ C[tµ] for a TRS R containing rules (1)-(4), where µ = {x/cons(z, y), y/cons(z, x), z/0}. Let D[chk(eq(x, y))] → D[false] be a step of the loop. Then, for an innermost loop we must ensure that the term eq(x, y)µn does not contain a redex w.r.t. R, especially not w.r.t. rule (2).
The above decision procedure works in three phases: ﬁrst, redex problems are simpliﬁed into a set of matching problems. Then, a modiﬁed matching algorithm is employed, where in the end identity problems have to be solved. Finally, a decision procedure for identity problems is applied.
In the remainder, let µ be an arbitrary but ﬁxed substitution (usually originating from some loop t →+R C[tµ]).
Deﬁnition 13.7 (Redex, Matching, and Identity Problems). Let s, t, and be terms. Then a redex problem is a pair t | , a generalized matching problem is a set of pairs {t1 1, . . . , tk k} (we call a generalized matching problem having only one pair, a matching problem, and drop the surrounding braces), and an identity problem is a pair s t.
A redex problem t | is solvable iﬀ there is a context C, a substitution σ, and an n ∈ N such that tµn = C[ σ]. A (generalized) matching problem is solvable iﬀ there is a substitution σ and an n ∈ N such that tiµn = iσ for all pairs ti i. An identity problem is solvable iﬀ there is an n ∈ N such that sµn = tµn. In those respective cases, we call (C, σ, n), (σ, n), and n, the solution.

13.5. Formalization
In [109] a straightforward certiﬁcation algorithm for loops is described which does nothing else than checking rewrite steps. We extend this result signiﬁcantly by also formalizing the necessary machinery to decide whether a loop is innermost. In the following, we discuss the three phases of the decision procedure from [103].
From Redex Problems to Matching Problems. A redex problem t | with ∈ V is trivially solvable using the solution (P, { /t}, 0). Thus, in the following we assume that
∈/ V. Then, solvability of t | is equivalent to the existence of a non-variable subterm s of tµn such that s = σ (i.e., matches s). In order to simplify redex problems, we represent these subterms in a ﬁnite way and consequently generate only ﬁnitely many matching problems.
Either, s starts inside t, so s = uµn for some u F t, or s is completely inside µn. But then, it must be of the form uµn for some u F xµ and x in W(t) = n V(tµn), where W(t) collects all variables which can possibly occur in a term of the form tµn. In both

148 Chapter 13. Certiﬁcation of Nontermination Proofs

cases, the equality s = σ can be reformulated to uµn = σ, i.e., solvability of the matching problem u . In total, the redex problem is solvable iﬀ one of the matching problems u is solvable for some u ∈ U (t), where U (t) = {u | t F u or xµ F u ∧ x ∈ W(t)}.
The following theorem (whose formalization was straightforward), corresponds to [103, Theorem 10].

Theorem 13.8. Let t | be a redex problem. Let

Minit (t, ) = if ∈ V then {t } else {u | u ∈ U (t)}

be the set of initial matching problems. Then t | problems in Minit (t, ) is solvable.

is solvable iﬀ one of the matching

Example 13.9. Continuing with Example 13.6, from each redex problem eq(x, y) | we obtain the matching problems eq(x, y) , cons(z, y) , cons(z, x) , and 0 where is an arbitrary left-hand side of the TRS.

Theorem 13.8 shows a way to convert redex problems into matching problems. However, for certiﬁcation, it remains to develop an algorithm that actually computes Minit . To this end, we need to compute U(t), which in turn requires to enumerate all subterms of a term and to compute W(t). Whereas the former is straightforward, computing W(t) is a bit
more diﬃcult: its original deﬁnition contains an inﬁnite union. Note that W(t) is only ﬁnite since we restrict to substitutions of ﬁnite domain and can
be computed by a ﬁxpoint algorithm: iteratively compute V(t), V(tµ), V(tµ2), . . . , until some V(tµk) is reached where no new variables are detected. In principle, it is possible
to formalize this algorithm directly, but we expect such a formalization to require tedious manual termination and soundness proofs. Thus instead, we characterize W(t) by the
following reﬂexive transitive closure.

Lemma 13.10. Let R = {(x, y) | x = y, x ∈ V, y ∈ V(xµ)}. Then W(t) = {y | ∃x ∈ V(t), (x, y) ∈ R∗}.

Note that R in Lemma 13.10 can easily be computed since whenever (x, y) ∈ R then x ∈ dom(µ). Moreover, R is ﬁnite since we only consider substitutions of ﬁnite domain. Hence, the above characterization allows us to compute W by the algorithm of [98] (generating the reﬂexive transitive closure of ﬁnite relations).
Note that W(t) can also be deﬁned inductively as the least set such that V(t) ⊆ W(t) and x ∈ W(t) =⇒ V(xµ) ⊆ W(t). And whenever a ﬁnite set S is deﬁned inductively, instead of implementing an executable algorithm for S manually, it might be easier to characterize S via reﬂexive transitive closures and afterwards execute it via the algorithm of [98]. This approach is not restricted to W: it has been applied in the next paragraph and also in other parts of IsaFoR.
An alternative might be Isabelle/HOL’s predicate compiler [10]. It can be used to obtain executable functions for inductively deﬁned predicates and sets. However, without manual tuning we were not able to obtain appropriate equations for the code generator. Furthermore, additional tuning is required to ensure termination of the resulting code in the target language. Ultimately, the current version of the predicate compiler provides a ﬁxed execution model for predicates and sets (goal-oriented depth-ﬁrst search) which might not yield the best performance for the desired application. Thus, for the time being we use our proposed solution via reﬂexive transitive closures, but perhaps in future versions of Isabelle/HOL, the predicate compiler will be a more convenient alternative.

13.5. Formalization

149

From Matching Problems to Identity Problems. To decide solvability of a (generalized) matching problem {t1 1, . . . , tk k}, in [103], a variant of a standard matching algorithm is used which simpliﬁes (generalized) matching problems until they are in solved form, i.e., all right-hand sides i are variables (or ⊥ is obtained which represents a matching problem without solution).

Deﬁnition 13.11 (Transformation of Matching Problems). In [103] the following transformation ⇒ on general matching problems is deﬁned. If M is a general matching problem with M = {t } M where ∈/ V, then

(i) M ⇒ {t1 1, . . . , tk k} ∪ M , if t = f (t1, . . . , tk) and = f ( 1, . . . , k) (ii) M ⇒ ⊥, if t = f (. . .), = g(. . .), and f = g

(iii) M ⇒ ⊥, if t ∈ V \ Vincr

(iv) M ⇒ {t µ | t

∈ M}, if t ∈ Vincr

The ﬁrst two rules are the standard decomposition and clash rules. Moreover, there are two special rules to handle the case where t is a variable. Here, the set of increasing variables Vincr = {x | ∃n. xµn ∈/ V} plays a crucial role. It collects all those variables for which µ, if applied often enough, introduces a non-variable term. In other words, xµn will always be a variable for x ∈/ Vincr.
In our development, instead of using the above relation, we formalized the rules directly as a function simplify-mp applying the transformation rules deterministically (thereby avoiding the need for a conﬂuence proof, as was required in [103]). As input it takes two generalized matching problems (represented by lists) where the second problem is assumed to be in solved form. Here, [] and · are the list constructors, and @ denotes list concatenation. The possibility of failure is encoded using Isabelle/HOL’s option type, which is either None, in case of an error, or Some r for the result r. In contrast to Deﬁnition 13.11 of [103], our algorithm also returns an integer i which provides a lower bound on how often µ has to be applied to get a solution. The function is given by the following equations (where for brevity do-notation in the option-monad is used):

simplify-mp []

s = return (s, 0)

simplify-mp ((t, x) · mp)

s = simplify-mp mp ((t, x) · s)

simplify-mp ((f (ss), g(ts)) · mp) s = do { guard (f = g); ps ← zip-option ss ts;

simplify-mp (ps@mp) s }

simplify-mp ((x, g(ts)) · mp) s = do { guard (x ∈ Vincr); (mp , i) ← simplify-mp (map-µ ((x, g(ts)) · mp)) (map-µ s); return (mp , i + 1) }
where, map-µ = map (λ(t, ).(tµ, )) using the standard map function for lists, zip-option combines two lists of equal length into Some list of pairs and yields None otherwise, and guard aborts with None if the given predicate is not satisﬁed.
Example 13.12. For = eq(x, x), only one of the redex problems of Example 13.9 remains (all others are simpliﬁed to None), namely eq(x, y) eq(x, x), for which we obtain the simpliﬁed matching problem {x x, y x}.

150 Chapter 13. Certiﬁcation of Nontermination Proofs
In our formalization we show all relevant properties of simplify-mp, i.e., termination, preservation of solvability, and that simplify-mp mp [], if successful, is in solved form. Moreover, we prove the computed lower bound to be sound.
Theorem 13.13. The function simplify-mp satisﬁes the following properties:
• It is terminating.
• It is complete, i.e., if (n, σ) is a solution for mp then there are mp and i such that simplify-mp mp [] = Some (mp , i), i n, and (n − i, σ) is a solution for mp ;
• It is sound, i.e., if (n, σ) is a solution for mp and simplify-mp mp [] = Some (mp , i) then (n + i, σ) is a solution for mp;
• If simplify-mp mp [] = Some (mp , i) then mp is in solved form.
Proof. For termination of simplify-mp mp s, where mp = [(t1, 1), . . . , (tk, k)], we use the lexicographic combination of the following two measures: ﬁrst, we measure the sum of the sizes of the i; and second, we measure the sum of the distances of the ti before turning into non-variables. Here, the distance of some term ti before turning into a non-variable is 0 if ti ∈ V \ Vincr and the least number d such that tiµd ∈/ V, otherwise.
For this lexicographic measure, we get a decrease in the ﬁrst component for the ﬁrst and the second recursive call, and a decrease in the second component for the third recursive call.
Proving soundness and completeness is done via the following property which is proven by induction on the call structure of simplify-mp.
Whenever simplify-mp mp s = r then
• if r = None then mp ∪ s is not solvable,
• if r = Some (mp , i), there is no solution (n, σ) for mp ∪ s where n < i, and (n, σ) is a solution for mp iﬀ (n + i, σ) is a solution for mp ∪ s.
Finally, the fact that simplify-mp mp [] is in solved form is shown by an easy induction proof on the call structure of simplify-mp, where [] is generalized to an arbitrary generalized matching problem that is in solved form.
Although simplify-mp is deﬁned as a recursive function, it cannot directly be used as a certiﬁcation algorithm, due to the following two problems:
The ﬁrst problem is that Vincr is not executable, since it contains an existential statement (remember that we had a similar problem for W earlier). Again, Vincr could be computed via a ﬁxpoint computation accompanied by a tedious manual termination proof. Instead, we once more employ reﬂexive transitive closures to characterize Vincr, which allows us to use the algorithm of [98] to compute it.
Lemma 13.14. Let R = {(x, y) | x = y, x = yµ, x ∈ V, y ∈ V}. Then Vincr = {y | ∃x ∈ V, xµ ∈/ V, (x, y) ∈ R∗}.
The second problem is the usage of implicit parameters. Recall that at the end of Sect. 13.4 we just ﬁxed some substitution µ (which corresponds to what we did in our formalization using Isabelle/HOL’s locale mechanism). Obviously, both Vincr and simplify-mp

13.5. Formalization

151

depend on µ. Hence, we have to pass µ as argument to both. As a result, the modiﬁed version of the last equation of simplify-mp looks as follows:

simplify-mp µ ((x, g(ts)) · mp) s = do {guard (x ∈ Vincr(µ)); (mp , i) ← simplify-mp µ (map-µ ((x, g(ts)) · mp)) (map-µ s); return (mp , i + 1) }

(7)

The problem of equation (7) is its ineﬃciency: In every recursive call, the set of increasing variables Vincr(µ) is newly computed. Therefore, the obvious idea is to compute Vincr(µ) once and for all and pass it as an additional argument V .

simplify-mp µ V ((x, g(ts)) · mp) s = do {guard (x ∈ V ); (mp , i) ← simplify-mp µ V (map-µ ((x, g(ts)) · mp)) (map-µ s); return (mp , i + 1) }

(8)

This version does not have the problem of recomputing Vincr(µ) and we just have to replace the initial call simplify-mp µ mp [] by simplify-mp µ Vincr(µ) mp [].
Although, this looks straightforward and maybe not even worth mentioning, we stress that this solution does not work properly. The problem is that by introducing V , we can call simplify-mp using some V = Vincr(µ), which can cause nontermination. Take for example µ as the empty substitution and V = {x}, then the function call simplify-mp µ V [(x, g(ts))] [] directly leads to exactly the same function call via (8). Hence, termination of simplify-mp deﬁned by (8) cannot be proven. Therefore, the function package of Isabelle/HOL [69] weakens equality (8) by the assumption that simplify-mp has to be terminating on the arguments µ, V , ((x, g(ts)) · mp), and s.
Of course, we can instantiate (8) by V = Vincr(µ). Then we can get rid of the additional assumption. But still, the corresponding unconditional equation is not suitable for code generation, since Vincr on the left-hand side is not a constructor.
Our ﬁnal solution is to use the recent partial-function [70] command of Isabelle/HOL which generates unconditional equations even for nonterminating functions, provided that some syntactic restrictions are met (only one deﬁning equation and the function must either return an option type or be tail-recursive).
Since simplify-mp already returns an option type, we just had to merge all equations into a single case statement. (If the result is not of option type, we can just wrap the original return type into an option type). Afterwards the partial-function command is applicable and we obtain an equation similar to (8) which can be processed by the code generator and eﬃciently computes simplify-mp without recomputing Vincr(µ). Moreover, since we have already shown termination of the ineﬃcient version of simplify-mp, we know that also the eﬃcient version does terminate whenever it is called with V = Vincr(µ). In our formalization we actually have two versions of simplify-mp: an abstract version which is unsuitable for code generation (and also ineﬃcient) and a concrete version. All the above properties are proven on the abstract version neglecting any eﬃciency problems. Afterwards it is shown that the concrete version computes the same results as the abstract one (which is relatively easy since the call-structure is the same). In this way, we get the best of two worlds: abstraction and ease of reasoning from the abstract version (using sets, existential statements, and the induction rules from the function package), and eﬃciency from the concrete version (using lists and concrete functions to obtain witnesses).
The above mentioned problem is not restricted to simplify-mp. Whenever the termination of a function relies on the correct initialization of some precomputed values, a similar problem arises. Currently, this can be solved by writing a second function via the

152 Chapter 13. Certiﬁcation of Nontermination Proofs
partial-function command, as shown above. Although the second deﬁnition is mainly a copy of the original one, we can currently not recommend to use it as a replacement, since the function package provides much more convenience for standard deﬁnitions than when using the partial-function command. If the functionality of partial functions is extended, the situation might change (and we would welcome any eﬀort in that direction).
Continuing with deciding matching problems, we are in the situation, that by using simplify-mp we can either directly detect that a matching problem is unsolvable or obtain an equivalent generalized matching problem in solved form M = {t1 x1, . . . , tk xk}. In principle, M has the solution (n, σ) where n is arbitrary and σ(xi) = tiµn. However, this deﬁnition of σ is not always well-deﬁned if there are i and j such that xi = xj and i = j. To decide whether it is possible to adapt the proposed solution, we must know whether tiµn = tjµn for some n, i.e., we must solve the identify problem ti tj.
The following result of [103, Theorem 14 (iv)] is easily formalized and also poses no challenges for certiﬁcation. Afterwards it remains to decide identity problems.
Theorem 13.15. Let M = {t1 x1, . . . , tk xk} be a generalized matching problem in solved form. Deﬁne Iinit = {ti tj | 1 i < j k, xi = xj}. Then M is solvable iﬀ all identity problems in Iinit are solvable.
To prove this theorem, the key observation is that we can always combine several solutions of identity problems: Whenever nij are solutions to the identity problems ti tj, respectively, then the maximum n of all nij is a solution to all identity problems ti tj. And then also (n, σ) is a solution to M where σ(xi) = tiµn is guaranteed to be welldeﬁned.
Example 13.16. For the remaining matching problem of Example 13.12 we generate one identity problem: x y.
Deciding Identity Problems. In [103, Section 3.4] a complicated algorithm is presented to decide solvability of an identity problem s t. The main idea is to iteratively generate (s, t), (sµ, tµ), (sµ2, tµ2), . . . until either some (sµi, tµi) with sµi = tµi is generated, or it can be detected that no solution exists. For the latter, some easy conditions for unsolvability are identiﬁed, e.g., sµi = C[f (ss)] and tµi = C[x] where x ∈/ Vincr. However, these conditions do not suﬃce to detect all unsolvable identity problems. Therefore, in each iteration conﬂicts (indicating which subterms have to become equal after applying µ several times, to obtain overall equality), are stored in a set S, and two suﬃcient conditions on pairs of conﬂicts from S are presented that allow to conclude unsolvability.
For the overall algorithm, soundness is rather easy to establish, completeness is more challenging, and the termination proof is the most diﬃcult part. To be more precise, it is shown that nontermination of the algorithm allows to construct an inﬁnite sequence of terms where no two terms are embedded into each other (which is not possible due to Kruskal’s tree theorem). Hence, the formalization would require a formalization of the tree theorem. Moreover, the implicit complexity bound on the number of required iterations is quite high.
The reason for using Kruskal’s tree theorem is that in [103] the conﬂicts in S consist of a variable, a position, and a term which is not bounded in its size. So, there is no a priori bound on S. We were able to simplify the decision procedure for s t considerably since we only store conﬂicts whose constituting terms are in the set of conﬂict terms
CT (s, t) = {u | v u, v ∈ {s, t} ∪ ran(µ)}.

13.5. Formalization

153

To be more precise, all conﬂicts are of the form (u, v, m) where (u, v) is contained in the ﬁnite set S = (CT (s, t) ∩ V) × CT (s, t). Whenever we see a conﬂict (u, v, ) for the second time, the algorithm stops. Thus, we get a decision procedure which needs at most |S| iterations and whose termination proof is easy. In contrast to [103], our procedure does neither require any preprocessing on µ nor uniﬁcation.
The key idea to get an a priory bound on the set of conﬂicts, is to consider identity problems of a generalized form s tµn which can be represented by the triple (s, t, n). Then applying substitutions can be done by increasing n, and all terms that are generated during an execution of the algorithm are terms from CT (s, t).
Before presenting the main algorithm for deciding identity problems s tµn, we require an auxiliary algorithm conﬂicts (s, t, n) that computes the set of conﬂicts for an identity problem, i.e., subterms of s and tµn with diﬀerent roots.

conﬂicts conﬂicts conﬂicts conﬂicts conﬂicts

(s, y, n + 1) (x, y, 0) (f (ss), y, 0) (x, g(ts), n) (f (ss), g(ts), n)

= conﬂicts (s, µ(y), n) = if x = y then ∅ else {(x, y, 0)} = {(y, f (ss), 0)} = {(x, g(ts), n)} = if f = g ∧ |ss| = |ts|
then (si,ti)∈zip ss ts conﬂicts (si, ti, n) else {(f (ss), g(ts), n)}

We identiﬁed and formalized the following properties of conﬂicts and CT . Lemma 13.17. • sσ = tµnσ iﬀ ∀(u, v, m) ∈ conﬂicts (s, t, n). uσ = vµmσ.

• if (u, v, m) ∈ conﬂicts (s, t, n) then – root(u) = root(v) – v ∈ V implies m = 0 ∧ u ∈ V – ∃k p. n = m + k ∧ ((s|p, tµk|p) = (u, v) ∨ ((s|p, tµk|p) = (v, u) ∧ m = 0)) – {u, v} ⊆ CT (s, t)

• {u, v} ⊆ CT (s, t) implies CT (u, v) ⊆ CT (s, t)

• CT (u, v) ⊆ CT (uµ, v) whenever u ∈ V

Using conﬂicts we can now formulate the algorithm ident-solve which decides identity problem s t if invoked with ident-solve ∅ (s, t, 0).

ident-solve S idp = let C = conﬂicts idp in if (f (us), , ) ∈ C ∨ ((u, v, ) ∈ C ∧ (u, v, ) ∈ S) then None else do { ns ← map-option (λ(u, v, m). ident-solve ({(u, v, m)} ∪ S) (uµ, v, m + 1)) C; return (max {n + 1 | n ∈ ns}) }

where map-option is a variant of the map function on lists whose overall result is None if the supplied function returns None for any element of the given list.

Example 13.18. We continue Example 13.16 by invoking ident-solve ∅ (x, y, 0). This leads to the conﬂict (x, y, 0). Afterwards, ident-solve {(x, y, 0)} (cons(z, y), y, 1) is invoked which results in the conﬂict (y, x, 0). Finally, the conﬂict (x, y, 0) is generated again when calling ident-solve {(x, y, 0), (y, x, 0)} (cons(z, x), x, 1) and the result None is obtained.

154 Chapter 13. Certiﬁcation of Nontermination Proofs

We formalized termination, soundness, and completeness of ident-solve.

Lemma 13.19 (Termination). ident-solve is terminating.
Proof. Take the measure function λS (s, t, ). |(CT (s, t)∩V)×CT (s, t)\{(a, b) | (a, b, ) ∈ S}|. Then the actual termination proof boils down to showing

L := (CT (s, t) ∩ V) × CT (s, t) \ {(a, b) | (a, b, ) ∈ S} ⊃ (CT (uµ, v) ∩ V) × CT (uµ, v) \ {(a, b) | (a, b, ) ∈ {(u, v, m)} ∪ S} =: R

whenever ident-solve S (s, t, n) leads to a call ident-solve ({(u, v, m)} ∪ S) (uµ, v, m + 1), i.e., whenever (u, v, m) ∈ conﬂicts (s, t, n), (u, v, ) ∈/ S, and u ∈ V. By Lemma 13.17 we obtain {u, v} ⊆ CT (s, t) and CT (uµ, v) ⊆ CT (u, v) ⊆ CT (s, t). Hence, L ⊇ R and since (u, v) ∈ L \ R we even have L ⊃ R.
Lemma 13.20 (Soundness). If ident-solve S (s, t, n) = Some i then sµi = tµnµi.

Proof. We perform induction on the call-structure of ident-solve. So, assume ident-solve S
(s, t, n) = Some i. By deﬁnition of ident-solve, for all (u, v, m) ∈ conﬂicts (s, t, n) there
is some j such that ident-solve ({(u, v, m)} ∪ S) (uµ, v, m + 1) = Some j and i is the maximum of all j + 1. Using the induction hypothesis, we conclude uµj+1 = uµµj = vµm+1µj = vµmµj+1 for all (u, v, m) ∈ conﬂicts (s, t, n), and since i ≥ j + 1 we also achieve uµi = vµmµi. But this is equivalent to sµi = tµnµi by Lemma 13.17 (where σ = µi).

Lemma 13.21 (Completeness). Whenever the identity problem s t is solvable then ident-solve ∅ (s, t, 0) = None.

Proof. If s t is solvable then there is some N such that sµN = tµN . Our actual proof shows the following property ( ) for all S, s , t , n, n , and p where (a, b) =↔ (c, d) abbreviates (a, b) = (c, d) ∨ (a, b) = (d, c).1

(sµn|p, tµn|p) =↔ (s , t µn ) −→ (∀(u, v, m) ∈ S. (m = 0 ∨ v ∈/ V) ∧ root(u) = root(v) ∧
(∃q1 q2 n1. p = q1q2 ∧ n1 < n ∧ (sµn1|q1, tµn1|q1) =↔ (u, vµm))) −→ ident-solve S (s , t , n ) = None

(9) (10)
(11)

Once ( ) is established, the lemma immediately follows from ( ) which is instantiated by

S = ∅, s = s, t = t, n = n = 0, and p = (the empty position).

To prove ( ), we perform induction on the call-structure of ident-solve. So, we assume

(9) and (10), and have to show (11). By sµN = tµN we conclude sµn|pµN = sµN µn|p = tµN µn|p = tµn|pµN , and thus s µN = t µn µN by (9). By Lemma 13.17 this shows uµN = vµmµN for all (u, v, m) ∈ conﬂicts (s , t , n ) =: C. In a similar way we prove uµN = vµmµN

for all (u, v, m) ∈ S using (10).

Next we consider an arbitrary (u, v, m) ∈ C. By Lemma 13.17 we have root(u) =

root (v), (u, v) ∨

m=0 ((s |q1,

∨ v ∈/ t µk|q1

V, and ) = (v,

there are q1 and k such that u) ∧ m = 0). In particular,

n= this

m + k and (s implies (s |q1,

|q1 , t µk|q1 ) t µk|q1µm)

= =↔

(u, vµm). Moreover, we know uµN = vµmµN .

1In the formalization, ( ) looks even more complicated, since here we dropped all parts that restrict p and q1 to valid positions.

13.5. Formalization

155

First, we show that u ∈ V, and hence the condition (f (us), , ) ∈ C is not satisﬁed. The reason is that u ∈/ V also implies v ∈/ V by Lemma 13.17 which implies the contradiction root(u) = root(uµN ) = root(vµmµN ) = root(v) = root(u).
Second, ident-solve ({(u, v, m)} ∪ S) (uµ, v, m + 1) = None. To show this, we just apply
the induction hypothesis where it remains to show that (9) and (10) are satisﬁed (where the values of S, s ,t ,n, n , p are {(u, v, m)}∪S, uµ, v, n+1, m+1, and pq1, respectively). To this end, we derive the following equality.

(sµn|pq1 , tµn|pq1 )

= (sµn|p|q1, tµn|p|q1) = (s |q1, t µk|q1µm)

=↔ =↔

(s |q1, t µn (u, vµm).

|q1 )

(12)

Using (12), root(u) = root(v), and m = 0 ∨ v ∈/ V, we conclude that (10) is satisﬁed for

the new conﬂict (u, v, m). Moreover, (10) is trivially satisﬁed for all (old) conﬂicts in S,

by using (10) (for the old inputs (s , t , n ), . . .). Finally, by applying µ on all terms in (12) we obtain (sµn+1|pq1, tµn+1|pq1) =↔ (uµ, vµm+1) which is exactly the required (9).
The last potential reason for ident-solve S (s , t , n ) to be None, is that there is some

m such that (u, v, m ) ∈ S. We assume that such an m exists and eventually show a

contradiction (the most diﬃcult part of this proof). By (10) we conclude that m = 0∨v ∈/ V and there are p1, q3, and n2 where p = p1q3, n2 < n, and (sµn2|p1, tµn2|p1) =↔ (u, vµm ). Since n2 < n there is some k1 with n = n2 + k1 and k1 > 0. Starting from (12) we derive

(u, vµm)

=↔ (sµn|pq1 , tµn|pq1 )

= =↔

(sµn2+k1 |p1q3q1 , tµn2+k1 |p1q3q1 ) (uσ|q, vµm σ|q)

= (sµn2|p1σ|q, tµn2|p1σ|q)

(13)

where σ and q are abbreviations for µk1 and q3q1, respectively. Using (13) it is possible to derive a contradiction via a case analysis.
If m = m then (13) yields both uσσ|qq = u and vµmσσ|qq = vµm. Thus, u(σσ)i|(qq)i = u and vµm(σσ)i|(qq)i = vµm for all i. For m = m we can further show u = vµm and hence, u(σσ)i|(qq)i = vµm(σσ)i|(qq)i for all i. This leads to the desired contradiction since we know that uµN = vµmµN , and hence u(σσ)N = uµ2k1N = uµN µ(2k1−1)N = vµmµN µ(2k1−1)N = vµmµ2k1N = vµm(σσ)N , which shows that for i = N the previous inequality does not
hold.
Otherwise m = m . Hence, m = 0 ∨ m = 0 and in combination with m = 0 ∨ v ∈/ V
and m = 0 ∨ v ∈/ V we conclude v ∈/ V. Thus, u ∈ V by Lemma 13.17 as (u, v, m) ∈
conﬂicts (s , t , n ). Then by a case analysis on (13) we can show that there are i and j such that uµi uµj. Moreover, from uµN = vµmµN and uµN = vµm µN we obtain uµN+m = uµN+m . In combination with m = m and uµi uµj this leads to the desired
contradiction.

Putting all lemmas on ident-solve together, we can even give a decision procedure for identity problems which does not require ident-solve at all, and shows an explicit bound on a solution.
Theorem 13.22. An identity problem s t is solvable iﬀ sµn = tµn where n = |CT (s, t)∩ V| · |CT (s, t)|.
Proof. If an identity problem is solvable, then the result of ident-solve ∅ (s, t, 0) = Some i for some i by Lemma 13.21. From the termination proof in Lemma 13.19 we know that i |CT (s, t)∩V|·|CT (s, t)| = n (unfortunately, in Isabelle/HOL we could not extract this knowledge from the termination proof and had to formalize this simple result separately). And by Lemma 13.20 we infer that sµi = tµi. But then also sµn = tµn.

156 Chapter 13. Certiﬁcation of Nontermination Proofs
Note that |CT (s, t)| |s| + |t| + |µ| where |µ| is the size of all terms in the range of µ. Hence, the value of n in Theorem 13.22 is quadratic in the size of the input problem. We conjecture that even a linear bound exists, although some proof attempts failed. As an example, we tried to replace the condition (u, v, ) ∈ C ∧ (u, v, ) ∈ S by (u, , ) ∈ C ∧ (u, , ) ∈ S in ident-solve to get a linear number of iterations. However, then ident-solve is not complete anymore.
13.6. Conclusions
We have formalized several techniques to certify compositional (innermost) nontermination proofs, where the hardest part was the decision procedure of [103], which decides whether a loop is an innermost loop. In our formalization, we were able to simplify the algorithm and the proofs for identity problems considerably: a complex algorithm can be replaced by a single line due to Theorem 13.22.
With this result we can also show (but have not formalized) that all considered decision problems of this paper are in P.
Theorem 13.23. Deciding whether an identity problem, a matching problem, or a redex problem is solvable is in P. Moreover, deciding whether a loop is an innermost loop is in P.
Proof. We start with identity problems. By Theorem 13.22 we just have to check sµn = tµn for n = |CT (s, t)∩V|·|CT (s, t)|. When using DAG compressed terms we can represent sµn and tµn in polynomial space and in turn use the algorithms of [19,91] to check equality in polynomial time. Note that even if the input (s, t, µ) is already DAG compressed, the problem is still in P. The reason is that |CT (s, t)| |s| + |t| + |µ| also holds when sizes of terms are measured according to their DAG representation.
For matching problems t , we ﬁrst observe that simplify-mp [(t, )] [] requires at most |Vincr| · | | many iterations, and when using DAG compression, the resulting simpliﬁed matching problem can be represented in polynomial space. Hence, the resulting identity problems can all be solved in polynomial time.
Using the result for matching problems, by Theorem 13.8 it follows that redex problems t | are decidable in P: The number of matching problems in Minit as well as the size of each element of Minit is linear in the sizes of t, , and µ.
Finally, since redex problems can be decided in P, by Lemma 13.5 this also holds for the question, whether a loop is an innermost loop.
We have also shown how reﬂexive transitive closures can be used to avoid termination proofs, and how partial functions help to develop eﬃcient algorithms.
We tested our algorithms within our certiﬁer CeTA (version 2.3) in combination with the termination analyzer AProVE [39], which is (as far as we know) currently the only tool, that can prove innermost nontermination of term rewrite systems. Through our experiments, a major soundness bug in AProVE was revealed: one of the two loop-ﬁnding methods completely ignored the strategy. After this bug was ﬁxed, all generated nontermination proofs could be certiﬁed. Since the overhead for certiﬁcation is negligible (AProVE required 151 minutes to generate all proofs, whereas CeTA required 4 seconds to certify them), we encourage termination tool users to always certify their proofs. For more details on the experiments, we refer to http://cl-informatik.uibk.ac.at/software/ceta/ experiments/nonterm/.

13.6. Conclusions

157

Future work consists of integrating further techniques for which completeness is not obvious into our framework. Examples are innermost narrowing [3] and the switch from innermost termination to termination for TRSs and DPPs.
Acknowledgments We thank Lukas Bulwahn for helpful information on Isabelle/HOL’s predicate compiler.

Bibliography
[1] E. Albert, P. Arenas, S. Genaim, M. Go´mez-Zamalloa, G. Puebla, D. V. Ram´ırezDeantes, G. Rom´an, and D. Zanardini. Termination and cost analysis with COSTA and its user interfaces. ENTCS, 258(1):109–121, 2009.
[2] T. Aoto, T. Yoshida, and J. Toyama. Proving conﬂuence of term rewriting systems automatically. In Proc. of the 20th International Conference on Rewriting Techniques and Applications (RTA 2009), volume 5595 of LNCS, pages 93–102, 2009.
[3] T. Arts and J. Giesl. Termination of term rewriting using dependency pairs. Theor. Comput. Sci., 236(1-2):133–178, 2000.
[4] M. Avanzini, G. Moser, and A. Schnabl. Automated implicit computational complexity analysis (system description). In Proc. of the 4th International Joint Conference on Automated Reasoning (IJCAR 2008), volume 5195 of LNAI, pages 132–138, 2008.
[5] F. Baader and T. Nipkow. Term Rewriting and All That. Cambridge University Press, 1998.
[6] G. Barthe, J. Forest, D. Pichardie, and V. Rusu. Deﬁning and reasoning about recursive functions: a practical tool for the Coq proof assistant. In Proc. of the 8th International Symposium on Functional and Logic Programming (FLOPS 2006), volume 3945 of LNCS, pages 114–129, 2006.
[7] A. M. Ben-Amram and M. Codish. A SAT-based approach to size change termination with global ranking functions. In Proc. of the 16th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS 2008), volume 4963 of LNCS, pages 218–232, 2008.
[8] A. M. Ben-Amram and C. S. Lee. Program termination analysis in polynomial time. ACM Trans. Program. Lang. Syst., 29(1), 2007.
[9] A. Ben Cherifa and P. Lescanne. Termination of rewriting systems by polynomial interpretations and its implementation. Science of Computer Programming, 9(2):137– 159, 1987.
[10] S. Berghofer, L. Bulwahn, and F. Haftmann. Turning inductive into equational speciﬁcations. In Proc. of the 22nd International Conference on Theorem Proving in Higher Order Logics (TPHOLs 2009), volume 5674 of LNCS, pages 131–146, 2009.
[11] Y. Bertot and P. Cast´eran. Interactive Theorem Proving and Program Development; Coq’Art: The Calculus of Inductive Constructions. TCS Texts. Springer, 2004.

160 Bibliography
[12] F. Blanqui, W. Delobel, S. Coupet-Grimal, S. Hinderer, and A. Koprowski. CoLoR, a Coq library on rewriting and termination. In Proc. of the 8th International Workshop on Termination (WST 2006), pages 69–73, 2006.
[13] F. Blanqui and A. Koprowski. CoLoR: a Coq library on well-founded rewrite relations and its application to the automated veriﬁcation of termination certiﬁcates. Mathematical Structures in Computer Science, 21(4):827–859, 2011.
[14] R. S. Boyer and J. S. Moore. A Computational Logic. Academic Press, 1980.
[15] M. Brockschmidt, R. Musiol, C. Otto, and J. Giesl. Automated termination proofs for Java programs with cyclic data. In Proc. of the 24th International Conference on Computer Aided Veriﬁcation (CAV 2012), volume 7358 of LNCS, pages 105–122, 2012.
[16] M. Brockschmidt, C. Otto, C. von Essen, and J. Giesl. Termination graphs for Java bytecode. In Veriﬁcation, Induction, Termination Analysis - Festschrift for Christoph Walther on the Occasion of His 60th Birthday, volume 6463 of LNCS, pages 17–37, 2010.
[17] W. Buchholz. Proof-theoretic analysis of termination proofs. Ann. Pure Appl. Logic, 75(1-2):57–65, 1995.
[18] L. Bulwahn, A. Krauss, and T. Nipkow. Finding lexicographic orders for termination proofs in Isabelle/HOL. In Proc. of the 20th International Conference on Theorem Proving in Higher Order Logics (TPHOLs 2007), volume 4732 of LNCS, pages 38– 53, 2007.
[19] G. Busatto, M. Lohrey, and S. Maneth. Eﬃcient memory representation of XML documents. In Proc. of the 10th International Symposium on Database Programming Languages (DBPL 2005), volume 3774 of LNCS, pages 199–216, 2005.
[20] M. Codish, C. Fuhs, J. Giesl, and P. Schneider-Kamp. Lazy abstraction for sizechange termination. In Proc. of the 17th International Conference on Logic for Programming, Artiﬁcial Intelligence and Reasoning (LPAR 2010), volume 6397 of LNCS, pages 217–232, 2010.
[21] E. Contejean, P. Courtieu, J. Forest, O. Pons, and X. Urbain. Certiﬁcation of automated termination proofs. In Proc. of the 6th International Symposium on Frontiers of Combining Systems (FroCoS 2007), volume 4720 of LNAI, pages 148– 162, 2007.
[22] E. Contejean, C. March´e, B. Monate, and X. Urbain. CiME. http://cime.lri.fr.
[23] E´. Contejean, A. Paskevich, X. Urbain, P. Courtieu, O. Pons, and J. Forest. A3PAT, an approach for certiﬁed automated termination proofs. In Proc. of the ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation (PEPM 2010), pages 63–72, 2010.
[24] B. Cook, A. Podelski, and A. Rybalchenko. Termination proofs for systems code. In Proc. of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2006), pages 415–426, 2006.

Bibliography

161

[25] P. Courtieu, J. Forest, and X. Urbain. Certifying a termination criterion based on graphs, without graphs. In Proc. of the 21st International Conference on Theorem Proving in Higher Order Logics (TPHOLs 2008), volume 5170 of LNCS, pages 183– 198, 2008.
[26] N. Dershowitz. Orderings for term-rewriting systems. Theor. Comput. Sci., 17:279– 301, 1982.
[27] N. Dershowitz. Termination of rewriting. J. Symb. Comp., 3(1-2):69–116, 1987.
[28] N. Dershowitz. Termination dependencies. In Proc. of the 6th International Workshop on Termination (WST 2003), pages 27–30, 2003.
[29] N. Dershowitz and Z. Manna. Proving termination with multiset orderings. Comm. of the ACM, 22(8):465–476, 1979.
[30] F. Emmes, T. Enger, and J. Giesl. Proving non-looping non-termination automatically. In Proc. of the 6th International Joint Conference on Automated Reasoning (IJCAR 2012), volume 7364 of LNAI, pages 225–240, 2012.
[31] J. Endrullis. Jambox. Available from http://joerg.endrullis.de.
[32] J. Endrullis, J. Waldmann, and H. Zantema. Matrix interpretations for proving termination of term rewriting. Journal of Automated Reasoning, 40(2-3):195–220, 2008.
[33] C. Fuhs, J. Giesl, A. Middeldorp, P. Schneider-Kamp, R. Thiemann, and H. Zankl. Maximal termination. In Proc. of the 19th International Conference on Rewriting Techniques and Applications (RTA 2008), volume 5117 of LNCS, pages 110–125, 2008.
[34] A. L. Galdino and M. Ayala-Rinco´n. A PVS theory for term rewriting systems. ENTCS, 247:67–83, 2009.
[35] A. L. Galdino and M. Ayala-Rinco´n. A formalization of the Knuth-Bendix(-Huet) critical pair theorem. Journal of Automated Reasoning, 45(3):301–325, 2010.
[36] A. Geser, D. Hofbauer, J. Waldmann, and H. Zantema. On tree automata that certify termination of left-linear term rewriting systems. Information and Computation, 205(4):512–534, 2007.
[37] J. Giesl and T. Arts. Veriﬁcation of Erlang processes by dependency pairs. Appl. Alg. Eng. Comm. Comput., 12(1,2):39–72, 2001.
[38] J. Giesl, M. Raﬀelsieper, P. Schneider-Kamp, S. Swiderski, and R. Thiemann. Automated termination proofs for Haskell by term rewriting. ACM Transactions on Programming Languages and Systems, 33(2):7:1–7:39, 2011.
[39] J. Giesl, P. Schneider-Kamp, and R. Thiemann. AProVE 1.2: Automatic termination proofs in the dependency pair framework. In Proc. of the 3rd International Joint Conference on Automated Reasoning (IJCAR 2006), volume 4130 of LNAI, pages 281–286, 2006.

162 Bibliography
[40] J. Giesl, R. Thiemann, and P. Schneider-Kamp. The dependency pair framework: Combining techniques for automated termination proofs. In Proc. of the 11th International Conference on Logic for Programming, Artiﬁcial Intelligence and Reasoning (LPAR 2004), volume 3452 of LNAI, pages 301–331, 2005.
[41] J. Giesl, R. Thiemann, and P. Schneider-Kamp. Proving and disproving termination of higher-order functions. In Proc. of the 5th International Symposium on Frontiers of Combining Systems (FroCoS 2005), volume 3717 of LNAI, pages 216–231, 2005.
[42] J. Giesl, R. Thiemann, P. Schneider-Kamp, and S. Falke. Mechanizing and improving dependency pairs. Journal of Automated Reasoning, 37(3):155–203, 2006.
[43] M. Gordon. From LCF to HOL: a short history. In Proof, Language, and Interaction, Essays In Honour of Robin Milner, pages 169–186. MIT Press, 2000.
[44] M. Gordon. From LCF to HOL: A short history. In Proof, Language, and Interaction, pages 169–185. MIT Press, 2000.
[45] M. J. C. Gordon, R. Milner, and C. P. Wadsworth. Edinburgh LCF, volume 78 of LNCS. Springer, 1979.
[46] B. Gramlich. Abstract relations between restricted termination and conﬂuence properties of rewrite systems. Fundamenta Informaticae, 24:3–23, 1995.
[47] S. Gulwani, K. Mehra, and T. Chilimbi. SPEED: precise and eﬃcient static estimation of program computational complexity. In Proc. of the 36th ACM Symposium on Principles of Programming Languages (POPL 2009), pages 127–139, 2009.
[48] F. Haftmann. Code generation from Isabelle/HOL theories, Apr. 2009. http:// isabelle.in.tum.de/doc/codegen.pdf.
[49] F. Haftmann and T. Nipkow. Code generation via higher-order rewrite systems. In Proc. of the 10th International Symposium on Functional and Logic Programming (FLOPS 2010), volume 6009 of LNCS, pages 103–117, 2010.
[50] N. Hirokawa and A. Middeldorp. Automating the dependency pair method. Information and Computation, 199(1-2):172–199, 2005.
[51] N. Hirokawa and A. Middeldorp. Tyrolean Termination Tool: Techniques and features. Information and Computation, 205(4):474–511, 2007.
[52] N. Hirokawa and A. Middeldorp. Decreasing diagrams and relative termination. Journal of Automated Reasoning, 47(4):481–501, 2011.
[53] N. Hirokawa, A. Middeldorp, and H. Zankl. Uncurrying for termination. In Proc. of the 15th International Conference on Logic for Programming, Artiﬁcial Intelligence and Reasoning (LPAR 2008), volume 5330 of LNAI, pages 667–681, 2008.
[54] D. Hofbauer and J. Waldmann. Match-bounds for relative termination. In Proc. of the 11th International Workshop on Termination (WST 2010), 2010.
[55] J. Hoﬀmann, K. Aehlig, and M. Hofmann. Resource aware ML. In Proc. of the 24th International Conference on Computer Aided Veriﬁcation (CAV 2012), volume 7358 of LNCS, pages 781–786, 2012.

Bibliography

163

[56] H. Hong and D. Jakuˇs. Testing positiveness of polynomials. Journal of Automated Reasoning, 21(1):23–38, 1998.
[57] J.-P. Jouannaud and A. Rubio. The higher-order recursive path ordering. In Proc. of the 14.th Annual IEEE Symposium on Logic in Computer Science (LICS 1999), pages 402–411. IEEE Computer Society Press, 1999.
[58] S. Kamin and J. J. L´evy. Two generalizations of the recursive path ordering. Unpublished manuscript, University of Illinois, 1980.
[59] R. Kennaway, J. W. Klop, R. Sleep, and F.-J. de Vries. Comparing curried and uncurried rewriting. Journal of Symbolic Computation, 21(1):15–39, 1996.
[60] G. Klein, J. Andronick, K. Elphinstone, G. Heiser, D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Norrish, T. Sewell, H. Tuch, and S. Winwood. seL4: formal veriﬁcation of an operating-system kernel. Comm. of the ACM, 53(6):107– 115, 2010.
[61] G. Klein and T. Nipkow. A machine-checked model for a Java-like language, virtual machine, and compiler. ACM Transactions on Programming Languages and Systems, 28(4):619–695, 2006.
[62] A. Koprowski. TPA: Termination proved automatically. In Proc. of the 17th International Conference on Rewriting Techniques and Applications (RTA 2006), volume 4098 of LNCS, pages 257–266, 2006.
[63] A. Koprowski. Coq formalization of the higher-order recursive path ordering. Appl. Alg. Eng. Comm. Comput., 20(5-6):379–425, 2009.
[64] A. Koprowski and J. Waldmann. Arctic termination . . . below zero. In Proc. of the 19th International Conference on Rewriting Techniques and Applications (RTA 2008), volume 5117 of LNCS, pages 202–216, 2008.
[65] K. Korovin and A. Voronkov. Orienting rewrite rules with the Knuth-Bendix order. Information and Computation, 183(2):165–186, 2003.
[66] M. Korp and A. Middeldorp. Match-bounds revisited. Information and Computation, 207(11):1259–1283, 2009.
[67] M. Korp, C. Sternagel, H. Zankl, and A. Middeldorp. Tyrolean Termination Tool 2. In Proc. of the 20th International Conference on Rewriting Techniques and Applications (RTA 2009), volume 5595 of LNCS, pages 295–304, 2009.
[68] A. Krauss. Certiﬁed size-change termination. In Proc. of the 21st International Conference on Automated Deduction (CADE 2007), volume 4603 of LNCS, pages 460–475, 2007.
[69] A. Krauss. Partial and nested recursive function deﬁnitions in higher-order logic. Journal of Automated Reasoning, 44(4):303–336, 2010.
[70] A. Krauss. Recursive deﬁnitions of monadic functions. In Proc. of the Workshop on Partiality and Recursion in Interactive Theorem Proving (PAR 2010), volume 43 of EPTCS, pages 1–13, 2010.

164 Bibliography
[71] M. S. Krishnamoorthy and P. Narendran. On recursive path ordering. Theor. Comput. Sci., 40:323–328, 1985.
[72] J. B. Kruskal. Well-quasi-ordering, the tree theorem, and Vazsonyi’s conjecture. Transactions of the American Mathematical Society, 95(2):210–225, 1960.
[73] D. Lankford. On proving term rewrite systems are noetherian. Technical Report MTP-3, Louisiana Technical University, Ruston, LA, USA, 1979.
[74] C. S. Lee, N. D. Jones, and A. M. Ben-Amram. The size-change principle for program termination. In Proc. of the 28th ACM Symposium on Principles of Programming Languages (POPL 2001), pages 81–92, 2001.
[75] A. Lochbihler. Verifying a compiler for Java threads. In Proc. of the 19th European Symposium on Programming (ESOP 2010), volume 6012 of LNCS, pages 427–447, 2010.
[76] S. Lucas. Polynomials over the reals in proofs of termination: From theory to practice. RAIRO Theor. Inf. Appl., 39(3):547–586, 2005.
[77] M. Marchiori. Logic programs as term rewriting systems. In Proc. of the 4th International Conference on Algebraic and Logic Programming (ALP 1994), volume 850 of LNCS, pages 223–241, 1994.
[78] A. Martelli and U. Montanari. An eﬃcient uniﬁcation algorithm. ACM Transactions on Programming Languages and Systems, 4(2):258–282, 1982.
[79] A. Middeldorp. Modular Properties of Term Rewriting Systems. PhD thesis, Vrije Universiteit, Amsterdam, 1990.
[80] M. O. Myreen and J. Davis. A veriﬁed runtime for a veriﬁed theorem prover. In Proc. of the 2nd International Conference on Interactive Theorem Proving (ITP 2011), volume 6898 of LNCS, pages 265–280, 2011.
[81] F. Neurauter, H. Zankl, and A. Middeldorp. Revisiting matrix interpretations for polynomial derivational complexity of term rewriting. In Proc. of the 17th International Conference on Logic for Programming, Artiﬁcial Intelligence and Reasoning (LPAR 2010), volume 6397 of LNCS, pages 550–564, 2010.
[82] M. Nguyen, D. D. Schreye, J. Giesl, and P. Schneider-Kamp. Polytool: Polynomial interpretations as a basis for termination analysis of logic programs. Theory and Practice of Logic Programming, 11(1):33–63, 2011.
[83] T. Nipkow. An inductive proof of the wellfoundedness of the multiset order, 1998. Available at http://www4.in.tum.de/~nipkow/misc/multiset.ps.
[84] T. Nipkow, L. C. Paulson, and M. Wenzel. Isabelle/HOL – A Proof Assistant for Higher-Order Logic, volume 2283 of LNCS. Springer, 2002.
[85] E. Ohlebusch. A simple proof of suﬃcient conditions for the termination of the disjoint union of term rewriting systems. Bulletin of the EATCS, 50:223–228, 1993.
[86] E. Ohlebusch. Termination of logic programs: Transformational methods revisited. Appl. Alg. Eng. Comm. Comput., 12(1-2):73–116, 2001.

Bibliography

165

[87] C. Otto, M. Brockschmidt, C. von Essen, and J. Giesl. Automated termination analysis of Java bytecode by term rewriting. In Proc. of the 21st International Conference on Rewriting Techniques and Applications (RTA 2010), volume 6 of LIPIcs, pages 259–276, 2010.
[88] S. Owre, J. M. Rushby, and N. Shankar. PVS: A prototype veriﬁcation system. In Proc. of the 11th International Conference on Automated Deduction (CADE 1992), volume 607 of LNAI, pages 748–752, 1992.
[89] L. C. Paulson. Logic and Computation: Interactive Proof with Cambridge LCF. Cambridge University Press, 1987.
[90] S. Peyton Jones et al. The Haskell 98 language and libraries: The revised report. Journal of Functional Programming, 13(1):0–255, Jan 2003.
[91] M. Schmidt-Schauß. Polynomial equality testing for terms with shared substructures. Frank report 21, Institut fu¨r Informatik. FB Informatik und Mathematik. J.W. Goethe-Universit¨at, Frankfurt am Main, 2005.
[92] P. Schneider-Kamp, R. Thiemann, E. Annov, M. Codish, and J. Giesl. Proving termination using recursive path orders and SAT solving. In Proc. of the 6th International Symposium on Frontiers of Combining Systems (FroCoS 2007), volume 4720 of LNAI, pages 267–282, 2007.
[93] F. Spoto, F. Mesnard, and E´. Payet. A termination analyzer for Java bytecode based on path-length. ACM Transactions on Programming Languages and Systems, 32(3), 2010.
[94] C. Sternagel. Automatic Certiﬁcation of Termination Proofs. PhD thesis, Institut fu¨r Informatik, Universita¨t Innsbruck, Austria, 2010.
[95] C. Sternagel and A. Middeldorp. Root-Labeling. In Proc. of the 19th International Conference on Rewriting Techniques and Applications (RTA 2008), volume 5117 of LNCS, pages 336–350, 2008.
[96] C. Sternagel and R. Thiemann. Certiﬁed subterm criterion and certiﬁed usable rules. In Proc. of the 21st International Conference on Rewriting Techniques and Applications (RTA 2010), volume 6 of LIPIcs, pages 325–340, 2010.
[97] C. Sternagel and R. Thiemann. Signature extensions preserve termination. In Proc. of the 19th Annual Conference of the EACSL on Computer Science Logic (CSL 2010), volume 6247 of LNCS, pages 514–528, 2010.
[98] C. Sternagel and R. Thiemann. Executable Transitive Closures of Finite Relations. In The Archive of Formal Proofs. http://afp.sf.net/entries/ Transitive-Closure.shtml, 2011. Formalization.
[99] T. Sternagel and H. Zankl. KBCV - Knuth-Bendix completion visualizer. In Proc. of the 6th International Joint Conference on Automated Reasoning (IJCAR 2012), volume 7364 of LNAI, pages 530–536, 2012.
[100] Terese. Term Rewriting Systems, volume 55 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 2003.

166
[101] R. Thiemann. The DP Framework for Proving Termination of Term Rewriting. PhD thesis, RWTH Aachen, 2007. Available as technical report AIB-2007-17.
[102] R. Thiemann and J. Giesl. The size-change principle and dependency pairs for termination of term rewriting. Appl. Alg. Eng. Comm. Comput., 16(4):229–270, 2005.
[103] R. Thiemann, J. Giesl, and P. Schneider-Kamp. Deciding innermost loops. In Proc. of the 19th International Conference on Rewriting Techniques and Applications (RTA 2008), volume 5117 of LNCS, pages 366–380, 2008.
[104] R. Thiemann and C. Sternagel. Certiﬁcation of termination proofs using CeTA. In Proc. of the 22nd International Conference on Theorem Proving in Higher Order Logics (TPHOLs 2009), volume 5674 of LNCS, pages 452–468, 2009.
[105] X. Urbain. Modular & incremental automated termination proofs. Journal of Automated Reasoning, 32(4):315–355, 2004.
[106] J. Waldmann. Matchbox: A tool for match-bounded string rewriting. In Proc. of the 15th International Conference on Rewriting Techniques and Applications (RTA 2004), volume 3091 of LNCS, pages 85–94, 2004.
[107] S. Winkler, H. Sato, A. Middeldorp, and M. Kurihara. Optimizing mkbTT (system description). In Proc. of the 21st International Conference on Rewriting Techniques and Applications (RTA 2010), volume 6 of LIPIcs, pages 373–384, 2010.
[108] H. Zankl, B. Felgenhauer, and A. Middeldorp. CSI – A conﬂuence tool. In Proc. of the 23rd International Conference on Automated Deduction (CADE 2011), LNAI, 2011.
[109] H. Zankl, C. Sternagel, D. Hofbauer, and A. Middeldorp. Finding and certifying loops. In Proc. of the 36th International Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM 2010), volume 5901 of LNCS, pages 755–766, 2009.
[110] H. Zantema. Termination of term rewriting by semantic labelling. Fund. Inform., 24(1-2):89–105, 1995.
[111] H. Zantema. Termination of string rewriting proved automatically. Journal of Automated Reasoning, 34(2):105–139, 2005.


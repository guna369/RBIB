REVISTA COLOMBIANA DE COMPUTACIÓN Volumen 11, número 2 Págs. 116 - 127
Representation of information using Kohonen's SOM
(Self-Organizing Maps)
Hernán González Acuña*, Max Suell Dutra†, Omar Lengerke*, Magda J. Morales‡.
Fecha de Recibido: 17/06/2010 Fecha de Aprobación: 22/09/2010
Resumen
En este artículo se presenta una demostración de los mapas auto organizados de Kohonen's también llamados SOM. Así mismo es realizado un estudio del funcionamiento de los mapas de kohonen en una y dos dimensiones y las características de este tipo de redes que trabajan de forma similar al cerebro humano. Finalmente, son detalladas las características necesarias para realizar el entrenamiento de las redes y la forma como son utilizados sus resultados, con la finalidad de descubrir características de la información de entrada, como por ejemplo, la distribución, la densidad y la forma de la información.
Palabras Claves: Algoritmo de Kohonen, Redes Neuronales no Supervisadas,
Entrenamiento Competitivo.
Abstract
In this paper is presented a demonstration of Kohonen's self-organizing maps, also known as SOM. Likewise is prepared a study of the functioning of Kohonen's maps in one and two dimensions and the most important characteristics of this type of network that works in similar way that the human brain. Finally, this paper details the characteristics necessaries for the network's training and how is possible use the results of the neural networks to discover the characteristics of the information input for instance, how is your distribution, the densityand shape. Keywords: Kohonen, Algorithm, Unsupervised neural networks, Competitive Training.
* Autonomous University of Bucaramanga, Colombia. hgonzalez3@unab.edu.co, olengerke@unab.edu.co †Federal University of Rio de Janeiro, Brasil, m max@mecanica.coppe.ufrj.br, mtavera@ufrj.br ‡ Se concede autorización para copiar gratuitamente parte o todo el material publicado en la Revista Colombiana de Computación siempre y cuando las copias no sean usadas para fines comerciales, y que se especifique que la copia se realiza con el consentimiento de la Revista Colombiana de Computación

Representation of information using Kohonen's SOM (Self-Organizing Maps)

117

1 Introduction
The information is a set of data organized that integrate a message about a particular entity or phenomenon. At present the large quantities of information makes more complex their analysis and interpretation. For this reason is important the collaboration of tools that make more easy the information, for instance, the visualization of information, that allows information about their distribution, these tools can be used to detect errors and improve the take of decisions [1]. This paper presents a demonstration of Kohonen's maps [2], which is based on unsupervised neural networks, in order to debug the data or the information. This debug allows the representation of the pattern of information with Kohonen's maps and a smaller amount of data, but including all characteristics of the input data. The Kohonen´s maps can be structures of information of one dimension, two dimensions or three dimension and each map there are zones with different density that is proportional to the repetition of data, for instance the zone with more information repetitive has more density than the information with less repetitive data.
2 Kohonen`smaps
Kohonen's maps are incorporated into an algorithm of neural networks that can create cluster of data for a training process. This group creates the projection of these data on a map distributed the features gradually. Kohonen's maps have different applications such as [3] ,[4] :
Clustering: Can be created groups of inputs data by different
criteria.
View [5] : These groups were done in orderly manner allowing see
and discover new features or relations that weren't referred.
Classification [6] : Done a Kohonen map, you can label and
separated the data in different groups.
Interpolation function: Providing numerical values to each node
of the Kohonen network can avail these values in the vectors of the input.
Vector quantization [7]: is used for obtain any vector or the
vector close to a data set previously knew
2.1 ArchitectureofKohonen`smaps
The Kohonen's maps have architecture divided on layers: Input layer.

118 Hernán González Acuña, Max Suell Dutra, Omar Lengerke, Magda J. Morales
Competition layer. In the Kohonen's maps each neuron of the input layer is connected to all neurons in the layer of competition (Fig. 1).
Fig. 1. Architecture of Kohonen maps
The difference with other types of networks [8], the neurons of Kohonen's maps are distributed throughout the input space, so when there are data in the input, it activates the neuron more close to the input data in the distribution area. The important characteristic of these maps is the capacity to self organizes as part of the learning process. When one neuron is active for one data, the neuron is developed in the space going to the point in the hyperspace of the input data. In this movement change the active neuron and his neighboring have a small displacement to organize the mesh created. The neurons can be moved through the space to the entrance remain grouped, forming a mesh in areas of the input space, which correspond to the input data. For this the Kohonen's maps can detect relations between the data and common characteristics or also can be used for data compression and optimization.
3 Definition oftypeofneighborhood
The definition of the type of neighborhood [9] allows identify the neurons that learn when one neuron is active. For each neuron can be define one type of neighborhood, and each neuron can have 2 neighbors (Fig. 2), 6 neighbors (Fig. 3) or 8 neighbors (Fig. 4). The configuration

Representation of information using Kohonen's SOM (Self-Organizing Maps)

119

most used is the square configuration but the hexagonal configuration has the best theory base. The neighborhood has another important factor is the shape of the function that changes the values of the vectors that she has. A Gaussian type function does that change the values decrease with distance. A function type circular change in the same way all vectors are in the neighborhood

Fig. 2. Linear architecture
of the neighborhood.

Fig. 3. Square architecture of the
neighborhood

Fig. 4. Hexagonal architecture of neighborhood.
4 TrainingofKohonen`smaps
For the training in the Kohonen's maps should follow the next steps [10]: (i) organizes a data set that is the input in the network, (ii) is realized the scale of the data input, (iii) training the network, where presenting each one of the data on the network that moves the neuron that is close to the input data, and (iv) the training cycle is repetitive until the variation of the Kohonen map is low.

120 Hernán González Acuña, Max Suell Dutra, Omar Lengerke, Magda J. Morales

4.1 Initialization ofvariables.
The initialization of variables can be randomly within a range of variation and distribution determined (uniform, Gaussian, random distribution with small values)

4.2 Scaletheinputs.
Sometimes the sets have different scales, so it should be do (before to training) a pre-processing of data in order that the variables have approximate the same range and standard deviation [11], guaranteeing that the input of the network are always on the same range. So should apply some methods, for example, the scale of the input variables is a statistic scale that has the following characteristics:

Zero mean. The most values in the range (-1, +1). By the scale between (-1,+1) will be the 95% of the input data.

The scale of the variable is do for:

xi

=

1 2σ Xi

( Xi

− µ Xi )

Where,

Xi = Original variable. xi = Scale variable.

σ xi = Standard deviation of the original variable. µ xi= mean of the input data.

(1)

4.3 Network Training
The main steps for the training of the network are shown in the diagram Figure 5 and described as follows:

(i) Initialization of the weights in random form,
Ui = [ui ,1, ui ,2...ui,n ]

(ii) Its show a vector of data for training,
E = [e1, e2...en ]

(iii) Calculate the distance between the input and each of the output neurons j,

( )N−1 2
d j = Xi(t) − Wi, j(t) i =0

(2)

Representation of information using Kohonen's SOM (Self-Organizing Maps)

121

Where Xi (t) is the nth input in the time t and Wij (t) is the neuron weight i to the neuron of output j.

(iv) Selection of the neuron with the shortest distance j* that is the winner neuron.

(v) Update the weights of winner neuron j* and the weights of the neighbours according of the type of the neighbourhood.

( )Wi, j(t + 1) = Wi, j (t) + α (t) Xi(t) −Wi, j (t)

(3)

Where α (t ) (0 < α( t ) < 1) is the rate of learn and decreases in the time.

(vi) Modifying the parameters of the neural network [12]: learning and the neighbourhood that are reduces when the number of iterations increases. The learning rate is calculated by equation (4).

αt

= α0

1

−

t T

(4)

Where T is the number of iterations and α0 is the initial value of α. The value of the neighbourhood is calculated with:

Ωt

= Ω0

1−

t T

(5)

Where,

Ω0

∪

1 2

n

o

neurons in the layer.

All the process of the neural network training is presented in the

diagram block in the Fig. 5, in this diagram there are all the steps that

were presented.

Fig. 5. Sequence for the application of neural network

122 Hernán González Acuña, Max Suell Dutra, Omar Lengerke, Magda J. Morales

4.4 Scaleofthenetwork.
In the finish of the training and testing process, the output signal shows the values that are within the range of (-1, +1). It is necessary that the output back in the same proportion as were the entries. For this is necessary doing the scaling in the output signal. To return the input data in the initial range applies equation (8) which is to take equation (1) and isolate the variable Xi.

Xi = 2σ Xi 〈 xi + µXi

(6)

5 Evaluation oftraining'snetwork

To do the test algorithm training is developed a program in Matlab, in order to do the variation of the sets of input data in the system (Fig. 6 ). Likewise, it makes the selection of the size of the representation of the input data that will be used for make the Kohonen's maps.

Fig. 6. Graphical interface.
After are created many sets of data for the test, in this set are modified in the density and distribution of input data and seeks to observe the variation of network behaviour to represent the input data. Four different sets of data are testing with the Kohonen's maps: Asquare with uniform distribution of data. Asquare where one half has a higher density than the other. Half cross with uniform distribution of data.
Two rounds with a separation between them.

Representation of information using Kohonen's SOM (Self-Organizing Maps)

123

5.1 Results
The following results were obtained with a mesh of 216 neurons and trained with a learning rate of 0.4. The input data in each of the sets have a ratio of 20 input data for each of the neurons of the mesh.
TEST 1: Asquare with the data uniformly distributed (Fig. 7 and Fig. 8).

Fig. 7. Input square with uniform distribution

Fig. 8. Kohonen's maps for test 1 in one and two dimensions.
TEST 2: A square where one half has a higher density than the other (
Fig. 9 and Fig. 10 )

124 Hernán González Acuña, Max Suell Dutra, Omar Lengerke, Magda J. Morales
Fig. 9. Input square halves of different density.
Fig 10. Kohonen's maps for test 2 in one and two dimensions.
TEST 3: Half cross with uniform distribution of data (Fig. 11 and
Fug. 12 ).
Fig. 11. Input half cross with uniform distribution

Representation of information using Kohonen's SOM (Self-Organizing Maps)

125

Fig.12 . Kohonen's maps for test 3 one and two dimensions.
TEST 4: Two circles that have a separation between them ( Fig. 13 and
Fig. 14 )
Fig. 13. Input data of two separate circles
Fig. 14 . Kohonen's maps for test 4 one and two dimensions.

126 Hernán González Acuña, Max Suell Dutra, Omar Lengerke, Magda J. Morales
6 Conclusions
The Kohonen's maps are very used when do not have knowledge of the information, the results obtained with the kononen's maps not are better than the result of an analysis of a case where we have the knowledge of the characteristics of information. Therefore, is recommend working with this type of unsupervised procedures when nobody has knowledge of information The density in the mesh of Kohonen's maps is an important characteristic that determines the absence of information in one range when the mesh in the neural network is sparse or otherwise when the presence of information with similar characteristics there are Kohonen's map with nodes very close between them. In this article we can see the importance of the implementation of Kohonen maps in information processing, since it can have a representation of the distribution of input data allows to obtain a better knowledge of data input in a process for make decisions.
7 Acknowledge
Our thanks to the direction in research of the university autonomous of Bucaramanga (UNAB)-Colombia, the coordination with COPPE-
UFRJ and Robotics laboratory UFRJ-Brazil.
8. References
[1] A. J. Smola, P. L. Bartlett, “Advances in neural information Processing systems”, Massachusetts Institute of Technology, 2000
[2] T. Kohonen, Self-Organization and Associative Memory, Springer Verlag, Berlin Heildeberg New York, 1989.
[3] T. Kohonen, E. Oja, O. Simula, A. Visa, and J. Kangas, “Engineering applications of the self-organizing map,” in Proc. IEEE, Oct. 1996, vol. 84, pp. 1358–1384.
[4] Gonzalez acuña, H. ;Dutra M.S; Lengerke, O.. Identification and modeling for non-linear dynamic system using neural networks type MLP. In: Euro American Conference On Telematics And Information Systems, Prague,

Representation of information using Kohonen's SOM (Self-Organizing Maps)

127

Czech Republic. 2009. v. 26._Ref259054579 [5] S.V.N.Vishwanathan and M.Narasimha Murty. Use of kohonen map for
dimensionality reduction. Technical Report TR No. IISC-CSA-1999-8, Indian Institute of Science, Computer Science and Automation Department, Bangalore, December 1999. [6] D. Michie, D.J. Spieegelhalter, C.C Taylor. “Machine learning, Neural and statistical Clasification”. February 17, 1994. [7] R. M. Gray, “Vector quantization,” IEEE Acoust., Speech, Signal Processing Mag., pp. 9–31,Apr. 1984. [8] S. Haykin, “Nueral Networks: A comprehensive Foundation”. 2nd edition, Prentice Hall. [9] Sameer A. Nene and Shree K. Nayar. A simple algorithm for nearest neighbor search in high dimensions. IEEE transactions on Pattern Analysis and Machine Intelligence, 19(9):989–1003, September 1997. [10] P.D.Wasserman. “Neural Computing theory and practice”, Vannostrand
reinhold, New York 1989. [11] G. Burel, “A new approach for neural networks: The scalar distributed
representation,” Traitement du Signal, vol. 10, no. 1, pp. 41–51, Apr. 1993. [12] T. Kohonen, Speech Recognition Based on Topology-Preserving Neural Maps, in I. Aleksander (Ed.), Neural Computing Architectures, MIT Press, Cambridge, MA, 1989, pp. 26-40..


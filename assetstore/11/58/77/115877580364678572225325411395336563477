wRe
niver
ity of
tech
confi
mic
the new R&D direction advocated here. For ac-
applications, like multimedia, telemetry and oth-
systems, doubling every 10 months, is growing
had a market share of 38% (Fig. 2).
The capacity of batteries is growing extremely
slow (doubling every 30 years: Fig. 1). A new
computing performance metrics has to be intro-
duced. Instead of MIPS we use MOPS per milli-
Watts (MOPS/mW: Fig. 3). Will future fuel cells
*Corresponding author. Tel.: +49-721-6082502; fax: +49-
721-607438.
Journal of Systems Architecture 4E-mail addresses: becker@itiv.uni-karlsruhe.de (J. Becker),
reiner@hartenstein.de (R. Hartenstein).ronyms see Fig. 5.
Computing requirements are rapidly changing for
embedded systems. Doubling every 12 months,
performance requirements for wireless communi-
cation are growing very fast (Fig. 1): Hansens law
is faster than Moores law. Also in other embedded
faster than for any other application area (Fig. 1),
where entertainment electronics and communica-
tion, mainly wireless, are the largest segment of the
semiconductor market (Fig. 2). Together already
in 1999 these fastest growing market segments,
where a major part of products is battery-driven,1. Preface
This paper introduces a mind set fundamentally
different from (yet) current mainstream digital
systems. This roadmap paper may be viewed as a
kind of digest of a forthcoming book, or, from a
full day tutorial usually required to do this effi-
ciently. That is why this paper is hard to read for
readers not daily working directly in the center of
ers, performance requirements are growing rap-
idly. With the growth rate recently slowing down,
the integration density of microprocessors is more
and more falling back behind Moores law (Fig. 1).
Accelerators occupy most of the silicon chip area.
Compared to hardwired accelerators more flexi-
bility is provided by morphware, which will be
explained later.
The amount of software written for embeddedConfigware and morph
J€urgen Becker a,*,
a Institut fuer Technik der Informationsverabeitung (ITIV), U
b Kaiserslautern Univers
Abstract
The paper addresses a broad readership in information
introduction to fine grain and coarse grain morphware, re
science and business models. It points out trends driven by
stream-based computing.
 2003 Elsevier B.V. All rights reserved.URLs: http://www.itiv.uni-karlsruhe.de, http://www.harten-
stein.de.
1383-7621/$ - see front matter  2003 Elsevier B.V. All rights reserv
doi:10.1016/S1383-7621(03)00073-0are going mainstream
iner Hartenstein b
sitaet Karlsruhe (TH), D-761288 Karlsruhe (TH), Germany
Technology, Germany
nology, computer science and related areas, and gives an
gurable computing, and its impact on classical computer
roelectronics technology, EDA, and the mind set of data-
9 (2003) 127–142
www.elsevier.com/locate/sysarcsoon solve this problem? Maybe for laptops, but
not for handys.
ed.
tion
den
)
4y
ale
)
f Syst1
0                               10    12                  18
months
inte
gra
*) Department of Trade and Industry, London
batterycapacity
memory ba
ndwidth (x1.0
7/y)
µproce
ssor 
integr.
 
density
(x1.2/y
(x1.03/y)
10y
30y
(lo
g s
c
Fig. 1. Moores Law and other ‘‘laws’’ (ST microelectronics,
MorphICs).2
factor embedded
software
[DTI*]
sity
[Moo
re ’
s la
w]
communication
bandwidth[Hansen’s law]
128 J. Becker, R. Hartenstein / Journal oThe memory bottleneck. Doubling approxi-
mately only every 10 years the growth of memory
communication bandwidth is extremely slow (Fig.
1). Because of the von Neumann bottleneck,
memory bandwidth is an important issue. Avoid-
ing this memory bottleneck not only by using ac-
celerators, but also by innovative computing
architectures, or even by breaking the dominance
of the von Neumann machine paradigm is a
promising goal of new trends in embedded system
development and CSE education.
2. Introduction
The dominance of the procedural mind set in
computer science stems from the general purpose
properties of the ubiquitous von Neumann (vN)
microprocessor. Because of its RAM-based ex-
PC: 25%
communication: 22%consumer: 16%automotive: 6%
Others: 31%
Fig. 2. Semiconductor market sectors in 1999 (Dataquest).treme flexibility no application-specific silicon is
needed, so that for software-based products no
mass production is needed for profitability.
Throughput is the only limitation because of its
sequential nature of operation. But now a second
RAM-based computing paradigm is heading for
mainstream: the application of morphware [1],
which is also a kind of general purpose platform.
Morphware (Fig. 4a), sometimes called soft hard-
ware, is programmable by reconfiguration of its
structure: programming in space––in contrast to
vN-based programming in time. Already a single
morphware device may provide massive parallel-
Fig. 3. Energy efficiency vs. flexibility.
ems Architecture 49 (2003) 127–142ism at logic level or at operator level, often more
efficient than vN-based process level parallelism.
Morphware has introduced a number of funda-
mental issues to computing and microelectronics
[2] and more will come.
Booming conferences on morphware and its
applications like FPL [3], the eldest and largest, as
well as the adoption of this topic area by con-
gresses like ASP-DAC, DAC, DATE, and others
[4] indicate, that morphware is heading from niche
to mainstream. This is a viable challenge to CS
curricula innovators, an occasion to reconsider vN
culture criticism, partly dating back to the 1980s
[5].
The vN microprocessor is indispensable. But
because of its monopoly our CS graduates
are no more professionals.
Currently often it is still more difficult to obtain
ca
mor
econf
ogic (
econf
ompu
b)
and
SystToward a dichotomy of basic computing para-
digms. From this starting point Computing Sci-
ences (CS) are slowly taking off to explore new
horizons: toward a dichotomy of basic computing
paradigms, by removing the blinders of the still
strong vN-only mind set. Within our CS curricula
this dominance is still emphasized, but by a slowly
decaying majority. It has been predicted, that by
the year 2010 more than 90% of programmers will
implement applications for embedded systems
(Fig. 1) [6,7], where a procedural/structural double
rail approach is a pre-requisite, providing new
chances.
Missing algorithmic cleverness. Currently most
programmers do not yet really have the back-
ground required. Typical programmers have severe
problems to map an application onto platforms
other than relying on state-machine-based mech-
anisms like vN or similar. This education gap can
be easily bridged only by a curricular transition
from the von-Neumann-only mind set toward this
dichotomy of two basic computing paradigms,
where the vN paradigm is taught together with a
R
L
R
C
platform 
category
source “running” 
on platform
machine 
paradigm
hardware (hardwired)
morphware* configware
ISP** software von  Neumann
AMP*** flowware
anti machine
rAMP flowware & 
configware
*) term used by DARPA-funded program
ACS (adaptable computing systems).
**) instruction stream processor
***) anti machine processor (datastream processor).
((a)
Fig. 4. (a) Platform categories
J. Becker, R. Hartenstein / Journal ofsecond machine paradigm which we call anti-ma-
chine [8,9]. A rich supply of tools and research
results is available to adapt fundamental courses,
lab courses and exercises. Not the time needed for
such modifications is the problem, since there are a
lot of similarities between both branches, like be-
tween matter and anti-matter.
Our educators are the problem. The vN mi-
croprocessor is indispensable. But because of its
monopoly our CS graduates are no more profes-
sionals. The key problem is, that educators need
to be a more open-minded to be able to cope
with the few asymmetries between both branches.high speed-up by morphware. But there is a wide
variety of acceleration factors. For example the
migration of a cell metabolism simulator [10]
from 1,3 GHz Pentium to 42 MHz implementa-
tion on Virtex II (a Xilinx FPGA) brought a
factor of 8, and a FIR filter on a 4-by-4 coarse
grain reconfigurable array a factor of 9 [11].
Compared to an implementation on a VAX-11/A concise and comprehensive basic terminology,
an important help to remove barriers, will be
summarized by this paper, along with an intro-
duction to morphware, the new computing para-
digm, its history, its commercial and scientific
backgrounds, as well as to its key issues and future
aspects.
3. Morphware
Currently the most important motivation to
replace hardwired accelerators by morphware is
more flexibility. But performance is the key issue.
tegory of 
phware use
granularity
(path width)
(re)configurable
blocks used
igurable 
fig. 5 b)
fine grain 
(~1 bit)
CLBs: configurable logic 
blocks
igurable
ting
coarse grain
(example: 16 
or 32 bits)
rDPUs: reconfigurable 
data path units (e.g. 
ALU-like)
multi-granular: 
supporting 
slice bundling 
with carry 
connect
rDPU slices
(example: 4 bits)
(b) categories of morphware.
ems Architecture 49 (2003) 127–142 129750 a grid-based design rule check [12] with a re-
programmable PLA (which to-day could well be
replaced by an FPGA), runs on a different ma-
chine paradigm to avoid the vN bottleneck [13]
brought a factor by three orders of magnitude––
by processing a complex Boolean expression in a
single clock cycle and using a generic address
generator to implement a 4-by-4 pixel video scan
window [14].
In morphware application the lack of algo-
rithmic cleverness is an urgent educational
problem.
configware for structural reconfiguration of the
t.
r)
Acro
f SystReconfigurable logic versus reconfigurable com-
puting. A growing consensus on terminology (Fig.
4) and acronyms (Fig. 5) clearly distinguishes re-
configurable logic (RL) from reconfigurable com-
puting (RC, Fig. 4), as well as different platform
categories (Fig. 4). With data stream processor
terminology has a dilemma: ‘‘dataflow machine’’
[15] and digital signal processing (DSP) have been
occupied decades ago by other areas. So we prefer
the acronym anti-machine (AM) of AMP.
Terminology. Even more important is the ter-
minology from a more global point of view, not to
get confused between ‘‘programmable’’, ‘‘micro-
programmable’’, ‘‘field-programmable’’ and con-
tradictory ‘‘soft hardware’’, and, to clarify the
dichotomy of fundamental models, as well as, to
alleviate the merger of RC and classical CS (Fig.
4a). To add more confusion some authors even use
the term ‘‘reconfigurable’’ (because processors can
AM anti machine (DS machine)
AMP data stream processor*
ASIC application-specific integrated ck
asMB autosequencing Memory Bank
CPU “central” processing unit: DPU
(with instruction sequencer)
cSoC configurable SoC
DPA  data path array (DPU array)
DS data stream
DPU  data path unit (without sequence
ecDPU emulation-capable DPU
EM evolutionary methods
EDA electronic design automation
Fig. 5.
130 J. Becker, R. Hartenstein / Journal obe addressed) for micro-miniaturized classical
parallel computing systems with all its classical
CPUs on a single chip. Whereas classical CS deals
with software running on hardware, the new
branch of CS deals with flowware running on
hardware, was well as with configware [16,17] and
flowware [18] ‘‘running’’ on morphware. This paper
gives introductions for a broad readership mainly
with a CS background, rather than for hardware
specialists.
Reconfigurability. Although it is not seriously
taught to most CS students, it is not new, that
algorithms can be implemented in software, or in
hardware. Implementing an application by soft-
ware means procedural programming, i.e. pro-morphware device (mapper output, see Fig. 6) and
flowware for scheduling the data streams (sched-
uler output, see Fig. 6). The acronym field-pro-
grammable gate array (FPGA) indicates, that
structural re-programming can be practised any-
where, like e.g. at the customers site. This also is
an important commercial aspect already now,
which will have an even greater impact in the
future. Following chapter will illustrate the re-
configurability mechanism within morphware
platforms by an example of reconfigurable logic
(fine grain morphware).
4. Reconfigurable logicgramming in time. But implementing an algorithm
on morphware means structural programming by
EH evolvable morphware
(“evolvable hardware”)
FPGA  field-programmable gate array
ISP  instruction stream processor
rDPU  reconfigurable DPU
rDPA  reconfigurable DPA
RA reconfigurable array
RAM random access memory
rAMP reconfigurable AMP
RC reconfigurable computing
RL reconfigurable logic
RTR  run time reconfiguration
SoC (an entire) System on a Chip
*) no “dataflow machine” [15]
nyms.
ems Architecture 49 (2003) 127–142Fig. 7 illustrates the basic mechanisms of RL im-
plementation on a morphware platform, which can
be changed at any time after fabrication by down-
loading configware into the configuration RAM.
CLBs and interconnect fabrics. A minor part of
the area is used by configurable logic blocks
(CLBs), which are the logic resources. Major part
of the area is covered by a reconfigurable inter-
connect fabrics, providing wire pieces, switch
boxes, and connect boxes to connect a pin of a
CLB, with a pin of another CLB by programming
a ‘‘soft wire’’ (an example shown in Fig. 7). The
state of each switching transistor is controlled by a
Flip-flop (FF) which is part of ‘‘hidden’’ configu-
ration RAM (not shown in Fig. 7), also used to
MM MM M
M
(b)
onfigw
ompile
(a)
in
s
fr
owwa
*)
achi
J. Becker, R. Hartenstein / Journal of SystM
M
M
M MM M
M
M
M config-
flowware
ware
M asM
auto-seqencing M
data sequencer rDPU
M Mmemory bank
rDPA c
c
fl
Fig. 6. FC/software co-compilation: (a) anti-mprogram the CLBs to select the particular logic
function of each. By new configware all this can re-
programmed anywhere and at any time.
The history of fine grain morphware. The do-
main of morphware platforms and their applica-
tions has undergone a long sequence of transitions,
where we may distinguish different subareas (Fig.
4b): fine grain morphware (FPGAs) and coarse
grain morphware. First FPGAs appeared as cheap
replacements of Mask Programmable Gate Arrays
(MPGAs). Still today FPGAs are the reason for
shrinking ASIC 1 (Fig. 5) markets, since for
FPGAs no application-specific silicon is needed––
1 ASIC fabrication cost is much lower (only a few specific
masks needed) than that of other integrated circuits (Fig. 8).
Fig. 7. Illustration of morphware reconfigurability resohigh level source program
automatic Partitioner
Analyzer
/ Profiler
softwareare
software for
compilerr*
termediate
mapper
cheduler
ont end
vN µprocessor
re/
 FC compiler
ne example and (b) partitioning co-compiler.
ems Architecture 49 (2003) 127–142 131a dominating cost factor in low production volume
products. Later the area proceeded into a new
model of computing possible with FPGAs. Next
step was making use of the possibility for debug-
ging or modifications the last day or week, which
also leads to the adoption by the rapid prototyping
community which also has lead to the introduction
of ASIC emulators faster than simulators. Next
step is direct in-circuit execution for debugging and
patching the last minute.
Fastest growing market. Morphware is the
fastest growing segment of the integrated circuit
(IC) market, currently relying on a growing large
user base of HDL-savvy designers. Cost differ-
ences between volume FPGAs and volume ASICs
are shrinking. Driven by a growing large user base
innovations occur more and more rapidly. FPGA
urces (FPGA: only 1 configured ‘‘wire’’ shown).
designs less complex emulation boards are used,
like logic emulation PWB (based on Xilinx Virtex,
revenue
/ month
product
update 1
update 2
update 3
morphware product with
hardware
downloadable upgrades
time / monthsproduct
10 20 300
Fig. 9. Morphware product longevity by updates.
f Systems Architecture 49 (2003) 127–142vendors are heading for the forefront of platform-
based design.
New business models. Morphware brings a new
dimension to digital system development and has a
strong impact on system-on-chip (SoC) design.
Performance by parallelism is only one part of the
story. The time has come to fully exploit morph-
ware flexibility to support very short turn-around
time for real-time in-system debugging, profiling,
verification, tuning, field-maintenance, and field-
upgrades. The consequence is a new business
0.35 0.25 0.15 0.13 0.1 0.070.8 0.6 0.18
4
3
2
1
12 12 26 28 30 >302016
# of masks cost / mio $
NRE an d
mask cost
mask
set
cost
source: 
eASIC
source:
Dataquest
µ feature size
Fig. 8. Increasing mask set cost and total NRE cost.
132 J. Becker, R. Hartenstein / Journal omodel for all kinds of electronics products, where
patches and upgrades are carried out at the cus-
tomers site––even via the internet using run time
reconfiguration (RTR). This is an important
remedy of the current embedded system design
crisis caused by skyrocketing design cost coincides
with decreasing product lifetime, by providing
product longevity (Fig. 9).
Rapid prototyping and ASIC emulation. Since in
integrated circuit design flow simulation may take
days or even weeks, the next step has been ASIC
emulation, using huge emulation machines called
ASIC emulators. By acquisitions the three major
EDA vendors offer ASIC emulators, along with
compilers: Cadence has acquired Quickturn, Syn-
opsys has acquired IKOS, and Mentor Graphics
bought Celaro, also offering such service over the
internet. Another R&D scene and market segment
is calls itself rapid prototyping, where for smallercan emulate up to three million gates), and, the
DN3000k10 ASIC Emulator from the Dini
Group.
Retro emulation. A future application of emu-
lation may serve to solve the long term microchip
spare part problem in areas like industrial equip-
ment, military, aerospace, automotive, etc. with
product lifetimes up to several decades. The in-
creasing spare part demand (Fig. 10) stems from
increasing amount of embedded systems, limited
life time of microchip fab lines (mostly less than 7–
10 years), and decreasing life time of unused mi-
crochips (Fig. 10). When a modern car with several
dozens of embedded microchips needs electronic
spare parts 10 or 15 years later, the microchip fab
line is no more existing, and major percentage or
all of the parts kept in a spare parts storehouse
have faded away. To keep an old fab line alive,
which would deliver long-lasting robust products
at low NRE cost, seems to be an illusion. Retro
emulation might be the only viable solution, where
automotive spare part
IC market volume / $2000 2005 2010 2015 year
newest technology
life expectance / years
IC spare part demand
years after introduction / $
2020
IC’s physical
0,13µ 0,07µ 0,045µ
5 years 10 years 15 years0 years years after
introduction
Fig. 10. The emerging automotive IC spare part problem.
with a path width of multiples of the slice path
Systreverse engineered products are emulated on
FPGAs, since application-specific silicon will not
be affordable because of low microchip production
volumes in these areas and rapidly increasing mask
cost (Fig. 8).
cSoC and general purpose chip. The most im-
portant business factor of morphware is its general
purpose property, so that application-specific sili-
con with its extremely high NRE cost can be
avoided. With the continuous technology progress
even entire SoC can be implemented on morph-
ware configurable SoC (cSoC). Due to Moores law
(Fig. 1) the FPGA vendors offer more and more
products having microcontrollers like ARM,
MIPS, PowerPC, or other RISC architectures,
memory, peripheral circuitry and others, together
with the FPGA on board of the same chip. A
Xilinx FPGA, for example, has four PowerPCs on
board and 24 Conexant 3.125 Gb/s serial link
cores providing a total of 75 Gb/s/chip link
bandwidth.
Power efficiency. More recently the research is
heading toward low power FPGAs from the algo-
rithms side. By transfer of an algorithm from a high
performance DSP to an experimental low power
FPGA a performance improvement by factors be-
tween 5 and 22 has been obtained by Jan Rabaey
et al. [19]. A reduction of clock frequency by a factor
of n yields a reduction of power dissipation by a
factor of n3 [20], when also the optimum technol-
ogy is selected––if its fab line is still available.
Evolvable morphware. The terms evolvable mor-
phware, and evolutionary methods (EM, also called
Darwinistic Methods), and biologically inspired
electronic systems stand for a new application area
of morphware in research––a resurrection of cy-
bernetics or bionics, stimulated by the new avail-
ability of morphware technology, where the effect
of chromosomes within evolution is emulated by
continuously changing configware [51]. The label-
ling ‘‘evolutionary’’ and the ‘‘DNA’’ metaphor
helped to start and polularize new conference se-
ries [52], and to raise research funds in the EU,
Japan, Korea, and the US. Critics disfavor the
trend to love genetic algorithms in other applica-
tions areas, even where simulated annealing is
more efficient, like e.g. for simultaneous placement
J. Becker, R. Hartenstein / Journal ofand routing which could be viewed as a kind ofwidth (e.g. 16, 20, or 24 bits).
Commercial architectures. Especially in appli-
cation areas like multimedia, wireless telecommu-mutational morphing [26]. The new morphware-
driven EM scene is still in its visionary phase.
5. Reconfigurable computing
Fine grain morphware lacks area/power-effi-
ciency (Fig. 3). The physical integration density
(transistors per chip) of FPGAs is roughly two
orders of magnitude worse than the Gordon
Moore Curve. Due to reconfigurability overhead
roughly about only one percent of these transistors
deserve the real application, so that the logical
integration density is about four orders of mag-
nitude behind Gordon Moore. For very high
throughput requirements RC using coarse grain
morphware is the drastically more powerful and
more area-efficient alternative, also providing a
massive reduction of memory and time needed for
configuration [21]. Coarse grain morphware is also
about one order of magnitude more energy-effi-
cient than fine grain (Fig. 3 [22–24]). Whereas RL
based on fine grain morphware (FPGAs) uses
single bit wide CLBs (Fig. 4b), reconfigurable
computing (RC) uses reconfigurable datapath units
(rDPUs), which, similar to ALUs, have major path
widths, like 32 bits, for instance––or even rDPU
arrays (rDPAs). Important applications stem from
the performance limits of the ‘‘general purpose’’
processor, creating a demand for accelerators.
Coarse-grain reconfigurable architectures. Sev-
eral interconnect topology categories of coarse
grain morphware for reconfigurable computing
(survey: [21]) may be distinguished: universal
RAM-based topologies of regular arrays (URA),
customized regular RAM-based arrays (CRA), hi-
erarchical irregular RAM-based macro-blocks
(IMB). Another class of resources for reconfigu-
rable computing is called multi-granular morph-
ware, where several nibble pathwidth rDPU slices
(4 bits, for instance) with slice bundling capability
including carry signal propagation can be config-
ured to be merged into datapath units (DPUs)
ems Architecture 49 (2003) 127–142 133nication, data communication and others, the
(b
unit) from PACT: (a) array structure and (b) rDPU.
f Systthroughput requirements are growing faster than
Moores law (growth of required bandwidth: Fig.
1), along with growing flexibility requirements due
to unstable standards and multi-standard opera-
tion [27]. Currently the requirements can be met
only by rDPAs from a provider like PACT (Fig. 11
[28]). Since a general purpose rDPA still seems to be
an illusion, such applications need domain-specific
architectures, so that specific silicon is needed, com-
mercially feasible to very high production volume
products like in consumer electronics.
Different routes to DPAs. In design and imple-
mentation of embedded systems using DPAs dif-
ferent business models are possible: using hardware
DPUs and DPAs designed all the flow down to
(a)
Fig. 11. Configurable XPU (xtreme processing
134 J. Becker, R. Hartenstein / Journal ophysical layout and tape-out directly or from a
library, or morphware using rDPUs and rDPAs.
With morphware using a fully universal rDPA of
universal rDPUs is not area-efficient nor resource-
efficient, unless done with a multi-granular plat-
form which permits configuring narrow path
rDPUs into wider compound rDPUs. This sup-
ports a business model, where personalization is
carried out after fabrication, and many different
design can be implemented onto the same platform.
The domain-specific rDPA approach is another
solution [21], where rDPU architecture and other
features are optimized for a particular application
domain. This may provide very high area efficiency
by structured VLSI design [25] of the DPUs sup-
porting regular array layout and wiring by abut-
ment, like the cells of the KressArray family [26].A design space explorer may help to derive and
compare within a few days several alternative
DPU and array architectures by a benchmark or a
domain-typical set of applications [29]. A desirable
new solution for the far future could be the soft
array approach mapping DPAs onto a very large
FPGAs, providing highest flexibility (structured
configware design: see Section 10).
The same models for hardware and morphware.
All four routes have in common, that mainly the
same mapping design flow part may be used for all
of them. With the tendency toward data-stream-
based DPAs there is in principle no difference,
whether the DPU array is hardwired or reconfig-
urable––except the binding time of placement andC
)
C
F GALU Ctrl
G
ems Architecture 49 (2003) 127–142routing: before, or, after fabrication.
6. Data-stream-based computing
The world of traditional instruction-stream-
based informatics is based on computing in the
time domain, where software schedules the in-
structions for execution. The classical machine
model, which we call vN paradigm, locates in-
struction sequencer and datapath in the same CPU
(Fig. 12a). We need only one program source for
vN machines (Fig. 13a), the software which pro-
vides the implementation of the instruction stream
executed at run time. For hardwired data-stream-
based computing platforms, however, we need
another programming source. Instead of software
I/O
ata ad
enerat
ata seq
emor
sM*
data streams
(a) v
(d) w
SystM M
I/O
instruction
sequencer
data
path
(ALU)
CPU
(a) (b)
data
path
DPU or
rDPU
unit
d
g
(d
m
instruction
stream
data
streamI/O
a
*) auto-sequencing memory
“von Neumann” machine
Fig. 12. Illustrating basic machine paradigms (legend: Fig. 6):
DPU; (c) with rDPU and distributed memory architecture and
terminology:Makimoto’s 2nd wave
(a)
J. Becker, R. Hartenstein / Journal ofwe need flowware to implement data streams in-
stead of an instruction stream. For a reconfigura-
ble data-stream-based platform, however, we need
two different ‘‘programming sources’’ (Fig. 13b): a
configware source to configure (before run time)
the resources, and flowware to determine, what has
to be executed at run time.
Why we need this second machine paradigm. Due
to the availability of morphware a second basic
machine model is needed, since the instruction-
stream-based vN model does not support changing
datapath units. We have to introduce a data-
stream-based second model which we call anti-
machine so that we now have a dichotomy of two
models: instruction-stream-based (vN) computing
vs. data-stream-based computing. There are a lot
of similarities between both models, so that each
of the 2 models is a kind of mirror image of the
other model––like with matter and anti-matter
resources fixed
algorithms variable
resources variable
algorithms variable
hardware
software
(Nick Tredennicks view):
configware
streamware
terminology:Makimoto’s 3rd wave
(Nick Tredennicks view):
(b)
morphware
Fig. 13. Terminology explained [36,37].[22]. The instructions are the electrons circulating
around the CPU of the vN machine, whereas data
streams circulating around the (r)DPU or (r)DPA
are the positrons of the AM.
Similarities and asymmetries. Data-stream-
based computing, the counterpart of instruction-
stream-based vN computing, however, uses a data
counter instead of a program counter (example in
Fig. 12b). However, there are some asymmetries,
like predicted by Paul Dirac for anti-matter. Fig.
6a shows the block diagram of data stream ma-
chine with 16 autosequencing memory banks. The
basic model allows this example machine to have
16 data counters, where as a vN machine cannot
(d)
(r)DPA
on Neumann; (b) data-stream-based anti-machine with simple
. DPU array (DPA or rDPA).(c)
I/O
MM MM M
MM MM M memory
(r)DPU
dress
or
uencer)
y
data streams
anti machine
ems Architecture 49 (2003) 127–142 135have more that one program counter. The parti-
tioning scheme of the data stream machine model
assigns a sequencer (address generator) always to a
memory bank (Fig. 12b), never to a DPU. This
modelling scheme goes fully conform with the area
of embedded distributed memory design and
management (see Section 6.2).
6.1. Flowware
Data streams, also efficiently supporting loop-
level parallelism [30], have been popularized
throughout the 1980s by systolic arrays, later by
the super systolic array (pioneered by the Kress-
Array [26], the generalization of the systolic array
[31]), and by projects like Streams-C Configurable
Computing (SCCC) [32], Stream Computations
control overhead. Compared to real logic func-
f SystOrganized for Reconfigurable Execution (SCORE)
[33], Adapting Software Pipelining for Reconfig-
urable Computing (ASPRC), Biggascale Emulation
Engine (BEE) [34], the KressArray Xplorer [29]
and many others. In a similar way like instruction
streams can be programmed from software sources,
also data streams can be programmed, but from
flowware sources. High level programming lan-
guages for flowware [35] and for software join the
same language principles and have a lot in com-
mon, no matter, wether finally the program
counter or a data counter is manipulated. The data
schedule generated from a flowware source deter-
mines, which data object has to enter or leave
which DPA port (or DPU port) at which time, so
that the embedded distributed autosequencing
memory has to generate the data streams.
Two programming sources. vN machine needs
just software as the only programming source,
since its hardwired resources are not programma-
ble. A reconfigurable data-stream-based machine,
however, needs two programming sources: con-
figware to program (reconfigure) the operational
resources, and, flowware to schedule the data
streams. Fig. 6b illustrates the structure of a
compiler [16,26] generating the code of both
sources from a high level programming language
source: phase 1 performs routing and placement to
configure the rDPA, resource, and phase 2 gener-
ates the flowware code needed to program the
autosequencing distributed memory accordingly.
6.2. Embedded memory
Increasing use of data-stream-based architec-
tures coming with the growing embedded system
market has recently stimulated distributed em-
bedded memory as a growing market segment and
important R&D area. Because of using distributed
memory the AM has no vN bottleneck. Together
with application-specific distributed memory ar-
chitecture synthesis also flowware implementation
deserves performance and power dissipation opti-
mization [14]. Good flowware may be also ob-
tained after optimized mapping an application
onto rDPA, where both, data sequencers and the
application can be mapped (physically, not con-
136 J. Becker, R. Hartenstein / Journal oceptually) onto the same rDPA [21].tions the overhead going into caches and other
auxiliary hardware is several orders of magnitude
higher [34]. Processor chips are almost all memory,
because the architecture is wrong. The metric for
what is a good solution has been wrong all the
time.
Contra concurrent computing. Arrays or other
ensembles of CPUs are difficult to program, and
often the run-time overhead is too high, except for
a few special application areas favored by Am-
dahls Law. Amdahls Law explains just one of
several reasons of inefficient resource utilization.
Each CPU comes with a central vN bottleneck,
whereas a DPU may have many ports active si-
multaneously. Like with a systolic array, DPU
array computing means parallelism by an appli-
cation-specific pipe network. There is no CPU.
There is nothing ‘‘central’’. Data-stream-based
computing with (r)DPAs provides a drasticallyThe anti-machine has no von Neumann bot-
tleneck.
Address generators. To solve the memory com-
munication bandwidth problem the AM paradigm
(data-stream-based computing) is much more ef-
ficient than ‘‘von Neumann’’. Two alternative
methodologies are available [14]: specialized ar-
chitectures using synthesized address generators
(e.g. APT by IMEC [14]), or, flexible architectures
with programmable general purpose address gen-
erators [13], which do not need memory cycles
even during complex address computations [14]––
an important performance issue. E.g. a kind of
image processing (for a grid-based design rule
check) on a VAX 11/750 moving a 4-by-4 pixel
window in a video scan mode took 94% of com-
putation time for address computation [12].
7. Data-stream-based vs. concurrent computing
Software processor solutions are inefficient rel-
ative to hardwired solutions (Fig. 3). Fundamental
flaws are: time multiplexing a single piece of logic,
data memory/processor traffic overhead, control
flow overhead, and intra-processor pipelining
ems Architecture 49 (2003) 127–142more efficient way to cope with memory commu-
nication bandwidth problems than classical con-
current computing.
7.1. The key to massive parallelization
However, today for DPA synthesis or mapping
applications onto rDPAs linear projection is no
more used, but simulated annealing instead, to
avoid the limitation to regular data dependencies
[26]. This (‘‘super systolic’’) generalization of the
systolic array also supports inhomogeneous irreg-
a factor of about 100 in area efficiency.
configware compilers (FC compilers [26,38], com-
pare Fig. 6 and Fig. 15, which explains the need
for two sources).
Co-compilation. When reconfigurable accelera-
tors are involved instead of the hardwired acceler-
ators, hardware/software co-design turns into FC/
software co-design. Using compilation techniques
for both sides we turn co-design into FC/software
co-compilation. Using coarse grain morphware
(rDPAs) as accelerators changes the scenario: im-
plementations onto both, host and accelerator(s)
n” mac
roced
n strea
encing
r
J. Becker, R. Hartenstein / Journal of Systems Architecture 49 (2003) 127–142 1378. Configware compilers
We already had ‘‘silicon compilers’’, which,
however, had been datapath layout generators,
rather than compilers. But taking into account,
that we now have two classes of RAM-based
programmable platforms, classical instruction
stream processors and morphware, we should
distinguish two classes of compilers; classical
software compilers (SW compilers), and, flowware/
machine category “von Neuman
general property p
driven by single instructio
engine principles instruction sequ
state register program counte
communication path set-up at run time
data path
resource single ALU
operation sequential
morphware support noular arrays [29]––in contrast to classical systolic
arrays.
Hardwired DPAs. On hardwired DPU array
basis the BWRC [34] has trumped a ‘‘chip in a
day’’ design methodology by direct mapping of
algorithms onto high-level, pre-characterized
macros wired together from a Simulink dataflow
diagram. An automated flow goes through module
generation, synthesis and layout. With system level
optimization BWRC got rid of difficult problems
by using relatively low clock rates––but for gainingFig. 14. Comparing the propertare RAM-based, which allows turn-around times
of minutes for the entire system, instead of months,
and, supporting a migration of accelerator imple-
mentation from IC vendor to customer, who usu-
ally does not have hardware experts. This creates a
demand for compilers accepting high level pro-
gramming language (HLL) sources [26,38]. Partly
dating back to the 1970s and 1980s know-how is
available from the classical parallelizing compiler
scene, like software pipelining, and, loop transfor-
mations and other concepts [39–44].
Automatic partitioning. Currently, not only for
hardware/software co-design, but also for soft-
ware/configware design, the compiler is a more or
less isolated tool used for the host only. But most
accelerators are still implemented by CAD. Soft-
ware/configware partitioning is still done manually,
requiring massive hardware expertise, particularly
when hardware description language (HDL) and
similar sources are used. Partitioning compilation
[26,32] and co-compilation from HLL sources [45]
still stems from academic efforts, as well as
the first automatic co-compilation from HLL
sources including automatic software/configware
hine anti machine
ural sequencing: deterministic*
m data stream(s) (no “dataflow”*)
data sequencing
(multiple) data counter(s)
at load time
array of ALUs and others
parallel
yes
*) arbitration-driven the “dataflow”
machine is indeterministic [15]ies of machine paradigms.
essor
)
e
ines
achin
f Systpartitioning (Fig. 6a) by identifying parallelizable
loops, having been implemented for the data-
stream-based MoM (Map-oriented Machine) [13].
OS. Also operating systems for morphware to
provide run time management are an interesting
issue, especially for scheduling problems, multi-
tasking on morphware coprocessors, dynamic re-
configuration, like for exploiting operation level
parallelism through dynamically reconfigurable
datapaths, embedded memory management, I/O
organization, etc.
(a) instruction set proc
machine paradigm von Neumann (vN
reconfigurability
support no
programming procedural
program source software
“instruction” fetch at run time
execution at run time instruction schedul
operation spini nstruction flow
operation resources CPUhardwired
parallelism only by multiple mach
state register program counter
state register located within CPU
Fig. 15. Asymmetry between m
138 J. Becker, R. Hartenstein / Journal o8.1. Machine paradigms and other general models
Machine paradigms are important models to
alleviate CS education and for understanding im-
plementation flows or design flows. The simplicity
of the vN paradigm helped a lot to educate zillions
of programmers. Fig. 12a shows the simplicity of
the block diagram, which has exactly one CPU
and exactly one RAM module (memory M). The
instruction sequencer and the DPU are merged to
be the CPU (central processing unit), whereas the
RAM (memory M) does not include a sequencing
mechanism. Other important attributes are the
RNI mode (read next instruction) and a branching
mechanism for sequential operation (computing in
the time domain). Fig. 14 compares both machine
paradigms. Since compilers based on the ‘‘von
Neumann’’ machine paradigm do not supportmorphware we need the data-stream-based ma-
chine paradigm for the rDPA side, (based on data
sequencer [14]).
The AM paradigm for morphware [46] and even
for hardwired AMs the data-stream-based AM
paradigm is the better counterpart (Fig. 12b) of
the vN paradigm (Fig. 12a). Instead of a CPU the
AM has only a DPU (datapath unit) without any
sequencer, or a rDPU (reconfigurable DPU)
without a sequencer. The AM model locates data
sequencers on the memory side (Fig. 12b). AMs do
(b) data stream processor
hardware morphware
anti machine
yes
no structural (super 
“instruction” fetch*)
data scheduling
flowware flowware &
configware
before run time (at loading time)
data schedule
data stream(s)
DPU, or, DPA rDPU, or, rDPA
hardwired reconfigurable
by single machine or multiple machines
one or more data counter(s)
outside DPU or DPA outside rDPU or rDPA
*) before run time
e and anti-machine paradigms.
ems Architecture 49 (2003) 127–142not have an instruction sequencer. Unlike ‘‘von
Neumann’’ the AM has no vN bottleneck, since it
also allows multiple sequencers (Fig. 12c) to sup-
port multiple data streams interfaced to multiple
memory banks, which is typical to data-stream-
based computing (Fig. 12c) allowing operational
resources much more powerful than ALU or
simple DPU: major DPAs or rDPAs (Fig. 12d).
General purpose anti-machine. The AM is as uni-
versal as the vN machine. The anti-programming
language is as powerful as vN-based languages.
But instead of a ‘‘control flow’’ sublanguage a
‘‘data stream’’ sublanguage like MoPL [35] re-
cursively defines data goto, data jumps, data loops,
nested data loops, and parallel data loops. For the
AM paradigm all execution mechanisms are
available to run such an anti-language. Its address
generator methodology includes a variety of es-
cape mechanisms needed to interrupt data streams
Also the configware market is taking off for
already existing, since Cadence, Mentor Graphics
Systmain-stream. Because FPGA-based designs be-
come more and more complex, even entire systems
on a chip, a good designer productivity and design
quality cannot be obtained without good config-
ware libraries with soft IP cores from various
application areas.
EDA is the key enabler. For the customer EDA
is the key enabler to obtain high quality FPGA-
based products with good designer productivity. Aby decision data or tagged control words inserted
in the data streams [46].
The AM model, where the DPUs are transport-
triggered by arriving data, goes conform with the
new and rapidly expanding R&D area of embed-
ded memories [14], including application-specific
or programmable data sequencers. Fig. 14 com-
pares the properties of both paradigms.
9. Configware industry
RAM-based success story. The secret of success
of software industry is based on RAM, vN para-
digm, compatibility, relocatability, and, scalabil-
ity. Due to the simplicity of the von Neumann
machine paradigm zillions of programmers can be
educated. The processor can be fabricated in vol-
ume since all personalization (programming) is
downloadable to scalable RAM. Relocatability
of machine code is provided by the machine
paradigm. Compatibility as a quasi-standard is
achieved by business strategy.
Configware industry, emerging as a counterpart
to software industry. Being RAM-based config-
ware industry is taking off to repeat the success
story known from software. Tsugio Makimoto has
predicted this more than 10 years ago [36,47]. Like
software, also configware may be downloaded
over the internet, or even via wireless channels.
FPGA functionality can be defined and even up-
graded later at the customers site––in contrast to
the hardware it replaces: configware use means a
change of the business model––providing shorter
time to market and (FPGA) product longevity
(Fig. 9). Many system-level integrated products
without reconfigurability will not be competitive.
J. Becker, R. Hartenstein / Journal ofgood morphware architecture is difficult to utilize,and Synopsys just jumped into it by closing the
back end gap down to creating configware code.
Structured configware design. The main problem
of configware industry in competing with software
industry is the lack of FPGA-based compatibility,
scalability, and code relocatability. Re-compila-
tion and re-debugging is required for another
FPGA type. Rather than by new FPGA archi-
tectures a future solution will be feasible by a
new EDA approach, which supports structured
configware design based on soft macro cells sup-
porting wiring by abutment made possible by
placement strategies being successful also for
rDPAs (compare Section 5).
10. Soft CPUs and rDPAs
Configware providers meanwhile offer CPUs as
soft IP cores also called FPGA CPU, or, soft
CPU, to be mapped onto an FPGA, like Micro-
Blaze (32 bits, 125 MHz, Xilinx), the Nios (multi-
granular, Altera), Leon (32 bit RISC, SPARC V8
compatible, public domain). Using the usual
FPGA design flow the soft CPU IP cores can be
generated from VHDL or Verilog originally tar-
geted at a hardwired implementation.
An application example is a configurable system-
on-chip (CSoC) consisting of a soft Leon core,
an XPP-array of suitable size, global and local
memory cores and efficient multi-layer Amba-if it is not efficiently supported by the EDA envi-
ronment(s) available. But EDA still often has
problems with designer productivity and quality of
designs. Zero maintenance bullet-proof self-sup-
porting tools for masses of designers, with a low
EDA budge are still missing.
EDA software market. Currently most of the
configware (reusable soft IP cores) is provided by
the FPGA vendors for free as a service to their
customers: the top FPGA vendors are the key in-
novators. But the number of independent config-
ware houses (soft IP core vendors) and design
services is growing. Also a separate EDA software
market, comparable to the compiler and OS mar-
ket in computers, separate from the morphware is
ems Architecture 49 (2003) 127–142 139based communication interfaces having been
million gates will be available commercially, onto
2002 (invited paper).
[10] Y. Osana et al., Implementation of ReCSiP: a ReConfig-
f Systems Architecture 49 (2003) 127–142which more than a 100 soft processor cores can be
mapped, leaving plenty of area to other on-chip
resources, so that a coarse grain morphware like
from PACT [48] [49,50] can be mapped onto it [38],
maybe in 5–10 years from now. The performance
disadvantage of lower FPGA clock frequency can
be fixed by a higher degree of parallelism obtained
through algorithmic cleverness.
11. Conclusions
The paper has given an introduction to recon-
figurable logic and reconfigurable computing, and
its impact on classical computer science. It also has
pointed out future trends driven by technology
progress and EDA innovations. It has tried to
highlight, that deep submicron allows SoC imple-
mentation, and the silicon IP business reduces
entry barriers for newcomers and turns infra-
structures of existing players into liability.
Many system-level integrated future products
without reconfigurability will not be competitive.
Instead of technology progress better architectures
by reconfigurable platform usage will often be the
key to keep up the current innovation speed be-
yond the limits of silicon. It is time to revisit past
results from morphware-related R&D to derivesynthesized and analyzed onto 0.13 lm UMC
CMOS (8 layer) copper standard cell process at
University of Karlsruhe [49]. The obtained syn-
thesis and optimization results were used within
the first silicon-right 0.13 lm STMicro CMOS
Chips from PACT [53]. The actual applications
currently mapped and analyzed onto this XPP64A
resp. XPP16 (4 · 4 ALU PAE array) are different
MPEG algorithms, W-LAN (Hiperlan-2), Soft-
ware Radio including RAKE-Receiver and Base-
band Filtering. Implementing a 16 tap FIR filter
on a XPP 16 array yielded a speed-up by a factor
of 4 and a power efficiency improvement (MOPS/
mW) by a factor of 16––compared to a state of the
art digital signal processor (SC140 Starcore, [53]).
The Giga FPGA. Already now 32 MicroBlaze or
64 Nios can be mapped onto a single FPGA.
Within a few years FPGAs with more than 100
140 J. Becker, R. Hartenstein / Journal opromising commercial solutions and curricularurable Cell Simulation Platform, FPL, 2003.
[11] R. Enzler et al., Virtualizing hardware with multi-context
reconfigurable arrays, FPL, 2003.
[12] W. Nebel et al., PISA, a CAD Package and Special Hard-
ware for Pixel-oriented Layout Analysis, ICCAD, 1984.
[13] M. Weber et al., MOM––map oriented machine, in: E.
Chiricozzi, A. DAmico (Eds.), Parallel Processing and
Applications, North-Holland, Amsterdam, 1988.
[14] M. Herz, Hartenstein, M. Miranda, E. Brockmeyer, F.
Catthoor, Memory organization for data-stream-based
reconfigurable computing, ICECS, 2002 (invited paper).
[15] D. Gajski et al., A second opinion on dataflow machines,
IEEE Comput. February (1982).
[16] J. Becker et al., Parallelization in co-compilation for config-
urable accelerators, in: Proceedings of ASP-DAC, 1998.updates in basic CS education. Exponentially in-
creasing CMOS mask costs demand adaptive and
re-usable silicon, which can be efficiently realized
by integrating morphware of different granularities
into cSoCs, providing a potential for short time-to-
market (risk minimization), multi-purpose/-stan-
dard features including comfortable application
updates within product life cycles (volume increase:
cost decrease). This results in the fact that several
major industry players are currently integrating
reconfigurable cores/datapaths into their processor
architectures and system-on-chip solutions.
References
[1] Available from <http://morphware.net/>.
[2] W. Mangione-Smith et al., Current issues in configurable
computing research, IEEE Comput. (1997).
[3] Available from <http://fpl.org>.
[4] Available from <http://www.aspdac.com/, http://www.dac.
com/, http://www.date-conference.com/>.
[5] P. Naur, Computing: A Human Activity––Selected Writ-
ings From 1951 To 1990, ACM Press/Addison-Wesley,
New York, 1992.
[6] H. Hansson, DTI prediction, Department of Trade and
Industry (DTI), London, UK 2001.
[7] F. Rammig, Eingebettete Systeme, in: 10th Anniversary
Workshop, Fraunhofer EAS Dresden, April, 2002.
[8] R. Hartenstein, Reconfigurable computing: urging a revision
of basic CS curricula, in: Proceedings of the 15th Interna-
tional Conference on Systems Engineering (ICSENG02),
Las Vegas, USA, 6–8 August 2002 (invited paper).
[9] R. Hartenstein, Data-stream-based computing: Antimate-
rie der Informatik, in: 60th Semester Anniversary Work-
shop of Prof. Reusch, University of Dortmund, 18–19 July[17] Available from <http://configware.org>.
J. Becker, R. Hartenstein / Journal of Systems Architecture 49 (2003) 127–142 141[18] Available from <http://flowware.net>.
[19] J. Rabaey, Reconfigurable processing: the solution to low-
power programmableDSP, in: Proceedings of ICASSP, 1997.
[20] M. Flynn et al., Deep submicron microprocessor design
issues, IEEE Micro July–August (1999).
[21] R. Hartenstein, A decade of research on reconfigurable
architectures, DATE, 2001.
[22] R. Hartenstein, The microprocessor is no more general
purpose, in: Proceedings of the ISIS 1997 (invited).
[23] R. Hartenstein, Trends in reconfigurable logic and recon-
figurable computing, in: ICECS 2002 (invited).
[24] A. DeHon, The density advantage of configurable com-
puting, IEEE Comput. April (2000).
[25] C. Mead, L. Conway, VLSI Systems Design, Addison-
Wesley, Reading, MA, 1980.
[26] R. Kress et al., A datapath synthesis system for the
reconfigurable datapath architecture, in: ASP-DAC95.
[27] J. Becker, T. Pionteck, M. Glesner, An application-tailored
dynamically reconfigurable hardware architecture for dig-
ital baseband processing, SBCCI, 2000.
[28] Available from <http://pactcorp.com>.
[29] U. Nageldinger et al., Generation of design suggestions for
coarse-grain reconfigurable architectures, FPL, 2000.
[30] B. Mei et al., Exploiting loop-level parallelism on coarse-
grained reconfigurable architectures using modulo sched-
uling, DATE, 2003.
[31] Available from <http://kressarray.de>.
[32] J. Frigo et al., Evaluation of the streams-C C-to-FPGA
compiler: an applications perspective, FPGA, 2001.
[33] E. Caspi et al., Extended version of: stream computations
organized for reconfigurable execution (SCORE),FPL, 2000.
[34] C. Chang, K. Kuusilinna, R. Broderson, G. Wright, The
biggascale emulation engine, FPGA, 2002.
[35] A. Ast et al., Data-procedural languages for FPL-based
machines, FPL, 1994.
[36] T. Makimoto, The rising wave of field-programmability,
in: Proceedings of the FPL 2000, Villach, Austria, 27–30
August 2000, Springer Verlag, Heidelberg, 2000 (keynote).
[37] N. Tredennick, Technology and business: forces driving
microprocessor evolution, in: Proceedings of the IEEE,
December, 1995.
[38] J. Cardoso, M. Weinhardt, From C programs to the
configure-execute model, DATE, 2003.
[39] L. Lamport, The parallel execution of Do-Loops, C. ACM
17 (2) (1974) 83–93.
[40] D.B. Loveman, Program improvement by source-to-source
transformation, J. ACM 24 (1) (1977).
[41] W.A. Abu-Sufah, D.J. Kuck, D.H. Lawrie, On the perfor-
mance enhancement of paging systems through program
analysis and transformations, IEEE-Trans. C30 (5) (1981).
[42] U. Banerjee, Speed-up of ordinary programs, Ph.D. Thesis,
University of Illinois at Urbana-Champaign, DCS Report
No. UIUCDCS-R-79-989, October 1979.
[43] J.R. Allen, K. Kennedy, Automatic loop interchange, in:
Proceedings of ACM SIGPLAN84, Symposium on Com-
piler Construction, Montreal, CanadaSIGPLAN Notices19 (6) (1984).[49] J. Becker, M. Vorbach, Architecture, memory and inter-
face technology integration of an industrial/academic
configurable system-on-chip (CSoC), IEEE Computer
Society Annual Workshop on VLSI (WVLSI 2003),
Tampa, Florida, USA, February, 2003.
[50] J. Becker, A. Thomas, M. Vorbach, V. Baumgarte, An
industrial/academic configurable system-on-chip project
(CSoC): coarse-grain XPP-/Leon-based architecture inte-
gration, in: Design, Automation and Test in Europe
Conference (DATE 2003), Munich, March 2003.
[51] M. Perkowski et al., Learning hardware using multiple-
valued logic, IEEE Micro May/June (2002).
[52] Available from <http://evonet.dcs.napier.ac.uk/evoweb/
news_events/conferences/index.html>.
[53] M. Vorbach, J. Becker, Reconfigurable processor architec-
tures for mobile phones, in: Reconfigurable Architectures
Workshop (RAW 2003), Nice, France, April, 2003.
Juergen Becker received the Diploma
degree in 1992, and his Ph.D. (Dr.-Ing.)
degree in 1997, both at Kaiserslautern
University, Germany. From 1997 to
2001 Dr. Becker was Assistant Profes-
sor at the Institute of Microelectro-
nic Systems at Darmstadt University
of Technology, Germany, where he
taught CAD algorithms for VLSI
design and did research in Systems-
on-Chip (SoC) integration and re-
configurable technologies for mobile
communication systems. Since 2001
Juergen Becker is full professor for
embedded electronic systems and head of the Institute for In-
formation Processing (ITIV) at theUniversitaet Karlsruhe (TH).
His actual research is focused on industrial-driven SoCs with
emphasis on adaptive embedded systems, e.g. dynamically re-
configurable hardware architectures. This includes correspond-
ing hardware/software co-design and co-synthesis techniques
from high-level specifications, as well as low power SoC opti-
mization. Prof. Becker is managing Director of the International
Department at Universitaet Karlsruhe (TH) and Vice Depart-
ment Director of Electronic Systems and Microsystems (ESM)
at the Computer Science Research Center (FZI) at the Univer-
sitaet Karlsruhe (TH). He is author and co-author of more than
100 scientific papers, published in peer-reviewed international
journals and conferences. Prof. Becker is active in several tech-
nical program and steering committees of international confer-
ences and workshops. He is a Member of the german GI and
SeniorMember of the IEEE. Prof. Becker is Chair of the GI/ITG[44] J. Becker, K. Schmidt, Automatic parallelism exploitation
for FPL-based accelerators, in: Hawaii International Con-
ference on System Sciences (HICSS98), Big Island,
Hawaii, 1998.
[45] J. Becker, A partitioning compiler for computers with
Xputer-based accelerators, Ph.D. Dissertation, University
of Kaiserslautern, 1997.
[46] K. Schmidt et al., A novel ASIC design approach based on
a new machine paradigm, J. SSC (1991).
[47] D. Manners, T. Makimoto, Living with the Chip, Chap-
man & Hall, London, 1995.
[48] V. Baumgarte et al., PACT XPP––a self-reconfigurable
data processing architecture, ERSA, 2001.Technical Committee of Architectures for VLSI Circuits.
Dr.-Ing. Reiner W. Hartenstein is pro-
fessor of CS&E at Kaiserslautern
University of Technology. He is IEEE
fellow, and member of ACM, GI, and
ITG. All his academic degrees are
from the EE Department at Karlsruhe
University, where he later became
Associate Professor of CS. He has
worked in character recognition, im-
age processing, digital and hybrid
systems, computer architecture, hard-
ware description languages, VLSI
design methods, reconfigurable com-
puting and related compilation tech-
niques. Supported by various funding agencies his groups
achievements are: the hardware design language KARL and its
graphical companion language ABL, the Xputer machine par-
adigm, the KressArray family––a reconfigurable generalization
of systolic arrays, and related structured configware design
tools, configware/software co-compilation, and others. He has
helped to organize numerous international conferences as a
program chair (10 times), industrial chair, steering committee
member, general chair, program committee member and/or
reviewer. He is founder of the PATMOS international work-
shop series, and of the German Multi University E.I.S.-Project
(Mead-&-Conway style VLSI-Design), and co-founder of
IFIP working group 10.2 (now 10.5), the FPL Interna-
tional Conference Series, and of EUROMICRO. He has au-
thored, co-authored, or co-edited 14 books and almost 400
papers.
142 J. Becker, R. Hartenstein / Journal of Systems Architecture 49 (2003) 127–142

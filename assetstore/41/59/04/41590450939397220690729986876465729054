Journal of Discrete Algorithms 25 (2014) 2–20
Contents lists available at SciVerse ScienceDirect
W
G
De
a
Ar
Av
Ke
Co
1.
na
do
st
th
th
up
th
co
Th
da
th
qu
da
tw
up
in
an
✩
1
15
htJournal of Discrete Algorithms
www.elsevier.com/locate/jda
avelet trees for all✩
onzalo Navarro 1
partment of Computer Science, University of Chile, Chile
r t i c l e i n f o a b s t r a c t
ticle history:
ailable online 19 July 2013
ywords:
mpact data structures
The wavelet tree is a versatile data structure that serves a number of purposes, from string
processing to computational geometry. It can be regarded as a device that represents a
sequence, a reordering, or a grid of points. In addition, its space adapts to various entropy
measures of the data it encodes, enabling compressed representations. New competitive
solutions to a number of problems, based on wavelet trees, are appearing every year.
In this survey we give an overview of wavelet trees and the surprising number of
applications in which we have found them useful: basic and weighted point grids, sets
of rectangles, strings, permutations, binary relations, graphs, inverted indexes, document
retrieval indexes, full-text indexes, XML indexes, and general numeric sequences.
© 2013 Elsevier B.V. All rights reserved.
Introduction
If one looks around for “wavelet trees”, one is likely to end up reviewing signal processing bibliography [1,24,104]. The
me wavelet is given to a time-bounded oscillation, which is used to decompose an input signal into time and frequency
mains simultaneously. More elementarily, the wavelet is “found” within the input signal at different starting positions and
retches. The input signal is then transformed into another (called the “wavelet transform” of the input signal) that indicates
e stretches that are used to align the wavelet at each starting position, so that the input signal is expressed as the sum of
e stretched wavelets. The “low frequencies” (wavelet stretched to a long time interval and thus with a “soft” shape) make
a coarse description of the original signal, whereas the “high frequencies” (wavelet stretched to a short time interval and
us with a “crisp” shape) describe the “details”. One can then, for example, discard the high frequencies to achieve lossy
mpression. In the so-called multiresolution analysis, instead, one recursively analyzes low and high frequencies separately.
e resulting representation is called a “wavelet packet decomposition” or a “wavelet packet tree”.
The wavelet tree we describe in this survey was invented in 2003 by Grossi, Gupta, and Vitter [57], as a space-efficient
ta structure to represent a sequence and answer some queries on it. Its name reveals some affinity with the idea of
e wavelet packet decomposition: “high” and “low” symbol values of the sequence are separated and the resulting subse-
ences are recursively subdivided.
It is interesting that, after some thought, one can see that the wavelet tree is a slight generalization of an old (1988)
ta structure by Chazelle [27], heavily used in Computational Geometry. This data structure represents a set of points on a
o-dimensional grid: it describes a successive reshuffling process where the points start sorted by one coordinate and end
sorted by the other (by recursively separating “low” and “high” coordinate values in the latter dimension). Kärkkäinen,
1999 [70], was the first to put this structure in use in the completely different context of text indexing. Still, the concept
d usage were totally different from the one Grossi et al. would propose four years later.
An early version of this article appeared in Proc. CPM 2012.
E-mail address: gnavarro@dcc.uchile.cl.
Partially funded by Millennium Nucleus Information and Coordination in Networks ICM/FIC P10-024F, Chile.
70-8667/$ – see front matter © 2013 Elsevier B.V. All rights reserved.
tp://dx.doi.org/10.1016/j.jda.2013.07.004
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 3
Fi
ed
(i
w
ar
in
in
re
be
re
co
2.
Σ
de
2.
[a
in
th
an
tr
[1
{‘
it
bo
en
ho
2.
a
pe
fir
wg. 1. A wavelet tree on string S = “alabar a la alabarda”. We draw the spaces as underscores. The subsequences of S and the subsets of Σ labeling the
ges are drawn (in gray) for illustration purposes; the tree stores only the topology and the bitmaps (in black).
We have already mentioned three ways in which wavelet trees can be regarded: (i) as a representation of a sequence;
i) as a representation of a reordering of elements; (iii) as a representation of a grid of points. Since 2003, these views of
avelet trees, and their interactions, have been fruitful in a surprisingly wide range of problems, extending well beyond the
eas of text indexing and computational geometry where the structure was conceived.
Our goal in this article is to give an overview of this marvelous data structure and its many applications. We aim to
troduce, to an audience with a general algorithmic background, the basic data organization used by wavelet trees, the
formation they can model, and the wide range of problems they can solve. We will also mention the most technical
sults and give the references to be followed by the more knowledgeable readers, advising the rest what to skip.
Being ourselves big fans of wavelet trees, and having squeezed them out for several years, it is inevitable that there will
many references to our own work in this survey. We apologize in advance for this, as well as for oversights of others’
sults, which are likely to occur despite our efforts. We also call the attention of the reader to another thoughtful and
mprehensive survey on wavelet trees which has recently appeared [80].
Data structure
Let S[1,n] = s1s2 . . . sn be a sequence of symbols si ∈ Σ , where Σ is a finite alphabet of size σ . To simplify we assume
= [1..σ ]. Then S can be represented in plain form using nlgσ  = n lgσ + O (n) bits (we use lg x = log2 x). We first
scribe the wavelet tree as a balanced binary tree over ordered leaves; later we will lift those restrictions.
1. Structure
A wavelet tree [57] for sequence S[1,n] over alphabet [1..σ ] can be described recursively. A wavelet tree over alphabet
..b] ⊆ [1, σ ] is a binary balanced tree with b − a + 1 leaves. If a = b, the tree is just a leaf labeled a. Else it has an
ternal root node, vroot , that represents S[1,n]. This root stores a bitmap Bvroot [1,n] defined as follows: if S[i] (a + b)/2
en Bvroot [i] = 0, else Bvroot [i] = 1. We define S0[1,n0] as the subsequence of S[1,n] formed by the symbols c  (a + b)/2,
d S1[1,n1] as the subsequence of S[1,n] formed by the symbols c > (a + b)/2. Then, the left child of vroot is a wavelet
ee for S0[1,n0] over alphabet [a..(a + b)/2] and the right child of vroot is a wavelet tree for S1[1,n1] over alphabet
+ (a + b)/2..b].
Fig. 1 displays a wavelet tree for the sequence S = “alabar a la alabarda”. Here for legibility we are using Σ =
’,a,b,d, l, r}, so n = 19 and σ = 6.
Note that this wavelet tree has height lgσ , and it has σ leaves and σ − 1 internal nodes. If we regard it level by level,
is not hard to see that it stores exactly n bits at each level, and at most n bits in the last one. Thus, nlgσ  is an upper
und to the total number of bits it stores. Storing the topology of the tree requires O (σ lgn) further bits, if we are careful
ough to use O (lgn) bits for the pointers. This extra space may be a problem on large alphabets. We show in Section 2.3
w to save it.
2. Tracking symbols
The wavelet tree we have described represents S , in the sense that one can recover S from it. More than that, it is
succinct data structure for S , in the sense that it takes space asymptotically equal to a plain representation of S , and it
rmits accessing any S[i] in time O (lgσ), as follows.
To extract S[i], we first examine Bvroot [i]. If it is a 0, we know that S[i] (σ + 1)/2, otherwise S[i] > (σ + 1)/2. In the
st case, we must continue recursively on the left child; in the second case, on the right child. The problem is to determine
here has position i been mapped to on the left (or right) child. In the case of the left child, where Bvroot [i] = 0, i has been
4 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
Fig
ac
Fig
sto
m
co
bi
of
[a
bi
i −
sp
st
sa
a
se
an
to
of
po
it
is
Th
to
th
us
st. 2. Accessing the symbol S[11] from our example wavelet tree, and finding out it is an ‘a’. Recall that only the tree topology and the bitmaps are
tually stored.
. 3. Tracking upwards to find the position of the second ‘b’ in S , finding it at S[16]. Recall that only the tree topology and the bitmaps are actually
red.
apped to position i0, which is the number of 0s in Bvroot up to position i. For the right child, where Bvroot [i] = 1, this
rresponds to position i1, the number of 1s in Bvroot up to position i. The number of 0s (resp. 1s) up to position i in a
tmap B is called rank0(B, i) (resp. rank1(B, i)). We continue this process recursively until we arrive at a leaf. The label
this leaf is S[i]. Note that we do not store the leaf labels; those are deduced as we successively restrict the subrange
..b] of [1..σ ] as we descend. Fig. 2 illustrates the process.
Operation rank was already considered by Chazelle [27], who gave a simple data structure using O (n) bits for a
tmap B[1,n], that computed rank in constant time (note that we only have to solve rank1(B, i), since rank0(B, i) =
rank1(B, i)). Jacobson [67] improved the space to n + O (n lg lgn/ lgn) = n + o(n) bits, and Golynski [51,52] proved this
ace is optimal as long as we maintain B in plain form and build extra data structures on it. The solution is, essentially,
oring rank answers every s = lg2 n bits of B (using lgn bits per sample), then storing rank answers relative to the last
mple every (lgn)/2 bits (using lg s = 2 lg lgn bits per sub-sample), and using a universal table to complete the answer to
rank query within a sub-sample. We will use in this survey the notation rankb(B, i, j) = rankb(B, j)−rankb(B, i − 1).
Above, we have tracked a position from the root to a leaf, and as a consequence we have discovered the symbol repre-
nted at the root position. It is also useful to carry out the inverse process: given a position at a leaf, we track it upwards
d find out where it is on the root bitmap, and hence on string S . This is done as follows.
Assume we start at a given leaf, at position i. If the leaf is the left child of its parent v , then the position i′ corresponding
i at v is the i-th occurrence of a 0 in its bitmap Bv . If the leaf is the right child of its parent v , then i′ is the position
the i-th occurrence of a 1 in Bv . This procedure is repeated from v until we reach the root, where we find the final
sition. The operation of finding the i-th 0 (resp. 1) in a bitmap B[1,n] is called select0(B, i) (resp. select1(B, i)), and
can also be solved in constant time using the n bits of B plus o(n) bits [29,84]. Thus the time to track a position upwards
also O (lgσ). Fig. 3 illustrates this process.
The constant-time solution for select [29,84] is analogous to that of rank. The bitmap is cut into blocks with s 1s.
ose that are long enough to store all their answers within sublinear space are handled in this way. The others are not
o long (i.e., O (lgO (1) n)) and thus encoding positions inside them require fewer bits (i.e., O (lg lgn)). This permits repeating
e idea recursively a second time. The third time, the remaining blocks are so short that can be handled in constant time
ing universal tables. Golynski [51,52] reduced the o(n) extra space to O (n lg lgn/ lgn) and proved this is optimal if B is
ored in plain form.
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 5
Fi
sto
o(
2.
sh
w
at
bi
co
at
ha
an
bi
op
of
th
sy
O
n
in
n
2.
tr
bu
ov
sp
sp
B
s
2
fro
o(
n l
in
fog. 4. A pointerless wavelet tree. Now the tree topology (in gray, also surrounding the nodes in rectangles for clarity) is not represented; the structure just
res one bitmap per level and concatenates them. Note that the alphabet partition is slightly modified.
With the support for rank and select, the space required by the basic binary balanced wavelet tree reaches nlgσ +
n) lgσ + O (σ lgn) bits. This completes a basic wavelet tree description; the rest of the section is more technical.
3. Reducing redundancy
As mentioned, the O (σ lgn) term can be removed if necessary [76,77]. We slightly alter the balanced wavelet tree
ape, so that all the leaves at the last level are grouped to the left, becoming a contiguous block of leaves (for this sake
e divide the interval [a..b] of [1..σ ] into [a..a + 2lg(b−a+1)−1 − 1] and [a + 2lg(b−a+1)−1..b]). Once there are no holes
any tree level, the bitmaps at all the levels can all be concatenated into a large bitmap B[1,nlgσ ]. We know the
tmap of level  starts at position 1 + n( − 1). Moreover, if we have determined that the bitmap of a wavelet tree node
rresponds to B[l, r], then the bitmap of its left child is at B[n + l,n + l + rank0(B, l, r) − 1], and that of the right child is
B[n+ l+rank0(B, l, r),n+ r]. Moving to the parent of a node is more complicated, but upward traversals can always be
ndled by first going down from the root to the desired leaf, so as to discover all the ranges in B of the nodes in the path,
d then doing the upward processing as one returns from the recursion. Fig. 4 illustrates this arrangement.
Using just one bitmap, we do not need pointers for the topology, and the overall space becomes nlgσ  + o(n) lgσ
ts. The time complexities do not change. However, in practice the operations are slowed down due to the extra rank
erations needed to navigate. A recent solution to this is the so-called “wavelet matrix” [33]. The idea is to send all the 0s
a row to the left of all the 1s. Then the wavelet tree nodes are scrambled across the level bitmap, but are still contiguous,
us any algorithm on the wavelet tree can be emulated in the wavelet matrix. Some simple ones, like accessing a sequence
mbol, can be carried out with just one rank operation per level, which is as fast as on the explicit tree representation.
ther operations are also speeded up.
The redundancy can be further reduced by representing the bitmaps using a structure by Golynski et al. [53], which uses
+ O (n lg lgn/ lg2 n) bits and supports constant-time rank and select (this representation does not leave the bitmap
plain form, and thus it can break the lower bound [52]). Added over all the wavelet tree bitmaps, the space becomes
lgσ + O (n lgσ lg lgn/ lg2 n) = n lgσ + o(n) bits.2 This structure has not been implemented as far as we know.
4. Speeding up traversals
Increasing the arity of wavelet trees reduces their height, which dictates the complexity of the downward and upward
aversals. If the wavelet tree is d-ary, then its height is lgd σ . However, the wavelet tree does not store bitmaps anymore,
t rather sequences Bv over alphabet [1..d], so that the symbol at Sv [i] is stored at the child numbered Bv [i] of node v .
In order to obtain time complexities O (1+ lgd σ) for the operations, we need to handle rank and select on sequences
er alphabet [1..d], in constant time. Ferragina et al. [43] showed that this is indeed possible, while maintaining the overall
ace within n lgσ + o(n) lgσ , for d = o(lgn/ lg lgn). Using, for example, d = lg1− n for any constant 0 <  < 1, the overall
ace is n lgσ + O (n lgσ/ lg n) bits. Golynski et al. [53] reduced the space to n lgσ + o(n) bits.
To support symbol rank and select on a sequence R[1,n] over alphabet [1..d], we assume we have d bitmaps
c[1,n], for c ∈ [1..d], where Bc[i] = 1 iff R[i] = c. Then rankc(R, i) and selectc(R, i) are reduced to rank1(Bc, i) and
elect1(Bc, i). We cannot afford to store those Bc , but we can store their extra o(n) data for binary rank and select.
We assume lgσ = O (lgn) here; otherwise there are many symbols that do not appear in S . If this turns out to be the case, one should use a mapping
m Σ to the range [1..σ ′], where σ ′  n is the number of symbols actually appearing in S . Such a mapping takes constant time and σ ′ lg(σ/σ ′) +
σ ′) + O (lg lgσ) bits of space using the “indexable dictionaries” of Raman et al. [99]. Added to the n lgσ ′ + o(n) bits of the wavelet tree, we are within
gσ +o(n)+ O (lg lgσ) bits. This is n lgσ +o(n) unless n = O (lg lgσ), in which case a plain representation of S using nlgσ  bits solves all the operations
O (lg lgσ) time. To simplify, a recent analysis [48] claims n lgσ + O (n) bits under similar assumptions. We will ignore the issue from now, and assume
r simplicity that all symbols in [1..σ ] do appear in S .
6 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
Ea
ta
al
O
ad
or
pr
ar
re
u
us
up
o(
O
n l
m
fo
to
2.
do
re
sp
3.
ar
m
3.1
fre
of
w
le
th
No
is
No
3
indch time we need access to Bc , we access instead R and use a universal table to simulate the bitmap’s content. Such
ble gives constant-time access to chunks of length lgd(n)/2 instead of lg(n)/2, so the overall space using Golynski et
.’s bitmap index representation [51,52] is O (dn lg lgn/ lgd n), which added over the lgd σ levels of the wavelet tree gives
(n lgσ · d lgd lg lgn/ lgn). This is o(n lgσ) for any d = lg1− n. Further reducing the redundancy to o(n) bits requires more
vanced techniques [53].
Thus, the O (lgσ) upward/downward traversal times become O (lgσ/ lg lgn) with multiary wavelet trees. Although the-
etically attractive, it is not easy to translate their advantages to practice (see, e.g., a recent work studying interesting
actical alternatives [18]). An exception, for a particular application, is described in Section 5.2.
The upward traversal can be speeded up further, using techniques known in computational geometry [27]. Imagine we
e at a leaf u representing a sequence S[1,nu] and want to directly track position i to an ancestor v at distance t , which
presents sequence S[1,nv ]. We can store at the leaf u a bitmap Bu[1,nv ], so that the nu positions corresponding to leaf
are marked as 1s in Bu . This bitmap is sparse, so it is stored in compressed form as an “indexable dictionary” [99], which
es nu lg(nv/nu) + o(nu) + O (lg lgnv) bits and can answer select1(Bu, i) queries in O (1) time. Thus we track position i
wards for t levels in O (1) time.
The space required for all the bitmaps that point to node v is the sum, over at most 2t leaves u, of those nu lg(nv/nu)+
nu) + O (lg lgnv) bits. This is maximized when nu = nv/2t for all those u, where the space becomes t · nv + o(nv) +
(2t lg lgnv). Added over all the wavelet tree nodes with height multiple of t , we get n lgσ + o(n lgσ) + O (σ lg lgn) =
gσ + o(n lgσ). This is in addition to those n lgσ + o(n) bits already used by the wavelet tree.
If we want to track only from the leaves to the root, we may just use t = lgσ and do the tracking in constant time. In
any cases, however, one wishes to track from arbitrary to arbitrary nodes. In this case we can use 1/ values of t = lgi σ ,
r i ∈ [1..1/ − 1], so as to carry out O (lg σ ) upward steps with one value of t before reaching the next one. This gives a
tal complexity for upward traversals of O ((1/) lg σ ) using O ((1/)n lgσ) bits of space.
5. Construction
It is easy to build a wavelet tree in O (n lgσ) time, by a linear-time processing at each node. It is less obvious how to
it in little extra space, which may be important for succinct data structures. Two recent results [34,102] offer various
levant space–time tradeoffs, building the wavelet tree within the time given, or close, and asymptotically negligible extra
ace.
Compression
The wavelet tree adapts elegantly to the compressibility of the data in many ways. Two key techniques to achieve this
e using specific encodings on bitmaps, and altering the tree shape. This whole section is technical, yet nonexpert readers
ay find inspiring the beginning of Sections 3.1 and 3.2.
. Entropy coding
Consider again Fig. 1. The fact that the ’a’ is much more frequent than the other symbols translates into unbalanced 0/1
quencies in various bitmaps. Dissimilarities in symbol frequencies are an important source of compressibility. The amount
compression that can be reached is measured by the so-called empirical zero-order entropy of a sequence S[1,n]:
H0(S) =
∑
c∈Σ
(nc/n) lg(n/nc) lgσ
here nc is the number of occurrences of c in S and we sum only the symbols that do appear in S . Then nH0(S) is the
ast number of bits into which S can be compressed by always encoding the same symbol in the same way.3
Grossi et al. [57] already showed that, if the bitmaps of the wavelet tree are compressed to their zero-order entropy,
en their overall space is nH0(S). Let Bvroot contain n0 0s and n1 1s. Then zero-order compressing it yields space
n0 lg(n/n0) + n1 lg(n/n1).
w consider its left child vl . Its bitmap, Bvl , is of length n0, and say it contains n00 0s and n01 1s. Similarly, the right child
of length n1 and contains n10 0s and n11 1s. Adding up the zero-order compressed space of both children yields
n00 lg(n0/n00) + n01 lg(n0/n01) + n10 lg(n1/n10) + n11 lg(n1/n11).
w adding the space of the root bitmap yields
In classical information theory [35], H0 is the least number of bits per symbol achievable by any compressor on an infinite source that emits symbols
ependently and randomly with probabilities nc/n.
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 7
Th
w
is
A
co
an
in
re
nH
w
sp
us
Se
3.
an
pa
to
a∑
w
st
in
of
da
us
th
Fu
tr
do
po
isFig. 5. A wavelet tree with Huffman shape for our string S .
n00 lg(n/n00) + n01 lg(n/n01) + n10 lg(n/n10) + n11 lg(n/n11).
is would already be nH0(S) if σ = 4. It is easy to see that, by splitting the spaces of the internal nodes until reaching the
avelet tree leaves, we arrive at
∑
c∈Σ nc lg(n/nc) = nH0(S).
This enables using any zero-order entropy coding for the bitmaps that supports constant-time rank and select. One
the “fully indexable dictionary” of Raman et al. [99], which for a bitmap B[1,n] requires nH0(B) + O (n lg lgn/ lgn) bits.
theoretically better one is that of Golynski et al. [53], which we have already mentioned without yet telling that it actually
mpresses the bitmap, to nH0(B)+ O (n lg lgn/ lg2 n). Pa˘tras¸cu [97] showed this can be squeezed up to nH0(B)+ O (n/ lgc n),
swering rank and select in time O (c), for any constant c, and that this is essentially optimal [98].
Using the second or third encoding, the wavelet tree represents S within nH0(S)+o(n) bits, still supporting the traversals
time O (lgσ). Ferragina et al. [43] showed that the zero-order compression can be extended to multiary wavelet trees,
aching nH0(S)+o(n lgσ) bits and time O (1+ lgσ/ lg lgn) for the operations, and Golynski et al. [53] reduced the space to
0(S)+o(n) bits. Recently, Belazzougui and Navarro [13] showed that the times can be reduced to O (1+ lgσ/ lgw), where
= Ω(lgn) is the size of the machine word. Basically they replace the universal tables with bit-parallel operations. Their
ace grows to nH0(S)+o(n(H0(S)+1)). (They also prove and match the lower bound time complexity Θ(1+ lg(lgσ/ lgw))
ing techniques that are beyond wavelet trees and this survey, but that do build on wavelet trees [5,8].)
It should not be hard to see at this point that the sums of nu lg(nv/nu) spaces used for fast upward traversals in
ction 2.4 also add up to (1/)nH0(S).
2. Changing shape
The algorithms for traversing the wavelet tree work independently of its balanced shape. Furthermore, our previous
alysis of the entropy coding of the bitmap also shows that the resulting space, at least with respect to the entropy
rt, is independent of the shape of the tree. This was already noted by Grossi et al. [58], who proposed using the shape
optimize average query time: If we know the relative frequencies fc with which each leaf c is sought, we can create
wavelet tree with the shape of the Huffman tree [66] of those frequencies, thus reducing the average access time to
c∈Σ fc lg(1/ fc) lgσ .
Mäkinen and Navarro [74, Section 3.3], instead, proposed giving the wavelet tree the Huffman shape of the frequencies
ith which the symbols appear in S . This has interesting consequences. First, it is easy to see that the total number of bits
ored in the wavelet tree is exactly the number of bits output by a Huffman compressor that takes the symbol frequencies
S , which is upper-bounded by n(H0(S) + 1). Therefore, even using plain bitmap representations taking n + o(n) bits
space, the total space becomes at most n(H0(S) + 1) + o(n(H0(S) + 1)) + O (σ lgn), that is, we compress not only the
ta, but also the redundancy space. This may seem irrelevant compared to the nH0(S) + o(n) bits that can be obtained
ing Golynski et al. [53] over a balanced wavelet tree. However, it is unclear whether that approach is practical; only
at of Raman et al. [99] has successful implementations [30,91,95], and this one leads to total space nH0(S) + o(n lgσ).
rthermore, plain bitmap representations are significantly faster than compressed ones, and thus compressing the wavelet
ee by giving it a Huffman shape leads to a much faster implementation in practice. Fig. 5 illustrates this shape.
Another consequence of using Huffman shape, implied by Grossi et al. [58], is that if the accesses to the leaves are
ne with frequency proportional to their number of occurrences in S (which occurs, for example, if we access at random
sitions in S), then the average access time is O (1+ H0(S)), better than the O (lgσ) of balanced wavelet trees. A problem
that the worst case could be as bad as O (lgn) if a very infrequent symbol is sought [74]. However, one can balance
8 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
w
nu
tr
tim
m
w
c
en
po
an
c(
th
w
tr
3.
th
w
on
th
sy
fo
th
w
co
le
k
se
in
w
th
us
fa
sp
yi
en
by
th
th
th
3.
th
pr
ha
Sb
thavelet subtrees after some depth, so that the average depth is O (1+ H0(S)), the maximum depth is O (lgσ), and the total
mber of bits is at most n(H0(S) + 2) [74].
Recently, Barbay and Navarro [11] showed that Huffman shapes can be combined with multiary wavelet trees and en-
opy compression of the bitmaps, to achieve space nH0(S)+o(n) bits, worst-case time O (1+ lgσ/ lg lgn), and average case
e O (1+ H0(S)/ lg lgn).
An interesting extension of Huffman-shaped wavelet trees that has not been emphasized much is to use them as a
echanism to give direct access on any variable-length prefix-free coding. Let S = s1, s2, . . . , sn be a sequence of symbols,
hich are encoded in some way into a bit-stream C = c(s1)c(s2) . . . c(sn). For example, S may be a numeric sequence and
can be a δ-code, to favor small numbers [14], or c can be a Huffman or another prefix-free encoding. Any prefix-free
coding ensures that we can retrieve S from C , but if we want to maintain the compressed form C and access arbitrary
sitions of S , we need tricks like sampling S at regular intervals and store pointers to C .
Instead, a wavelet tree representation of S , where for each si we rather encode c(si), uses the same number of bits of C
d gives direct access to any S[i] in time O (|c(si)|). More precisely, at the bitmap root position Bvroot [i] we write a 0 if
si) starts with a 0, and 1 otherwise. In the first case we continue by the left child and in the second case we continue by
e right child, from the second bit of c(si), until the code is exhausted. Gagie et al. [46] combined this idea with multiary
avelet trees to obtain a faster decoding.
Very recently, Grossi and Ottaviano [59] extended wavelet trees to handle a sequence of strings, and gave the wavelet
ee the shape of the trie of those strings. The idea extends a previous, more limited, approach [76,77].
3. High-order entropy coding
High-order compression extends zero-order compression by encoding each symbol according to a context of length k
at precedes or follows it. The k-th order empirical entropy of S [82] is defined as Hk(S) =∑A∈Σk (|S A |/n)H0(S A) Hk−1(S),
here S A is the string of symbols preceding context A in S . Any statistical compressor assigning fixed codes that depend
a context of length k outputs at least nHk(S) bits to encode S .
The Burrows–Wheeler transform [23] is a useful tool to achieve high-order entropy. It is a reversible transformation
at permutes the symbols of a string S[1,n] as follows. First sort all the suffixes S[i,n] lexicographically, and then list the
mbols that precede each suffix (where S[n] precedes S[1,n]). The result, Sbwt[1,n], is the concatenation of the strings S A
r all the contexts A. By definition, if we compress each substring S A of Sbwt to its zero-order entropy, the total space is
e k-th order entropy of S , for k = |A|.
The first [57] and second [42] reported use of wavelet trees used a similar partitioning to represent each range of Sbwt
ith a zero-order compressed wavelet tree, so as to reach nHk(S) + o(n lgσ) bits of space, for any k  α lgσ n and any
nstant 0 < α < 1. In the second case [42], the use of Sbwt was explicit. The partitioning was not with a fixed context
ngth, but instead an optimal partitioning was used [39]. This way, they obtained the given space simultaneously for any
in the range. In the first case [57], they made no reference to the Burrows–Wheeler transform, but also compressed the
quences S A of the k-th order entropy formula, for a fixed k. We give more details on the reasons behind the use of Sbwt
Section 5.
Already in 2004, Grossi et al. [58] realized that the careful partitioning into many small wavelet trees, one per context,
as not really necessary to achieve k-th order compression. By using a proper encoding on its bitmaps, a wavelet tree on
e whole Sbwt could reach k-th order entropy compression of a string S . They obtained 2nHk(S) bits, plus redundancy, by
ing γ -codes [14] on the runs of 0s and 1s in the wavelet tree bitmaps. Mäkinen and Navarro [78] observed the same
ct when encoding the bitmaps using Raman et al. [99] fully indexable dictionaries. They reached nHk(S)+ o(n lgσ) bits of
ace, simultaneously for any k α lgσ n and any constant 0<α < 1, using just one wavelet tree for the whole string. This
elded simpler and faster indexes in practice [30].
The key property is that some entropy-compression methods are local, that is, their space is the sum of the zero-order
tropies of short substrings of Sbwt . This can be shown to be upper-bounded by the entropy of the whole string, but also
the sum of the entropies of the substrings S A . Even more surprisingly, Kärkkäinen and Puglisi [71] recently showed that
e k-th order entropy is still reached if one cuts Sbwt into equally-spaced regions of appropriate length, and thus simplified
ese indexes further by using the faster and more practical Huffman-shaped wavelet trees on each region.
There are also more recent and systematic studies [38,62] of the compressibility properties of wavelet trees, and how
ey relate to gap and run-length encodings of the bitmaps, as well to the balancing and the arity.
4. Exploiting repetitions
Another relevant source of compressibility is repetitiveness, that is, that S[1,n] can be decomposed into a few substrings
at have appeared earlier in S , or alternatively, that there is a small context-free grammar that generates S . Many com-
essors build on these principles [14], but supporting wavelet tree functionality on such compressed representations is
rder.
Mäkinen and Navarro [75] studied the effect of repetitions in the Burrows–Wheeler transform of S . They showed that
wt could be partitioned into at most nHk(S) + σ k runs of equal letters in Sbwt , for any k. It is not hard to see that
ose runs are inherited by the wavelet tree bitmaps, where run-length compression would take proper advantage of them.
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 9
M
to
M
W
ti
en
tr
af
th
st
4.
m
4.
op
pl
an
an
w
is
to
th
4.
le
or
th
up
[1
O
idFig. 6. Computing rank‘l’(S,11), that is, the number of occurrences of ‘l’ up to position 11 in S , using a wavelet tree, and finding out it is 2.
äkinen and Navarro followed a different path: they built a wavelet tree on the run heads and used a couple of bitmaps
simulate the operations on the original strings. The compressibility of those two bitmaps has been further studied by
äkinen et al. [79,101] in the context of highly repetitive sequence collections, and also by Gog [50, Section 3.6.1].
In some cases, however, we need the wavelet tree of the very same string S that contains the repetition, not its Burrows–
heeler transform. We describe such an application in Section 6.4.
Recently, Navarro et al. [93] proposed a grammar-compressed wavelet tree for this problem. The key point is that repeti-
ons in S[1,n] induce repetitions in Bvroot [1,n]. They used Re-Pair [73], a grammar-based compressor, on the bitmaps, and
hanced a Re-Pair-based compressed sequence representation [56] to support binary rank (they only needed downward
aversals). This time, the wavelet tree partitioning into left and right children cuts each repetition into two, so quickly
ter a few levels such regularities are destroyed and another type of bitmap compression (or none) is preferred. While the
eoretical space analysis is too weak to be useful, the result is good in practice and leaves open the challenge of achieving
ronger theoretical and practical results.
We will find even more specific wavelet tree compression problems later.
Sequences, reorderings, or point grids?
Now that we have established the basic structure, operations, and encodings of wavelet trees, let us take a view with
ore perspective. Various applications we have mentioned display different ways to regard a wavelet tree representation.
1. As a sequence of values
This is the most basic one. The wavelet tree on a sequence S = s1, . . . , sn represents the values si . The most important
erations that the wavelet tree must offer to support this view are, apart from accessing any S[i] (that we already ex-
ained in Section 2), rank and select on S . For example, the second main usage of wavelet trees [42,43] used access
d rank on the wavelet tree built on sequence Sbwt in order to support searches on S .
The process to support rankc(S, i) is similar to that for access, with a subtle difference. We start at position i in Bvroot ,
d decide whether to go left or right depending on where is the leaf corresponding to c (and not depending on Bvroot [i]). If
e go left, we rewrite i ← rank0(Bvroot , i), else we rewrite i ← rank1(Bvroot , i). When we arrive at the leaf c, the value of i
the final answer. Fig. 6 illustrates operation rank. The time complexity for this operation is that of a downward traversal
wards the leaf labeled c. To support selectc(S, i) we apply the upward tracking, as described in Section 2, starting at
e i-th position of the leaf labeled c.
2. As a reordering
Less obviously, the wavelet tree structure describes a stable ordering of the symbols in S , so that if one traverses the
aves one finds first all the occurrences of the smaller symbols, and within the same symbol (i.e., the same leaf), they are
dered by original position. As it will be clear in Section 5, one can argue that this is the usage of wavelet trees made by
eir creators [57].
In this case, tracking a position downwards in the wavelet tree tells where it goes after sorting, and tracking a position
wards tells where each symbol is placed in the sequence. An obvious application is to encode a permutation π over
..n]. Fig. 7 illustrates the idea. Our best wavelet tree takes n lgn+ o(n) bits and can compute any π(i) and π−1(i) in time
(lgn/ lg lgn) by carrying out, respectively, downward and upward tracking of position i. We will see improvements on this
ea later.
10 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
4.
w
or
so
w
re
th
[y
al
No
an
ar
w
fa
al
co
y-
effi
5.
5.1
A
le
in
m
oc
Fe
th
of
O
ve
re
by
σ
fu
so
fro
eaFig. 7. Representing permutation π = (7 4 1 6 8 3 5 2) using a wavelet tree. As before, the grayed values are not actually stored.
3. As a grid of points
The slightly less general structure of Chazelle [27] can be taken as the representation of a set of points supported by
avelet trees. It is generally assumed that we have an n × n grid with n points so that no two points share the same row
column (i.e., a permutation). A general set of n points is mapped to such a discrete grid by storing the real coordinates
mewhere else and breaking ties somehow (arbitrarily is fine in most cases).
Take the set of points (xi, yi), in x-coordinate order (i.e., xi < xi+1). Now define string S[1,n] = y1, y2, . . . , yn . Then
e can find the i-th point in x-coordinate order by accessing S[i]. Moreover, since the wavelet tree is representing the
ordering of the points according to y-coordinate, one can find the i-th point in y-coordinate order by tracking upwards
e i-th point in the leaves.
Unlike permutations, here the emphasis is in counting and reporting the points that lie within a rectangle [xmin, xmax] ×
min, ymax]. This is solved through a more complicated tracking mechanism, well-known in computational geometry and
so described explicitly on wavelet trees [76]. We start at the root bitmap range Bvroot [xl, xr], where xl = xmin and xr = xmax.
w we map the interval to the left and to the right, using xl ← rank0/1(Bvroot , xl − 1) + 1 and xr ← rank0/1(Bvroot , xr),
d continue recursively. At any node along the recursion, we may stop if (i) the interval [xl, xr] becomes empty (thus there
e no points to report); (ii) the interval of leaves (i.e., y-coordinate values) represented by the node has no intersection
ith [ymin, ymax]; (iii) the interval of leaves is contained in [ymin, ymax]. In case (iii) we can count the number of points
lling in this sub-rectangle as xr − xl + 1. As it is well known that we visit only O (lgn) wavelet tree nodes before stopping
l the recursive calls (see, e.g., a recent detailed proof, among other more sophisticated wavelet tree properties [48]), the
unting time is O (lgn). Each of the xr − xl + 1 points found in each node can be tracked up and down to find their x- and
coordinates, in O (lgn) time per reported occurrence. Fig. 8 illustrates the structure and query process. There are more
cient variants of this technique that we will cover in Section 7, but they build on this basic idea.
Applications as sequences
. Full-text indexes
A full-text index built on a string S[1,n] is able to count and locate the occurrences of arbitrary patterns P [1,m] in S .
classical index is the suffix array [55,81], A[1,n], which lists the starting positions of all the suffixes of S , S[A[i],n], in
xicographic order, using nlgn bits. The starting positions of the occurrences of P in S appear in a contiguous range
A, which can be binary searched in time O (m lgn), or O (m + lgn) by doubling the space. A suffix tree [2,83,105] is a
ore space-consuming structure (yet still O (n lgn) bits) that can find the range in time O (m). After finding the range, each
currence is reported in constant time, both in suffix trees and arrays.
The suffix array of S is closely related to its Burrows–Wheeler transform: Sbwt[i] = S[A[i] − 1] (taking S[0] = S[n]).
rragina and Manzini [40,41] showed how, using at most 2m access and rank operations on Sbwt , one could count
e number of occurrences in S of a pattern P [1,m]. Using multiary wavelet trees [43,53] this gives a counting time
O (m) on polylog-sized alphabets, and O (m lgσ/ lg lgn) in general. Each such occurrence can then be located in time
(lg1+ n lgσ/ lg lgn) for any  > 0, at the price of O (n/ lg n) = o(n) further bits of space. This result has been superseded
ry recently [5,8,12,13], in some cases using wavelet trees as a part of the solution, and in all cases with some extra
dundancy, such as o(nHk(S)) and O (n) further bits.
Grossi et al. [57,60,61] used wavelet trees to obtain a similar result via a quite different strategy. They represented A
means of a permutation Ψ (i) = A−1[A[i] + 1], that is, the cell in A pointing to A[i] + 1. Ψ turns out to be formed by
contiguous ascending runs. The suffix array search can be simulated in O (m lgn) accesses to Ψ . Fig. 9 illustrates a Ψ
nction, and Fig. 12 shows how it can be represented in compressed form using a wavelet tree. They encode Ψ in a more
phisticated form, however: separately for the range of each context S A (recall Section 3.3). As all the Ψ pointers coming
m each run are increasing, a wavelet tree is used to describe how the σ ascending sequences of pointers coming from
ch run are intermingled in the range of S A . This turns out to be, precisely, the wavelet tree of S A . This is why both
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 11
Fi
se
va
y-
ar
y-
th
Fi
ea
Fe
th
su
of
otg. 8. Representing a grid with a wavelet tree, and a range search. (a) The grid and the range as the intersection of an x- and a y-range. On top, the
quence of y-coordinates sorted by x-coordinate. The final structure is just the wavelet tree of that sequence. (b) Partitioning into small and large y
lues, and the subsequences belonging to each partition. (c) First wavelet tree decomposition, at the root. The bitmap indicates to which partition each
coordinate belongs, in x-coordinate order. (d) The final wavelet tree, indicating in bold arrows the edges traversed to count the points in the range. Nodes
e followed as long as they contain some nonempty part of the x-range, they have nonempty intersection with the y-range, and are not contained in the
range. We highlight the maximal nodes covering the query y-range (round-corner hatched boxes, in bold those reached), and the query x-range inside
e sequence of each involved node (sharp-corner hatched boxes, in bold those traversed).
g. 9. The suffix array A and the Ψ array of our text. We have separated the areas where A starts with the same letter. Ψ is monotone increasing within
ch such area.
rragina et al. and Grossi et al. obtain basically the same space, nHk(S)+o(n lgσ) bits. Due to the different search strategy,
e counting time of Grossi et al. is higher. On the other hand, the representation of Ψ allows them to locate patterns in
blogarithmic time, still using O (nHk(S)) + o(n lgσ) bits.
This is the best known usage of wavelet trees as sequences, and it is well covered in earlier surveys [87]. New extensions
these basic concepts, supporting more sophisticated search problems, appear every year (e.g., [15,100]). We cover next
her completely different applications.
12 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
Fig
re
th
5.
lis
en
te
in
ar
is
an
ac
is
effi
ap
p
p′
um
pr
en
th
im
Th
te
su
5.
e
th
m
bi
O
of
th
ad
O
tw
of
in
to
co
Th
al. 10. Representing text ‘to be or not to be’ with an inverted list (left) or with a wavelet tree (right). Words are mapped to 1, 2, 3, 4 and the wavelet tree
presents the sequence of mapped words. Then we access any text word (e.g., the 4th text word is 4, or ‘not’), and successive select2 operations retrieve
e content of the inverted list of word 2, or ‘be’.
2. Positional inverted indexes
Consider a natural language text collection. A positional inverted index is a data structure that stores, for each word, the
t of the positions where it appears in the collection [4]. In compressed form [106] it takes space close to the zero-order
tropy of the text seen as a sequence of words [87]. This entropy yields very competitive compression in natural language
xts. Yet, we need to store both the text (usually zero-order compressed, so that direct access is possible) and the inverted
dex, adding up to at least 2nH0(S) bits, where S is the text regarded as a sequence of word identifiers. Inverted indexes
e by far the most popular data structures to index natural language text collections, so reducing their space requirements
of high relevance.
By representing the sequence of word identifiers using a wavelet tree, we obtain a single representation for both the text
d the inverted index, all within nH0(S)+o(n) bits [30]. In order to access any text word, we just compute S[i]. In order to
cess the i-th element of the inverted list of any word c, we compute selectc(S, i). Furthermore, operation rankc(S, i)
useful to implement some list intersection algorithms [9], as it finds the position i in the inverted list of word c more
ciently than with a binary or exponential search. Fig. 10 illustrates the basic idea.
Arroyuelo et al. [3] extended this functionality to document retrieval: retrieve the distinct documents where a word
pears. They use a special symbol “$” to mark document boundaries. Then, given the first occurrence of a word c,
= selectc(S,1), the document where this occurrence lies is j = rank$(S, p) + 1, document j ends at position
= select$(S, j), it contains o = rankc(S, p, p′) occurrences of the word c, and the search for further relevant doc-
ents can continue from query selectc(S,o + 1).
An improvement over the basic idea is to use multiary wavelet trees, more precisely of arity up to 256, and using the
operty that wavelet trees give direct access to any variable-length code. Brisaboa et al. [20] started with a byte-oriented
coding of the text words (using either Huffman with 256 target symbols, or other practical encoding methods [21]) and
en organized the sequence of codes into a wavelet tree, as described in Section 3.2. A naive byte-based rank and select
plementation on the wavelet tree levels gives good results in this application, with the bytes represented in plain form.
e resulting structure is indeed competitive with positional inverted indexes in many cases. A variant specialized on XML
xt collections, where the codes are also used to distinguish structural elements (tags, content, attributes, etc.) in order to
pport some XPath queries, is also being developed [19].
3. Graphs
Another simple application of this idea is the representation of directed graphs [30]. Let G be a graph with n nodes and
edges. An adjacency list, using n lg e + e lgn bits (the n pointers to the lists plus the e target nodes) gives direct access to
e neighbors of any node v . If we want also to perform reverse navigation, that is, to know which nodes point to v , we
ust spend other n lg e + e lgn bits to represent the transposed graph.
Once again, representing with a wavelet tree the sequence S[1, e] concatenating all the adjacency lists, plus a compressed
tmap B[1, e] marking the beginnings of the lists, gives access to both types of neighbors within space n lg(e/n) + e lgn +
(n) + o(e), which is close to the space of the plain representation (actually, possibly less). To retrieve the i-th neighbor
a node v , we compute the starting point of the list of v , l ← select1(B, v), and then access S[l + i − 1]. To retrieve
e i-th reverse neighbor of a node v , we compute p ← selectv(S, i) to find the i-th time that v is mentioned in an
jacency list, and then compute with rank1(B, p) the owner of the list where v is mentioned. Both operations take time
(lgn/ lg lgn). This is also useful to represent undirected graphs, where adjacency lists must usually represent each edge
ice. With a wavelet tree we can choose any direction for an edge, and at query time we join direct and reverse neighbors
nodes to build their list. Fig. 11 illustrates this arrangement.
Note, finally, that the wavelet tree can compress S to its zero-order entropy, which corresponds to the distribution of
-degrees of the nodes. A more sophisticated variant of this idea, combined with Re-Pair compression [73], was shown
be competitive with current Web graph compression methods [31]. A different arrangement [65] has been useful to
mpress social networks, by extracting dense subgraphs (Si,Ci) where all the nodes in Si point to all the nodes in Ci .
en by storing a sequence S1 : C1 : S2 : C2 : . . . of node identifiers, and a bitmap that marks their limits, one can reproduce
l the edges using access and select on the node sequence, and rank/select on the bitmap.
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 13
Fi
(s
(st
re
Fi
sto
6.
ap
6.
en
pe
th
as
an
th
po
po
th
va
Th
[1
sy
th
6.
m
la
re
w
thg. 11. Representing a graph (left) with a wavelet tree plus a bitmap (right). The direct neighbors of v3 are obtained by accessing part of the sequence
quare-corner rectangle). The reverse neighbors of v5 are found by successive select5 operations (curved arrows, on top) and rank on the bitmap
raight arrows, on the bottom). The presence of an edge from v1 to v2 is decided via rank2 in the area of the edges leaving v1 (rounded-corner
ctangle).
g. 12. Representing a permutation with runs (on the bottom, the runs are shown with rectangles). Only the tree shape and the bitmaps, in black, are
red.
Applications as reorderings
Apart from its first usage [57], that can be regarded as encoding a reordering, wavelet trees offer various interesting
plications when seen in this way.
1. Permutations
As explained in Section 4, one can easily encode a permutation with a wavelet tree. It is more interesting that the
coding can take less space when the permutation is, in a sense, compressible. Barbay and Navarro [10,11] considered
rmutations π of [1..n] that can be decomposed into ρ contiguous ascending runs, of lengths r1, r2, . . . , rρ . They define
e entropy of such a permutation as H(π) =∑ρi=1(ri/n) lg(n/ri), and show that it is possible to sort an array with such
cending runs in time O (n(H(π)+1)). This is obtained by building a Huffman tree on the run lengths (seen as frequencies)
d running a mergesort-like algorithm that follows the Huffman tree shape.
They note that, if we encode with 0 or 1 the results of the comparisons of the mergesort algorithm at each node of
e merging tree, the resulting structure contains at most n(H(π) + 1) bits, and it represents the permutation. Starting at
sition i in the top bitmap Bvroot one can track down the position exactly as done with wavelet trees, so as to arrive at
sition j of the t-th leaf (i.e., run). By storing, in O (ρ lgn) bits, the starting position of each run in π , we can convert
e leaf position into a position in π . Therefore the downward traversal solves operation π−1(i), because it starts from
lue i (i.e., position i after sorting π ), and gives the position in π from where it started before the merging took place.
e corresponding upward traversal, consequently, solves π(i). Other types of runs, more and less general, are also studied
0,11]. Fig. 12 illustrates how a permutation is encoded.
Some thought reveals that this structure is indeed the wavelet tree of a sequence formed by replacing, in π−1, each
mbol belonging to the i-th run, by the run identifier i. Then the fact that a downward traversal yields π−1(i) and that
e upward traversal yields π(i) are natural consequences. This relation is made more explicit in a later article [5,8].
2. Generic numeric sequences
There are several basic problems on sequences of numbers that can be solved in nontrivial ways using wavelet trees. We
ention a few that have received attention in the literature.
One such problem is the range quantile query: Preprocess a sequence of numbers S[1,n] on the domain [1..σ ] so that
ter, given a range [l, r] and a value i, we can compute the i-th smallest element in S[l, r].
Classical solutions to this problem have used nearly quadratic space and constant time. Only a very recent solution [69]
aches O (n lgn) bits of space (apart from storing S) and O (lgn/ lg lgn) time. We show that, by representing S with a
avelet tree, we can solve the problem in O (lgσ) time and just o(n) extra bits [48,49]. This is close to O (lgn/ lg lgn) (in
is problem, we can always make σ  n hold), and it can be even better if σ is small compared to n.
14 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
Fig
inc
ele
be
st
(s
w
in
so
sp
im
If
an
pr
so
ho
m
ca
th
co
ch
ra
an
rig
6.
In
in
as
w
al
of
m
B[
S
th. 13. Representing a non-positional inverted list for a collection of four documents. The L lists are sorted by decreasing frequency; the F lists by
reasing document identifier. The wavelet tree on the right represents both lists simultaneously. An L list is obtained by just accessing the sequence. An
ment of the F list (in this example, the 2nd element of the list Fma) by a range quantile query for the 2nd smallest value in the range. We first go left
cause there are 2 or more 0s in the range, then right because there are less than 2 0s.
Starting from the range S[l, r], we compute rank0(Bvroot , l, r). If this is i or more, then the i-th value in this range is
ored in the left subtree, so we go to the left child and remap the interval [l, r] as done for counting points in a range
ee Section 4). Otherwise we go right, subtracting rank0(Bvroot , l, r) from i and remapping [l, r] in the same way. When
e arrive at a leaf, its label is the i-th smallest element in S[l, r]. Fig. 13 illustrates this operation within an application to
verted lists.
Another fundamental problem is called range next value: Preprocess a sequence of numbers S[1,n] on the domain [1..σ ]
that later, given a range [l, r] and a value x, we return the smallest value in S[l, r] that is larger than x.
The state of the art also includes superlinear-space and constant-time solutions, as well as one using O (n lgn) bits of
ace and O (lgn/ lg lgn) time [107]. Once again, we achieve o(n) extra bits and O (lgσ) time using wavelet trees [48] (we
prove this time in Section 7.2).
Starting at the root from the range S[l, r], we see if value x labels a leaf descending from the left or from the right child.
x descends from the right child, then no value on the left child can be useful, so we recursively descend to the right child
d remap the interval [l, r] as done for counting points in a range. Else, there may be values > x on both children, but we
efer those on the left, if any. So we first descend to the left child looking for an answer (there may be no answer if, at
me node, the interval [l, r] becomes empty). If the left child returns an answer, this is what we seek and we return it. If,
wever, there is no value > x on the left child, we seek the smallest value on the right child. We then enter into another
ode where we see if there is any 0-bit in Bv [l, r]. If there is one, we go to the left child, else we go to the right child. It
n be shown that the overall process takes O (lgσ) time.
A variant of the range next value problem is called prevLess [72]: return the rightmost value in S[1, r] that is smaller
an x. Here we start with S[1, r]. If value x labels a leaf descending from the left, we map the interval to the left child and
ntinue recursively from there. If, instead, x descends from the right child, then the answer may be on the left or the right
ild, and we prefer the rightmost in [1, r]. Any 0-bit in Bv [1, r] is a value smaller than x and thus a valid answer. We use
nk and select to find the rightmost 0 in Bv [1, r]. We also continue recursively by the right child, and if it returns an
swer, we map it to the bitmap Bv [1, r]. Then we choose the rightmost between the answer from the right child and the
htmost zero. The overall time is O (lgσ).
3. Non-positional inverted indexes
These indexes store only the list of distinct documents where each word appears, and come in two flavors [4,106].
the first, the documents for each word are sorted by increasing identifier. This is useful to implement list unions and
tersections for boolean, phrase and proximity queries. In the second, a “weight” (measuring importance somehow) is
signed to each document where a word appears. The lists of each word store those weights and are sorted by decreasing
eight. This is useful to implement ranked bag-of-word queries, which give the documents with highest weights added over
l the query words. It would seem that, unless one stores two inverted indexes, one must choose one order in detriment
the queries of the other type.
By representing a reordering, wavelet trees can store both orderings simultaneously [48,92]. Let us represent the docu-
ents where each word appears in decreasing weight order, and concatenate all the lists into a sequence S[1,n]. A bitmap
1,n] marks the starting positions of the lists, and the weights are stored separately. Then, a wavelet tree representation of
simulates, within the space of just one list, both orderings. By accessing S[l + i − 1], where l = select1(B, c), we obtain
e i-th element of the inverted list of word c, in decreasing weight order. To access the i-th element of the inverted list of
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 15
Fi
D
a
qu
da
6.
in
do
so
be
C
oc
of
ti
D
co
d
ap
el
f1
co
ra
se
w
Fi
gi
th
w
do
w
[4
co
7.
7.
sog. 14. The suffix array S A of a collection, its document array D , and its wavelet tree representation on the bottom. A query for “la” yields an interval
[21,25], and the wavelet tree is used to spot all the different values of D in that interval.
word in increasing document order, we also compute the end of its list, r = select1(B, c + 1) − 1, and then run a range
antile query for the i-th smallest value in the range [l, r]. Fig. 13 illustrates this operation.
Many other operations of interest, like list intersections, can be carried out with this representation and little auxiliary
ta [48,92].
4. Document retrieval indexes
An interesting extension to full-text retrieval is document retrieval, where a collection S[1,n] of general strings (so
verted indexes cannot be used) is to be indexed to answer different document retrieval queries. The most basic one,
cument listing, is to output the distinct documents where a pattern P [1,m] appears. Muthukrishnan [85] defined a
-called document array D[1,n], where D[i] gives the document to which the i-th lexicographically smallest suffix of S
longs (i.e., where the suffix S[A[i],n] belongs, where A is the suffix array of S). He also defined an array C[1,n], where
[i] points to the previous occurrence of D[i] in D . A suffix tree was used to identify the range A[l, r] of the pattern
currences, so that we seek to report the distinct elements in D[l, r]. With further structures to find minima in ranges
C [16], Muthukrishnan gave an O (m + occ) algorithm to find the occ distinct documents where P appears. This is
me-optimal, yet the space is impractical.
Wavelet trees have also proved extremely useful here. Mäkinen and Välimäki [103] showed that, if one implemented
as a wavelet tree, then array C was not necessary, since C[i] = selectD[i](D,rankD[i](D, i − 1)). They also used a
mpressed full-text index [42] to identify the range D[l, r], so the total time turned out to be O (m lgσ + occ lgd), where
is the number of documents in S . Moreover, for each document c output, rankc(D, l, r) gave the number of times P
peared in c, which is important for ranked document retrieval.
Gagie et al. [48,49] showed that an application of range quantile queries enabled the wavelet tree to solve this problem
egantly and without any range minima structure: The first distinct document is the smallest value in D[l, r]. If it occurs
times, then the second distinct document is the (1 + f1)-th smallest value in D[l, r], and so on. They retained the
mplexities of Mäkinen and Välimäki, but the solution used less space and time in practice. Later [48] they replaced the
nge quantile queries by a depth-first traversal of the wavelet tree that reduced the time complexity, after the suffix array
arch, to O (occ lg(d/occ)). The technique is similar to the two-dimensional range searches: recursively enter into every
avelet tree branch where the mapped interval [l, r] is not empty, and report the leaves found, with frequency r − l + 1.
g. 14 illustrates this process.
This depth-first search method can easily be extended to support more complex queries, for example t-thresholded ones:
ven s patterns, we want the documents where at least t of the terms appear. We can first identify the s ranges in D and
en traverse the wavelet tree while maintaining the s ranges, stopping when less than t intervals are nonempty, or when
e arrive at leaves (where we report the document). Other sophisticated traversals have been proposed for retrieving the
cuments ranked by number of occurrences of the patterns [36].
An interesting problem is how to compress the wavelet tree of D effectively. The zero-order entropy of D has to do
ith document lengths, which is generally uninteresting, and unrelated to the compressibility of S . It has been shown
7,93] that the compressibility of S shows up as repetitions in D , which has stimulated the development of wavelet tree
mpression methods that take advantage of the repetitiveness of D , as described at the end of Section 3.
Applications as grids
1. Discrete grids
Much work has been done in Computational Geometry over structures very similar to wavelet trees. We only highlight
me results of interest, generally focusing on structures that use linear space. We assume here that we have an n × n grid
16 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
Fig
en
x-
a r
Fig
as
wi
w
be
O
Re
to
in
oc
a
re
so
la
[9
su
in
ex
of
on
gi
w
th
m
th. 15. An LZ78 parsing of the text "alabar a la alabarda$", into 11 phrases. To find pattern "bar" split over phrases, we look for a phrase
ding with "b" followed by another starting with "ar" (partition ("ba", "r") must also be tried). The grid connects those consecutive phrases: on the
coordinate, the reversed preceding phrases; on the y-coordinate, the following phrases. By sorting both sets in lexicographic order, the problem becomes
ange search, where our pattern is found in phrases 3 and 9.
. 16. A grid with weights (left) and its wavelet tree representation (right). The wavelet tree represents the points; the weights are represented separately
an array on top, sorted by x-coordinate. A range is decomposed into maximal wavelet tree nodes (shaded), and the position of the minimum weight
thin each range is obtained. Then these positions are tracked to the top sequence to find the actual weights and give the global minimum, 1.
ith n points not sharing rows nor columns. Interestingly, these grids with range counting and reporting operations have
en intensively used in compressed text indexing data structures [17,28,32,41,70,72,76,86]. Fig. 15 gives a typical example.
Range counting can be done in time O (lgn/ lg lgn) and O (n lgn) bits [68]. This time cannot be improved within space
(n lgO (1) n) [96], but it can be matched with a multiary wavelet-tree-like structure using just n lgn + o(n lgn) bits [17].
aching this time, instead of the easy O (lgn) time bound we have explained in Section 4, requires a sophisticated solution
the problem of doing the range counting among several consecutive children of a node, that are completely contained
the x-range of the query. They [17] also obtain a range reporting time (for the occ points in the range) of O ((1 +
c) lgn/ lg lgn). This is not surprising once counting has been solved: it is a matter of upward or downward tracking on
multiary wavelet tree. The technique for faster upward tracking we described in Section 2.4 can be used to improve the
porting time to O ((1+ occ) lg n), using O ((1/)n lgn) bits of space [26].
Wavelet trees offer relevant solutions to other geometric problems, such as finding the dominant points in a grid, or
lving visibility queries. Those problems can be recast as a sequence of queries of the form “find the smallest element
rger than x in a range”, described in Section 6.2, and therefore solved in time O (lgn) per point retrieved [90]. That paper
0] also studies extensions of geometric queries where the points have weights and statistical queries on them are posed,
ch as finding range sums, averages, minima, quantiles, majorities, and so on. The way those queries are solved open
teresting new avenues in the use of wavelet trees.
Some queries, such as finding the minimum value of a two-dimensional range, are solved by enriching wavelet trees with
tra information aligned to the bitmaps. Recall that each wavelet tree node v handles a subsequence Sv of the sequence
points S[1,n]. To each node v with bitmap Bv [1,nv ] we associate a data structure using 2nv + o(nv) bits that answers
e-dimensional range minimum queries [44] on Sv [1,nv ]. Once built, this structure does not need to access Sv , yet it
ves the position of the minimum in constant time. Since, as explained, a two-dimensional range is covered by O (lgn)
avelet tree nodes, only those O (lgn) minima must be tracked upwards, where the actual weights are stored, to obtain
e final result. Thus the query requires O (lg1+ n) time and O ((1/)n lgn) bits of space by using the fast upward tracking
echanism. Fig. 16 illustrates this idea.
Other queries, such as finding the i-th smallest value of a two-dimensional range, are handled with a wavelet tree on
e weight values. Each wavelet tree node stores a grid with the points whose weights are in the range handled by that
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 17
no
(i
O
m
in
sp
x-
Tw
th
is
te
Bo
re
co
lo
x-
th
ch
re
re
7.
re
la
w
a
vi
Fo
ex
re
gr
ra
tr
in
tr
no
pr
ra
O
Tw
ro
th
cl
bi
7.
a
[4
S
th
nde. Then, by doing range counting queries on those grids, one can descend left or right, looking for the rightmost leaf
.e., value) such that the counts of the children to the left of the path followed add up to less than i. The total time is
(lg2 n/ lg lgn), however the space becomes superlinear, O (n lg2 n) bits.
Finally, an interesting extension to the typical point grids are grids of rectangles, which are used in geographic infor-
ation systems as minimum bounding rectangles of complex objects. Then one wishes to find the set of rectangles that
tersect a query rectangle. This is well solved with an R-tree data structure [63], but a wavelet tree may offer interesting
ace reductions. Brisaboa et al. [22] describe a technique to store n rectangles where one does not contain another in the
coordinate range (so the set is first separated into maximal “x-independent” subsets and each subset is queried separately).
o arrays with the ascending lower and upper x-coordinates of the rectangles are stored (as the sets are x-independent,
e same position in both arrays corresponds to the same rectangle). A wavelet tree on those x-coordinate-sorted rectangles
set up, so that each node handles a range of y-coordinate values. This wavelet tree stores two bitmaps per node v: one
lls whether the rectangle Sv [i] extends to the y-range of the left child, and the other whether it extends to the right child.
th bitmaps can store a 1 at a position i, and thus the rectangle is stored in both subtrees. To avoid representing a large
ctangle more than O (lgn) times, both bits are set to 0 (which is otherwise impossible) when the rectangle completely
ntains the y-range of the current node. The total space is O (n lgn) bits.
Given a query [xmin, xmax] × [ymin, ymax], we look for xmin in the array of upper x-coordinates, to find position xl , and
ok for xmax in the array of lower x-coordinates, to find position xr . This is because a query intersects a rectangle on the
axis if the query does not start after the rectangle ends and the query does not end before the rectangle starts. Now
e range [xl, xr] is used to traverse the wavelet tree almost like on a typical range search, except that we map to the left
ild using rank1 on one bitmap, and to the right child using rank1 on the other bitmap. Furthermore, we report all the
ctangles where both bitmaps contain a 0-bit, and we remove duplicates by merging results at each node, as the same
ctangle can be encountered several times. The overall time to report the occ rectangles is still O ((1+ occ) lgn).
2. Binary relations
A binary relation R between two sets A and B can be thought of as a grid of size |A| × |B|, containing |R| points. Binary
lations are not only good abstractions of strings, permutations and our grids, which are particular cases, but also of a
rge number of more applied structures. For example, a non-positional inverted index is a binary relation between a set of
ords and a set of documents, so that a word is related to the documents where it appears. As another example, a graph is
binary relation between the set of nodes and itself.
The most typical operations on binary relations are determining the elements b ∈ B that are related to some a ∈ A and
ce versa, and determining whether a pair (a,b) ∈ A× B is related in R . However, more complex queries are also of interest.
r example, counting or retrieving the documents related to any term in a range enables on-the-fly stemming and query
pansion. Retrieving the terms associated to a document permits vocabulary analyses. Accessing the documents in a range
lated to a term enables searches local to subcollections. Range counting and reporting allows regarding graphs at different
anularities (e.g., a Web graph can be regarded as a graph of hosts, or of pages, on the fly).
Barbay et al. [6,7] studied a large number of complex queries for binary relations, including accessing the points in a
nge in various orders, as well as reporting rows or columns containing points in a range. They proposed two wavelet-
ee-like data structures for handling the operations. One is basically a wavelet tree of the set of points (plus a bitmap that
dicates when we move from one column to the next). It turns out that almost all the solutions described so far on wavelet
ees find application to solve some of the operations.
In the extended version [7] they use multiary wavelet trees to reduce the times of most of the operations. Several
ntrivial structures and algorithms are designed in order to divide the times of various operations by lg lgn (the only
ecedent we know of is that of counting the number of points in a range [17]). For example, it is shown how to solve the
nge next value problem (recall Section 6.2) in time O (lgn/ lg lgn). Others, like the range quantile query, stay no better than
(lgn).
Barbay et al. also propose a second data structure that is analogous to the one described for rectangles in Section 7.1.
o bitmaps are stored per node, indicating whether a given column has points in the first and in the second range of
ws. This extension of a wavelet tree is less powerful than the previous structure, but it is shown that its space is close to
e entropy of the binary relation: (1+ √2)H + O (|A| + |B| + |R|) bits, where H = lg( |A|·|B||R|
)
. This is not achieved with the
assical wavelet tree. A separate work [37] builds on this to obtain a fully compressed grid representation, within H +o(H)
ts.
3. Colored range queries
A problem of interest in areas like query log and web mining is to count the different colors in a sequence S[1,n] over
universe of σ colors. Inspired in the idea of Muthukrishnan [85] for document retrieval (recall Section 6.4), Gagie et al.
7] showed that this is a matter of counting how many values smaller than l are there in C[l, r], where C[i] = max{ j < i,
[ j] = S[i]}. This is a range counting query for [l, r] × [1, l−1] on C seen as a grid, that can be solved in time O (lgn) using
e wavelet tree of C . Note that this wavelet tree, unlike that of S , uses n lgn + o(n) bits. Gagie et al. compressed it to
lgσ + O (n lg lgn) bits, by taking advantage of the particular structure of C , which shows up in the bit-vectors. Gagie and
18 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
Kä
th
8.
th
in
O
de
un
bi
un
w
Th
ca
ca
gr
Ω
of
9.
of
w
un
of
fo
co
pr
th
lo
st
so
ha
qu
(s
to
53
Ac
re
Re
4rkkäinen [45] then reduced the space to nH0(S) + o(nH0(S)) + O (n) with more advanced techniques, and also reduced
e query time to O (lg(r − l + 1)).
Dynamism
We have described the wavelet tree as a static data structure. However, if the bitmaps or sequences stored at
e nodes support insertions and deletions (“indels”) in time indel(n), then a wavelet tree of height h also supports
dels in the sequence S[1,n] it represents, in time O (h · indel(n)). This has been used to support indels in time
((1+ lgσ/ lg lgn) lgn/ lg lgn) [64,94].
The alphabet, however, is still fixed in those solutions. While such a limitation may seem natural for sequences, it looks
finitely artificial when representing grids: one can insert and delete new x-coordinates and points, but the y-coordinate
iverse cannot change. Creating or removing alphabet symbols requires changing the shape of the wavelet tree, and the
tmaps or sequences stored at the nodes undergo extensive modifications upon small tree shape changes (e.g., when
dergoing rotations as in AVL or Red–Black trees).
Very recently [89], compressed dynamic wavelet tree representations have been achieved, reaching O (logn/ log logn)
orst-case time for the operations rank, select, and access, and the same time complexity, yet amortized, for indels.
is time is optimal even for binary alphabets [25]. In addition, the space used is nH0(S)+o(n(1+H0(S))) and the alphabet
n be dynamic. This does not yet close the problem, because apart from the amortized update time,4 the representation
nnot emulate any wavelet tree operation, but just those mostly associated to sequences (Section 5). Those related to
ids, for example (Section 7), cannot be emulated (this is hardly achievable with this solution, given lower bounds like
((logn/ log logn)2) for dynamic range counting [96]).
A recent wavelet-tree-inspired scheme for storing sequences of strings has been proposed [59]. It is related with the idea
Section 3.2, as they use a wavelet tree with the shape of the Patricia tree of the strings stored.
Conclusions and further challenges
We have described the wavelet tree, a surprisingly versatile data structure that offers nontrivial solutions to a wide range
problems in areas like string processing, computational geometry, and many more. An important additional asset of the
avelet tree is its simplicity to understand, teach, and program. This makes it a good data structure to be introduced at an
dergraduate level, at least in its more basic variants. In many cases, solutions with better time complexity than the ones
fered by wavelet trees are not so practical nor easy to implement.
Wavelet trees seem to be unable to reach access and rank/select times of the form O (lg lgσ), as other structures
r representing sequences do [54], close to the lower bounds [13]. However, both have been combined to offer those time
mplexities and good zero-order compression of data and redundancy [5,8,13]. Yet, the lower bounds on some geometric
oblems [26], matched with current wavelet trees [7,17], suggest that this combination cannot be carried out much further
an those three operations. Still, there are some complex operations where it is not clear that wavelet trees have matched
wer bounds [48].
A path that, in our opinion, has only started to be exploited, is to enhance the wavelet tree with “one-dimensional” data
ructures at its nodes v , so that, by efficiently solving some kind of query over the corresponding subsequences Sv , we
lve a more complex query on the original sequence S . In most cases along this survey, these one-dimensional queries
ve been rank and select on the bitmaps, but we have already shown some examples involving more complicated
eries [47,90]. This approach may prove to be very fruitful.
In terms of practice, although there are many successful and publicly available implementations of wavelet tree variants
ee, e.g., libcds.recoded.cl and github.com/simongog/sdsl), there are some challenges ahead, such as carrying
practice the theoretical results that promise fast and small multiary wavelet trees [18,43,53] and lower redundancies [52,
,97].
knowledgements
Thanks to Jérémy Barbay, Ana Cerdeira, Travis Gagie, Juha Kärkkäinen, Susana Ladra, Simon Puglisi, the anonymous
viewers, and all those who took the time to read early versions of this manuscript.
ferences
[1] A. Akansu, R. Haddad, Multiresolution Signal Decomposition: Transforms, Subbands, and Wavelets, Academic Press, 1992.
[2] A. Apostolico, The myriad virtues of subword trees, in: Combinatorial Algorithms on Words, in: NATO ISI Ser., Springer-Verlag, 1985, pp. 85–96.
[3] D. Arroyuelo, S. González, M. Oyarzún, Compressed self-indices supporting conjunctive queries on document collections, in: Proc. 17th SPIRE, 2010,
pp. 43–54.
[4] R. Baeza-Yates, B. Ribeiro-Neto, Modern Information Retrieval, 2nd edition, Addison–Wesley, 2011.
This was recently improved to worst-case for all the operations, but now rank and updates cost time O (logn) [88].
G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20 19
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[[5] J. Barbay, F. Claude, T. Gagie, G. Navarro, Y. Nekrich, Efficient fully-compressed sequence representations, Algorithmica (2013), http://dx.doi.org/
10.1007/s00453-012-9726-3, in press, arXiv:0911.4981v4.
[6] J. Barbay, F. Claude, G. Navarro, Compact rich-functional binary relation representations, in: Proc. 9th LATIN, in: LNCS, vol. 6034, 2010, pp. 170–183.
[7] J. Barbay, F. Claude, G. Navarro, Compact binary relation representations with rich functionality, Inf. Comput. (2013), in press, arXiv:1201.3602.
[8] J. Barbay, T. Gagie, G. Navarro, Y. Nekrich, Alphabet partitioning for compressed rank/select and applications, in: Proc. 21st ISAAC, in: LNCS, vol. 6507,
2010, pp. 315–326 (part II).
[9] J. Barbay, A. López-Ortiz, T. Lu, A. Salinger, An experimental investigation of set intersection algorithms for text searching, ACM J. Exp. Algorithmics
14 (2009), article 7.
10] J. Barbay, G. Navarro, Compressed representations of permutations, and applications, in: Proc. 26th STACS, 2009, pp. 111–122.
11] J. Barbay, G. Navarro, On compressing permutations and adaptive sorting, arXiv:1108.4408.
12] D. Belazzougui, G. Navarro, Alphabet-independent compressed text indexing, in: Proc. 19th ESA, in: LNCS, vol. 6942, 2011, pp. 748–759.
13] D. Belazzougui, G. Navarro, New lower and upper bounds for representing sequences, in: Proc. 20th ESA, in: LNCS, vol. 7501, 2012, pp. 181–192.
14] T. Bell, J. Cleary, I. Witten, Text Compression, Prentice Hall, 1990.
15] T. Beller, S. Gog, E. Ohlebusch, T. Schnattinger, Computing the longest common prefix array based on the Burrows–Wheeler transform, in: Proc. 18th
SPIRE, 2011, pp. 197–208.
16] M. Bender, M. Farach-Colton, The LCA problem revisited, in: Proc. 4th LATIN, 2000, pp. 88–94.
17] P. Bose, M. He, A. Maheshwari, P. Morin, Succinct orthogonal range search structures on a grid with applications to text indexing, in: Proc. 11th
WADS, 2009, pp. 98–109.
18] A. Bowe, Multiary wavelet trees in practice, Honours thesis, RMIT University, Australia, 2010.
19] N. Brisaboa, A. Cerdeira, G. Navarro, A compressed self-indexed representation of XML documents, in: Proc. 13th ECDL, in: LNCS, vol. 5714, 2009,
pp. 273–284.
20] N. Brisaboa, A. Fariña, S. Ladra, G. Navarro, Reorganizing compressed text, in: Proc. 31st SIGIR, 2008, pp. 139–146.
21] N. Brisaboa, A. Fariña, G. Navarro, J. Paramá, Lightweight natural language text compression, Inf. Retr. 10 (2007) 1–33.
22] N. Brisaboa, M. Luaces, G. Navarro, D. Seco, A fun application of compact data structures to indexing geographic data, in: Proc. 5th FUN, in: LNCS,
vol. 6099, 2010, pp. 77–88.
23] M. Burrows, D. Wheeler, A block sorting lossless data compression algorithm, Tech. Rep. 124, Digital Equipment Corporation, 1994.
24] C. Burrus, R. Gopinath, H. Guo, Introduction to Wavelets and Wavelet Transforms: A Primer, Prentice–Hall, 1997.
25] H.-L. Chan, W.-K. Hon, T.-W. Lam, K. Sadakane, Compressed indexes for dynamic text collections, ACM Trans. Algorithms 3 (2) (2007), article 21.
26] T. Chan, K.G. Larsen, M. Pa˘tras¸cu, Orthogonal range searching on the RAM, revisited, in: Proc. 27th SoCG, 2011, pp. 1–10.
27] B. Chazelle, A functional approach to data structures and its use in multidimensional searching, SIAM J. Comput. 17 (3) (1988) 427–462.
28] Y.-F. Chien, W.-K. Hon, R. Shah, J. Vitter, Geometric Burrows–Wheeler transform: Linking range searching and text indexing, in: Proc. 18th DCC, 2008,
pp. 252–261.
29] D. Clark, Compact pat trees, PhD thesis, University of Waterloo, Canada, 1996.
30] F. Claude, G. Navarro, Practical rank/select queries over arbitrary sequences, in: Proc. 15th SPIRE, in: LNCS, vol. 5280, 2008, pp. 176–187.
31] F. Claude, G. Navarro, Extended compact Web graph representations, in: Algorithms and Applications (Ukkonen Festschrift), in: LNCS, vol. 6060, 2010,
pp. 77–91.
32] F. Claude, G. Navarro, Self-indexed grammar-based compression, Fundam. Inform. 111 (3) (2010) 313–337.
33] F. Claude, G. Navarro, The wavelet matrix, in: Proc. 19th SPIRE, in: LNCS, vol. 7608, 2012, pp. 167–179.
34] F. Claude, P. Nicholson, D. Seco, Space efficient wavelet tree construction, in: Proc. 18th SPIRE, 2011, pp. 185–196.
35] T. Cover, J. Thomas, Elements of Information Theory, Wiley, 1991.
36] J.S. Culpepper, G. Navarro, S.J. Puglisi, A. Turpin, Top-k ranked document search in general text databases, in: Proc. 18th ESA, in: LNCS, vol. 6347,
2010, pp. 194–205 (part II).
37] A. Farzan, T. Gagie, G. Navarro, Entropy-bounded representation of point grids, in: Proc. 21st ISAAC, in: LNCS, vol. 6507, 2010, pp. 327–338 (part II).
38] P. Ferragina, R. Giancarlo, G. Manzini, The myriad virtues of wavelet trees, Inf. Comput. 207 (8) (2009) 849–866.
39] P. Ferragina, R. Giancarlo, G. Manzini, M. Sciortino, Boosting textual compression in optimal linear time, J. ACM 52 (4) (2005) 688–713.
40] P. Ferragina, G. Manzini, Opportunistic data structures with applications, in: Proc. 41st FOCS, 2000, pp. 390–398.
41] P. Ferragina, G. Manzini, Indexing compressed texts, J. ACM 52 (4) (2005) 552–581.
42] P. Ferragina, G. Manzini, V. Mäkinen, G. Navarro, An alphabet-friendly FM-index, in: Proc. 11th SPIRE, in: LNCS, vol. 3246, 2004, pp. 150–160.
43] P. Ferragina, G. Manzini, V. Mäkinen, G. Navarro, Compressed representations of sequences and full-text indexes, ACM Trans. Algorithms 3 (2) (2007),
article 20.
44] J. Fischer, Optimal succinctness for range minimum queries, in: Proc. 9th LATIN, 2010, pp. 158–169.
45] T. Gagie, J. Kärkkäinen, Counting colours in compressed strings, in: Proc. 22nd CPM, 2011, pp. 197–207.
46] T. Gagie, G. Navarro, Y. Nekrich, Fast and compact prefix codes, in: Proc. 36th SOFSEM, in: LNCS, vol. 5901, 2010, pp. 419–427.
47] T. Gagie, G. Navarro, S.J. Puglisi, Colored range queries and document retrieval, in: Proc. 17th SPIRE, in: LNCS, vol. 6393, 2010, pp. 67–81.
48] T. Gagie, G. Navarro, S.J. Puglisi, New algorithms on wavelet trees and applications to information retrieval, Theor. Comput. Sci. 426–427 (2012) 25–41.
49] T. Gagie, S.J. Puglisi, A. Turpin, Range quantile queries: Another virtue of wavelet trees, in: Proc. 16th SPIRE, 2009, pp. 1–6.
50] S. Gog, Compressed suffix trees: Design, construction, and applications, PhD thesis, University of Ulm, Germany, 2011.
51] A. Golynski, Optimal lower bounds for rank and select indexes, in: Proc. 33th ICALP, in: LNCS, vol. 4051, 2006, pp. 370–381.
52] A. Golynski, Optimal lower bounds for rank and select indexes, Theor. Comput. Sci. 387 (3) (2007) 348–359.
53] A. Golynski, R. Grossi, A. Gupta, R. Raman, S.S. Rao, On the size of succinct indices, in: Proc. 15th ESA, in: LNCS, vol. 4698, 2007, pp. 371–382.
54] A. Golynski, J.I. Munro, S.S. Rao, Rank/select operations on large alphabets: a tool for text indexing, in: Proc. 17th SODA, 2006, pp. 368–373.
55] G. Gonnet, R. Baeza-Yates, T. Snider, New indices for text: Pat trees and Pat arrays, in: Information Retrieval: Data Structures and Algorithms, Prentice–
Hall, 1992, pp. 66–82.
56] R. González, G. Navarro, Compressed text indexes with fast locate, in: Proc. 18th CPM, in: LNCS, vol. 4580, 2007, pp. 216–227.
57] R. Grossi, A. Gupta, J. Vitter, High-order entropy-compressed text indexes, in: Proc. 14th SODA, 2003, pp. 841–850.
58] R. Grossi, A. Gupta, J. Vitter, When indexing equals compression: Experiments with compressing suffix arrays and applications, in: Proc. 15th SODA,
2004, pp. 636–645.
59] R. Grossi, G. Ottaviano, The wavelet trie: Maintaining an indexed sequence of strings in compressed space, in: Proc. 31st PODS, 2012, pp. 203–214.
60] R. Grossi, J. Vitter, Compressed suffix arrays and suffix trees with applications to text indexing and string matching, in: Proc. 32nd STOC, 2000,
pp. 397–406.
61] R. Grossi, J. Vitter, Compressed suffix arrays and suffix trees with applications to text indexing and string matching, SIAM J. Comput. 35 (2) (2006)
378–407.
62] R. Grossi, J. Vitter, B. Xu, Wavelet trees: From theory to practice, in: Proc. 1st CCP, 2011, pp. 210–221.
63] A. Guttman, R-trees: A dynamic index structure for spatial searching, in: Proc. 10th SIGMOD, 1984, pp. 47–57.
20 G. Navarro / Journal of Discrete Algorithms 25 (2014) 2–20
[64] M. He, I. Munro, Succinct representations of dynamic strings, in: Proc. 17th SPIRE, 2010, pp. 334–346.
[65] C. Hernández, G. Navarro, Compressed representation of web and social networks via dense subgraphs, in: Proc. 19th SPIRE, in: LNCS, vol. 7608, 2012,
pp. 264–276.
[66] D. Huffman, A method for the construction of minimum-redundancy codes, Proc. IRE 40 (9) (1952) 1090–1101.
[67] G. Jacobson, Space-efficient static trees and graphs, in: Proc. 30th FOCS, 1989, pp. 549–554.
[68] J. JáJá, C. Mortensen, Q. Shi, Space-efficient and fast algorithms for multidimensional dominance reporting and counting, in: Proc. 15th ISAAC, 2004,
pp. 558–568.
[69] A.G. Jørgensen, K.D. Larsen, Range selection and median: Tight cell probe lower bounds and adaptive data structures, in: Proc. 22nd SODA, 2011,
pp. 805–813.
[70] J. Kärkkäinen, Repetition-based text indexing, PhD thesis, University of Helsinki, Finland, 1999.
[71] J. Kärkkäinen, S.J. Puglisi, Fixed block compression boosting in FM-indexes, in: Proc. 18th SPIRE, 2011, pp. 174–184.
[72] S. Kreft, G. Navarro, Self-indexing based on LZ77, in: Proc. 22nd CPM, in: LNCS, vol. 6661, 2011, pp. 41–54.
[73] J. Larsson, A. Moffat, Off-line dictionary-based compression, Proc. IEEE 88 (11) (2000) 1722–1732.
[74] V. Mäkinen, G. Navarro, New search algorithms and time/space tradeoffs for succinct suffix arrays, Tech. Rep. C-2004-20, University of Helsinki,
Finland, April 2004.
[75] V. Mäkinen, G. Navarro, Succinct suffix arrays based on run-length encoding, Nord. J. Comput. 12 (1) (2005) 40–66.
[76] V. Mäkinen, G. Navarro, Position-restricted substring searching, in: Proc. 7th LATIN, in: LNCS, vol. 3887, 2006, pp. 703–714.
[77] V. Mäkinen, G. Navarro, Rank and select revisited and extended, Theor. Comput. Sci. 387 (3) (2007) 332–347.
[78] V. Mäkinen, G. Navarro, Implicit compression boosting with applications to self-indexing, in: Proc. 14th SPIRE, in: LNCS, vol. 4726, 2007, pp. 214–226.
[79] V. Mäkinen, G. Navarro, J. Sirén, N. Välimäki, Storage and retrieval of highly repetitive sequence collections, J. Comput. Biol. 17 (3) (2010) 281–308.
[80] C. Makris, Wavelet trees: a survey, Comput. Sci. Inf. Syst. 9 (2) (2012) 585–625.
[81] U. Manber, G. Myers, Suffix arrays: a new method for on-line string searches, SIAM J. Comput. 22 (5) (1993) 935–948.
[82] G. Manzini, An analysis of the Burrows–Wheeler transform, J. ACM 48 (3) (2001) 407–430.
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[
[1
[1
[1
[1
[1
[1
[1
[183] E. McCreight, A space-economical suffix tree construction algorithm, J. ACM 23 (2) (1976) 262–272.
84] I. Munro, Tables, in: Proc. 16th FSTTCS, in: LNCS, vol. 1180, 1996, pp. 37–42.
85] S. Muthukrishnan, Efficient algorithms for document retrieval problems, in: Proc. 13th SODA, 2002, pp. 657–666.
86] G. Navarro, Indexing text using the Ziv–Lempel trie, J. Discrete Algorithms 2 (1) (2004) 87–114.
87] G. Navarro, V. Mäkinen, Compressed full-text indexes, ACM Comput. Surv. 39 (1) (2007), article 2.
88] G. Navarro, Y. Nekrich, Optimal dynamic sequence representations, arXiv:1206.6982v2.
89] G. Navarro, Y. Nekrich, Optimal dynamic sequence representations, in: Proc. 24th SODA, 2013, pp. 865–876.
90] G. Navarro, Y. Nekrich, L. Russo, Space-efficient data-analysis queries on grids, Theor. Comput. Sci. 482 (2013) 60–72.
91] G. Navarro, E. Providel, Fast, small, simple rank/select on bitmaps, in: Proc. 11th SEA, in: LNCS, vol. 7276, 2012, pp. 295–306.
92] G. Navarro, S.J. Puglisi, Dual-sorted inverted lists, in: Proc. 17th SPIRE, in: LNCS, vol. 6393, 2010, pp. 310–322.
93] G. Navarro, S.J. Puglisi, D. Valenzuela, Practical compressed document retrieval, in: Proc. 10th SEA, in: LNCS, vol. 6630, 2011, pp. 193–205.
94] G. Navarro, K. Sadakane, Fully-functional static and dynamic succinct trees, ACM Trans. Algorithms (2013), in press, arXiv:0905.0768v5.
95] D. Okanohara, K. Sadakane, Practical entropy-compressed rank/select dictionary, in: Proc. 9th ALENEX, 2007, pp. 60–70.
96] M. Pa˘tras¸cu, Lower bounds for 2-dimensional range counting, in: Proc. 39th STOC, 2007, pp. 40–46.
97] M. Pa˘tras¸cu, Succincter, in: Proc. 49th FOCS, 2008, pp. 305–313.
98] M. Pa˘tras¸cu, E. Viola, Cell-probe lower bounds for succinct partial sums, in: Proc. 21st SODA, 2010, pp. 117–122.
99] R. Raman, V. Raman, S. Rao, Succinct indexable dictionaries with applications to encoding k-ary trees and multisets, in: Proc. 13th SODA, 2002,
pp. 233–242.
00] T. Schnattinger, E. Ohlebusch, S. Gog, Bidirectional search in a string with wavelet trees, in: Proc. 21st CPM, 2010, pp. 40–50.
01] J. Sirén, N. Välimäki, V. Mäkinen, G. Navarro, Run-length compressed indexes are superior for highly repetitive sequence collections, in: Proc. 15th
SPIRE, in: LNCS, vol. 5280, 2008, pp. 164–175.
02] G. Tischler, On wavelet tree construction, in: Proc. 22nd CPM, 2011, pp. 208–218.
03] N. Välimäki, V. Mäkinen, Space-efficient algorithms for document retrieval, in: Proc. 18th CPM, 2007, pp. 205–215.
04] J. Walker, A Primer on Wavelets and Their Scientific Applications, 2nd edition, Chapman and Hall/CRC, 2008.
05] P. Weiner, Linear pattern matching algorithm, in: Proc. 14th Annual IEEE Symposium on Switching and Automata Theory, 1973, pp. 1–11.
06] I. Witten, A. Moffat, T. Bell, Managing Gigabytes, 2nd edition, Morgan Kaufmann, 1999.
07] C.-C. Yu, W.-K. Hon, B.-F. Wang, Efficient data structures for the orthogonal range successor problem, in: Proc. 15th COCOON, 2009, pp. 96–105.

Operation-Centric Hardware Description and Synthesis
by
James C. Hoe
B.S., University of California at Berkeley (1992) M.S., Massachusetts Institute of Technology (1994)
Submitted to the Department of Electrical Engineering and Computer Science
in partial ful llment of the requirements for the degree of Doctor of Philosophy in Electrical Engineering and Computer Science
at the MASSACHUSETTS INSTITUTE OF TECHNOLOGY
June 2000 c Massachusetts Institute of Technology 2000. All rights reserved.
Author : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : Department of Electrical Engineering and Computer Science April 28, 2000
Certi ed by : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : Arvind
Johnson Professor of Computer Science and Engineering Thesis Supervisor
Accepted by : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : Arthur C. Smith
Chairman, Department Committee on Graduate Students

2

Operation-Centric Hardware Description and Synthesis
by James C. Hoe
Submitted to the Department of Electrical Engineering and Computer Science
on April 28, 2000, in partial ful llment of the requirements for the degree of
Doctor of Philosophy in Electrical Engineering and Computer Science
Abstract
In an operation-centric framework, the behavior of a system is decomposed and described as a collection of operations. An operation is de ned by a predicate condition and an e ect on the system's state. An execution of the system corresponds to some sequential interleaving of the operations such that each operation in the sequence produces a state that enables the next operation. An operation's e ect on the system is global and atomic. In other words, an operation \reads" the state of the system in one step, and, if enabled, the operation updates the state in the same step. This atomic semantics simpli es the task of hardware description by permitting the designer to formulate each operation as if the system were otherwise static.
This thesis develops a method for hardware synthesis from an operation-centric description. The crux of the synthesis problem is in nding a synchronous state transition system that carries out multiple parallelizable operations per clock cycle and yet maintains a behavior that is consistent with the atomic and sequential semantics of the operations. The thesis rst de nes an Abstract Transition System (ATS), an operation-centric state machine abstraction. The thesis next develops the theories and algorithms to synthesize an e cient synchronous digital circuit implementation of an
ATS. Finally, the thesis presents TRSpec, a source-level operation-centric hardware
description language based on the formalism of Term Rewriting Systems. The results of this thesis show that an operation-centric framework o ers a sig-
ni cant improvement over traditional hardware design ows. The TRSpec language
and synthesis algorithms developed in this thesis have been realized in the Term Rewriting Architectural Compiler (TRAC). This thesis presents the results of several
operation-centric design exercises using TRSpec and TRAC. In an example based
on a 32-bit MIPS integer core, the operation-centric description can be developed ve times faster than a hand-coded structural Verilog description. Nevertheless, the circuit implementation produced by the operation-centric framework is comparable to the hand-coded design in terms of speed and area. Thesis Supervisor: Arvind Title: Johnson Professor of Computer Science and Engineering
3

4

Acknowledgments
First and foremost, I would like to thank Professor Arvind for supervising this research. His insight and criticism added greatly to this thesis. Furthermore, his guidance and support have been invaluable to my academic development over the last eight years. I wish him the best of luck in the Sandburst venture.
I want to thank Professor Srinivas Devadas and Professor Martin Rinard for generously spending their time to advise me on this thesis. They have been both my mentors and friends.
My fellow data owers from the Computation Structures Group (CSG) have helped me in so many ways. I want to thank Dr. Larry Rudolph who has always been a source of good ideas. I will always treasure the company of Shail Aditya, Andy Shaw, Boon Ang, Alejandro Caro, Derek Chiou, Xiaowei Shen, Jan Maessen, Mike Ehrlich and Daniel Rosenband,1 my long-time fellow CSG students. Daniel Rosenband provided the hand-written Verilog descriptions in this thesis and generated many of the synthesis results. Mieszko Lis reviewed many drafts of this thesis. I also want to thank Dr. Andy Boughton for looking after everyone in the group. CSG and everyone associated with it have been a big part of my life for nearly a decade. I hope that, one day, I will nally come to know what exactly is a \computation structure".
I want to thank the Intel Corporation and the Intel Foundation for supporting the research in this thesis by providing research grants and a fellowship. I also appreciate the many useful exchanges with Dr. Timothy Kam and other members of Intel's Strategic CAD Laboratories.
I owe much to my family for a ording me the luxury of \just hanging around" as a graduate student for so many years. As always, I am indebted to my parents for their love, support, and encouragement through the years. I am grateful for their hard work and sacri ces that have allowed me to be who I am today. Last but not the least, I must thank my in nitely patient wife for her love and support during my studies at MIT. She encourages me when I am down, understands me when I am upset, and tolerates all of my many quirks. As my most loyal reader, she has tirelessly proofread every document I have produced at MIT.
I would like to nish by telling a short story with me in it. It was the summer of 1987 in Pasadena, California. I was attending an informal lecture given by Richard Feynman to a class of SSSSP2 high school students. Near the end of the lecture, he challenged us with a puzzle to nd the aw in an odd but apparently sound explanation for universal gravitation. As soon as the lecture ended, I charged to the front of the room to check my answer with him. Instead of agreeing with me, he gave me a knowing smirk and put both of his hands on my head. To my surprise, he then grabbed my head, as one would with a basketball, and proceeded to shake
it in comic exaggeration. Dumbfounded, I heard him say in a playful voice, \Don't just ask me. Use your head to think about it. That is what it's there for!"
At that instant, I realized the point in learning is not so one can come up with the
1Names appear in the chronological order of their realized or pending graduation dates. 2Caltech's Summer Secondary School Science Project
5

same answers as the answer key on a nal exam. What could be the point in solving a cooked up problem that has been solved hundreds of times by hundreds of people? I acquired a whole new mindset where the end result of learning is not rote knowledge but the ability to solve real-world problems that have not been conveniently prepared from their right answers. For me, nding answers to open questions is not only the true point in learning, but it is also where the true act of learning begins. Finding things out is indeed pleasurable Fey99]. Incidentally, for those of you who are wondering, I did come up with the right answer on that day.
6

In memory of my grandfather, Hoe Kwan-Wu, M.D., 1900 1996
7

8

Contents

1 Introduction
1.1 Operation-Centric Hardware Description . . . . . . . . . . . . . . . . 1.2 Limitations of Operation-Centric Frameworks . . . . . . . . . . . . . 1.3 Comparison to CFSM-based Frameworks . . . . . . . . . . . . . . . . 1.4 Comparison to Other High-Level Frameworks . . . . . . . . . . . . . 1.5 Thesis Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6 Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15
15 16 17 17 19 20

2 Operation-Centric Design Examples
2.1 Euclid's Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 A Simple Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 A Pipelined Processor . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Another Pipelined Processor . . . . . . . . . . . . . . . . . . . . . . .

21
21 24 25 28

3 Hardware Synthesis and Scheduling

3.1 Abstract Transition Systems . . . . . . . . . . . . . . . . . . . . . . .

3.1.1 State Elements . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.2 State Transitions . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.3 Operational Semantics . . . . . . . . . . . . . . . . . . . . . .

3.1.4 Functional Interpretation . . . . . . . . . . . . . . . . . . . . .

3.2 Reference Implementation of an ATS . . . . . . . . . . . . . . . . . .

3.2.1 State Storage . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.2 State Transition Logic . . . . . . . . . . . . . . . . . . . . . .

3.2.3 RTL Description . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.4 Correctness of a Synchronous Implementation . . . . . . . . .

3.2.5 Performance Considerations . . . . . . . . . . . . . . . . . . .

3.3 Optimization I: Parallel Compositions . . . . . . . . . . . . . . . . . .

3.3.1 Con ict-Free Transitions . . . . . . . . . . . . . . . . . . . . .

3.3.2 3.3.3

Static Deduction of
Scheduling of <>CF

<>CF . . .
Transitions

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

3.3.4 Performance Gain . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Optimization II: Sequential Compositions . . . . . . . . . . . . . . . .

3.4.1 Sequentially-Composible Transitions . . . . . . . . . . . . . .

3.4.2 3.4.3

SStcahteidcuDlinedguocfti<onSCofT<raSnCsit.io.ns.

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

31
31 31 33 33 33 36 36 37 39 42 42 43 43 46 48 50 51 51 54 54

9

3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

4 TRSpec Hardware Description Language
4.1 Term Rewriting Systems . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 TRS for Hardware Description . . . . . . . . . . . . . . . . . . . . . . 4.3 Type System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.1 Simple Types . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.2 Abstract Types . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Rewrite Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Abstract Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.2 Type Restrictions . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.3 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Source Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Input and Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.7 Mapping TRSpec to ATS . . . . . . . . . . . . . . . . . . . . . . . .
4.7.1 Terms to ATS State Elements . . . . . . . . . . . . . . . . . . 4.7.2 Rules to ATS Transitions . . . . . . . . . . . . . . . . . . . . . 4.7.3 Local Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

59
59 60 61 61 62 64 64 64 65 66 66 68 69 70 71 71

5 Examples of TRSpec Descriptions and Synthesis
5.1 Euclid's Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Design Capture . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.3 Synthesis Results . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 A Simple Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Adaptation for Synthesis . . . . . . . . . . . . . . . . . . . . . 5.2.2 Synthesis Results . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 MIPS R2000 Processor . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.1 MIPS Integer Subset . . . . . . . . . . . . . . . . . . . . . . . 5.3.2 Microarchitecture . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.3 Synthesis Results . . . . . . . . . . . . . . . . . . . . . . . . .
5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73
73 73 74 75 78 78 82 83 87 87 88 89

6 Microprocessor Design Exploration

91

6.1 Design Flow Overview . . . . . . . . . . . . . . . . . . . . . . . . . . 91

6.2 Step 1: ISA Speci cation . . . . . . . . . . . . . . . . . . . . . . . . . 92

6.3 Step 2: Pipelining Transformation . . . . . . . . . . . . . . . . . . . . 92

6.4 Step 3: Superscalar Transformation . . . . . . . . . . . . . . . . . . . 95

6.4.1 Derivation of Composite Rules . . . . . . . . . . . . . . . . . . 96

6.4.2 A Composite Rule Example . . . . . . . . . . . . . . . . . . . 98

6.4.3 Derivation of a Two-Way Superscalar Processor . . . . . . . . 99

6.5 Synthesis and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 102

6.5.1 TRSpec to RTL . . . . . . . . . . . . . . . . . . . . . . . . . 102

6.5.2 GTECH RTL Analysis . . . . . . . . . . . . . . . . . . . . . . 103

10

6.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106

7 Conclusions

107

7.1 Summary of Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

7.2.1 Language Issues . . . . . . . . . . . . . . . . . . . . . . . . . . 108

7.2.2 Synthesis Issues . . . . . . . . . . . . . . . . . . . . . . . . . . 109

7.2.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

7.3 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . 112

A TRSpec Language Syntax

115

A.1 Keywords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

A.1.1 Keywords in Type De nitions . . . . . . . . . . . . . . . . . . 115

A.1.2 Keywords in Rule and Source Term Declarations . . . . . . . . 115

A.2 TRS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

A.3 Type De nitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

A.3.1 Built-In Type . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

A.3.2 Algebraic Type . . . . . . . . . . . . . . . . . . . . . . . . . . 116

A.3.3 Abstract Type . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

A.3.4 I/O Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

A.3.5 Type Synonym . . . . . . . . . . . . . . . . . . . . . . . . . . 117

A.3.6 Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

A.4 Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

A.4.1 Left Hand Side . . . . . . . . . . . . . . . . . . . . . . . . . . 117

A.4.2 Right Hand Side . . . . . . . . . . . . . . . . . . . . . . . . . 118

A.4.3 Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

A.4.4 Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

A.5 Source Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

B TRSpec Description of a MIPS Processor

119

B.1 Type De nitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

B.1.1 Processor States . . . . . . . . . . . . . . . . . . . . . . . . . . 119

B.1.2 Instruction Set Architecture . . . . . . . . . . . . . . . . . . . 120

B.2 Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

B.2.1 M4 Macros . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

B.2.2 Fetch Stage Rules . . . . . . . . . . . . . . . . . . . . . . . . . 122

B.2.3 Decode Stage Rules . . . . . . . . . . . . . . . . . . . . . . . . 123

B.2.4 Execute Stage Rules . . . . . . . . . . . . . . . . . . . . . . . 130

B.2.5 Memory Stage Rules . . . . . . . . . . . . . . . . . . . . . . . 132

B.2.6 Write-Back Stage Rules . . . . . . . . . . . . . . . . . . . . . 133

B.3 Source Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

11

12

List of Figures

2-1 Next-state logic corresponding to (a) the Mod operation and (b) the Flip operation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2-2 Circuit for computing Gcd(a b). . . . . . . . . . . . . . . . . . . . . .
2-3 TRS rules for a simple ISA. . . . . . . . . . . . . . . . . . . . . . . . 2-4 A simple non-pipelined processor datapath, shown without control sig-
nals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3-1 ATS summary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3-2 Synchronous state elements. . . . . . . . . . . . . . . . . . . . . . . .

3-3 A monolithic scheduler for an M-transition ATS. . . . . . . . . . . .

3-4 Circuits for merging two transitions' actions on the same register. . .

3-5 The GCD circuit from Example 3.4. . . . . . . . . . . . . . . . . . . .

3-6 Scheduling con ict-free rules: (a) <>CF graph (b) Corresponding con-

ict graph and its connected components. . . . . . . . . . . . . . . .

3-7 Enumerated encoder table for Example 3.7. . . . . . . . . . . . . . .

3-8

SCcohrerdesuplionngdisnegquaecnytcialilclyd-ciroemctpeodsi<blSeCruglreasp:h(a()c)DCiroercrteesdpo<nSdCinggracpohn

(b) ict

graph and its connected components. . . . . . . . . . . . . . . . . . .

3-9 Enumerated encoder table for Example 3.8. . . . . . . . . . . . . . .

4-1 Type de nitions for a simple non-pipelined processor. . . . . . . . . .
4-2 TRSpec description of I/O . . . . . . . . . . . . . . . . . . . . . . .
4-3 I/O interfaces synthesized for TOP. . . . . . . . . . . . . . . . . . . . 4-4 A tree graph representing the GCD type structure from Example 4.2.

5-1 TRSpec description of Euclid's Algorithm. . . . . . . . . . . . . . .
5-2 Scheme implementation of Euclid's Algorithm. . . . . . . . . . . . . . 5-3 Excerpt from the hand-coded Verilog RTL description of Euclid's Al-
gorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5-4 Summary of GCD synthesis results. . . . . . . . . . . . . . . . . . . .
5-5 Another TRSpec description of Euclid's Algorithm. . . . . . . . . . 5-6 TRSpec type de nitions for a processor with instruction and data
memory interfaces. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5-7 TRSpec rewrite rules for a processor with instruction and data mem-
ory interfaces. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5-8 A processor's memory interfaces and their connections. . . . . . . . .

23 24 26 27 32 37 38 38 41 49 50
55 56 63 67 68 69 74 75 76 77 78 79 80 81

13

5-9 Summary of the processor synthesis results. . . . . . . . . . . . . . . 5-10 Hand-coded Verilog description of a simple processor. (Part 1) . . . . 5-11 Hand-coded Verilog description of a simple processor. (Part 2) . . . . 5-12 Block diagram of the ve-stage pipelined MIPS processor core. . . . . 5-13 Summary of MIPS synthesis results. . . . . . . . . . . . . . . . . . . .

82 84 85 86 88

6-1 Type de nitions for a simple non-pipelined processor. . . . . . . . . . 92 6-2 Rules for a simple non-pipelined processor. . . . . . . . . . . . . . . . 93 6-3 A simple processor datapath shown without its control paths. . . . . 93 6-4 Additional type de nitions for the two-stage pipelined processor. . . . 94 6-5 Rules for the execute stage of the two-stage pipelined processor. . . . 96 6-6 Combining the Add Execute rule with other execute rules. . . . . . . 100 6-7 Combining the Bz-Not-Taken Execute rule with other execute rules. . 101 6-8 Combining the Load Execute rule with other execute rules. . . . . . . 101 6-9 Combining the Store Execute rule with other execute rules. . . . . . . 102 6-10 Synchronous pipeline with local feedback ow control. . . . . . . . . . 103 6-11 Synchronous pipeline with combinational multi-stage feedback ow
control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 6-12 Circuit area distributions for ve processors. . . . . . . . . . . . . . . 104 6-13 Critical path delays for ve processors. . . . . . . . . . . . . . . . . . 105

14

Chapter 1 Introduction
This thesis presents a method for hardware synthesis from an \operation centric" description. In an operation-centric description, the behavior of a system is decomposed and described as a collection of operations. This operation-centric view of hardware is new in synthesis but not in description. Most high-level hardware speci cations intended for human reading are given operation-centrically. A typical assembly programmer's manual for a microprocessor is an example where the behavior of the processor is broken down into per-instruction operations. Although informal, the written material in most computer architecture textbooks also presents ideas and designs operation-centrically. This thesis improves the usefulness of the operation-centric approach to hardware description by developing a formal description framework and by enabling automatic synthesis of an e cient synchronous circuit implementation. The results of this thesis show that an operation-centric hardware development framework o ers a signi cant reduction in design time and e ort, without loss in implementation quality, when compared to traditional frameworks.
1.1 Operation-Centric Hardware Description
An operation is de ned by a predicate condition and an e ect. The interpretation is that an operation's e ect on the system state can take place when the predicate condition is satis ed. Although an implementation may execute multiple operations concurrently, the end result of an execution must correspond to some sequential interleaving of the operations such that each operation in the sequence produces a state that enables the next operation.
For an unambiguous interpretation, the e ect of an operation is taken to be atomic. In other words, an operation \reads" the entire state of the system in one step, and, if the operation is enabled, the operation updates the state in the same step. If several operations are enabled in a state, any one of the operations can be selected to update the state in one step, and afterwards a new step begins with the updated state. This atomic semantics simpli es the task of hardware description by permitting the designer to formulate each operation as if the system were otherwise static. The designer does not have to worry about unexpected interactions with other concurrent
15

operations. For related reasons, the atomic semantics of operations also makes an operational-centric description easier to interpret by a human. It is important to re-emphasize that this sequential and atomic interpretation of a description does not prevent a legal implementation from executing several operations concurrently, provided the concurrent execution does not introduce new behaviors that are not producible by sequential executions.
The instruction reorder bu er (ROB) of a modern out-of-order microprocessor exempli es complexity and concurrency in hardware behavior.1 Describing an ROB poses a great challenge for traditional hardware description frameworks where concurrency needs to be managed explicitly. However, in an operation-centric framework, the behavior of an ROB can be perspicuously described as a collection of atomic operations including dispatch, complete, commit, etc. AS99]. For example, the dispatch operation is speci ed to take place if there is an instruction that has all of its operands and is waiting to execute, and, furthermore, the execution unit needed by the instruction is available. The e ect of the dispatch operation is to send the instruction to the execution unit. The speci cation of the dispatch operation need not include information about how to resolve potential con icts arising from the concurrent execution of other operations.
1.2 Limitations of Operation-Centric Frameworks
Instead of marking the progress of time with an explicit global clock, the behavior of an operation-centric description is interpreted as a sequence of operations executed in atomic steps. This abstract model of time permits an unambiguous sequential interpretation by the designer but at the same time enables a compiler to automatically exploit parallelism by scheduling multiple parallelizable operations into the same clock cycle in a synchronous implementation. Unfortunately, at times, this normally simplifying abstraction gets in the way of describing hardware designs whose correctness criteria include a speci c synchronous execution timing. For example, a microprocessor often has to interface with the memory controller via a synchronous bus protocol that assumes prede ned latencies between the exchanges of bus signals. As another example, suppose the description of the out-of-order processor above contains a fully-pipelined multiplier unit with a latency of exactly four clock cycles. After a dispatch operation launches a multiply instruction, a corresponding complete operation should be triggered exactly four cycles later. This kind of temporal relationships between operations cannot be expressed directly in an operation-centric framework. In more severe cases, glue logic expressed in a lower-level synchronous representation becomes necessary. However, in many cases, this limitation can be avoided by using an asynchronous handshake between the interacting elements. For example, an operation-centrically described processor can interface with a bus protocol where the initiations of exchanges are demarcated by appropriate strobe signals. The multiplication unit in the out-of-order processor can be out tted with ready and done status
1Refer to HP96] and Joh91] for background information on the operation of an ROB.
16

signals that can be tested by the predicates of the dispatch and complete operations.
1.3 Comparison to CFSM-based Frameworks
Digital hardware designs inherently embody highly concurrent behaviors. The implementation of any non-trivial design invariably consists of a collection of cooperating nite state machines (CFSM). Hence, most hardware description frameworks, whether schematic or textual, use CFSM as the underlying abstraction. In a CFSM framework, a designer explicitly manages concurrency by scheduling the exact cycleby-cycle interactions between multiple concurrent state machines. It is easy to make a mistake in coordinating interactions between two state machines because transitions in di erent state machines are not coupled semantically. It is also di cult to modify one state machine without considering its interaction with the rest of the system.
The advantage of a CFSM framework lies in its resemblance to the underlying circuit implementation. This makes automatic circuit synthesis simpler and more e cient. The close correlation between description and implementation also gives designers tighter control over lower-level implementation choices. On the other hand, a CFSM description cannot be easily correlated to the functionality of the design. Producing a CFSM-based description from a high-level design speci cation requires considerable time and expertise because the designer has to translate between two very di erent abstractions. The development of a CFSM-based description is also errorprone due to its explicit concurrency model. The disadvantages of CFSM frameworks can quickly outweigh their advantages as hardware design complexity increases.
1.4 Comparison to Other High-Level Frameworks
RTL Description and Synthesis: Synthesizing hardware from textual register-
transfer level (RTL) descriptions is currently the standard practice in the development of digital integrated circuits. Many high-end integrated-circuit companies own proprietary hardware description languages (HDLs) and synthesis tools to cater to their applications and fabrication technologies. On the other hand, Verilog TM96] and VHDL2 Ins88] are two standardized HDLs that are supported by commercial tools and are widely used in both industry and academia. The Synopsis Design Compiler Synb] and Synplify Synd] are commercial synthesis tools that compile the RTL (a.k.a. structural) subsets of Verilog and VHDL. Synthesizable RTL descriptions rely on the same synchronous CFSM abstraction as traditional schematic frameworks. In comparison to schematic capture, the productivity gain of an RTL-based design ow comes from the relative compactness of textual descriptions and from automatic logic optimizations by synthesis tools. The operation-centric synthesis framework in this thesis outputs RTL descriptions and relies on commercial hardware compilers to produce the nal circuit implementations.
2VHSIC Hardware Description Language, where VHSIC stands for Very High Speed Integrated Circuits.
17

Behavioral Description and Synthesis: The term behavioral description typi-
cally refers to describing hardware, or hardware/software systems, as multiple threads of computation that communicate via message passing or a shared-memory paradigm. The underlying multithreaded abstraction can be traced to Communicating Sequential Processes Hoa85]. The objective of behavioral synthesis is to infer and allocate the necessary hardware resources and schedule their usage by the threads. Topics relating to this style of high-level synthesis are discussed in GDWL92] and MLD92]. The behavioral frameworks describe each hardware thread using high-level sequential languages like C or behavioral Verilog. A comprehensive summary of early e orts can be found in WC91] and CW91]. More recent e orts are represented by HardwareC Sta90], Esterel Ber98], ECL LS99], SpecC GZD+00], and a hybrid C/Verilog co-speci cation framework by Thomas, et al. TPPW99]. Commercially, besides behavioral Verilog and VHDL, SystemC LTG97] and Superlog Co-], based on C/C++ and C/Verilog respectively, are two threaded behavioral languages that are currently gaining acceptance. The multithreaded behavioral abstraction is an improvement over the CFSM frameworks. Instead of synchronizing cycle-by-cycle, the threaded abstraction allows hardware threads to synchronize at coarser granularity or only at communication points. Nevertheless, a designer still needs to coordinate the interactions between concurrent threads of computations explicitly.
Software Languages for Hardware Description and Synthesis: Both sequen-
tial and parallel programming languages have been used to capture functionalities for hardware implementation. Transmogri er-C is based on C plus additional hardwarespeci c constructs Gal95]. The Programmable Active Memory (PAM) project uses C++ syntax for RTL description VBR+96]. These rst two usages of software languages are strongly in uenced by hardware description needs. Although these languages leverage the familiar syntax of C and C++, a description is typically illegal or not meaningful as a C or C++ program. Semantically and syntactically correct sequential C and Fortran programs have been automatically parallelized to target an array of con gurable structures in the RAW project BRM+99]. The SpC project attempts to synthesize full ANSI-compliant C programs and, in particular, addresses C programs with pointers to variables SM98, Mic99]. Data-parallel C languages have also been used to program an array of FPGAs in Splash 2 GM93] and CLAy GG97]. These latter examples synthesize hardware from semantically correct programs and thus are convenient tools for implementing algorithms. However, these program-based descriptions are not suitable for describing microarchitectural mechanisms. In the operation-centric framework of this thesis, both algorithms and microarchitectures
can be expressed in the TRSpec language for synthesis. Processor-Speci c High-level Description and Synthesis: High-level hard-
ware description and synthesis are employed in the development of application speci c instruction set processors (ASIPs). For example, the ADAS PSH+92] environment accepts an instruction set architecture (ISA) described in Prolog and emits a VLSI implementation using a combination of tools in stages. During behavioral
18

synthesis, the Piper tool attempts to pipeline the microarchitecture while taking into account factors like instruction issue frequencies, pipeline stage latencies, etc. The whole ADAS development suite is driven at the front-end by ASIA HHD93], a system that automatically generates a custom ISA for a particular application program. Other processor-speci c high-level description and synthesis frameworks include Mimola Mar84], Dagar Raj89], nML FPF95], and ISDL HHD97]. Although not domain-speci c to ASIP developments, the operation-centric framework in this thesis can also be used to give concise speci cations of ISA semantics and processor microarchitectures. Furthermore, an operation-centric ISA speci cation is amenable to automatic synthesis and architectural transformations.
Other Representative Work: Hardware description frameworks with formal ba-
sis have been used in the context of formal design speci cation and veri cation. For example, the speci cation language of the HOL theorem proving system SRI97] has been used to describe a pipelined processor, and a methodology has been developed to verify a pipelined processor description against its non-pipelined counterpart Win95]. The Hawk language, based on Haskell JHA+98], can be used to create executable speci cations of processor microarchitectures MLC98] a Hawk pipelined processor speci cation can be reduced into a non-pipelined equivalent for veri cation ML99].
The TRSpec language described in this thesis is based on the formalism of Term Rewriting Systems (TRS). Besides being synthesizable, a TRSpec description is also
amenable to formal veri cation. In a closely related research, Marinescu and Rinard address the problem of synthesizing a synchronous pipeline from a description of loosely-coupled modules connected by queues MR99].
1.5 Thesis Contributions
The operation-centric view of hardware has existed in many forms of hardware speci cations, usually to convey high-level architectural concepts. This thesis creates a new hardware development framework where an e cient circuit implementation can be synthesized automatically from an operation-centric description. Speci cally, this thesis makes the following contributions:
Identi cation of key properties and advantages of operation-centric hardware description frameworks
Design of TRSpec, a source-level operation-centric hardware description lan-
guage based on the TRS formalism De nition of an operation-centric state machine abstraction that serves as the syntax-independent intermediate representation during hardware synthesis Formulation of the theories and algorithms necessary to create an e cient hardware implementation from an operation-centric description
19

Implementation of the Term Rewriting Architectural Compiler (TRAC), a com-
piler for TRSpec Investigation of the e ectiveness of TRSpec and TRAC in comparison to tra-
ditional RTL-based development ows Preliminary investigation of pipelined superscalar processor development via
source-to-source transformations of TRSpec descriptions
In this early step towards the study of operation-centric hardware development frameworks, several important and interesting points of research are not fully addressed. These open issues are summarized in Section 7.2
1.6 Thesis Organization
Following this introductory chapter, the next chapter rst provides a further introduction to operation-centric hardware descriptions using four design examples. Chapter 3 then explains the synthesis of operation-centric hardware descriptions. The chapter rst develops a formal representation for operation-centric hardware description and then describes the synthesis algorithms implemented in TRAC. Chapter 4 presents
TRSpec, the source-level TRS language accepted by TRAC. Chapter 5 presents the results from applying TRAC to TRSpec descriptions. Chapter 6 presents the application of TRSpec and TRAC to the design of a pipelined superscalar processor.
Chapter 7 concludes with a summary of this thesis and identi es areas for continued investigation.
20

Chapter 2 Operation-Centric Design Examples
Four design examples are explored to provide a concrete demonstration of operationcentric hardware description. The rst example develops a hardware implementation of Euclid's Algorithm. The second example describes a simple instruction set architecture (ISA). The third and fourth examples describe two di erent pipelined implementations of the same ISA. In this chapter, the operation-centric descriptions borrow the notation of Term Rewriting Systems (TRS) BN98, Klo92].
2.1 Euclid's Algorithm
This example develops a nite state machine (FSM) that computes the greatest common divisor (GCD) of two positive integers using Euclid's Algorithm.1 The FSM's
state elements consist of two 32-bit registers, a and b. The behavior of this FSM
can be captured operation-centrically using a TRS whose terms has the signature Gcd(a,b) where the variables a and b correspond to the unsigned integer contents of
registers a and b. The FSM has two operations, Mod and Flip. The Mod operation
can be described by the rewrite rule: Mod Rule:
Gcd(a,b) if (a b) ^ (b6=0) ! Gcd(a-b,b)
In general, a rewrite rule has three components: a left-hand-side pattern term, a right-hand-side rewrite term, and an optional predicate. In the Mod rule, the pattern is Gcd(a,b), the rewrite term is Gcd(a-b,b), and the predicate is the expression
(a b) ^ (b6=0). A rule can be applied if the pattern matches the current state value
and if the predicate is true. In the context of operations, the pattern and the predicate describe the condition of an operation, i.e., the Mod operation is enabled only
1Euclid's Algorithm states (Gcd a ) = (b Gcd b (a mod b)) if b 6= 0. This equality can be used
to repeatedly reduce the problem of computing (Gcd a b) to a simpler problem. For example, (2Gcd 4) = (4Gcd 2) = (2Gcd 0) = 2.
21

when (a b)^(b 6= 0). The right-hand-side rewrite term speci es the new state value
after a rewrite. In describing the behavior of an FSM, the rewrite term conveys how an operation updates the state elements. The e ect of the Mod operation is to update
the register a by the value a ; b.
The Flip operation can be described by the rewrite rule: Flip Rule:
Gcd(a,b) if (a<b) ! Gcd(b,a) The e ect of the Flip operation is to exchange the contents of registers a and b. This operation is enabled only when (a < b).
Starting with two integers in registers a and b, an execution of the FSM computes
the GCD of these initial values. The execution of the FSM must correspond to some sequential and atomic interleaving of the Mod operation and the Flip operation. The
sequence of operations stops when register b contains 0 because neither operation can be enabled. The GCD of the initial values is in register a at the end of the sequence.
For example, starting with an initial state corresponding to the term Gcd(2,4), the sequence of operations that follows is
Gcd(2,4) !Flip Gcd(4,2) !Mod Gcd(2,2) !Mod Gcd(0,2) !Flip Gcd(2,0)
This operation-centric description of an FSM is not only easy to understand, but it also allows certain properties of the whole system to be proven by examining the property of each operation individually. For example, it can be shown that if an FSM correctly implements this two-operation system, then it maintains the invariant
that the GCD of the contents of registers a and b never changes. To show this, one
can make the argument that neither the Mod operation nor the Flip operation can
change the GCD of a and b.2 Since an execution of a correctly implemented FSM
must correspond to some sequential and atomic interleaving of these two operations,
the GCD of a and b should never change from operation to operation.
In a TRS, the state of the system is represented by a collection of values, and a rule rewrites values to values. Given a collective state value s, a TRS rule computes a new value s0 such that
s0=if (s) then (s) else s
where the function captures the ring condition and the function captures the e ect of the rule. It is also possible to view a rule as a state-transformer in a statebased system. In the state-transformer view, the applicability of a rule is determined by computing the function on the current state, and the next-state logic consists of a set of actions that alter the contents of the state elements to match the value

2For the Mod operation, if (a b)^(b 6= 0) then (a mod b) = ((a ; b) mod b), and thus,

; ;( ) = ( mod ) = ((Gcd a b

Gcd a

bb

Gcd a

b) mod b b) = (Gcd a

b b).

For the Flip operation,

( ) = ( ).Gcd a b

Gcd b a

22

δ Mod,a

πMod
ce
a

b
ce

δ Mod,a =0

πMod

(a)

δFlip,a

πFlip
ce
a

δFlip,b

πFlip

δFlip,b

b
ce
πFlip

δFlip,a

(b) Figure 2-1: Next-state logic corresponding to (a) the Mod operation and (b) the Flip operation.

of (s). The execution semantics of operations remains sequential and atomic in this action-on-state interpretation.
The operations in this example can be restated in terms of actions. In the notation
below, Mod is the predicate of the Mod operation, aMod a is the action on register a
according to the Mod operation, etc.

Mod aMod a

= =

(a b) ^ (b 6= 0) set(a ; b)

F lip aFlip a
aFlip b

= = =

(a < b) set(b) set(a)

If an operation is mapped to a synchronous state transition of the FSM, Mod and aMod a map to the next-state logic in circuit (a) of Figure 2-1. The Flip operation maps to the next-state logic in circuit (b) of Figure 2-1.

23

πMod πFlip +πMod

δ Mod,a

ce
a

δFlip,a

π
Flip

δFlip,b δ Mod,a

δFlip,b

b
ce
πFlip

δFlip,a

=0

π
Flip
πMod

Figure 2-2: Circuit for computing Gcd(a b).

The nal FSM implementation can be derived by combining the two circuits from

Figure 2-1. Both the Mod operation and the Flip operation a ect register a, but the

two operations are never enabled in the same state. (Their predicates contradict.)

Since the two operations are mutually exclusive, the latch enable for a is simply the

lvsotaagluticeeavlo-afOluaReisaonfsdetlheleactttecwhdoefrnooapmbelreeaitotihfoenbrsi'sapMcrooenddtaircooalrtleeadsF(bliiyp.eat.,htehrMFooluidgp+hopaeFmrlaiput)ilotainpnladelxotenhree..

next-state The next-

Figure 2-2 shows the merged circuit. The circuit behaves like circuit (a) of Fig-

ure 2-1 in a cycle when the condition (a b)^(b 6= 0), required by the Mod operation,

is satis ed. In a cycle when the condition (a < b), required by the Flip operation,

is satis ed, the circuit behaves like circuit (b) of Figure 2-1 instead. Since the two

operations are mutually exclusive, the atomic semantics of operations is maintained

in this implementation automatically.

2.2 A Simple Processor
A simple ISA can be speci ed operation-centrically as a TRS whose terms have the signature Proc(pc, rf, imem, dmem). The four elds of a processor term correspond to the values of the programmer-visible states of the ISA. The variable pc represents the program counter register. The rf variable corresponds to the register le, an array of integer values. In an expression, rf r] gives the value stored in location r of rf, and rf r:=v] gives the new value of the array after location r has been updated by the value v. The imem variable is an array of instructions. The dmem variable corresponds to the data memory, another array of integer values.
The behavior of the processor can be described by giving a rewrite rule for each instruction in the ISA. The following rule describes the execution of an Add instruction.
Add Rule: Proc(pc, rf, imem, dmem) where Add(rd,r1,r2)=imem pc]
! Proc(pc+1, rf rd:=(rf r1]+rf r2])], imem, dmem)
This rule conveys the same level of information as one would nd in a programmer's
24

manual. The rule is enabled for a processor term whose current instruction, imem pc], is an Add instruction. (An Add instruction is represented by a term with the signature Add(rd,r1,r2). The rule uses a pattern-matching where statement to require imem pc] to match the pattern Add(rd,r1,r2).) Given a processor term that satis es this condition, the rule's rewrite term speci es that the processor term's pc eld should be incremented by 1 and the destination register rd in rf should be updated with the sum of rf r1] and rf r2]. In the state-transformer view, the operation described by the Add rule can be stated as

Add = (imem pc]=Add(rd,r1,r2))

aAdd pc aAdd rf

= =

set(pc+1) a-set(rd,rf r1]+rf r2])

Figure 2-3 gives the rewrite rules for all instructions in the ISA. The instructions are move PC to register, load immediate, register-to-register addition and subtraction, branch if zero, memory load and store. The complete TRS not only describes an ISA, but it also constitutes an operation-centric description of an FSM that implements the ISA. The datapath for the processor can be derived automatically and is depicted in Figure 2-4.

2.3 A Pipelined Processor
The ISA description from the previous example can be transformed into the description of a pipelined processor by adding FIFOs as pipeline bu ers and by systematically splitting every instruction operation into sub-operations local to the pipeline stages.
For example, a two-stage pipelined processor can be speci ed as a TRS whose terms have the signature Proc(pc, rf, bf, imem, dmem). A FIFO bf is included as the pipeline bu er between the fetch stage and the execute stage. The FIFO is abstracted to have a nite but unspeci ed size. Using a list-like syntax, a FIFO containing three elements can be expressed as b1 b2 b3. The two main operations on a FIFO are enqueue and dequeue. Enqueuing b to bf yields bf b while dequeuing from b bf leaves bf. An empty FIFO is expressed as ` '.
Using a FIFO to separate pipeline stages provides the necessary isolation that allows the operations in one stage to be described independently of the other stages. Although this style of pipeline description re ects a pipeline that is asynchronous and elastic, it is possible to infer a legal implementation that is fully-synchronous and has stages separated by simple registers.
The Add rule from the previous example can be split into two sub-rules that describe the operations in the fetch stage and the execute stage of the pipeline. The fetch-stage sub-rule is
Fetch Rule: Proc(pc, rf, bf, imem, dmem)
! Proc(pc+1, rf, bf imem pc], imem, dmem)
25

Loadi Rule: Proc(pc, rf, imem, dmem) where Loadi(rd,const)=imem pc]
! Proc(pc+1, rf rd:=const], imem, dmem)
Loadpc Rule: Proc(pc, rf, imem, dmem) where Loadpc(rd)=imem pc]
! Proc(pc+1, rf rd:=pc], imem, dmem)
Add Rule: Proc(pc, rf, imem, dmem) where Add(rd,r1,r2)=imem pc]
! Proc(pc+1, rf rd:=(rf r1]+rf r2])], imem, dmem)
Sub Rule: Proc(pc, rf, imem, dmem) where Sub(rd,r1,r2)=imem pc]
! Proc(pc+1, rf rd:=(rf r1]-rf r2])], imem, dmem)
Bz Taken Rule: Proc(pc, rf, imem, dmem) if (rf rc]=0) where Bz(rc,ra)=imem pc]
! Proc(rf ra], rf, imem, dmem)
Bz Not-Taken Rule:
Proc(pc, rf, imem, dmem) if (rf rc]6=0) where Bz(rc,ra)=imem pc] ! Proc(pc+1, rf, imem, dmem)
Load Rule: Proc(pc, rf, imem, dmem) where Load(rd,ra)=imem pc]
! Proc(pc+1, rf rd:=dmem rf ra]]], imem, dmem)
Store Rule: Proc(pc, rf, imem, dmem) where Store(ra,r)=imem pc]
! Proc(pc+1, rf, imem, dmem rf ra]:=rf r]])
Figure 2-3: TRS rules for a simple ISA.
26

+1 pc

Program
ROM (imem)

Register
File (rf)

Data Memory (dmem)
ALU (+,-)

Figure 2-4: A simple non-pipelined processor datapath, shown without control signals.
The execute-stage sub-rule is Add Execute Rule: Proc(pc, rf, inst bf, imem, dmem) where Add(rd,r1,r2)=inst
! Proc(pc, rf rd:=(rf r1]+rf r2])], bf, imem, dmem)
The Fetch rule fetches instructions from consecutive instruction memory locations and enqueues them into bf. The Fetch rule is not concerned with what happens if a branch is taken or if the pipeline encounters an exception. The Add Execute rule, on the other hand, would process the next pending instruction in bf as long as it is an Add instruction.
In this TRS, more than one rule can be enabled on a given state. The Fetch rule is always ready to re, and at the same time, the Add Execute rule, or other execute-stage rules, may be ready to re as well. Even though conceptually only one rule should be red in each step, an implementation of this processor description must carry out the e ect of both rules in the same clock cycle. Without concurrent execution, the implementation does not behave like a pipeline. Nevertheless, the implementation must also ensure that a concurrent execution of multiple operations produces the same result as a sequential execution.
The Bz Taken rule and the Bz Not-Taken rule from the previous example can also be split into separate fetch and execute rules. The execute-stage components of the Bz Taken and Bz Not-Taken rules are
Bz Taken Execute Rule: Proc(pc, rf, inst bf, imem, dmem) if (rf rc]=0) where Bz(rc,ra)=inst
! Proc(rf ra], rf, , imem, dmem)
and Bz Not-Taken Execute Rule:
Proc(pc, rf, inst bf, imem, dmem) if (rf rc]6=0) where Bz(rc,ra)=inst ! Proc(pc, rf, bf, imem, dmem)
27

The Fetch rule performs a weak form of branch speculation by always incrementing pc. Consequently, in the execute stage, if a branch is resolved to be taken, besides setting pc to the branch target, all speculatively fetched instructions in bf need to be discarded. This is indicated by setting bf to ` ' in the Bz Taken Execute rule.
Although both the Fetch rule and the Bz Taken Execute rule can a ect pc and bf, the sequential semantics of rules allows the formulation of the Bz Taken Execute rule to ignore the possibility of contentions or race conditions with the Fetch rule. In a clock cycle where the processor state enables both rules, the description permits an implementation to behave as if the two operations are executed sequentially, in either order. This implementation choice determines whether one or two pipeline bubbles are inserted after a taken branch, but it does not a ect the processor's ability to correctly execute a program.
The operations in this example can also be restated in terms of actions:

F etch aFetch pc

= =

notfull (bf ) set(pc+1)

aFetch bf = enq(imem pc])

Add;Exec aAdd;Exec rf aAdd;Exec bf

= = =

( rst(bf)=Add(rd,r1,r2))^notempty(bf)
a-set(rd,rf r1]+rf r2]) deq( )

BzT aken;Exec aBzT aken;Exec pc

= =

( rst(bf)=Bz(rc,rt))^(rf rc]=0)^notempty(bf)
set(rf ra])

aBzTaken;Exec bf = clear( )

BzNotT aken;Exec aBzNotT aken;Exec bf

= =

( rst(bf)=Bz(rc,rt))^(rf rc]6=0)^notempty(bf)
deq( )

Null actions, represented as , on a state element are omitted from the action lists

above. (The full action list for the Add Execute rule is h apc, arf, abf, aimem, admem i

where apc, aimem been augmented

awnidthaadmnemexaprleicit's.t)estA,lsnootnfoutlilc(eb,f)eaocrh

operation's notempty (bf),

expression depending

has on

how the operation accesses bf.

2.4 Another Pipelined Processor
This example demonstrates the versatility of operation-centric descriptions by deriving a variation of the pipelined processor. In the previous example, instruction decode and register le read are performed in the execute stage. The processor described in this example performs instruction decode and register read in the fetch stage instead.
The new two-stage pipelined processor is speci ed as a TRS whose terms have the signature Proc(pc, rf, bf, imem, dmem). Decoded instructions and their operand values
28

are stored as instruction templates in the pipeline bu er bf. The instruction template terms have the signatures: TLoadi(rd,v), TAdd(rd,v1,v2), TSub(rd,v1,v2), TBz(vc,va), TLoad(rd,va), and TStore(va,v).
Below, the Add rule from Section 2.2 is split into two sub-rules such that instruction decode is included in the fetch-stage rule. The fetch-stage sub-rule is
Add Fetch Rule: Proc(pc, rf, bf, imem, dmem)
if (r12=Target(bf)) ^ (r22=Target(bf)) where Add(rd,r1,r2)=imem pc] ! Proc(pc+1, rf, bf TAdd(rd,rf r1],rf r2]), imem, dmem)
The execute-stage sub-rule is
Add Execute Rule: Proc(pc, rf, itemp bf, imem, dmem) where TAdd(rd,v1,v2)=itemp
! Proc(pc, rf rd:=(v1+v2)], bf, imem, dmem)
Splitting an operation into sub-operations destroys the atomicity of the original operation and can cause new behaviors that are not part of the original description. Thus, the sub-operations may need to resolve newly created hazards. In this case, the Add Fetch rule's predicate expression has been extended to check if the source register names, r1 and r2, are in Target(bf).3 This extra condition stalls instruction fetching when a RAW (read-after-write) hazard exists.
As another example, consider the Bz Taken rule and the Bz Not-Taken rule from Section 2.2. Again, the rules can be split into their fetch and execute sub-operations. Both Bz rules share the following instruction fetch rule:
Bz Fetch Rule: Proc(pc, rf, bf, imem, dmem)
if (rc2=Target(bf)) ^ (ra2=Target(bf)) where Bz(rc,ra)=imem pc] ! Proc(pc+1, rf, bf TBz(rf rc],rf ra]), imem, dmem)
The two execute rules for the Bz instruction template are
Bz Taken Execute Rule: Proc(pc, rf, itemp bf, imem, dmem) if (vc=0) where TBz(vc,va)=itemp
! Proc(va, rf, , imem, dmem)
and
Bz Not-Taken Execute Rule:
Proc(pc, rf, itemp bf, imem, dmem) if (vc6=0) where TBz(vc,va)=itemp ! Proc(pc, rf, bf, imem, dmem)
As in the previous examples, the operations in this example can also be restated
3Target(bf) is a shorthand for the set of destination register names in bf. Let bf contain entries
i1 ... in. `r2=Target(bf)' stands for `(r6=Dest(i1))^...^(r6=Dest(in))' where Dest(itemp) extracts the
destination register name of an instruction template.
29

in terms of actions:

Add;F etch

=

(imem pc]=Add(rd,r1,r2))
^(r12= Target(bf ))^(r22=Target(bf ))^notful l (bf )

aAdd;Fetch pc = set(pc+1)

aAdd;Fetch bf = enq(TAdd(rd,rf r1],rf r2]))

Add;Exec = ( rst(bf)=TAdd(rd,v1,v2))^notempty(bf)

aAdd;Exec rf aAdd;Exec bf

= =

a-set(rd,v1+v2) deq( )

Bz;F etch

=

(imem pc]=Bz(rc,ra))
^(rc2= Target(bf ))^(ra2=Target(bf ))^notful l (bf )

aBz;F etch pc aBz;F etch bf

= =

set(pc+1) enq(TBz(rf rc],rf ra]))

BzT aken;Exec aBzT aken;Exec pc
aBzT aken;Exec bf

= = =

( rst(bf)=TBz(vc,vt))^(vc=0)^notempty(bf)
set(va) clear( )

BzNotT aken;Exec aBzNotT aken;Exec bf

= =

( rst(bf)=TBz(vc,vt))^(vc6=0)^notempty(bf)
deq( )

30

Chapter 3 Hardware Synthesis and Scheduling
Implementing an operation-centric description involves combining the atomic operations into a single coherent next-state logic for a state machine. For performance reasons, an implementation should carry out multiple operations concurrently while still maintaining a behavior that is consistent with a sequential execution of the atomic operations. This chapter rst presents the Abstract Transition System (ATS), an abstract state machine whose transitions are speci ed operation-centrically. The chapter next develops the procedure to synthesize an ATS into an e cient synchronous digital implementation. A straightforward implementation that only executes one operation per clock cycle is presented rst. This is followed by two increasingly optimized implementations that support the concurrent execution of operations.
3.1 Abstract Transition Systems
ATS is designed to be the intermediate target in the compilation of source-level
operation-centric hardware description languages. An ATS is de ned by S, So and X . S is a list of state elements, and So is a list of initial values for the elements in S. X is a list of operation-centric transitions. The structure of an ATS is summarized
in Figure 3-1.
3.1.1 State Elements
This thesis de nes a restricted ATS where an element in S can only be a register,
an array, or a FIFO. For generality, the ATS abstraction can be extended with new elements and variations on existing elements.
A register R can store an integer value up to a speci ed maximum word size. The value stored in R can be referenced in an expression using the side-e ect-free R.get( ) query operator. For conciseness, R.get( ) can be abbreviated simply as R in an expression. R's content can be set to value by the R.set(value) action operator. Any number of queries are allowed in an atomic update step, but each register allows
31

AT S = h S, So, X i

S So

= =

h h

vRR1,1.,....,.R, vNRRN,RA, 1v,A..1.,,.A..N, vAA, NFA1,, v..F.,1F, .N..F,

v, OFN1F, .,.v.,OO1 ,N.O..,, vI1O, N..O.,

iINI

i

X T

= =

f h

T1,
,

...,
i

TM

g

= exp

= h aR1,...,aRNR,aA1,...,aANA,aF1,...,aFNF ,aO1,...,aONO ,aI1,...,aINI i

aR = ] set(exp)

aA = aF =

] ]

aen-sqe(te(xepx)pid]xd, eeqx(p)dat]a )en-deq (exp)

]

clear( )

aO = ] set(exp)

aI =

exp

= ]

constant
R.get( )

]]AP.ari-mgeitti(vide-xO) p(exp1,

...,

expn)

] F. rst( ) ] F.notfull( ) ] F.notempty( )

] O.get( ) ] I.get( )

Figure 3-1: ATS summary.

at most one set action in each atomic update step. An atomic update step is de ned in Section 3.1.3 where the operational semantics of an ATS is de ned.
An array A can store a speci ed number of values. The value stored in the idx'th entry of A can be referenced in an expression using the side-e ect-free A.a-get(idx) query operator. A.a-get(idx) can be abbreviated as A idx]. The content of the idx'th entry of A can be set to value using the A.a-set(idx,value) action operator. Out-ofbound queries or actions on an array are not allowed. In this thesis, each array allows at most one a-set action in each atomic update step, but any number of queries on an array are allowed. In general, variations of the array elements can be de ned to allow multiple a-set actions or to limit the number of a-get queries.
A FIFO F stores a nite but unspeci ed number of values in a rst-in, rst-out manner. The oldest value in F can be referenced in an expression using the side-e ectfree F. rst( ) query operator, and can be removed by the F.deq( ) action operator. A new value can be added to F using the F.enq(value) action operator. F.en-deq(value) is a compound action that enqueues a new value after dequeuing the oldest value. In addition, the entire contents of F can be cleared using the F.clear( ) action operator. Under owing or over owing a FIFO by an enq or a deq action is not allowed. The status of F can be queried using the side-e ect-free F.notfull( ) and F.notempty( ) query operators that return a Boolean status. Each FIFO allows at most one action in each atomic update step, but any number of queries are allowed. Again, variations of FIFO elements can be de ned to support multiple actions per update step.
Input elements and output elements are register-like state elements. An input element I is like a register without the set operator. I.get( ) returns the value of an external input port instead of the content of an actual register. An output element O
32

supports both set and get, and its content is visible outside of the ATS on an output port. Input and output elements can be treated exactly like registers except when the input and output ports are instantiated in the nal phase of circuit synthesis (described in Section 3.2.3).
3.1.2 State Transitions
An element in X is a transition. A transition is a pair, h , i. is a Boolean
value expression. In general, a value expression can contain arithmetic and logical operations on scalar values. A value in an expression can be a constant or a value
queried from a state element. Given an ATS whose S has N elements, is a list of N actions, one for each state element. An action is speci ed as an action operator
plus its arguments in terms of value expressions. A null action is represented by the
symbol ` '. For all actions in , the i'th action of must be valid for the i'th state element in S.
3.1.3 Operational Semantics
The initial value of the i'th state element (except for input elements) is taken from the i'th element of So. From this initial state, the execution of an ATS takes place
as a sequence of state transitions in atomic update steps.
At the start of an atomic update step, all expressions in X are evaluated using
the current contents of the state elements. In a given step, an applicable transition is one whose expression evaluates to true. All argument expressions to the actions
in 's are also evaluated using the current state of S.
At the end of an atomic update step, one transition is selected nondeterministically
from the applicable transitions. S is then modi ed according to the actions of the selected transition. For each action in the selected , the i'th action is applied to the i'th state element in S, using the argument values evaluated at the beginning of the
step. If an illegal action or combination of actions is performed on a state element, the system halts with an error. A valid ATS speci cation never produces an error.
The atomic update step repeats until S is in a state where none of the transitions
are applicable. In this case, the system halts without an error. Alternatively, a
sequence of transitions can lead S to a state where selecting any applicable transition leaves S unchanged. The system also halts without an error in this case. Some systems
may never halt. Due to nondeterminism, some systems could halt in di erent states from one execution to the next.
3.1.4 Functional Interpretation
ATS is de ned in terms of state elements and actions with side-e ects. In some circumstances, it is convenient to have a functional interpretation of ATS where an
execution produces a sequence of S values. In the functional domain, the state of S
is represented by a collection of values. R is represented by an integer value, A is an array of values, and F is an ordered list of values. The e ect of a transition on state
33

s can be expressed as a function of and such that s. if (s) then (s) else s
where is a function that computes a new S value based on the current S value. Given a transition whose is h aR1..., aA1..., aF1... i, its in the functional domain is
(s)= let
h vR1..., vA1..., vF1... i=s
in
h Apply(aR1, vR1), ..., Apply(aA1, vA1), ..., Apply(aF1, vF1), ... i

Applying an action in the functional domain entails computing the new value of a state element based on the old values of the state elements. Given a state element whose current value is v, the state's new value after receiving action a can be computed by

Apply(a, v)= case a of
set(exp) ) Eval(exp) aen-sqe(te(xepx)pi)dx,vexEpvdaatla()e)xp)v Eval(expidx):=Eval(expdata)] deq( ) ) RestOf(v) en-deq(exp) ) RestOf(v) Eval(exp) clear( ) ) empty list )v

where RestOf(list) is the remainder of list after the rst element is removed.

Example 3.1 (GCD)

This ATS corresponds to the TRS described in Section 2.1. The ATS describes the

computation of the greatest common divisor of two 32-bit integers using Euclid's

Algorithm. S is
transition pairs,

h h

Ra, 1,

R1 biianwdhehre

Ra 2,

2aindwhRebreare

32-bit

registers.

X

consists

of

two

1 1

= =

(hRsaet

(RRba)-R^b)(,Rbi6=

0)

2 2

= =

Rh sae<tR(Rb b),

set(Ra)

i

The two transitions correspond to the Mod operation and the Flip operation from
Section 2.1, respectively. The initial values So=h 2, 4 i initialize Ra and Rb for the
computation of Gcd(2, 4).
2

34

Example 3.2 (A Simple Processor)
This ATS corresponds to the ISA described in Section 2.2. The programmer-visible
processor states are represented by S=h RPC,ARF ,AIMEM,ADMEM i where RPC is a
16-bit program counter register, ARF is a general purpose register le of four 16bit values, AIMEM is a 216-entry array of 25-bit instruction words, and ADMEM is a 216-entry array of 16-bit values.
X consists of transitions that correspond to the rewrite rules from Section 2.2, i.e.,
each transition describes the execution of an instruction in the ISA. Let 0, 2 and 4 be the numerical values assigned to the instruction opcodes Loadi (Load Immediate), Add (Triadic Register Add) and Bz (Branch if Zero). Also, let the instruction word stored in AIMEM be decoded as
opcode = bits 24 down to 22 of AIMEM RPC] rd = bits 21 down to 20 of AIMEM RPC] r1 = bits 19 down to 18 of AIMEM RPC] r2 = bits 17 down to 16 of AIMEM RPC] const = bits 15 down to 0 of AIMEM RPC] 's and 's of the transitions corresponding to the execution of Loadi, Add and Bz are

Loadi Loadi

= =

(opcode=0)
h set(RPC+1),

a-set (rd ,

const),

,

i

Add Add

= =

(opcode=2)
h set(RPC+1),

a-set (rd ,

ARF

r1 ]+ARF

r2 ]),

,

i

BzT aken BzT aken

= =

(opcode=4)
h set(ARF r2

^
]),

(ARF ,,

r1 ]=0)
i

BzN otT aken BzN otT aken

= =

(opcode=4) ^ h set(RPC+1),

(ARF ,,

r1 ]6=0) i

2

Example 3.3 (A Pipelined Processor) This ATS corresponds to the two-stage pipelined processor from Section 2.3. S is h RPC,ARF ,FBF ,AIMEM,ADMEM i. FBF is the pipeline stage FIFO for holding fetched
instruction words. Separate transitions are used to describe the operations in the Fetch stage and the
Execute stage of the pipeline. and of the transition that correspond to instruction fetch are

F F

etch etch

= =

Fh BseFt.(nRoPtfCu+ll1()),

, enq(AIMEM RPC]),

,

i

Again, let 0, 2 and 4 be the numerical values assigned to the instruction opcodes Loadi (Load Immediate), Add (Triadic Register Add) and Bz (Branch if Zero), and

35

let the instruction word returned by FBF . rst( ) be decoded as

opcode = rd = bits

bits 24 down to 22 of FBF . 21 down to 20 of FBF . rst(

rst )

(

)

r1 = bits 19 down to 18 of FBF . rst( )

r2 =
const

bits 17 = bits

down to 15 down

16 to

of FBF . 0 of FBF

rst( ) . rst(

)

's and 's of the transitions that correspond to the execution of Loadi, Add and Bz in the Execute stage are

Loadi;Execute Loadi;Execute

= =

FhB,Fa.n-soette(mrdp,tyco(n)s^t),(odpeqco(d)e, =,0)i

Add;Execute Add;Execute

= =

Fh B,Fa.n-soette(mrdp,tyA(R)F

^ (opcode=2)
r1]+ARF r2]),

deq

(

),

,

i

BzT aken;Execute BzT aken;Execute

= =

Fh BseFt.(nAoRteFmr2p]t)y,

(

) ,

^ (opcode=4) clear( ), , i

^

(ARF

r1 ]=0)

BzNotT aken;Execute BzNotT aken;Execute

= =

Fh B,F

.notempty ( , deq( ), ,

)^ i

(opcode=4)

^

(ARF

r1 ]6=0)

2

3.2 Reference Implementation of an ATS
One straightforward implementation of an ATS is a nite state machine (FSM) that
performs one atomic update step per clock cycle. The elements of S, instantiated
as clock synchronous registers, arrays and FIFOs, are the state of the FSM. (The hardware manifestations of the ATS state elements are shown in Figure 3-2.) The
atomic transitions in X are combined to form the next-state logic of the FSM.
3.2.1 State Storage
The register primitive is rising-edge-triggered and has a clock enable. A register can take on a new value on every clock edge, but the current value of the register can fan out to many locations. This hardware usage paradigm is directly re ected in the de nition of the R element of an ATS where only one set action is allowed in an atomic update step, but any number of get queries are allowed. In general, the capability of the storage primitives should match their respective ATS counterparts. When the ATS state element models are extended or modi ed, their hardware representations need to be adjusted accordingly, and vice versa.
The array primitive supports combinational reads, that is, the read-data port (RD) continuously outputs the value stored at the entry selected by the read-address
36

(write addr) WA

(write data) WD

R AD

(write enable) WE Q

LE Clk

(read addr) RA 1
(read addr) RA... 2
(read addr) RA n

RD1 (read data)
R...D2 (read data)
RDn (read data)

Clk

(enq data) ED

first

F(enq enable) EE
(deq enable) DE

_full

(clear enable) CE

_empty

Clk
Figure 3-2: Synchronous state elements.

port (RA). The construction of the reference implementation assumes that an array primitive has as many read ports as necessary to support all a-get( ) queries of ATS transitions. After a register-transfer level (RTL) design is generated, the actual number of combinational read ports can be reduced by common subexpression elimination. The array primitive only has one synchronous write port, allowing one update per clock cycle. In general, the number of array write ports should match the speci ed capability of the ATS array element A. The data at the write-data port (WD) is stored to the entry selected by the write-address port (WA) on a rising clock edge if the write-enable (WE) is asserted.
A FIFO primitive has three output ports that output the oldest value ( rst), a Boolean \not full" status ( full), and a Boolean \not empty" status ( empty). The FIFO primitive also has three synchronous interfaces that change the state of the FIFO by enqueuing a new value, dequeuing the oldest value, and clearing the entire FIFO. Each synchronous interface has an enable that must be asserted at the rising clock edge for the state change to take e ect. New entries should not be enqueued to a FIFO if the FIFO is full. The exact size of the FIFO primitive does not have to be speci ed until the nal design is simulated or synthesized.

3.2.2 State Transition Logic

The next-state logic can be derived from the transitions in X in three steps:

Step 1: All value expressions in the ATS are mapped to combinational logic signals

on the current state of the state elements. In particular, this step creates a set of

sAiTgnSa.lTs,heTl1o,.g..i,c

mTMa,ppthinagt

are the signals of in this step assumes

transitions T1,...,TM of an
all required combinational

M -transition
resources are

available. RTL optimizations can be employed to simplify the combinational logic

37

π1 φ1 π2 φ2
Scheduler

πM φM

Figure 3-3: A monolithic scheduler for an M-transition ATS.

φφ21 latch enable

φ1 φ2 δ1 δ2

LE D

Q

Figure 3-4: Circuits for merging two transitions' actions on the same register.

and to share duplicated logic.
Step 2: A scheduler is created to generate the set of arbitrated enable signals,
FTig1,u..r.e, 3T-M3.,) bTahseedrefoenrencTe1,i.m..,plTeMm.en(tTathieonb'sloscckheddiualgerraamsseorftsaonsclyheodnuelersiigsnsahloinwenacihn clock cycle, re ecting the selection of one applicable transition. A priority encoder is a valid scheduler for the reference implementation.
Step 3: One conceptually creates M independent versions of next-state logic, each corresponding to one of the M transitions in the ATS. Next, the M sets of next-state
logic are merged, state-element by state-element, using the signals for arbitration.
For example, a register may have N transitions that could a ect it over time. (N M
because some transitions may not a ect the register.) The register takes on a new
value if any of the N relevant transitions is enabled in a clock cycle. Thus, the register's latch enable is the logical-OR of the signals of the N relevant transitions. The new value of the register is selected from the N candidate next-state values
via a decoded multiplexer controlled by the signals. Figure 3-4 illustrates the merging circuit for a register that is a ected by the set actions from two transitions. This merging scheme assumes at most one transition's action needs to be applied to a particular register element in a clock cycle. Furthermore, all actions of the same transition should be enabled everywhere in the same clock cycle to achieve the appearance of an atomic transition. The algorithm for generating the RTL description of a reference ATS implementation is given next.
38

3.2.3 RTL Description

Scheduler: Given an ATS with M transitions T1,...,TM, a scheduler generates ar-

bitrated transition
Ti. In any state s,

enable a valid

ssicghneadlus lerT1m,..u.,stT, Matwlehaesrte,

eTnisuisreustehdatto

select

the

actions

of

1. 2.

Ti T1

) (s)_

T..i.(s_)

TM (s)

)

T1_ ... _ TM

wschheerdeuleTri

(s) is that

the value of Ti's expression in state
selects one applicable transition per

s. A clock

priority cycle.

encoder is Since ATS

a valid allows

nondeterminism in the selection, the priority encoder could use a static, round-robin

or randomized priority.

Register Update Logic: Each R in S can be implemented using a synchronous register with clock enable (see Figure 3-2). For each R in S, the set of transitions
that update R is

f Txi j aRTxi =set(expxi) g where aRTxi is the action of Txi on R. The register's latch enable signal (LE) is
LE = Tx1 _ ... _ Txn
The register's data input signal (D) is

D = Tx1 expx1 + ... + Txn expxn

The data input enabled by Txi.

expression

corresponds

to

a

pass-gate

multiplexer

where

expxi

is

Array Update Logic: Each A in S can be implemented using a memory array
with a synchronous write port (see Figure 3-2). (Given an array implementation with su cient independent write ports, this scheme can also be generalized to support an
ATS that allows multiple array writes in an atomic step.) For each A in S, the set of
transitions that write A is

f Txi j aATxi =a-set(idxxi, dataxi) g
The array's write address (WA) and data (WD) are

WA = Tx1 idxx1 + ... + Txn idxxn WD = Tx1 datax1 + ... + Txn dataxn

39

The array's write enable signal (WE) is
WE = Tx1 _ ... _ Txn

FIFO Update Logic: Each F in S can be implemented using a rst-in, rst-out
queue with synchronous enqueue, dequeue and clear interfaces (see Figure 3-2). For each F, the inputs to the interfaces can be constructed as follows. Enqueue Interface: The set of transitions that enqueues a new value into F is

f Txi j (aFTxi =enq(expxi))_(aFTxi =en-deq(expxi)) g

Every transition pression. Hence, and enable (EE)

sTnigxoinathlTsxaitawreenilql ubeeueasssienrttoedFiifsFreiqsuailrreedadtoy

test full.

F.notfull( ) in its F's enqueue data

ex(ED)

ED = Tx1 expx1 + ... + Txn expxn
EE = Tx1 _ ... _ Txn
Dequeue Interface: The set of transitions that dequeues from F is

f Txi j (aFTxi =deq( ))_(aFTxi =en-deq(expxi)) g

Eexvperreysstiroann.siStiiomnilTarxliyw, nhoich
signal is

dequeues Txi will be

from F is asserted

required to test F.notempty( ) in if F is empty. F's dequeue enable

its (DE)

DE = Tx1 _ ... _ Txn
Clear Interface: The set of transitions that clears the contents of F is

f Txi j aFTxi =clear( ) g
F's clear enable (CE) is

CE = Tx1 _ ... _ Txn

Input and Output Ports: Thus far, input/output elements, I's and O's, have been
treated as R's. This is the only occasion where I and O elements require additional handling. To support output, the output of each O register is wired to an external output port of the same width. To support input, the net driven by the Q output of an I register is rewired to an external input port of the same width. The I register itself is only a placeholder structure that can be removed after its output connection has been rewired.

40

φMod φFlip +φMod

δ Mod,a

ce
a

δFlip,a

φ Flip

δFlip,b δ Mod,a

δFlip,b

b
ce
φ Flip

δFlip,a

=0

πFlip πMod

Figure 3-5: The GCD circuit from Example 3.4.

Example 3.4 (GCD)

A reference implementation of the ATS from Example 3.1 can be derived as follows.

Scheduler: The ATS consists of two transitions. For convenience, let the two tran-
sitions be named Mod and Flip. A two-way priority encoder su ces as a simple scheduler. The inputs to the priority encoder are

Mod F lip

= =

R(Ra<a RRb b)

^

(Rb6=

0)

The outputs of the priority enconder are Mod and Flip.

Register Update Logic: The ATS consists of two registers Ra and Rb. Ra is up-
dated by both transitions whereas Rb is only updated by the Flip transition. Thus, the latch enables for the two registers are

LERa = Mod _ Flip
LERb = Flip The registers' data inputs are

DRa = Mod (Ra-Rb) + Flip (Rb) DRb = Ra Rb is only updated by one rule, and hence, multiplexing of its data input is not required. The synthesized circuit is depicted in Figure 3-5. (The scheduler is not shown.)
2

41

3.2.4 Correctness of a Synchronous Implementation
An implementation is said to implement an ATS correctly if

1. The implementation's sequence of state transitions corresponds to some execution of the ATS.
2. The implementation maintains liveness of state transitions.

Suppose for any T the next-state logic of the reference implementation produces the

same state changes as an application of T, provided T is the only signal asserted

at the clock edge, the correctness of the reference implementation can be argued from

the properties of the priority-encoder scheduler. First, the priority encoder main-

tains liveness because the encoder asserts a signal whenever at least one signal

is asserted. (In general, liveness is maintained by any scheduler that satis es the

ccoonrrdeistpioonndsT1e(xsa)_ct.l.y._toTMan(s

)at)omiTc1

_up..d._ateTMst.)epSeocfotnhde,

an implementation clock cycle ATS because the priority en-

coder selects only one of the applicable transitions to update the state at each clock

edge. The sequence of state values constructed by sampling the reference implemen-

tation after every clock edge must correspond to some allowed sequence of ATS state

transitions.

A dimension of correctness that is not addressed in the requirements above is the

treatment of nondeterminism in an ATS. Unless the priority encoder in the reference

implementation has true randomization, the reference implementation is determin-

istic, that is, the implementation can only embody one of the behaviors allowed

by the ATS. Thus, an implementation can enter a livelock if the ATS depends on

nondeterminism to make progress. The reference implementation cannot guarantee

strong-fairness in the selection of transitions, that is, the reference implementation

cannot guarantee any one transition will always be selected eventually, regardless of

how many times the transition becomes applicable. However, using a round-robin

priority encoder as the scheduler is su cient to ensure weak-fairness, that is, if a

transition remains applicable over a bounded number of consecutive steps, it will be

selected at least once.

3.2.5 Performance Considerations
In a given atomic update step, if two simultaneously applicable transitions read and
write mutually disjoint parts of S, then the two transitions can be executed in any
order in two successive steps to produce the same nal state. In this scenario, although the ATS operational semantics prescribes a sequential execution in two atomic update steps, a synchronous hardware implementation can exploit the underlying parallelism and execute the two transitions concurrently in one clock cycle. In the case of the examples from Sections 2.3 and 2.4 where pipelined processors are described operation-centrically by stages, it is necessary to execute transitions from di erent pipeline stages concurrently to achieve pipelined executions. In general, it is dangerous to let two arbitrary transitions execute in the same clock cycle because of possible
42

data dependence and structural con icts. The next two sections, 3.3 and 3.4, formalize the conditions for the concurrent execution of transitions and suggest implementations with more aggressive scheduling schemes that execute multiple transitions in the same clock cycle.

3.3 Optimization I: Parallel Compositions

In a multiple-transitions-per-cycle implementation, the state transition in each clock

cycle must correspond to a sequential execution of the ATS transitions in some order.

If two transitions Ta and Tb become applicable in the same clock cycle when S is in

state select

sb,othTat(raTnb (ssit)i)onosr

forTbe(xeTac(ust)i)onm. uOstthbeerwtirsuee,

for an implementation to executing both transitions

correctly would be

inconsistent with any sequential execution in two atomic update steps.

of

Given
Ta and

TTbai(n

Ttbh(es)s)aomre

cTlbo(ckTac(ys c))le, .thTehree

are two approaches to execute the actions rst approach cascades the combinational

logic from the two transitions. However, arbitrary cascading does not always improve

the overall performance since it may lead to a longer cycle time. In a more practical

approach, Ta and Tb are executed in the same clock cycle only if the correct nal state

can be constructed from an independent evaluation of their combinational logic on

the same starting state.

3.3.1 Con ict-Free Transitions
Based on the intuition above, this section develops a scheduling algorithm based
on the con ict-free relationship (<>CF ). <>CF is a symmetrical relationship that
imposes a stronger requirement than necessary for executing two transitions con-
currently. However, the symmetry of <>CF permits a straightforward implementa-
tion that concurrently executes multiple transitions if they are pairwise con ict-free. (Section 3.4 will present an improvement based on a more exact condition.) The con ict-free relationship and the parallel composition function PC are de ned in De nition 3.1 and De nition 3.2.

De nition 3.1 (Con ict-Free Relationship)

Two transitions Ta and Tb are said to be con ict-free (Ta <>CF Tb) if

8 s.

Ta(s) ^

Tb (s )

)

( TTbb(( TTaa((ss))))^=

TTaa((

TTbb((ss))))

^
=

P C (s ))

where PC is the functional equivalent of PC( Ta, Tb).

2

43

De nition 3.2 (Parallel Composition)

PC ( a, b)= h pcR(aR1, bR1), ..., pcA(aA1, bA1), ..., pcF (aF1, bF1), ... i

where a=h aR1, ..., aA1, ..., aF1, ... i, b=h bR1, ..., bA1, ..., bF1, ... i

pcR(a,

b)=case

a,

b

of

a, ,

b

) )

a b

... ) unde ned

pcA(a,

b)=case

a,

b

of

a, ,b

) )

a b

... ) unde ned

pcF (a,

b)=case

a,

b

of

a, ,

b

) )

a b

enq(exp), deq( ) ) en-deq(exp)

deq( ), enq(exp) ) en-deq(exp)

... ) unde ned

2

The function PC computes a new by composing two 's that do not contain con-

icting actions on the same state element. It can be shown that PC is commutative

and associative.

Suppose Ta and Tb become applicable in the same state s. Ta <>CF Tb implies that

the two transitions can be applied in either order in two successive steps to produce

the same nal state .s0 Ta <>CF Tb further implies that an implementation could

produce s0 by applying the parallel composition of state s. Theorem 3.1 extends this result to multiple

paTiarwainsde coTnb

to the same initial ict-free transitions.

Theorem 3.1 (Composition of <>CF Transitions)

Given a collection of n transitions that are applicable in state s, if all n transitions are pairwise con ict-free, then the following condition holds for any ordering of Tx1,...,Txn.

Tx2((

^TTxxn1((sT)x)n;1

... ^
( ...

TxT3x(n (TxT2x(n;T1x(1 (.s..)))T.x.3.())Tx2=(

TPx1C((ss)))))

...

))

^

PC is the functional equivalent of the parallel composition of order.

Tx1 ,..., Txn, in any

Proof: Suppose Theorem 3.1 holds for n = K.

Part 1. Given a collection of K+1 transitions, T1,...,TK;1,TK,TK+1, that are all
applicable in state s and are pairwise con ict-free, it follows from the induction hypothesis that

TxK ( TxK;1 ( ... Tx3 ( Tx2 ( Tx1 (s))) ... ))

44

and also

(TxK+1 TxK;1 ( ... Tx3 ( Tx2 ( Tx1 (s))) ... ))

In It

footlhloewr swforordms,theTxdKe(s

)0 ^
nition

ToxfK+<1>(sC0)F

where that

s0

is

TxK;1 ( ...

Tx3 ( Tx2 ( Tx1 (s))) ... ).

(TxK+1 TxK (s0)) and hence

(TxK+1 TxK ( TxK;1 ( ... Tx3 ( Tx2 ( Tx1 (s))) ... )))

Part 2. It also follows from the induction hypothesis that

TxK ( TxK;1 (... Tx3 ( Tx2 ( Tx1 (s)))...))=(Apply(PC ( Tx1,..., TxK;1 , TxK )))(s)
and
(TxK+1 TxK;1 (... Tx3 ( Tx2 ( Tx1 (s)))...))=(Apply(PC ( Tx1,..., TxK;1 , TxK+1 )))(s) Following the de nition of PC( ), one can conclude from the second statement above
sHfsatoathTnealmxadlnKtotIeec;wt2e11easi,.(.slfeftt.aamre.hn.olrereymenTastTsxdttxt3aehKya(te+taeseT1chdx(t2oeoee(flwTdexnmnTeKoixt(1efniisr(noso0stb)n)m)ty)eho)aetfsa.hT.<c.sexataK)f>etm.+derC1resFtoisnapTtfnhxatbKoearyt+trt1ao(cfsTTTt0xxtx)eKKhK,d+++ea111onp((nsdrisT)obxtoyKnhafosu(ttsshT0a)aaxtf)Ktthcete=readTsnxadKTTmoxxn(tKKesh+0(be)a1ys(Ts^stxa0Ka)ft+TtTex1ewxK(rKohs;f+0e1)1xre,).(Ke..s.+i,0s1s)A(.0tTsnhxIi)y1ets.

= =

(AxxKKp++p11l((y(T(APxKpC(p(lTyx(TKPx;11C,(.(....,.Tx1TT,xxK3..(;.,1T,x2T(xTKxTK;x1,1,(sTx)TK)x)+K1.)..)))))())s()s)))

Therefore, the theorem holds for n = K+1 if the theorem holds for n = K. The

theorem holds for K = 2 by the de for any n greater or equal to 2.

nition of <>CF .

By induction,

Theorem 3.1 holds

2

45

3.3.2 Static Deduction of <>CF

The scheduling algorithm given later in this section can work with a conservative

<<>>CCFF

,

test, that is, if the conservative the algorithm might generate a

test fails to less optimal

identify but still

a pair correct

of transitions as implementation.

Analysis based on Domain and Range:

A of

static determination two transitions. The

of <>CF can be made
domain of a transition

by comparing the domains and
is the set of state elements in S

ranges \read"

by the expressions in either or . The domain of a transition can be further sub-

classi ed as -domain and -domain depending on whether the state element is read

by the -expression or an expression in .

The range of a transition is the set of state elements in S that are acted on by

. For this analysis, the head and the tail of a FIFO are considered to be separate

elements. The functions to extract the domain and range of a transition are de ned

below.

De nition 3.3 (Domain of and )

De(exp)

=

case exp of
constant

)

f

g

R.get( ) ) f R g

AFFFO....pnna(oor-egsttxfeetpu(mt1l(),lipd().t.x)y.,)()fe))xFp)fhnfeFa)Adtfa)gigFl hgeDadeD(geex(ipd1x)) ...

De(expn)

D

(

) = DR(aR1)
where

...
=h

aRD1A,

(aA1 ) ..., aA1

,

... ...,

aFD1 ,F.(..aiF1

)

...

DR(a)

=

case

a

of

)fg
set (exp)

)

De(exp)

DA(a)

=

case

a

of

)fg
a-set (idx ,data)

)

De(idx )

De(data)

DF

(a)

=

case

a

of

)fg eednneqq-d((ee)qx)p(e)xf)p)gD)e(Dexep(e)xp)

clear( ) ) f g

2

46

De nition 3.4 (Range of )

R

(

) = RR(aR1)
where

...
=h

aRR1A, (.a..A, 1a)A1

,

... ...,

aFR1,F.(.a. iF1)

RR (aR )

=

case

aR

of

)f
set(-)

g )

f

R

g

RA(aA )

=

case

aA

of

)fg
a-set(-,

-)

)

f

A

g

RF

(aF)

=

case

aF

of

)fg edecnlneeq-qad((re-)()q))()-))ff)FfFhFtfeahaiFdleaghgde,adF,taFiltagil

g

...

2

Using D( ) and R( ), a su cient condition that ensures two transitions are con ictfree is given in Theorem 3.2.

Theorem 3.2 (Su cient Condition for <>CF)

( (

(D( R(

T)aT)a)6(\TDaR(<( >TTaCb))F)

6\ R(
)
Tb)

Tb) ) ^ ( (D(

Tb )

D(

Tb)) 6\ R(

Ta) ) ^

2

If the domain and range of two transitions do not overlap, then the two transitions do not have any data dependence. Since their ranges do not overlap, a valid parallel composition of Ta and Tb must exist.

Analysis based on Mutual Exclusion:
If two transitions never become applicable on the same state, then they are said to be mutually exclusive.

De nition 3.5 (Mutually Exclusive Relationship)

Ta <>ME Tb if 8 s. :( Ta(s)^ Tb(s))

2

TfFoowrrot<ut>nraaMtneEsliytr,ieotqnhuseitrhesaetxdaperrteeesr<smi>oinnMiniEsgsuatshtuieasflsylyattahisecodanebjiulnintiytcitoioonfnothfofe<r>eexlCapFtrieotsnrsiaivolinaclol(ny.sTtAar(ansi)ne^txsaocTtbn(tste)hs)et.
current values of state elements. A conservative test that scans two expressions for contradicting constraints on any one state element works well in practice.

47

3.3.3 Scheduling of <>CF Transitions
Using Theorem 3.1, instead of selecting a single transition per clock cycle, a scheduler can select any number of applicable transitions that are pairwise con ict-free. In other words, in each clock cycle, the 's should satisfy the condition:

Ta ^ Tb ) Ta <>CF Tb where T is the arbitrated transition enable signal for transition T. Given a set of
applicable transitions in a clock cycle, many di erent subsets of pairwise con ict-free transitions could exist. Selecting the optimum subset requires evaluating the relative importance of the transitions. Alternatively, an objective metric simply optimizes the number of transitions executed in each clock cycle.

Partitioned Scheduler: In a partitioned scheduler, transitions in X are rst partitioned into as many disjoint scheduling groups, P1,...,Pk, as possible such that
(Ta 2 Pa) ^ (Tb 2 Pb) ) Ta <>CF Tb

Transitions in di erent scheduling groups are con ict-free, and hence each scheduling

group can be scheduled independently of other groups. For a given scheduling group

cporinotraiitnyinegncToxd1e,.r...,TIxnn,theTx1b,e..s.t,

cTaxnsec, aan

be generated from
transition T is con

icTtx1-f(rse)e,..w.,itThxne(vse)ryusointghear

transition in X . In which case, T is in a scheduling group by itself, and T= T

without arbitration.

X can be partitioned into scheduling groups by nding the connected components

of an
f (Ti,

undirected graph
Tj) j :(Ti <>CF

Twj)hgo.seEnaocdhecsoanrneecttreadnscitoimonpsonTe1n, t...i,s

TM and whose edges
a scheduling group.

are

Example 3.5 (Partitioned Scheduler)

The undirected graph (a) in Figure 3-6 depicts the con ict-free relationships in an
ATS with six transitions, X =f T1, T2, T3, T4, T5, T6 g. Each transition is represented
as a node in the graph. Two transitions are con ict-free if their corresponding nodes are connected, i.e.,

(((((TTTTT12345

<<<<<>>>>>CCCCCFFFFF

TTTTT23456))))),,,,

(((TTT123

<<<>>>CCCFFF

TTT345))),,,

(((TTT123

<<<>>>CCCFFF

TTT566

), ), ),

(T1

<>CF

T6),

Graph (b) in Figure 3-6 gives the corresponding con ict graph where two nodes are connected if they are not con ict-free. The con ict graph has three connected

48

T1 T6

Scheduling Group 1 T2 T1
T3 T6

Scheduling Group 2 T2
Scheduling Group 3 T3

T5 T4

T5 T4

(a) (b)
Figure 3-6: Scheduling con ict-free rules: (a) <>CF graph (b) Corresponding con ict
graph and its connected components.

components, corresponding
corresponding to T1, T4 and

to
T6

the three <>CF scheduling groups. The
can be generated using a priority encoding

signals of their

corresponding 's. Scheduling group 2 also requires a scheduler to ensure 2 and 5

are not asserted in the same clock cycle. However,

T3 =

T3

without any

arbitration.
2

Example 3.6 (Fully Partitioned Scheduling)

In Example 3.4, a two-way priority scheduler is instantiated for a two-transition ATS. The two transitions' -expressions are

Mod F lip

= =

R(Ra<a RRb b)

^

(Rb6=

0)

Notice, the two transitions' -expressions have contradicting requirements on the

value tions

of are

R<a>. M

F E

lip requires Ra and therefore

<R>bC, Fb.utTheMtordarnesqituiiorness

Ra<Rb.
are each

Thus in its

the two transiown scheduling

group. Hence, Mod= Mod and Flip= Flip without any arbitration.

2

Enumerated Scheduler: ttbsSrouceagchcnheaetsduthitusheielaorintTnag1sTsc1aglanor<nodn>nugTopCa6tF1sbaTroTeef64sc<gbihrsu>aetnpdCobhuFtloe(btsbdhee)ltieTnwicn1dteeeaeFpdnniegditnunhTdree6temhna3etsr-le6eyslavncomeoofsnteeata<acncli>hdoncsCoekvFtthehcwreryyreic,etlthTert.1arTanaT4nsn.hisdtAiiistoTilonts6hencsloseeauclfnegtcTihbot1eent,dhsTieesb4le,tyvhcTattrl6eheiddgee
other groups. In any given clock cycle, the scheduler for each group can independently select multiple transitions that are pairwise con ict-free within the scheduling group.

49

0T1 0T4 0T6 0T1 0T4 0T6 001001 010010 011010 100100 101101 110100 111101

Figure 3-7: Enumerated encoder table for Example 3.7.

tgbarybaaplFeho2ernwnathrnosycslhewoeoindtkohuudlpiiennstdgaNebgxrlebao1nui,ndp..d.,ewebdxnigtehcdeastbnEryabnaesrTiedxt1eido(tesen)rs,mn.T.e.ixd,n1e,aTd.x.s.nb,fT(yosxl)nl.o,nwTdsThi:nxe1g,d.a.a.,tcaliTqvxunaelcuiaennda1bn,e..u.c,ndodnmirapetuctttehedde

N E

= =

f f

T(Txxi ij,

index
Txj) j

b(Titxib2i Nis

asserted g ) ^ (Txj2N

)

^

(Txi

<>CF

Txj) g

For each Txi that is in the clique, assert data bit di.

Example 3.7 (Enumerated Encoder)

Scheduling group 1 from Example 3.5 can be scheduled by an enumerated encoder

(Figure 3-7) that allows T1 and T6 to execute concurrently.

2

3.3.4 Performance Gain
When X can be partitioned into scheduling groups, the partitioned scheduler is
smaller and faster than the monolithic encoder of the reference implementation from Section 3.2.3. The partitioned scheduler also reduces wiring cost and delay since 's and 's of unrelated transitions are not brought together for arbitration.
The property of the parallel composition function, PC, ensures that transitions are con ict-free only if their actions on state elements do not con ict. For example,
given a set of transitions that are all pairwise con ict-free, each R in S can be updated
by at most one of those transitions at a time. Hence, the state update logic described in Section 3.2.3 can be used without any modi cation. Consequently, parallel composition of actions does not increase the combinational delay of the datapath. All in all, the implementation in this section achieves better performance than the reference implementation by allowing more transitions to execute in a clock cycle without increasing the cycle time.
50

3.4 Optimization II: Sequential Compositions

Consider the following is well-de ned for any

ecxhaomiceploefwtwheoretrPaCns(itioTan,s

TTba)

and and

its
Tb

functional equivalent, P from the following ATS:

C

,

S X

= =

h f

Rh 1tr,uRe2,, h true, h true,

hhhRs3,,eits(,eRts(2eR+t(31R+)1,1+),,1)

i i i

i i i

g

pccAaorlunetvhsseiioso,utueignsnhtsgeaewclnltietitorhranalan,wtsoilTteuaiaol(dsntTsnbo(aonsrte)e)poae6=rlrdwmearTiytbso(TfaaTspaeap(qsnlui)dce)a.nTbtIbliteat,lcoatehnxeexebce<uecut>sithoCenoFwinosnfcthThtheeadaasutan,lmedfroerTpbcar.lololpHcsoke,sneccdyPecC,iln(etsh)tbeheiiesr-
concurrent execution can be allowed in an implementation. However, the concurrent

execution of all three transitions in a parallel composition does not always produce

a consistent new state due to circular data dependencies among the three transi-

tions. To capture these conditions, this section presents a more exact requirement for

concurrent execution based on the sequential composibility relationship.

3.4.1 Sequentially-Composible Transitions
De nition 3.6 (Sequential Composibility)

Two transitions Ta and Tb are said to be sequentially composible (Ta <SC Tb), if

8 s. Ta(s) ^ Tb(s) ) ( TTbb(( TTaa((ss))))^= SC(s))
where SC is the functional equivalent of SC( Ta, Tb).

2

De nition 3.7 (Sequential Composition)

SC ( a, b)= h scR(aR1, bR1), ..., scA(aA1, bA1), ..., scF (aF1, bF1), ... i

where a=h aR1, ..., aA1, ..., aF1, ... i, b=h bR1, ..., bA1, ..., bF1, ... i

scR(a,

b)=case a,

a,
)

b a

of

,b)b

... ) b

51

scA(a,

b)=case a,

a,
)

b a

of

,b)b

... ) unde ned

scF

(a,

b)=case a,

a,
)

b a

of

,b)b

enq(exp), deq( ) ) en-deq(exp)

deq( ), enq(exp) ) en-deq(exp)

a, clear( ) ) clear( )

... ) unde ned

2

The sequential composition function SC returns a new by composing actions on the same element from two 's. scR, scA and scF are the same as pcR, pcA and pcF except in two cases where SC allows two con icting actions to be sequentialized. First, scR(set(expa), set(expb)) is set(expb) since the e ect of the rst action is overwritten by the second in a sequential application. Second, scF (a, clear( )) returns clear( ) since regardless of a, applying clear( ) leaves the FIFO emptied.
SetrxuaepncTpsuhiottseiioeo<nnTSsoaCfcaaTrnneadlaaatnTlisdobonaThsrbhaeovipanepiscspotalniocraceiolbcartlxerineaisgtnpioaosnctntadoitofetn<oss>.tTChTbFa(at.<TcIanSa(Csnp)a)bT,rebtbiosucenutqllnyuaoerr,tne<tqiTuaSailC(rizeieTssdbnt(.hso)Tte)sh.cyeoTmonwrcmeuomer<trre3Sin.cC3t. extends this result to multiple transitions whose transitive closure on <SC is ordered.

Theorem 3.3 (Composition of <SC Transitions)

Given a
Txj <SC

sequence of
Txk for all j

n <

transitions,
k then

Tx1 ,...,Txn ,

that

are

all

applicable

in

state

s,

if

(

TTTxxx2nn(((

TTTxxx1nn(;;s11)(()

^
... ...

... ^
TTxx33((

TTxx22((

TTxx11((ss))))))

... ...

)) ))

^
=

SC(s))

where SC is the functional equivalent of the nested sequential composition SC ( ... SC (SC ( Tx1 , Tx2 ), Tx3), ... ).

Proof: The induction proof is similar to the proof of Theorem 3.1. Suppose Theo-

rem 3.3 holds for n = K.

Part 1. Given a sequence of K+1 transitions,

applicable in state hypothesis that

s

and

Txi

<SC

Txj

for

all

i

T<x1j,.,..i,Tt xfKo;ll1o,wTxsKf,rToxmK+1t,hethiantdaurcetiaolnl

TxK ( TxK;1 ( ... Tx3 ( Tx2 ( Tx1 (s))) ... ))

52

and also

(TxK+1 TxK;1 ( ... Tx3 ( Tx2 ( Tx1 (s))) ... ))

In It

footlhloewr swforordms,theTxdKe(s

)0 ^
nition

<ToxfK+1

(s 0
SC

) where that

s0

is

TxK;1 ( ...

Tx3 ( Tx2 ( Tx1 (s))) ... ).

(TxK+1 TxK (s0)) and hence

(TxK+1 TxK ( TxK;1 ( ... Tx3 ( Tx2 ( Tx1 (s))) ... )))

Part 2. It also follows from the induction hypothesis that

TxK ( TxK;1 (... Tx3 ( Tx2 ( Tx1 (s)))...))=(Apply(SC ( Tx1,..., TxK;1 , TxK )))(s)
and

(TxK+1 TxK;1 (... Tx3 ( Tx2 ( Tx1 (s)))...))=(Apply(SC ( Tx1,..., TxK;1 , TxK+1 )))(s) Following the de nition of SC( ), one can conclude from the second statement above
ftaAoohxTnfnlKxadleKyo+tI;wt1i1s2s1(ts.i.s(sata)fhtt.r.na.ehoe.lyHermesesleaTaetsntxmamdt3hcaty(eeeeet,nesdTaethxlfee2otoem(efwnraeniTectnxtiT1ftoeirx(sndeKos)m+toao)1hfc)n(ett<.ehb.Td.SseyxaCK)om.n(trTshesxb0ta)Ky)atp+f1aatseTrTctrxxaaKKonf++tf1T1eix(tgrcKhnaT+eonxT1rKxp(eiKsgr(+)otsnh10oo)(aef)rsset0a=)htca,hatf(aitteAoneanrpdcsTptxtlbiKhToyyxu(n(Ksss+0)TTtb1hxx(y^KKse0,)saTanTxwmxTKdKxh;eK+te1+h1ra,1.(ees.s).),0sas)(tf.0saTt0etxIi)1erts.

= =

(AxxKKp++p11l((y(T(ASxKpC(p(lTyxT(KxS;1C,1 (.(...,..Tx1TT,xxK.3.;(.,1

,TxT2xT(KxK;Tx1,1,(sT)Tx)xK)K+.)1.).))())s))())s))

Therefore, the theorem holds for n = K+1 if the theorem holds for n = K. The

theorem holds for K = 2 by the for any n greater or equal to 2.

de

nition

of

<SC .

By

induction,

Theorem

3.3

holds

2

53

3.4.2 Static Deduction of <SC
Using D( ) and R( ), a su cient condition for two transitions to be sequentially composible is given in Theorem 3.4.

Theorem 3.4 (Su cient Condition for <SC)

((D()Tb)TaD<(SCTbT))b6\R( Ta)) ^ (SC ( Ta, Tb) is de ned)

2

3.4.3 Scheduling of <SC Transitions
Incorporating the results from Theorem 3.3 into the partitioned scheduler from Section 3.3.3 allows additional transitions to execute in the same clock cycle. For each
tctahrolalentaessitdcsaettt-rreftareuendepsidstaci'osht,needleouTngyla1iinbc,.g,l.e.o,gusriTrogyunmapla,glosscuroicntThthxa1mti,nh..fia.un,trgtTThxTynexj.r1<,Ir.n.eS.q,CTeuvxiTenrrye,kysitiassf,ssjttchah<eteridcekau.mllleyHurscogthweobneseveeernaarn,tSetoCsor-dtosheriermdienaprgrlibinfoiyg-f Tz1,...,Tzn for each scheduling group such that

8 s. Tzj ^ Tzk ) Tzj <SC Tzk if j < k Scheduler:

dsTTuixorb1e,scc.e.ot.te,nTdosxtfnrgE,urSacoCtpnhetshuwecrhhs<ottsShceCoamtnsotpcdhhueeetsdegsuaralrteephrheTfgxios1rr,oa..auc.,ypTcc'xoslnincS.aCinEc-todS-rCfdwreeihseroidnssegcehuneedsdeidgunelgaisnsagEStgCorapocyoucllpoicgtiicshaatlthseocorltnatroagniensast

f (Txi, Txj) j (Txi <SC Txj )^ :(Txi <>CF Txj) g

Next, one needs to nd the connected components of an undirected graph whose
nodes are Tx1, ..., Txn and whose edges are

f (Txi, Txj) j :(((TTxxii, T<x>j)C62F TExSjC)acgyclic) ^ ((Txj, Txi) 62 ESCacyclic) ^

Each connected component forms a scheduling subgroup. Transitions in di erent

scheduling subgroups are either con ict-free or sequentially-composible. 's for the

transitions in a subgroup can be generated using a priority encoder. On each clock

cycle, the particular

stcrhaendsiutliionngsgTroy1u,p..,.,sTaytmis,fysetlhecetceodndcoitliloecntsivoeflTyhbeyoraelml

subgroup encoders of a 3.3 because the selected

transitions can always be ordered according to the static SC-ordering of the parent

scheduling group.

54

T1 T2

T1 T2

Scheduling Group 1 T1 T2

Scheduling Group 2

T3 T3

T3

T5 T4

T5 T4

T5 T4

(a) (b) (c)

FCiogrurreesp3o-n8d: inSgchaecdyuclliinc gdisreeqcuteednt<iaSllCy-cgorampphos(icb)leCorurrleess:po(nad)inDgirceocntedict<gSrCapghraapnhd

(b) its

connected components.

Example 3.8 (Sequentially Composed Implementation)

The directed graph (a) in Figure 3-8 depicts the sequential composibility relationships

in
Tb

an ATS implies

with ve transitions,
Ta <SC Tb, i.e.,

X =f

T1,

T2,

T3,

T4,

T5

g.

A

directed

edge

from

Ta

to

((((TTTT1234

<<<<SSSSCCCC

TTTT2341)))),,,

(((TTT123

<<<SSSCCC

TTT355

), ), ),

(T1

<SC

T5),

GiTs1r,raeTpmh2,o(vTbe3)d, .iTn4AFaintgoduprToel5o3.g-8(icTashlheoswoorrstdteohrfeotahfceTy4calcaiycncd<licSTC5<cgSarCanpgbhreawprhheveyenriestlehddes

edge from T4 to
the SC-ordering also.) Graph (c)

T1
of in

Figure 3-8 gives the corresponding con ict graph. The two connected components

of the tion.

con The

ict graph remaining

are the two scheduling groups. signals can be generated using

Ta3=priTo3riwtyitheonucotdainnyg

arbitraof their

corresponding 's. More transitions can be executed concurrently if the enumerated

encoder in Figure 3-9 is used instead.

2

State Update Logic:

When sequentially-composible transitions are allowed in the same clock cycle, the register update logic cannot assume that only one transition acts on a register in each clock cycle. When multiple actions are enabled for a register, the register update logic should ignore all except for the latest action with respect to the SC-ordering of some scheduling group. (All transitions that update the same register are in the same

55

0T1 0T2 0T4 0T5 0T1 0T2 0T4 0T5 00010001 00100010 00110010 01000100 01010101 01100100 01110101 10001000 10011001 10101000 10111001 11001100 11011101 11101100 11111101
Figure 3-9: Enumerated encoder table for Example 3.8.
scheduling group, except for a transition that is mutually exclusive with the rest.)
For each R in S, the set of transitions that update R is f Txi j aRTxi =set(expxi) g
The register's latch enable signal is
LE = Tx1 _ ... _ Txn
The register's data input signal is
D = Tx1 Tx1 expx1 + ... + Txn Txn expxn wtrhaenrseitioTnxsi:= :( _Ty1 _Ty2 ... ). The expression Txi contains Tyi's from the set of
f Tyi j RT:x(i2TcxRiom<( e>sTMybi)Eef^oTryei)Tgyi in the SC-ordering ^
In essence, the register's data input (D) is selected through a prioritized multiplexer that gives higher priority to transitions later in the SC-ordering. The update logic for arrays and FIFOs remain unchanged from Section 3.2.3.
56

3.5 Summary
This chapter describes the procedure to synthesize an operation-centric description given as an ATS into an e cient synchronous digital implementation. The chapter rst gives a straightforward implementation that executes one operation per clock cycle. The chapter next develops the necessary theories and algorithms to construct more optimized implementations that can carry out multiple operations concurrently while still maintaining a behavior that is consistent with a sequential execution of the atomic operations.
57

58

Chapter 4 TRSpec Hardware Description Language
The TRSpec language is an adaptation of Term Rewriting Systems (TRSs) BN98,
Klo92] for operation-centric hardware description. This synthesizable TRS language includes extensions beyond the standard TRS formalism to facilitate hardware description. On the other hand, the generality of TRS has to be restricted in some areas to ensure synthesizability. This chapter rst introduces the basic concepts in the TRS
formalism. Next, the chapter presents the TRSpec language. The discussion begins with TRSpec's type system and then moves onto the topics of rewrite rules, initialization and input/output. The concrete syntax of TRSpec is given separately in Appendix A. This chapter concludes with a discussion on how to map a TRSpec
description to an ATS.
4.1 Term Rewriting Systems
TRSs have been used extensively to give operational semantics of programming languages. More recently, TRSs have become a powerful tool, in research and in classrooms, to concisely describe the operation of computer hardware. For example, TRSs have made it possible to describe a processor with out-of-order and speculative execution in a page of text AS99]. Such hardware descriptions in TRSs are also amenable to formal veri cation techniques.
A TRS consists of a set of terms and a set of rewrite rules. The general structure of a rewrite rule is
!patlhs if exppred exprhs
A rule can be applied to rewrite a term s if the rule's left-hand-side pattern patlhs matches s (or a subterm in s) and the predicate exppred evaluates to true. A successful pattern match binds the variables in patlhs to the corresponding subterms in s. When a rule is applied, the new term is constructed by replacing the whole or part of s that matched patlhs with the resulting term from evaluating exprhs.
59

The e ect of a rewrite is atomic, that is, the whole term is \read" in one step and if the rule is applicable then a new term is returned in the same step. If several rules are applicable, then any one of them is chosen nondeterministically and applied. Afterwards, all rules are re-evaluated for applicability on the new term. Starting from a source term, successive rounds of rewrites continue until the nal term cannot be rewritten using any rule. TRSs that describe hardware are sometimes nondeterministic (\not con uent" in the programming language parlance) and nonterminating.
In a functional interpretation, a rule can be thought of as the function:
s. case s of
patlhs ) if exppred then
exprhs
else
s
... ) s
This function uses a pattern-matching case statement in which two patterns, patlhs and `...', are checked sequentially against s until the rst successful match. If patlhs matches s, the variables in patlhs are bound to the subterms in s, and the function returns the evaluation of the consequent expression exprhs. If patlhs fails to match s, the wild-card pattern `...' always matches s successfully, and the function returns a term identical to s.
4.2 TRS for Hardware Description
Chapter 2 uses the TRS notation to describe the behaviors of nite state machines. In those descriptions, the collective values of the state elements are symbolically represented as terms. Each rewrite rule describes an atomic operation by specifying when the operation could take place and what is the new value of the state elements afterwards. In general, a TRS for hardware description has to be restricted to have a nite number of nite-size terms, and the rewrite rules cannot change the size of the terms. These restrictions guarantee that a TRS can be synthesized using a nite amount of hardware. These restrictions can be enforced by augmenting the TRS formalism with a type system.
The rest of this chapter presents TRSpec, a strongly-typed TRS language for operation-centric hardware description. TRSpec features built-in integers, common
arithmetic and logical operators, non-recursive algebraic types and two built-in abstract datatypes, array and FIFO. User-de ned abstract datatypes, with both se-
quential and combinational functionalities, can also be imported into a TRSpec
description by providing an interface declaration and a separate implementation description.
60

4.3 Type System
The TRSpec language is strongly typed, that is, every term has a type that is dened by the user. The allowed type classes in TRSpec are listed below.
TYPE :: STYPE ] CPRODUCT ] ABSTRACT ] IOTYPE
CPRODUCT :: Cnk(TYPE1, ..., TYPEk) where k > 0
ABSTRACT :: Array STYPEidx] STYPE ] Fifo STYPE
IOTYPE :: ITYPE ] OTYPE

4.3.1 Simple Types
Simple types (STYPE) in TRSpec include built-in booleans, signed and unsigned
integers, as well as algebraic product and sum types. An integer type can be declared to have an arbitrary word width. Each product type has a unique constructor name and consists of one or more subtypes. A sum type, a disjoint union type, is made up of two or more disjuncts. A disjunct is similar to a product except a disjunct may have zero elements. A sum type with only zero-element disjuncts is an enumeration type. Product and sum types can be composed to construct arbitrary algebraic type structures, but recursive types are not allowed. Members of the STYPE class are listed below.

STYPE :: Bool

] Bit N]

] Int N]

] PRODUCT

] SUM

PRODUCT SUM

:: ::

CDnISkJ(SUTNYCPTEj1j,

..., STYPEk) DISJUNCT

where

k

>

0

] DISJUNCT jj SUM

DISJUNCT :: Cnk(STYPE1, ..., STYPEk) where k 0

Example 4.1 (Euclid's Algorithm)
The terms in the TRS from Section 2.1 has the type declared as GCD below.

Type GCD = Gcd(NUM, NUM) Type NUM = Bit 32] A GCD term ts the signature Gcd(a,b), where a and b are unsigned 32-bit integers.
2

61

Example 4.2 (Alternate Description of Euclid's Algorithm)
This example gives an alternate description of Euclid's Algorithm to illustrate some modularity and type-related issues. The following TRS describes the computation of the mod function via iterative subtraction.
Type VAL = Mod(NUM,NUM) jj Val(NUM)
Type NUM = Bit 32] Mod Iterate Rule:
Mod(a,b) if a b ! Mod(a-b,b)
Mod Done Rule:
Mod(a,b) if a<b ! Val(a)
Using this description of mod, Euclid's Algorithm can be rewritten as follows:

Type GCD = Gcd(VAL,VAL)
Flip & Mod Rule:
Gcd(Val(a), Val(b)) if b6=0 ! Gcd(Val(b), Mod(a,b))
To nd the greatest common divisor of two unsigned integers, Ca and Cb, the source term should be Gcd(Val(Ca),Val(Cb)). The normal term is Gcd(Val(Cgcd),Val(0)) where Cgcd is the greatest common divisor of Ca and Cb. The sequence of rewrites to nd the GCD of 2 and 4 is

Gcd(Val(2),Val(4)) ! Gcd(Val(4),Mod(2,4)) ! Gcd(Val(4),Val(2)) ! Gcd(Val(2),Mod(4,2)) ! Gcd(Val(2),Mod(2,2)) ! Gcd(Val(2),Mod(0,2)) !
Gcd(Val(2),Val(0))

2

4.3.2 Abstract Types
Abstract datatypes are de ned by their interfaces, without information about its internal operation or implementation. An interface can be classi ed as performing
either a combinational operation or a state-transforming operation. TRSpec has
built-in abstract datatypes of arrays and FIFOs. An array is used to model register les, memories, etc. An array term has only
two operations, read and write. In a rule, if a is an array, a idx] gives the value stored in the idx'th location of a, and a idx:=v], a state-transforming \write", is an array
identical to a except location idx has been changed to the value v. TRSpec supports
arrays of STYPE entries indexed by enumeration-type or integer-type indices. FIFOs provide the primary means of communication between di erent modules
and pipeline stages. The two main state-transforming operations on FIFOs are enqueue and dequeue. Enqueuing an element e to a FIFO q is written as q.enq(e) while dequeuing the rst element from q is written as q.deq( ). An additional statetransforming interface q.clear( ) clears the contents of q. The combinational query interface q. rst( ) returns the value of the oldest element in q. FIFOs are restricted to

62

Type PROC = Proc(PC, RF, IMEM, DMEM) Type PC = Bit 16] Type RF = Array RNAME] VAL
Type RNAME = Reg0( ) jj Reg1( ) jj Reg2( ) jj Reg3( )
Type VAL = Bit 16] Type IMEM = Array PC] INST Type INST = Loadi(RNAME, VAL)
jj Loadpc(RNAME) jj Add(RNAME, RNAME, RNAME) jj Sub(RNAME, RNAME, RNAME) jj Bz(RNAME, RNAME) jj Load(RNAME, RNAME) jj Store(RNAME, RNAME)
Type DMEM = Array ADDR] VAL Type ADDR = Bit 16]
Figure 4-1: Type de nitions for a simple non-pipelined processor.
have STYPE entries. In the description phase, a FIFO is abstracted to have a nite
but unspeci ed size. A TRSpec rule that uses a FIFO's interfaces has an implied
predicate that tests whether the FIFO is not empty or not full, as appropriate.
TRSpec allows abstract types to be included in the construction of new product
types, but an abstract type cannot be included in the algebraic type hierarchy de-
scending from a sum type. Hence, TRSpec distinguishes between a complex product
type (CPRODUCT) versus a simple product type (PRODUCT), depending on whether or not its algebraic structure contains an abstract type. Complex product types cannot be included in the disjunct of a sum type.
Example 4.3 (Simple Processor)
The TRS in Section 2.2 describes a simple ISA whose programmer-visible states consist of a program counter, a register le, instruction memory and data memory. The terms from that TRS have the types declared in Figure 4-1. A term that captures the processor state has the type PROC. Instruction terms have the type INST.
2
Example 4.4 (Pipelined Processor)
The TRS in Section 2.4 describes a two-stage pipelined processor. The processor performs instruction fetch and decode in the rst stage and instruction execute in the second stage. A FIFO stores decoded instruction templates between the two pipeline
63

stages. A term that captures the state of this pipelined processor has the type:

Type PROC = Proc(PC, RF, BF, IMEM, DMEM)

The type of the abstract FIFO (BF) and the type of the instruction templates (ITEMP) are de ned as

Type BF = Fifo ITEMP Type ITEMP = TLoadi(RNAME, VAL)
jj TAdd(RNAME, VAL, VAL) jj TSub(RNAME, VAL, VAL) jj TBz(VAL, PC) jj TLoad(RNAME, ADDR) jj TStore(ADDR, VAL)

2

4.4 Rewrite Rules
4.4.1 Abstract Syntax
Syntactically, a TRSpec rewrite rule is composed of a left-hand-side pattern and
a right-hand-side expression. A rule can optionally include a predicate expression and where bindings. The where bindings on the left-hand-side have pattern-matching
semantics. A failure in matching PATi to EXPlhs i in the i'th left-hand-side binding
also deems the rule inapplicable to a term. The right-hand-side where bindings are
simple irrefutable variable assignments. The concrete TRSpec syntax is given in Appendix A. The abstract syntax of a TRSpec rewrite rule is summarized below.
(`{' is the don't-care symbol.)
RULE :: LHS ! RHS
LHS :: PATlhs if EXPpred] where PAT1=EXPlhs 1,...,PATn=EXPlhs n] PAT :: `{' ] variable ] numerical constant ] Cn0( ) ] Cnk(PAT1,...,PATk) RHS :: EXPrhs where variable1=EXPrhs 1,...,variablen=EXPrhs m] EXP :: `{' ] variable ] numerical constant ] Cn0( ) ] Cnk(EXP1,...,EXPk)
] Primitive-Op (EXP1,...,EXPk) Primitive-Op :: Arithmetic ] Logical ] Array-Access ] FIFO-Access
4.4.2 Type Restrictions
The type of PATlhs must be PRODUCT, CPRODUCT or SUM. In addition, each rule must have PATlhs and EXPrhs of the same type. In Example 4.2, the Mod Done rule rewrites a Mod term to a Val term. The Mod Done rule does not violate this type discipline because both a Val term and a Mod term belong to the same sum type.
64

TRSpec's non-recursive type system and the type disciplines on rewrite rules ensure
the size of every term is nite and a rewrite does not change the size of the terms.

4.4.3 Semantics
In a functional interpretation, a TRSpec rewrite rule can be thought of as a function of typeof(PATlhs)!typeof(PATlhs). The function returns a term identical to the
input term if the rule is not applicable. If the rule is applicable, the return value is a new term based on the evaluation of the main right-hand-side expression EXPrhs. A rule of the form:

patlhs

!

if exppred where pat1 = explhs 1, ..., patn = explhs n
exprhs

where var1 = exprhs 1, ..., varm = exprhs m

corresponds to the function:

s. case s of
patlhs )

case

)explhs 1
pat 1

of

......

case epxaptinlfhs)enxpofpred then

let

var 1 = exprhs 1, ..., var m = exprhs m

in

else exprhs

......
... ) s ... ) s

s
... ) s

65

The previous function can be decomposed into its two components, and , where

= and

s. case s of
patlhs )

case

)explhs 1
pat 1

of

......

case ep..xa.pt)nlhs)fnaolesfxeppred

......

... ) false

... ) false

= s. let patlhs = s pat1 = explhs 1, ..., patn = explhs n var 1 = exprhs 1, ..., var m = exprhs m
in

exprhs

The function determines a rule's applicability to a term and has the type signature

typeof(pat typeof(pat

llhhss))!!Btyoople. oOf(npathtlehso)t,hdeertehramnidn,esthtehe

function, new term

with the type signature in case (s) evaluates to

true. Using and , a rule can also be written as the function:

s. if (s) then (s) else s

4.5 Source Term
A TRSpec description includes a source term to specify the initial state of a system.
The top-level type of the system is inferred from the type of the source term, which must be PRODUCT, CPRODUCT or SUM. A source term can specify an initial value for every subterm that is not an ABSTRACT type. A source term can include don'tcare symbols to leave some subterms unconstrained. Unless otherwise speci ed, the initial contents of an abstract array term are unde ned. An abstract FIFO term is initially empty.

4.6 Input and Output
Traditionally, a TRS models a closed system, that is, it does not interact outside of the
modeled system. TRSpec augments the TRS formalism to allow users to assign I/O semantics to STYPE terms. For example, the TRSpec fragment in Figure 4-2 can

66

Type TOP = Top(CNTRI, NUMI, NUMI, NUMO, GCD) IType CNTRI = CNTR
Type CNTR = Load( ) jj Run( )
IType NUMI = NUM OType NUMO = NUM GCD Start Rule:
Top(Load( ), x, y, {, {)
! Top(Load( ), x, y, 0, Gcd(Val(x),Val(y)))
GCD Done Rule: Top(Run( ), x, y, {, Gcd(Val(gcd), Val(0)))
! Top(Run( ), x, y, gcd, Gcd(Val(gcd), Val(0)))
Figure 4-2: TRSpec description of I/O
be combined with the TRSpec design from Example 4.2 to provide I/O capabilities.
When synthesized as hardware, this I/O wrapper description maps to a module with the I/O ports shown in Figure 4-3.
TOP is a collection of several I/O types along with GCD. NUMO is an output type derived from NUM. An output type can be derived from any STYPE, and the output type itself can be included in the construction of new CPRODUCT types. In the
implementation of a TRSpec description, the value of an output term is externally
visible. On the other hand, NUMI is an input type derived from NUM. Like an output
type, an input type can also be derived from any STYPE and be included in the construction of new CPRODUCT types. The value of an input term can only be rewritten by an agent external to the system. The e ect of an externally-triggered rewrite appears spontaneously but atomically between the atomic e ects of rewrite rules. Rewrite rules inside the system are not allowed to change the value of an input
term. In the implementation of a TRSpec description, input interfaces are created
to control the values of the input terms.
Although the TRSpec compiler has some liberty in the exact implementation of
the input and output interfaces, as a rule of thumb, one should be able to connect the output port from a correctly implemented module to the input port of another correctly implemented module, as long as the ports are derived from the same type.
The GCD Start and GCD Done rules describe the input and output operations. Given a TOP term, the GCD Start rule states that when the rst subterm is Load( ), the GCD subterm can be initialized using the second and third subterms. (The rst three subterms of a TOP term are all input terms.) The GCD Done rule states if the rst subterm is Run( ) and the GCD subterm looks like Gcd(Val(a),Val(0)) then the
67

CNTRL

NUM1 32
NUM2 32

NUMO 32

Figure 4-3: I/O interfaces synthesized for TOP.

NUMO output subterm should be set to a to report the answer.
Using the symbol `,!' to represent externally-triggered rewrites, a possible se-
quence of rewrites that computes the GCD of 2 and 4 is
.... !Top(Run(),9,3,3,Gcd(Val(3),Val(0))) ,!Top(Load(),2,4,3,Gcd(Val(3),Val(0))) !Top(Load(),2,4,0,Gcd(Val(2),Val(4))) ,!Top(Run(),2,4,0,Gcd(Val(2),Val(4))) !Top(Run(),2,4,0,Gcd(Val(4),Mod(2,4))) !Top(Run(),2,4,0,Gcd(Val(4),Val(2))) !Top(Run(),2,4,0,Gcd(Val(2),Mod(4,2))) !Top(Run(),2,4,0,Gcd(Val(2),Mod(2,2))) !Top(Run(),2,4,0,Gcd(Val(2),Mod(0,2))) !Top(Run(),2,4,0,Gcd(Val(2),Val(0))) !Top(Run(),2,4,2,Gcd(Val(2),Val(0))) ,! ....
At the start of the sequence, the rst external rewrite modi es the rst three subterms of Top(Run( ),9,3,3,Gcd(Val(3),Val(0))) to Top(Load( ),2,4,3,Gcd(Val(3),Val(0))). Afterwards, the GCD Start rule is applied to set the GCD subterm to Gcd(Val(2),Val(4)). Another external rewrite changes the rst subterm from Load( ) to Run( ) this disables the GCD Start rule. Applications of the rules from Example 4.2 ultimately reduce the GCD subterm from Gcd(Val(2),Val(4)) to Gcd(Val(2),Val(0)). At this point, the GCD Done rule becomes enabled and rewrites the NUMO subterm from 0 to 2.
Due to nondeterminism, many sequences are possible for the same external manip-
ulations of the input subterms. Therefore, I/O handshakes in a TRSpec description
must be asynchronous by design. For example, after the rst external rewrite there is no guarantee on how soon the GCD Start rule is applied. Hence, the external agents should not trigger the second external rewrite until it has observed a transition to 0 on the NUMO output. Also before the second external rewrite, the GCD Start rule could be applied several more times. Moreover, the GCD rules from Example 4.2 could also start rewriting the GCD subterm. Nevertheless, regardless of the sequence of events between the rst and the second external rewrites, this TRS produces the correct answer when the NUMO output term transitions out of 0.

4.7 Mapping TRSpec to ATS
The TRSpec type discipline requires each rewrite rule to have the same type on both sides of !. Therefore, all terms reachable from the source term by rewriting must have the same type. Given TRSpec's non-recursive type system, the terms in
68

GCD

VAL

Tag Mod

Val

NUM

NUM

NUM

‘1,Mod,2’

VAL

Tag Mod

Val

NUM

NUM

NUM

Figure 4-4: A tree graph representing the GCD type structure from Example 4.2.

a TRSpec TRS can always be encoded as the collective value of a nite set of ATS
state elements.

4.7.1 Terms to ATS State Elements

The ATS state elements needed to represent all possible terms of the same type as

the source term can be conceptually organized in a tree graph according to the source

term's algebraic type structure. In this tree, the ATS state elements, R, A and F,

appear at the leaves. For example, the tree graph of the algebraic type, GCD, from

Example 4.2 is shown in Figure 4-4. A product node, like the one labeled GCD, has a

child branch for each constituent subtype. A sum node, like the one labeled VAL, has

a branch for each disjunct. \tag register" node records

A sum which

node also has an extra
one of the d disjuncts

branch where a dlog2de-bit
is active. A disjunct node,

like the one labeled Mod, has a child branch for each of its constituent subtypes. In

this case, the Mod disjunct node has two leaf children that are 32-bit register nodes

corresponding to the 32-bit integer type NUM.

A sum node has a branch for each of the disjuncts, but, at any time, only the

branch whose tag matches the content of the tag register holds meaningful data. For

example, the active subtree corresponding to the term Gcd(Val(2), Mod(4,2)) is shaded

in Figure 4-4. As an optimization, registers on di erent disjuncts of a sum node can

share the same physical register. In Figure 4-4, the registers aligned horizontally can

be allocated to the same physical register.

A unique name can be assigned to each ATS state element based on the path (also

known as projection) from the root to the corresponding leaf node. For example, the

name for the second (from the left) NUM register in Figure 4-4 is `1 Mod 2'. Following

this naming scheme, the ATS state elements needed by a type can also be organized

as a set of name/state-element pairs where each pair h proj, e i speci es an ATS state

element e and its name proj.

69

The storage elements (and their names) needed to represent all possible term instances of GCD are given by the set:

f

h `1 h `2

tag', h `1 tag', h `2

iR1;bit ,
Mod 1',
iR1;bit ,
Mod 1',

R32;bit R32;bit

i, i,

h `1 h `2

Mod Mod

2', 2',

R32;bit R32;bit

i, i,

h `1 h `2

Val Val

1', 1',

R32;bit R32;bit

i, ig

Every name/state-element pair in this set has a one-to-one correspondence with a

leaf node in the tree graph in Figure 4-4.

4.7.2 Rules to ATS Transitions
With a nearly identical execution semantics as an ATS transition, a TRSpec rewrite
rule readily maps to an ATS transition. Using the functional interpretation of rules presented in Section 4.4.3, the and functions for the Flip&Mod rule from Example 4.2 are
= s. case s of
Gcd(Val(a),Val(b)) ) b6=0 ... ) false
and
= s. let Gcd(Val(a),Val(b)) = s
in Gcd(Val(b),Mod(a,b))
The function of a rule maps to the expression of an ATS transition. In an ATS, the condition computed by (s) is speci ed as a logic expression that tests the contents of the ATS state elements. The expression implied by the pattern matching and the predicate of the Flip&Mod rule is
(R1 tag = Val) ^ (R2 tag = Val) ^ (R2 V al 1 6= 0)
The function of a rule can be viewed as specifying actions on the ATS state elements. The function of the Flip&Mod rule can be mapped to the following set of
state-element/action pairs, h e, action i. (Elements with null actions are omitted.) f h R1h tRag2,tasge,ts(eVta(lM) io,dh)Ri,1 VhaRl 12,Msoedt1(,bs)eit,(a) i, h R2 Mod 2, set(b) i g
The variables a and b in the Flip&Mod rule's function are bound to the subterms in the input term s by pattern matching. In the ATS context, a and b are the contents of the corresponding ATS storage elements, namely R1 V al 1 and R2 V al 1.
Notice, the expression of this transition requires R1 tag=Val. Thus, the action
70

h R1 tag,
sition's

set(Val) i
predicate.

in the set above is The set action on

redundant in R1 tag can be

any state that satis es the tranreplaced by a null action without

changing the semantics of the transition. In general, the predicate of a transition

restricts the possible starting values of the state when the transition is applied. A

compiler can eliminate an action from a transition if the compiler can statically detect

that the action has no e ect on its state element anytime the transition's predicate

is satis ed.

In another example, the pipelined processor TRS from Example 4.4 requires the

following set of storage elements:

f h `1', RPC i, h `2', ARF i, h `3', FBF i, h `4', AIMEM i, h `5', ADMEM i g

The transition corresponding to the Fetch rule has the following actions:

f h RPC, set(RPC+1) i, h FBF , enq(AIMEM RPC]) i g

The transition corresponding to the Add Execute rule has the following actions:

f h ARF , a-set(ARF r1]+ARF . r2]) i, h FBF , deq( ) i g

4.7.3 Local Rules
A local rule may be applicable to many di erent parts of the top-level term. In these cases, a local rule maps to multiple ATS transitions. It is as if a local rule is replaced by multiple lifted instances of the rule, one for each possible application site. The effect of applying a particular lifted instance to the whole term is the same as applying the original local rule to the corresponding local site. For example, the Mod Done rule from Example 4.2 can be applied to both the rst and second subterms of a GCD term. The two lifted instances of the Mod Done rule are
Gcd(Mod(a,b),x) if a<b ! Gcd(Val(a),x)
and
Gcd(x,Mod(a,b)) if a<b ! Gcd(x,Val(a))

4.8 Summary
This chapter presents the TRSpec operation-centric hardware description language. The TRSpec language is an adaptation of the TRS notation to enable concise and precise descriptions of hardware behavior. The TRSpec language includes both
extensions and restrictions on the standard TRS formalism to increase its usefulness
71

in hardware descriptions. The concrete syntax of TRSpec is given in Appendix A. This chapter also shows how to translate a TRSpec description to an equivalent
ATS. 72

Chapter 5 Examples of TRSpec Descriptions and Synthesis
The procedures for synthesizing operation-centric descriptions, described in Chapter 3, has been implemented in the Term Rewriting Architectural Compiler (TRAC).
TRAC accepts TRSpec descriptions and outputs synthesizable structural descrip-
tions in the Verilog Hardware Description Language TM96]. This chapter presents
the results from applying TRAC to TRSpec examples. To produce the nal imple-
mentations, the TRAC-generated Verilog descriptions are further compiled by commercial hardware compilers to target a number of implementation technologies. The quality of TRAC-generated implementations is evaluated against reference implementations synthesized from hand-coded Verilog structural descriptions.
5.1 Euclid's Algorithm
TRSpec and TRAC combine to form a high-level hardware development framework.
Staying within the TRS formalism, even someone without a digital design background can capture an algorithm and produce a hardware implementation.
5.1.1 Design Capture
The TRSpec language o ers a level of abstraction and conciseness that is comparable
to high-level programming languages. Example 4.2 gives a description of Euclid's Al-
gorithm as a TRS. The description is reiterated in Figure 5-1 using concrete TRSpec syntax. This TRSpec description compares favorably against a software implementation of the same algorithm in Scheme (shown in Figure 5-2). In both TRSpec and
Scheme, a designer/programmer can rely on the high-level language abstractions to
express the algorithm in a direct and intuitive way. Notice that both the TRSpec
and the Scheme descriptions are easy to understand even without comments. In comparison, a hand-coded Verilog register-transfer level (RTL) description of
Euclid's Algorithm (excerpt shown in Figure 5-3) cannot be created or understood by someone who is not familiar with synchronous logic design. Besides the information
73

Type GCD = Gcd(VAL, VAL) Type VAL = Mod(NUM, NUM) || Val(NUM)
Type NUM = Bit 32]
Rule "Flip & Mod" Gcd(Val(a), Val(b)) if b!=0 ==> Gcd(Val(b), Mod(a,b))
Rule "Mod Iterate" Mod(a,b) if a>=b ==> Mod(a-b,b)
Rule "Mod Done" Mod(a,b) if a<b ==> Val(a)
Figure 5-1: TRSpec description of Euclid's Algorithm.
inherent to the algorithm, an RTL designer must also inject information about synchronous hardware implementation, such as exactly how the algorithm is scheduled over multiple clock cycles.
5.1.2 Debugging
As discussed in Section 2.1, the correctness of a TRSpec description can be es-
tablished by verifying the correctness of each rewrite rule independently. Each rule only has to individually maintain the invariant that given any valid term, if enabled, the rule must produce another valid term. TRAC guarantees that the synthesized implementation behaves according to some valid sequence of atomic rewrites, thus producing a corresponding sequence of valid states. In practice, TRAC also helps
designers uncover many mistakes by type checking the TRSpec sources.
Once the correctness of each individual rule is veri ed, the remaining mistakes are likely to be either due to unintended nondeterminism or missing rules. Executing a TRS with unintended nondeterminism can result in a livelock where the rewrite sequence repeats without making progress. On the other hand, the rewrite sequence of a TRS with an incomplete set of rules can terminate prematurely. The TRAC compiler can assist in debugging these two classes of errors by generating a simulatable Verilog description. The simulatable description has the same state structure and cycle-by-cycle behavior as the synthesizable description. However, the simulatable description can be instrumented to print an on-screen warning, or halt the simulation, whenever multiple rules are enabled in the same clock cycle. A designer can then verify if the nondeterminism exists by design. Likewise, the simulatable Verilog description can also issue a warning when none of the rules are enabled in a clock cycle. The designer can examine the state of a stopped simulation and determine if new rules
74

(define (gcd a b) (if (= b 0) a (gcd b (iter-remainder a b))))
(define (iter-remainder a b)
<(if ( a b)
a (iter-remainder (- a b) b)))
Figure 5-2: Scheme implementation of Euclid's Algorithm.
are needed to complete the description.
5.1.3 Synthesis Results
The algorithmic description in Figure 5-1 needs to be combined with the TRSpec
description of a top-level I/O wrapper (similar to the example in Section 4.6) to form
a usable hardware module. TRAC can compile the combined TRSpec description
into a synthesizable Verilog RTL description in less than 2 seconds on a workstation with a 500 MHz Pentium III processor. The TRAC-generated Verilog description can then be compiled for implementation on a Xilinx XC4010XL-09 FPGA Xila] using the Xilinx Foundation 1.5i tools Xilb]. For this example, the compilation time in the Xilinx tools is about 10 minutes.
The table in Figure 5-4 reports the circuit characteristics of the synthesized FPGA implementation in terms of the number of ip- ops, the overall FPGA resource uti-
lization, and the maximum clock frequency. The row labeled \TRSpec" in Figure 5-4 characterizes the implementation synthesized from the TRSpec description.
For comparison, an implementation is also synthesized from the hand-coded Verilog RTL description (Figure 5-3) its circuit characteristics are reported in the row labeled \Hand-coded RTL". To account for the algorithmic di erences between the two designs, the last column of the table reports the number of clock cycles needed to compute the GCD of 590,111,149 and 998,829,163.1
The results indicate that the implementation synthesized from the TRSpec de-
scription is much worse than the version synthesized from the hand-coded Verilog RTL description. In comparison to the hand-coded RTL design, the TRAC-synthesized design requires 38 additional bits of storage, has a longer critical path, and needs an extra cycle for every remainder computed. This di erence in circuit quality and
1590111149=53857 10957 and 998829163=91159 10957. 53857, 10957 and 91150 are prime numbers.
75

module GCD ( Gcd, Done, Start, Clk, Reset, Mode, A, B
)

// Outputs // Inputs

........

reg 0:31] reg 0:31]

x y

........

always @ (posedge Clk or negedge Reset) begin if (!Reset) begin x <= 0 y <= 0 end else if (newStart) begin x <= A y <= B end else if (x >= y) begin x <= x - y y <= y end else begin x <= y y <= x end end

........

endmodule

(Courtesy of D. L. Rosenband.)

Figure 5-3: Excerpt from the hand-coded Verilog RTL description of Euclid's Algorithm.

76

Version

FF Util. Freq. Elapse (bit) (%) (MHz) (cyc)

TRSpec

102 38 31.5 104

Hand-coded RTL

64 16 53.1

54

TRSpec (Optimized)

64

20

44.2

54

Figure 5-4: Summary of GCD synthesis results.
performance is a consequence of the trade-o between performance and ease of development in a high-level design framework. In creating a hand-coded RTL description, a human designer needs to do extra work to inject some minimal amount of implementation-related information, but, at the same time, the RTL designer can also inject \creative" optimizations to achieve a smaller and simpler implementation. TRAC does not have the same ingenuity after all, TRAC can only synthesize what has been described and cannot invent new mechanisms. For simple designs, a human designer can almost always create a better circuit directly in a lower-level abstraction than relying on automatic synthesis from a high-level abstraction. However, as the design complexity increases, the bene t of low-level hand optimizations quickly diminishes while the required e ort increases dramatically. The advantage of highlevel design and synthesis becomes more prominent in larger designs where managing complexity and concurrency becomes the dominant problem. This trend is already evident in the development of a simple processor in the next section.
Although TRAC cannot match the ingenuity of a human designer, the TRSpec
language does not prevent a performance-conscious designer from adding sourcelevel optimizations to a description. With an understanding of how TRAC maps
a TRSpec description to its RTL implementation, a designer can in uence the
synthesized outcome by making changes to the source-level description. Figure 5-5
gives another TRSpec description of Euclid's Algorithm that has been formulated to
match the hand-coded RTL design. (The rules and type de nitions in Figure 5-5 are rst shown in Section 2.1 and Example 4.1.) The implementation of the optimized
TRSpec description shows a large improvement over the more literal algorithmicstyle description. The implementation synthesized from the optimized TRSpec description is characterized in the row labeled \TRSpec (Optimized)" in Figure 5-4.
The source-level optimizations result in an implementation that is within 25% of the hand-coded RTL design in terms of overall circuit size and 17% in terms of circuit speed.
This design example serves to demonstrate the TRSpec design ow for imple-
menting an algorithm in hardware and also to compare the TRAC-synthesized implementations against traditional hand implementations. One caveat of this example is that a software implementation of Euclid's Algorithm on the latest microprocessor
77

Type GCD = Gcd(NUM, NUM) Type NUM = Bit 32]
Rule "Mod" Gcd(a,b) if (a>=b) && (b!=0) ==> Gcd(a-b,b)
Rule "Flip" Gcd(a,b) if (a<b) ==> Gcd(b,a)
Figure 5-5: Another TRSpec description of Euclid's Algorithm.
would, in fact, run faster than any FPGA implementation, provided the software implementation can directly compute the mod function on the processor's oating-point
unit. For algorithm synthesis, the TRSpec framework is better suited for problems
with a high degree of ne-grain concurrency and bit-level operations.
5.2 A Simple Processor
The rules and type de nitions from Section 2.2 and Example 4.3 describe a simple Instruction Set Architecture (ISA). As an architectural description, it is convenient to model the instruction memory and the data memory as abstract arrays internal to the system. As is, the description can be synthesized to a processor with an internal instruction ROM and an internal data RAM. However, in a realistic design, the processor module should access external memory modules through input and output ports.
5.2.1 Adaptation for Synthesis
A new processor description in concrete TRSpec syntax is given in Figures 5-6 and 5-
7. The rules and type de nitions are derived directly from the architectural-level ISA description of Section 2.2 and Example 4.3. The new description synthesizes to a processor with instruction and data memory interfaces shown in Figure 5-8.
Instruction Memory Interface:
In the new de nition of PROC, internal memory arrays have been replaced by I/O subterms. (The semantics of I/O types is described in Section 4.6.) PC O is an output type derived from PC. The PC O subterm in a PROC term is the program counter. Its value appears on a corresponding output port of the synthesized processor. INSTPORT is an input type derived from INST. The value of the INSTPORT subterm in a PROC term is taken from a corresponding input port of the synthesized processor. The
78

Type

PROC = Proc(PC O, RF, INSTPORT, RPORT, WPORT)

OType

PC O = PC

Type

PC = Bit 16]

Type

RF = Array RNAME] VAL

Type

RNAME = Reg0( ) || Reg1( ) || Reg2( ) || Reg3( )

Type

VAL = Bit 16]

IType INSTPORT = INST

Type

INST = Loadi(RNAME,VAL)

|| Loadpc(RNAME)

|| Add(RNAME,RNAME,RNAME)

|| Sub(RNAME,RNAME,RNAME)

|| Bz(RNAME,RNAME)

|| Load(RNAME,RNAME)

|| Store(RNAME,RNAME)

Type

RPORT = Rport(ADDR O, VAL I, BUSY)

OType

ADDR O = ADDR

Type

ADDR = Bit 16]

IType

VAL I = VAL

Type

BUSY = Busy( ) || NotBusy( )

Type

WPORT = Wport(ADDR O, VAL O, ENABLE O)

OType

VAL O = VAL

OType ENABLE O = ENABLE

Type

ENABLE = Enable( ) || Disable( )

Figure 5-6: TRSpec type de nitions for a processor with instruction and data mem-
ory interfaces.

79

Rule "Loadi" Proc(pc,rf,inst,rport,wport) where Loadi(rd,const)=inst
==> Proc(pc+1,rf rd:=const],inst,rport,wport) Rule "Loadpc"
Proc(pc,rf,inst,rport,wport) where Loadpc(rd)=inst ==> Proc(pc+1,rf rd:=pc],inst,rport,wport) Rule "Add"
Proc(pc,rf,inst,rport,wport) where Add(rd,r1,r2)=inst ==> Proc(pc+1,rf rd:=(rf r1]+rf r2])],inst,rport,wport) Rule "Sub"
Proc(pc,rf,inst,rport,wport) where Sub(rd,r1,r2)=inst ==> Proc(pc+1,rf rd:=(rf r1]-rf r2])],inst,rport,wport) Rule "Bz-Taken"
Proc(pc,rf,inst,rport,wport) if (rf rc]==0) where Bz(rc,ra)=inst
==> Proc(rf ra],rf,inst,rport,wport) Rule "Bz-Not-Taken"
Proc(pc,rf,inst,rport,wport) if (rf rc]!=0) where Bz(rc,ra)=inst
==> Proc(pc+1,rf,inst,rport,wport) Rule "Load Start"
Proc(pc,rf,inst,Rport(-,-,NotBusy( )),wport) where Load(rd,ra)=inst
==> Proc(pc,rf,inst,Rport(rf ra],-,Busy( )),wport) Rule "Load Finish"
Proc(pc,rf,inst,Rport(-,val,Busy( )),wport) where Load(rd,ra)=inst
==> Proc(pc+1,rf rd:=val],inst,Rport(-,val,NotBusy( )),wport) Rule "Store Enable"
Proc(pc,rf,inst,rport,wport) where Store(ra,r)=inst ==> Proc(pc+1,rf,inst,rport,Wport(rf ra],rf r],Enable( ))) Rule "Store Disable"
Proc(pc,rf,inst,rport,wport) if Store(-,-)!=inst ==> Proc(pc,rf,inst,rport,Wport(-,-,Disable( )))
Figure 5-7: TRSpec rewrite rules for a processor with instruction and data memory
interfaces.
80

DMEM
WE WA WD
RA RD

PROC

W_ENABLE
W_ADDR 16
W_DATA 16

PC

R_ADDR INSTPORT 16
R_DATA 16

16 25

IMEM
RA
RD

Figure 5-8: A processor's memory interfaces and their connections.

PC O output port and INSTPORT input port of the synthesized processor should be connected to the external instruction memory module as shown in Figure 5-8. This interface assumes an instruction memory module with combinational lookup.
The new rewrite rule for the execution of the Add instruction is
Rule "Add" Proc(pc, rf, inst, rport, wport) where Add(rd,r1,r2)=inst
==> Proc(pc+1, rf rd:=(rf r1]+rf r2])], inst, rport, wport)
The external instruction memory is indexed by the current value of the PC O subterm, and the rule looks to the INSTPORT subterm for the instruction returned by the external instruction memory. Besides this di erence in instruction lookup, the rule is otherwise identical to the original Add rule from Section 2.2.

Data Memory Interfaces:
In the new de nition of PROC, the internal data memory array has also been replaced by read and write interfaces to an external memory module. The data read-port term, RPORT, is a product term consisting of an output ADDR subterm, an input VAL subterm, and a BUSY status subterm. Their connections to the external memory module are shown in Figure 5-8. To execute a Load instruction, the following two rules are needed to manipulate the memory read interface in two steps.
Rule "Load Start" Proc(pc, rf, inst, Rport(-,-,NotBusy( )), wport) where Load(rd,ra)=inst
==> Proc(pc, rf, inst, Rport(rf ra],-,Busy( )), wport)
Rule "Load Finish" Proc(pc, rf, inst, Rport(-,val,Busy( )), wport) where Load(rd,ra)=inst
==> Proc(pc+1, rf rd:=val], inst, Rport(-,val,NotBusy( )), wport)

81

Version

FF Util. Freq. (bit) (%) (MHz)

TRSpec

161 60 %

40.0

Hand-coded RTL 160 50 %

41.0

Figure 5-9: Summary of the processor synthesis results.
When the current instruction is a Load instruction, the Load Start rule sets the ADDR subterm to the load address and sets the BUSY status subterm to Busy( ). In the second step, the Load Finish rule completes the Load instruction by updating the register le with the returned load value and by resetting the BUSY status term to .NotBusy( )
The data write-port term, WPORT, is a product term consisting of an output ADDR subterm, an output VAL subterm, and an output ENABLE status term. The following two rules are needed to control this synchronous write interface.
Rule "Store Enable" Proc(pc, rf, inst, rport, wport) where Store(ra,r)=inst
==> Proc(pc+1, rf, inst, rport, Wport(rf ra],rf r],Enable( )))
Rule "Store Disable" Proc(pc, rf, inst, rport, wport) if Store(-,-)!=inst
==> Proc(pc, rf, inst, rport, Wport(-,-,Disable( )))
When the current instruction is a Store instruction, the Store Enable rule sets the current store address and store data in the write-port term. Furthermore, the Store Enable rule enables the write interface by setting the ENABLE subterm to Enable( ). The Store Disable rule resets the ENABLE subterm to Disable( ) after a store operation is completed.
5.2.2 Synthesis Results
In this example, the TRSpec framework not only o ers the ease of a high-level
design ow but also produces a nal implementation that is comparable to a handcrafted e ort. A synthesizable Verilog RTL description of the processor can be generated by TRAC from the description in Figures 5-6 and 5-7. The TRAC-generated RTL description is further compiled for implementation in a Xilinx XC4013XL-08
FPGA Xila] using the Xilinx Foundation 1.5i tools Xilb]. The row labeled \TRSpec"
in Figure 5-9 characterizes the FPGA implementation in terms of the number of ip- ops, the overall resource utilization, and the maximum clock frequency. As a reference, a hand-coded Verilog RTL description of the same processor (included in
82

Figure 5-10 and 5-11) is also synthesized in this study. The row labeled \Hand-coded RTL" characterizes the implementation synthesized from the hand-coded Verilog de-
scription. The data indicate that the TRSpec description results in an FPGA im-
plementation that is similar in size and speed to the result of the hand-coded Verilog description. This similarity should not be surprising because, after all, both descriptions are describing the same processor ISA, albeit under very di erent design methodologies. In the manual Verilog design ow, a human designer has interpreted the ISA to create an RTL circuit description, but unlike in the simple GCD circuit of the previous section, it is hard for the designer to depart too far from the speci cation in a design with even modest complexity. Thus, if TRAC can correctly and e ciently interpret the operation-centric ISA description, one should expect TRAC to generate an RTL description that resembles the human designed circuit.
What is not apparent from the circuit characterizations is the di erence in devel-
opment time and e ort. The TRSpec and the hand-coded Verilog descriptions are comparable in length. However, the TRSpec description can be translated in a literal
fashion from an ISA manual. Whereas, although the hand-coded Verilog description is relatively simple, it has a much weaker correlation to the ISA speci cation. The hand-coded RTL description also includes circuit implementation information that
the RTL designer has to improvise. Whereas, in a TRSpec design ow, the designer can rely on TRAC to supply the implementation-related information. The TRSpec
framework permits a natural and intuitive decomposition of hardware behavior into atomic operations with a sequential interpretation. It is TRAC's responsibility to identify operations that can be implemented as concurrent hardware and to insert interlocks between operations that need to be sequentialized. In a traditional design framework, a similar type of analysis and translation must be performed manually by the designer. This not only creates more work for the designer but also creates more opportunity for error.
5.3 MIPS R2000 Processor
Appendix B gives the TRSpec description of a ve-stage pipelined integer processor
core based on the MIPS R2000 ISA (as described in Kan87]). The description corresponds to the elastic pipeline illustrated in Figure 5-12. The stages of the elastic pipeline are separated by abstract FIFOs, which have a nite but unspeci ed size. During synthesis, TRAC converts such an asynchronous elastic pipeline description into a synchronous pipeline by instantiating a special FIFO implementation that consists of a single stage of register and ow-control logic. Further information on creating a pipelined description is discussed in Section 6.3. The conversion from an asynchronous pipeline description to a synchronous implementation is described in Section 6.5.1.
83

module PROC ( WriteData, WriteAddr, WriteEnable, ReadAddr, ReadData, InstAddr, Inst, CLK, _RST
)

output 31:0] WriteData

output 31:0] WriteAddr

output

WriteEnable

output 31:0] ReadAddr input 31:0] ReadData

output 31:0] InstAddr input 22:0] Inst

input input

CLK _RST

reg 31:0] reg 31:0]

pc regFile 0:3]

wire 2:0] wire 1:0] wire 1:0] wire 1:0] wire 31:0] wire 31:0] wire 31:0] wire 31:0]

op rd r1 r2 rdv r1v r2v immediate

(Courtesy of D. L. Rosenband.)

Figure 5-10: Hand-coded Verilog description of a simple processor. (Part 1)

84

assign op=Inst 22:20]

assign rd=Inst 19:18]

assign r1=Inst 3:2]

assign r2=Inst 1:0]

assign immediate={{16{Inst 17]}},Inst 17:2]}

assign rdv=regFile rd]

assign r1v=regFile r1]

assign r2v=regFile r2]

assign

WriteData=r1v

// store data out

assign

WriteAddr=rdv

// store address out

assign

WriteEnable=(op==6)

assign

ReadAddr=r1v

assign

InstAddr=pc

always@(posedge CLK) begin case (op) 0: regFile rd]<=immediate 1: regFile rd]<=pc 2: regFile rd]<=regFile r1]+regFile r2] 3: regFile rd]<=regFile r1]-regFile r2] 5: regFile rd]<=ReadData endcase
end

always@(posedge CLK) begin if (_RST) begin if ((op==4) && (rdv==0)) begin pc<=r1v end else begin pc<=pc+1 end end else begin pc<=0 end
end endmodule

(Courtesy of D. L. Rosenband.)

Figure 5-11: Hand-coded Verilog description of a simple processor. (Part 2)

85

Instruction Memory Interface

PC +4

Fetch Stage

BD

Write Back

Register File

Decode Logic

Clear Jump Target

BE

Decode Stage

Bypass

ALU

Bypass

Barrel Shifter
BM
Data Memory Interface

Execute Stage Memory Stage

BW
Writeback Stage
Figure 5-12: Block diagram of the ve-stage pipelined MIPS processor core.

86

5.3.1 MIPS Integer Subset
The TRSpec description in Appendix B implements all MIPS R2000 instructions
except: 1. Integer multiple and divide instructions (MFHI, MTHI, MFLO, MTLO, MULT, MULTU, DIV, DIVU) 2. Half-word, byte and non-aligned load and store instructions (LB, LH, LWL, LBU, LHU, LWR, SB, SH, SWL, SWR) 3. Privileged instructions (SYSCALL, BREAK) 4. Coprocessor related instructions
The integer core description also does not support exception handling, privileged mode or memory management. The semantics of the memory load and branch/jump instructions has been altered to eliminate delay slots. In other words, the result of a load instruction is immediately available to the next instruction, and the e ect of a branch/jump instruction takes place immediately.
5.3.2 Microarchitecture
The description corresponds to an implementation of the MIPS ISA in a ve-stage pipelined Harvard microarchitecture. The description speci es separate external instruction and data memory interfaces that are similar to the scheme in Section 5.2. The rewrite rules imply a register le usage that requires two combinational read ports and one synchronous write port.
Fetch Stage: A single rule describes the sequential instruction fetch using the in-
struction fetch interface.
Decode Stage: Separate rules specify the decoding of the di erent instruction sub-
classes. When a read-after-write hazard is detected, the decode-stage rules attempt to bypass completed data from the execute, memory and write-back stages. If readafter-write hazards cannot be resolved by bypassing, the decode-stage rules stop ring. Branch or jump instructions are also carried out by decode-stage rules. After a control ow instruction, one bubble is inserted into the pipeline before the execution can restart at the correct jump target.
Execute Stage: Execute-stage rules describe the execution of various ALU instruc-
tions. Separate execute-stage rules also describe memory address calculations for load and store instructions. The type de nition of the MIPS processor term includes a user-de ned abstract type SHIFTER. The SHIFTER abstract type encapsulates a barrel shifter implemented in Verilog. The execute-stage rules access the SHIFTER term's interface to compute arithmetic and logical shifts of integer operands.
87

version
TRSpec
Hand-coded RTL

CBA tc6a area speed (cell) (MHz) 9059 96.6 7168 96.0

LSI 10K area speed (cell) (MHz) 34674 41.9 26543 42.1

Figure 5-13: Summary of MIPS synthesis results.
Memory Stage: Load and store instructions are executed. Other instructions
simply pass through this stage.
Write-Back Stage: All register updates are performed in the write-back stage.
5.3.3 Synthesis Results
The TRSpec description of the MIPS core can be compiled by TRAC into a syn-
thesizable Verilog RTL description. The synthesizable Verilog description can then be compiled by the Synopsys Design Compiler Synb] to target both the Synopsys CBA Syna] and LSI Logic 10K LSI] gate-array libraries. For comparison, a hand-coded Verilog RTL description of the same MIPS microarchitecture is also compiled for the same technology libraries. Figure 5-13 summarizes the pre-layout area and speed estimates reported by the Synopsys Design Compiler. The row labeled
\TRSpec" characterizes the implementation synthesizes from the TRSpec descrip-
tion. The row labeled \Hand-coded RTL" characterizes the implementation synthesized from the hand-coded Verilog description.
As is the case for the simple processor in the previous section, the results from
synthesizing the TRSpec description and the hand-coded Verilog description are in
good agreement, especially in terms of cycle time.2 The implementation synthesized from the hand-coded Verilog RTL description is 20 to 25 percent smaller than the
implementation synthesized from the TRSpec description. The TRSpec and the
hand-coded Verilog descriptions are similar in length (790 vs. 930 lines of source
code), but the TRSpec description is developed in less than one day (eight hours),
whereas the hand-coded Verilog description requires nearly ve days to complete.
2Both Synopsys synthesis runs are con gured for high-e ort on minimizing cycle time.
88

5.4 Summary
This chapter presents the results from applying TRAC to synthesize TRSpec descrip-
tions. The designs are targeted for implementation technologies that include Xilinx FPGAs and ASIC gate-array libraries. The quality of TRAC-generated implementations is evaluated against reference implementations synthesized from hand-coded Verilog RTL descriptions.
As part of this study, several examples have also been targeted for the Wildcard Recon gurable Computing Engine from Annapolis Micro Systems Ann]. The Wildcard hardware contains a single Xilinx Vertex XCV300-4 FPGA packaged in a PCMCIA form-factor. (Higher capacity devices are available on PCI cards.) The Wildcard hardware can be plugged into standard expansion slots of personal computers, and FPGA con gurations can be created and uploaded onto the Wildcard FPGA from the main processor. The FPGA con guration can include memory-mapped I/O and DMA functionalities so a software application on the main processor can interface with the hardware application on the FPGA interactively. Such a exible recon gurable hardware platform perfectly complements the ability to rapidly create
hardware designs in the TRSpec framework. In one scenario, algorithmic descriptions in TRSpec, like Euclid's Algorithm
from Section 5.1, can be synthesized for the Wildcard FPGA. This e ectively creates a hardware-software co-processing environment where an application running on the processor can launch hardware-assisted computations on the FPGA hardware. In this
usage, TRSpec provides the means for an application developer to retarget suitable
parts of an application for hardware acceleration, expending only comparable time and e ort as software development.
In another usage, an architect can create simulatable and synthesizable hardware
prototypes from architectural descriptions in TRSpec. For example, the TRSpec
description of the MIPS processor from Section 5.3 can be synthesized for execution on the Wildcard FPGA. In this context, the Wildcard FPGA becomes a hardware emulator where actual MIPS binaries can be executed. New mechanisms and ideas can be quickly added to the FPGA-emulated prototype by making high-level modi cations
to the architectural-level TRSpec description.
89

90

Chapter 6 Microprocessor Design Exploration
This chapter demonstrates the application of operation-centric hardware description
and synthesis in microprocessor design. A high-level TRSpec description of an in-
struction set architecture (ISA) is amenable to transformations that produce descriptions of pipelined and superscalar processors. This ability to rapidly create derivative designs enables a feedback-driven iterative approach to custom microprocessor development.
6.1 Design Flow Overview
In this design ow, an architect starts by formulating a high-level ISA speci cation as a TRS. The goal at this stage is to de ne an ISA precisely without injecting implementation details. For example, the rewrite rules from Section 2.2 and the type def-
initions from Example 4.3, together, constitute an ISA speci cation in the TRSpec
language. Interpreting the speci cation as is, TRAC synthesizes the register-transfer level (RTL) implementation of a single-issue, non-pipelined processor.
Based on this ISA description, the architect can further derive TRSpec descrip-
tions of pipelined processors by introducing pipeline bu ers (as described in Sec-
tions 2.3 and 2.4). The TRSpec framework simpli es the insertion of pipeline stages
by allowing the architect to create elastic pipelines where pipeline stages are separated by FIFOs. The operations in one pipeline stage can be described independently of the operations in the other stages. During synthesis, TRAC maps an asynchronous elastic pipeline description onto a synchronous pipeline where the stages are separated by simple pipeline registers.
A pipelined processor description in TRSpec can be further transformed into a
superscalar description by adding new rules derived from composing existing rules from the same pipeline stage. A composite rule, when applied, has the same e ect as the sequential in-order execution of its constituent rules. The predicate of a composite rule is only enabled in a state where the full sequence of rules can be applied. Thus, the correctness of the expanded description is guaranteed because adding composite rules cannot introduce any new behavior.
Both pipelining and superscalar design derivations are performed as source-to91

Type PROC = Proc(PC, RF, IMEM, DMEM) Type PC = Bit 32] Type RF = Array RNAME] VAL
Type RNAME = Reg0( ) jj Reg1( ) jj Reg2( ) jj Reg3( )
Type VAL = Bit 32] Type IMEM = Array PC] INST Type INST = Loadi(RNAME,VAL)
jj Loadpc(RNAME) jj Add(RNAME,RNAME,RNAME) jj Sub(RNAME,RNAME,RNAME) jj Bz(RNAME,RNAME) jj Load(RNAME,RNAME) jj Store(RNAME,RNAME)
Type DMEM = Array ADDR] VAL Type ADDR = Bit 32]
Figure 6-1: Type de nitions for a simple non-pipelined processor.
source transformations in the TRSpec language. The derived designs can be com-
piled into Verilog RTL descriptions using TRAC. For design feedback, the generated Verilog descriptions can be simulated and evaluated using commercial tools like the Cadence A rma NC Verilog Simulator Cad] and the Synopsys RTL Analyzer Sync].
6.2 Step 1: ISA Speci cation
Figures 6-1 and 6-2 repeat the rules and type de nitions of a simple ISA, already presented in Section 2.2 and Example 4.3. The type de nitions in Figure 6-1 have been altered to increase the processor data width from 16 to 32 bits. For conciseness, the rules in Figure 6-2 are given in an abbreviated format where all rules share a common left-hand-side pattern, given once at the top. When synthesized, the
TRSpec description roughly corresponds to the datapath shown in Figure 6-3.
6.3 Step 2: Pipelining Transformation
The TRSpec processor description from the previous section can be pipelined by
splitting each rule into multiple sub-rules where each sub-rule describes the suboperation in a pipeline stage. As in Section 2.4, the processing of an instruction can be broken down into separate fetch and execute sub-operations in a two-stage pipelined design. The pipelined design needs bu ers to hold partially executed instructions.
In a TRSpec description, the pipeline bu ers are modeled as FIFOs of a nite
92

Proc(pc, rf, imem, dmem)

Loadi: Loadpc: Add: Sub: Bz-Taken: Bz-Not-Taken: Load: Store:

where Loadi(rd,const) = imem pc]
! Proc(pc+1, rf rd:=const], imem, dmem)
where Loadpc(rd) = imem pc]
! Proc(pc+1, rf rd:=pc], imem, dmem)
where Add(rd,r1,r2) = imem pc]
! Proc(pc+1, rf rd:=rf r1]+rf r2]], imem, dmem)
where Sub(rd,r1,r2) = imem pc]
! Proc(pc+1, rf rd:=rf r1]-rf r2]], imem, dmem)
if rf rc]=0 where Bz(rc,rt) = imem pc]
! Proc(rf rt], rf, imem, dmem) if rf rc]6=0 where Bz(rc,rt) = imem pc] ! Proc(pc+1, rf, imem, dmem)
where Load(rd,ra) = imem pc]
! Proc(pc+1, rf rd:=dmem rf ra]]], imem, dmem)
where Store(ra,r) = imem pc]
! Proc(pc+1, rf, imem, dmem rf ra]:=rf r]])

Figure 6-2: Rules for a simple non-pipelined processor.

+1 PC

S0
Program ROM
(IMEM)

S1
Register File (RF)

Data Memory (DMEM)
ALU (+,-)

(S0 and S1 are potential sites for pipeline bu ers.) Figure 6-3: A simple processor datapath shown without its control paths.

93

Type PROC2 = Proc2(PC, RF, BF, IMEM, DMEM) Type BF = Fifo ITEMP Type ITEMP = TLoadi(RNAME,VAL)
jj TAdd(RNAME,VAL,VAL) jj TSub(RNAME,VAL,VAL) jj TBz(VAL,PC) jj TLoad(RNAME,ADDR) jj TStore(ADDR,VAL)

Figure 6-4: Additional type de nitions for the two-stage pipelined processor.

but unspeci ed size. In the synthesis phase, TRAC replaces these FIFOs by simple pipeline registers and ow control logic. In the description phase, the FIFO-based elastic pipeline abstraction allows the operations in di erent stages to be described independently without references to the operations in the other stages. A rule that describes an operation in a particular pipeline stage typically dequeues from the upstream FIFO and enqueues into the down-stream FIFO.
To describe a two-stage Fetch/Execute pipeline, the type of the processor term is rede ned as PROC2 in Figure 6-4. In contrast to PROC in Figure 6-1, a PROC2-typed term contains an additional BF-typed eld. BF is a FIFO that holds decoded instruction templates whose operands have been fetched from the register le. As discussed in Section 2.4, the original Add rule from the ISA speci cation may be replaced by the following two rules, corresponding to the fetch and execute sub-operations, respectively:

Add Fetch:

Proc2(pc,

rf,

bf,

imem,

dmem)
if r12=Target(bf)

^

r22=Target(bf)

where Add(rd,r1,r2) = imem pc]
! Proc2(pc+1, rf, bf TAdd(rd,rf r1],rf r2]), imem, dmem)

Add Execute:

!

Proc2(pc, rf, TAdd(rd,v1,v2) bf, imem, dmem) Proc2(pc, rf rd:=v1+v2], bf, imem, dmem)

Splitting the e ect of one rewrite rule into multiple rules destroys the atomicity of the

original rule and thus can cause new behaviors that may not conform to the original

speci cation. Therefore, in addition to determining the appropriate division of work

among the pipeline stages, the architect must also resolve any newly created hazards.

For example, the fetch rule's predicate expression has been extended to check if the

source register names, r1 and r2, are in Target(bf). (Target(bf) is the shorthand for the

set of target register names in bf.) This extra predicate condition stalls instruction

94

fetching when a RAW (read-after-write) hazard exists. The Bz-Taken rule and the Bz-Not-Taken rule in Figure 6-2 can also be split into
their fetch and execute sub-operations. Both Bz rules share the following instruction fetch rule:

Bz Fetch:

Proc2(pc,

rf,

bf,

imem,

dmem)
if rc2=Target(bf)

^

rt2=Target(bf)

where Bz(rc,rt) = imem pc]
! Proc2(pc+1, rf, bf TBz(rf rc],rf rt]), imem, dmem)

The two execute rules for the Bz instruction are

Bz-Taken Execute: Proc2(pc, rf, TBz(vc,vt) bf, imem, dmem) if vc = 0
! Proc2(vt, rf, , imem, dmem)
and

Bz-Not-Taken Execute:

Proc2(pc,

rf,

TBz(vc,vt) if

bf, imem,
vc6=0

dmem)

! Proc2(pc, rf, bf, imem, dmem)

All of the rules in Figure 6-2 can be partitioned into separate fetch and execute sub-rules to completely convey the operations of a two-stage pipelined processor. The current partitioning places the pipeline bu er (bf) at the position labeled S1 in Figure 6-3. Pipelines with di erent number of stages and bu er placements can also

be derived similarly.

A generic instruction fetch rule is

Proc2(pc,

rf,

bf,

imem,

dmem) if (Source(inst)

6

\

Target(bf)

)

where inst = imem pc]
! Proc2(pc+1, rf, bf Decode(inst), imem, dmem)

Source(inst) is the shorthand to extract the source register names from instruction inst. Decode(inst) is the shorthand that maps inst to its corresponding instruction template where the register operands have been fetched. For example, the expression `Decode(Add(rd,r1,r2))' is the same as `TAdd(rd,rf r1],rf r2])'. The execute-stage subrules for all instructions are given in Figure 6-5.

6.4 Step 3: Superscalar Transformation
This section describes the transformation from a pipelined microarchitecture to a pipelined superscalar microarchitecture. The transformation produces a microarchi-
95

Proc2(pc, rf, bf, imem, dmem) where itemp rest = bf

Loadi: Add: Sub: Bz-Taken: Bz-Not-Taken: Load: Store:

where TLoadi(rd,v) = itemp

! Proc2(pc, rf rd:=v], rest, imem, dmem)

where TAdd(rd,v1,v2) = itemp

! Proc2(pc, rf rd:=v1+v2], rest, imem, dmem)

where TSub(rd,v1,v2) = itemp

! Proc2(pc, rf rd:=v1-v2], rest, imem, dmem)

if vc=0 where TBz(vc,vt) = itemp

!if vcP6=ro0c2(wvhte, rref,

, imem, dmem) TBz(vc,vt) = itemp

! Proc2(pc, rf, rest, imem, dmem)

where TLoad(rd,va) = itemp

! Proc2(pc, rf rd:=dmem va]], rest, imem, dmem)

where TStore(va,v) = itemp

! Proc2(pc, rf, rest, imem, dmem va:=v])

Figure 6-5: Rules for the execute stage of the two-stage pipelined processor.
tecture similar to the DEC Alpha 21064 DWA+92] in that the microarchitecture processes multiple instructions in each pipeline stage when possible, but does not allow out-of-order execution. To derive a two-way superscalar processor description from a pipelined processor description, one needs to compose two rules from the same pipeline stage into a new composite rule that combines the e ects of both rules. Given that TRAC generates RTL descriptions where the entire e ect of a rule is executed in one clock cycle, the composite rule yields an RTL design that is capable of processing two instructions per clock cycle.
6.4.1 Derivation of Composite Rules
A TRS rule r on a set of terms T can be described by a function f whose domain D and image I are subsets of T . Given a rule:
s if p ! s0 the function f may be expressed as
f(s) = if (s) then (s) else s where ( ) represents the ring condition derived from the left-hand-side pattern s and the predicate expression p and ( ) represents the function that computes the
96

new term. Given two
function f1 2 where

rules

r1

and

r2,

the

composite

rule

r1

2

can

be

described

by

the

f1 2(s) = if i1f(s2)2((th11e((sns)))) then

else

s

else

s

= if

1(s)
2( 1

(^s))2(

1(s))

then

else

s

rRLiiaamm1ldeermtaaemaaggnDiodeetdvty1Ioeinr1dfca2go\nfbn2dcryD,ta1anItr22ih11nnmue2osbstadcirenya1obgntmeachrfnbaeer1deiea.ndptsrBoeliD2ammyacd1uaeto2ddilhenaeoistasbfeadnyddfnloo1edtrbc21ykinin2imisctotobrairtooneghandsceeeaulocosuicfuvufesetbecfliaosov1nsemecoytkampa.nonpoefdespwiDlbtDiice1ob2ahnettahh,aivonaaainvtoddsirdposIoirr2nfoscgordbs1uuirenl1cadcte2nehsdbteatoerlhdl2aeeo.tlrmTriHameRansoitiswSnnriaitetcitvahoteenenadrddts,.
Rule composition can also be described as a purely syntactic operation. Given the following two rewrite rules:

ss12

if if

pp12

! !

0
s1
0
s2

((rr12

) )

one

rst

derives

a

restricted

instance

of

r2

that

is

directly

applicable

to

s0 1

such

that

s p ! sif0 0 12

00
2

(restricted instance of r2)

This instance of r2 can then be composed with r1 as follows:

s1 if

(p1

^

p0 2

)

!

s00 2

(r1 2)

97

6.4.2 A Composite Rule Example
Consider the Add Fetch and Bz Fetch rules of the two-stage pipelined processor from Section 6.3.

Add Fetch:

Proc2(pc,

rf,

bf,

imem,

dmem)
if r12=Target(bf)

^

r22=Target(bf)

where Add(rd,r1,r2) = imem pc]

! Proc2(pc+1, rf, bf TAdd(rd,rf r1],rf r2]), imem, dmem)

Bz Fetch:

Proc2(pc,

rf,

bf,

imem,

dmem)
if rc2=Target(bf)

^

rt2=Target(bf)

where Bz(rc,rt) = imem pc]
! Proc2(pc+1, rf, bf TBz(rf rc],rf rt]), imem, dmem)

One can rewrite the Bz Fetch rule as if it is being applied to the right-hand-side

expression of the Add Fetch rule. The restricted Bz Fetch rule appears as

Proc2(pc+1,

rf,

bf

TAdd(rd,rf r1],rf r2]), imem, dmem)
if rc2=Target(bf TAdd(rd,rf r1],rf

r2]))

^ rt2=Target(bf TAdd(rd,rf r1],rf r2]))

where Bz(rc,rt) = imem pc+1]

! Proc2((pc+1)+1, rf,

(bf TAdd(rd,rf r1],rf r2])) TBz(rf rc],rf rt]),

imem, dmem)

This rule is more speci c than the original Bz Fetch rule because bf is required to contain an Add instruction template as the youngest entry. A more speci c instance of a TRS rule is guaranteed to be correct because it res under fewer conditions. The Add Fetch and Bz Fetch rules can be combined into a composite rule:

Proc2(pc,

rf,

bf,

imem,

dmem)
if r12=Target(bf)

^

r22=Target(bf)

^ rc2=Target(bf TAdd(rd,rf r1],rf r2]))

^ rt2=Target(bf TAdd(rd,rf r1],rf r2]))

where Add(rd,r1,r2) = imem pc]

Bz(rc,rt) = imem pc+1]

! Proc2((pc+1)+1, rf,

(bf TAdd(rd,rf r1],rf r2])) TBz(rf rc],rf rt]),

imem, dmem)

98

The predicate expression in the rule above can be simpli ed as shown in the rule below by interpreting the e ect of enqueuing to an abstract FIFO term.

Proc2(pc,

rf,

bf,

imem,

dmem)
if r12=Target(bf)

^

r22=Target(bf)

^ rt2=Target(bf) ^ rc2=Target(bf)

^ rc6=rd ^ rt6=rd

where Add(rd,r1,r2) = imem pc]

Bz(rc,rt) = imem pc+1]

! Proc2((pc+1)+1, rf,

(bf TAdd(rd,rf r1],rf r2])) TBz(rf rc],rf rt]),

imem, dmem)

In an implementation synthesized according to the procedures outlined in Chapter 3, the scheduler should give higher priority to a composite rule over its constituent rules when they are enabled in the same clock cycle.

6.4.3 Derivation of a Two-Way Superscalar Processor
This section presents the derivation of the composite rules for a two-way superscalar processor description. The derivations are based on the two-stage pipelined processor from Section 6.3. For each of the two pipeline stages, di erent combinations of two rules from the same stage are composed. In general, given a pipeline stage with
N rules, a superscalar transformation leads to an O(Ns) increase in the number of rules where s is the degree of superscalarity. Since superscalar transformation implies
an increase in hardware resources like register le ports, ALUs and memory ports, one may not want to compose all possible combinations of rules. For example, one may not want to compose a memory load rule with another memory load rule if the memory interface can only accept one operation per cycle.
This derivation assumes that there are no restrictions on hardware resources except that the data memory can only service one operation, a read or a write, in each clock cycle. The derivation also assumes the instruction memory can return two consecutive instruction words on any address alignment.
The generic instruction fetch rule from the end of Section 6.3 can be composed with itself to produce a two-way superscalar fetch rule:

Proc2(pc,

rf,

bf,

imem,

dmem)
if Source(inst)6

\Target(bf)

^ Source(inst')6\(Target(bf) Target(inst))

where inst = imem pc]

inst' = imem pc+1]

! Proc2((pc+1)+1, rf,

bf Decode(inst) Decode(inst'), imem, dmem)

The superscalar execute rules are derived by composing all legal combinations of the rules in Figure 6-5. A composite execute rule examines both the rst and second

99

Proc2(pc, rf, bf, imem, dmem) where TAdd(rd,v1,v2) itemp rest = bf

Loadi:

where TLoadi(rd',v) = itemp

! Proc2(pc, rf rd:=(v1+v2),rd':=v)], rest, imem, dmem)

Add: where TAdd(rd',v1',v2') = itemp

! Proc2(pc, rf rd:=(v1+v2),rd':=(v1'+v2')], rest, imem, dmem)

Sub: where TSub(rd',v1',v2') = itemp

! Proc2(pc, rf rd:=(v1+v2),rd':=(v1'-v2')], rest, imem, dmem)

Bz-Taken:

if vc=0 where TBz(vc,vt) = itemp

Bz-Not-Taken:

!if vcP6=ro0c2w(hvte,rerf

rd:=v1+v2], TBz(vc,vt) =

, imem, itemp

dmem)

! Proc2(pc, rf rd:=v1+v2], rest, imem, dmem)

Load: where TLoad(rd',va) = itemp

! Proc2(pc, rf rd:=(v1+v2),rd':=dmem va]], rest, imem, dmem)

Store: where TStore(va,v) = itemp

! Proc2(pc, rf rd:=v1+v2], rest, imem, dmem va:=v])

Figure 6-6: Combining the Add Execute rule with other execute rules.

instruction templates in the pipeline bu er bf. If the rst and second instruction templates satisfy the rule's predicate expression, the rule is applied to process both instruction templates simultaneously.
The table in Figure 6-6 gives the composition of the Add Execute rule with other execute rules. (Similar composite rules can be derived for the Loadi Execute or Sub Execute rules.) If the rst instruction template in bf is an Add instruction template then the second instruction template, when present, can always be executed concurrently. In the composite rules, the expression a i:=v,i':=v'] denotes a sequential update of location i and i' of array a. If i is the same as i' then a i:=v,i':=v'] has the same e ect as a i':=v'].
The Bz-Taken Execute rule cannot be composed with any other execute rule. If the rst position of bf contains the instruction template of a taken branch, bf will subsequently be cleared by the Bz-Taken Execute rule. Since every execute-stage rule requires the pipeline bu er to be not empty, none of the execute-stage rules can be applicable immediately after the Bz-Taken Execute rule has been applied.
Executing the Bz-Not-Taken Execute rule produces no side-e ects other than removing the current Bz instruction template from the head of bf. Hence, as shown in Figure 6-7, composing a Bz-Not-Taken Execute rule with any other rule results in a composite rule that is nearly identical to the second rule in the composition.
The tables in Figures 6-8 and 6-9 give the composition of the Load Execute and the Store Execute rules with other execute rules. Since the data memory only responds to one memory operation per clock cycle, one cannot compose the Load Execute rule or the Store Execute rule with another memory access rule.
100

Proc2(pc, rf, bf, imem, dmem) where TBz(vc,vt) itemp rest = bf

Loadi:

if vc6=0 where TLoadi(rd,v) = itemp

Add: i!f vcP6=ro0c2w(hpecr,erfTrAd:d=dv(r],dr,evs1t,,vi2m)e=m,itdemmepm)

Sub:

!if vcP6=ro0c2w(hpecr,erfTrdS:u=bv(r1d+,vv12,]v,2r)es=t,

imem, itemp

dmem)

Bz-Taken:

!if vcP6=ro0c2^(pvcc,'r=f 0rd:w=hve1r-ev2T],Brze(svt,c'im,vte'm) ,=dmiteemm)p

Bz-Not-Taken:

!if vcP6=ro0c2^(vvtc','6=rf,0

, imem, where

dmem) TBz(vc',vt')

=

itemp

Load:

!if vcP6=ro0c2

(pc, rf, rest, imem, dmem) where TLoad(rd,va) = itemp

Store:

i!f vcP6=ro0c2

(pc, rf where

rd:=dmem va]], TStore(va,v) =

rest, imem, itemp

dmem)

! Proc2(pc, rf, rest, imem, dmem va:=v])

Figure 6-7: Combining the Bz-Not-Taken Execute rule with other execute rules.

Proc2(pc, rf, bf, imem, dmem) where TLoad(rd,va) itemp rest = bf

Loadi:

where TLoadi(rd',v) = itemp

! Proc2(pc, rf rd:=dmem va],rd':=v], rest, imem, dmem)

Add: where TAdd(rd',v1,v2) = itemp

! Proc2(pc, rf rd:=dmem va],rd':=(v1+v2)], rest, imem, dmem)

Sub: where TSub(rd',v1,v2) = itemp

! Proc2(pc, rf rd:=dmem va],rd':=(v1-v2)], rest, imem, dmem)

Bz-Taken:

if vc=0 where TBz(vc,vt) = itemp

Bz-Not-Taken:

!if vcP6=ro0c2(wvht,er(erf

rd:=dmem TBz(vc,vt)

va]]), , imem, = itemp

dmem)

! Proc2(pc, (rf rd:=dmem va]]), rest, imem, dmem)

Figure 6-8: Combining the Load Execute rule with other execute rules.

101

Proc2(pc, rf, bf, imem, dmem) where TStore(va,v) itemp rest = bf

Loadi:

where TLoadi(rd,v') = itemp

! Proc2(pc, rf rd:=v'], rest, imem, dmem va:=v])

Add: where TAdd(rd,v1,v2) = itemp

! Proc2(pc, rf rd:=v1+v2], rest, imem, dmem va:=v])

Sub: where TSub(rd,v1,v2) = itemp

! Proc2(pc, rf rd:=v1-v2], rest, imem, dmem va:=v])

Bz-Taken:

if vc=0 where TBz(vc,vt) = itemp

Bz-Not-Taken:

!if vcP6=ro0c2

(vt, rf, where

, imem, dmem va:=v]) TBz(vc,vt) = itemp

! Proc2(pc, rf, rest, imem, dmem va:=v])

Figure 6-9: Combining the Store Execute rule with other execute rules.
6.5 Synthesis and Analysis
The TRSpec processor descriptions presented in this chapter can be compiled into
synthesizable Verilog RTL descriptions using TRAC. The TRAC-generated RTL descriptions can be further compiled, by commercial hardware compilers, for a number of target technologies ranging from ASICs to FPGAs. In addition, the RTL descriptions can also be targeted for the Synopsys GTECH Library, a technology-independent logic representation. The GTECH netlist of a design can then be processed by the Synopsys RTL Analyzer Sync] to provide quantitative feedback about the circuit's size and delay.
6.5.1 TRSpec to RTL
Input and Output:
As in the example from Section 5.2, realistically, the architectural-level descriptions in this chapter need to be modi ed to access external memory modules through input/output ports. For the single-issue pipelined and non-pipelined processor descriptions, modi cations to include instruction and data memory interfaces are similar to the presentation in Section 5.2. On the other hand, for a two-way superscalar processor description, the modi cation needs to provide an instruction fetch interface that can return two instructions at a time. It is su cient to have a single fetch interface that can return two consecutive instructions on any address alignment because the two-way superscalar fetch rule only references consecutive locations, pc and pc+1.
Synchronous Pipeline Synthesis:
In pipelined processor descriptions, the operation of the processors cannot depend on the exact depth of the pipeline FIFOs. This allows TRAC to instantiate onedeep FIFOs (i.e., a single register) as pipeline bu ers. Flow control logic is added to
102

_full?

_full?

_full?

deq deq

Logic

Logic

empty?

enq empty? 1-deep FIFO

enq

deq

Logic

empty?

enq

Stage K

Stage K+1

Stage K+2

Figure 6-10: Synchronous pipeline with local feedback ow control.

ensure a FIFO is not over owed or under owed by enqueue and dequeue operations. A straightforward attempt might lead to the circuit shown in Figure 6-10. In this naive mapping, the one-deep FIFO is full if its register holds valid data the FIFO is empty if its register holds a bubble. With only local ow control between neighboring stages, the overall pipeline would contain a bubble in every other stage during steady-
state execution. For example, if pipeline bu er K and K+1 are occupied and bu er K+2 is empty in some clock cycle, the operation in stage K+1 would be enabled to advance at the clock edge, but the operation in stage K is held back because bu er K+1 appears full during the clock cycle. The operation in stage K is not enabled until the next clock cycle when bu er K+1 has been emptied.
It is important that TRAC creates a ow control logic that includes a combinational multi-stage feedback path that propagates from the last pipeline stage to the rst pipeline stage. The cascaded feedback scheme shown in Figure 6-11 allows stage
K to advance both when pipeline bu er K+1 is actually empty and when bu er K+1 is going to be dequeued at the coming clock edge. This scheme allows the entire
pipeline to advance synchronously on each clock cycle. A stall in an intermediate pipeline stage causes all up-stream stages to stall at once. A caveat of this scheme is that this multi-stage feedback path could become the critical path, especially in a deeply pipelined design. In which case, one may want to break the feedback path at selected stages by using two-register-deep FIFOs with local ow control. A cyclic feedback path can also be broken by inserting two-register-deep FIFOs with local ow control.
6.5.2 GTECH RTL Analysis
Five TRSpec descriptions are included in this analysis. The rst three are the non-
pipelined processor, two-stage pipelined processor and two-stage two-way superscalar processor, presented in Sections 6.2, 6.3 and 6.4, respectively. Two additional descriptions describe a three-stage pipelined processor and its corresponding two-way superscalar derivative. The three-stage pipelined processor corresponds to the datapath in Figure 6-3 with pipeline bu ers inserted at both positions S0 and S1. The three-stage pipelined processor description is derived from the two-stage pipelined
103

_full?

deq

_full?

deq

_full?

deq

Logic

Logic

Logic

empty?

enq empty? 1-deep FIFO

enq empty?

enq

Stage K

Stage K+1

Stage K+2

Figure 6-11: Synchronous pipeline with combinational multi-stage feedback ow control.

Unpipelined 2-stage 2-stage 2-way 3-stage 3-stage 2-way area ( % ) area ( % ) area ( % ) area ( % ) area ( % )

Prog. Counter Reg. File Mem. Interface ALU Pipe. Bu er(s) Logic

321 ( 7.4) 1786 ( 41.2) 981 ( 22.6) 796 ( 18.3)
0 ( 0.0) 450 ( 10.4)

321 ( 5.6) 1792 ( 31.1) 985 ( 17.1) 796 ( 13.8) 737 ( 12.8) 1122 ( 19.5)

321 ( 4.0) 2157 ( 26.9) 985 ( 12.3) 1588 ( 19.8) 1858 ( 23.2) 1099 ( 13.7)

321 ( 5.1) 1792 ( 28.1) 985 ( 15.4) 796 ( 12.5) 1305 ( 20.4) 1179 ( 18.4)

321 ( 3.4) 2157 ( 22.7) 985 ( 10.4) 1588 ( 16.7) 3342 ( 35.2) 1099 ( 11.6)

Total 4334 (100.0) 5753 (100.0) 8008 (100.0) 6378 (100.0) 9492 (100.0)

Normalized Total 1.00

1.33

18.5

1.47

2.19

(Unit area = two-input NAND gate)

Figure 6-12: Circuit area distributions for ve processors.

processor descriptions following the same methodology presented in Section 6.3. All
TRSpec descriptions have been derived manually starting from the non-pipelined
processor description and have been altered to reference external instruction and data memories through I/O ports.
The ve TRSpec processor descriptions are compiled to GTECH netlists for area
and timing analyses by the Synopsys RTL Analyzer. The outputs of the Synopsys RTL Analyzer are tabulated in Figures 6-12 and 6-13. Figure 6-12 compares the amount of logic area needed by the ve designs. (One unit of area corresponds to the area needed by a two-input NAND gate.) The total area increases 2.2 times going from a nonpipelined implementation to a three-stage two-way superscalar pipeline. As expected, both pipelining and superscalarity increase the pipeline bu er requirements (from 0 to 3342) and control logic requirements (from 450 to 1099). Superscalarity also doubles the ALU requirements and increases the register- le size because of the additional read and write ports.
The two tables in Figure 6-13 break down the delay of each processor's critical path into contributions by di erent parts of the processor. (The Synopsys RTL Analyzer
104

unpipelined

2-stage

2-stage, 2-way

Stage 1 Stage 2 Stage 1 Stage 2

Program Counter Instruction Fetch Operand Fetch Raw Hazard PC increment S1 32-ALU Write Back

start X 4 20 6

start start XX 12
18 6 start 8 start
20 20 57

Total if X=20

30+X 18+X 25 26+X 27 50 38 25 46 27

(Unit delay = two-input NAND gate)

3-stage

3-stage, 2-way

Stage 1 Stage 2 Stage 3 Stage 1 Stage 2 Stage 3

Program Counter start

start

Inst Fetch or PC Inc X

X

S0

6 start

8 start

Instruction Decode

12

18

S1

8 start

11 start

32-ALU

20 23

Write Back

58

Total if X=20

6+X 20 26 20

25 8+X 29 25 28 29

31 31

(Unit delay = two-input NAND gate)

Figure 6-13: Critical path delays for ve processors.

105

reports the logic delay in units that correspond to the propagation delay of a twoinput NAND gate. The analysis does not consider load or fan-out.) For pipelined processors, separate critical paths are given for each stage. (A combinational path is assigned to a stage based on where the path starts). When a critical path involves an instruction fetch, X is used to represent the instruction memory lookup delay since the actual delay will vary with the instruction memory size and implementation. The critical path analysis does not consider the latencies of data memory operations since, if data memory latencies ever become a factor in the critical path, one should modify the synchronous data memory interface to spend more cycles rather than lengthening the cycle time.
The information generated by the Synopsys RTL Analyzer is helpful in deciding the e ectiveness of various architectural features and modi cations. Ideally, convert-
ing a non-pipelined microarchitecture to a p-stage pipelined microarchitecture should increase the clock frequency by p-fold, but this is rarely achieved in practice due to
unbalanced partitioning and pipeline logic overhead. Assuming X is 20 time units, the two-stage pipelined processor only achieves a clock frequency that is 39% higher than the non-pipelined version. The three-stage pipeline processor achieves a 92% improvement.
Overall, the peak performance of the two-stage two-way superscalar processor is approximately twice that of the non-pipelined processor at approximately twice the cost in terms of area. The three-stage two-way superscalar processor appears to have the best performance-area trade-o since it has nearly 3 times the performance of the non-pipelined processors while consuming only 2.2 times more area.
A caveat in this analysis is that the results only give an indication of the processors' peak performance. The e ects of instruction mix on the di erent microarchitectures must be analyzed by simulating the TRAC-generated RTL descriptions with benchmark applications. The combined feedback from circuit analysis and simulation can help steer the architect in an iterative exploration of a large number of architectural options and trade-o s.
6.6 Summary
This chapter describes how to generate a pipelined processor design from an ISA
speci cation by source-to-source transformations in the TRSpec operation-centric
framework. Transformations to create superscalar designs are also possible. Currently, the transformations described in this chapter have to be performed manually. An e ort to semi-automate the process is underway Lis00, Ros00]. In the future, the mechanics of the transformation would be automated, but human intervention would still be required to guide these transformations at a high level, such as selecting the degree of superscalarity and the positions of the pipeline stages.
106

Chapter 7 Conclusions
This thesis creates a new operation-centric hardware development framework that employs an easier and more intuitive hardware design abstraction.
7.1 Summary of Work
Operation-Centric Hardware Description: This thesis presents the concepts
and advantages of operation-centric hardware description where the behavior of a system is decomposed and described as a collection of operations. Typically, an operation is de ned by a predicate condition and an e ect. An operation a ects the state of the system globally and atomically. This atomic semantics simpli es the task of hardware description by permitting the designer to formulate each operation as if the system were otherwise static.
TRSpec Hardware Description Language: This thesis presents TRSpec, an
adaptation of Term Rewriting Systems (TRS) for operation-centric hardware description. This synthesizable TRS language includes extensions beyond the standard TRS formalism to increase its compactness and expressiveness in hardware description. On the other hand, in some areas, the generality of TRS has to be restricted with the help of a type system to ensure a description's synthesizability into a nite-state
machine. Speci cally, TRSpec disallows in nite-size terms and also disallows rules
that can change the size of the terms.
Abstract Transition Systems: This thesis de nes an Abstract Transition System
(ATS). ATS is an operation-centric state machine abstraction. ATS is a convenient
intermediate representation when mapping TRSpec, or other source-level operation-
centric languages, to hardware implementations. ATS has all of the essential properties of an operation-centric framework but without the syntactic complications of a source-level language. Generalized synthesis and optimization algorithms can be developed in the ATS abstraction, independent of source language variations.
107

Hardware Synthesis and Scheduling: This thesis develops the theories and al-
gorithms necessary to create an e cient hardware implementation from an operationcentric description. In particular, this thesis explains how to implement an ATS as a synchronous nite state machine. The crux of the synthesis problem involves nding a valid composition of the ATS transitions in a coherent state transition system that carries out as many ATS transitions concurrently as possible. This thesis presents both a straightforward reference implementation and two optimized implementations.
Term Rewriting Architectural Compiler: The ideas in this thesis are realized in the Term Rewriting Architectural Compiler (TRAC), a TRSpec-to-Verilog compiler. Design exercises employing TRSpec and TRAC have shown that an operation-
centric hardware development framework o ers a signi cant reduction in design time and e ort when compared to traditional frameworks, without loss in implementation quality.
Microarchitectural Transformations: This thesis investigates the application of TRSpec and TRAC to the design of pipelined superscalar processors. The design ow starts from a basic instruction set architecture given in TRSpec and progres-
sively incorporates descriptions of pipelining and superscalar mechanisms as sourceto-source transformations.
7.2 Future Work
This thesis is a preliminary investigation into operation-centric frameworks for hardware development. The results of this thesis provide a starting point for this research. This section points out the weaknesses in this thesis and proposes possible resolutions in follow-on research. This section also describes work stemming from applying the technologies in this thesis.
7.2.1 Language Issues
The TRSpec language supports the essential aspects of operation-centric hardware description. However, in many respects, TRSpec is a rudimentary language with-
out many of the common features in modern programming languages. Syntactically,
TRSpec borrows from the TRS notation, which is not suited for describing large or
modular designs. These and many other issues are being addressed by the development of BlueSpec, a new operation-centric hardware description language Aug00].
Like TRSpec, BlueSpec is a strongly typed language supporting algebraic types. BlueSpec is semantically similar to TRSpec, but it borrows from the syntax and fea-
tures of the Haskell JHA+98] functional language. The salient features of BlueSpec are outlined below.
Compact Syntax: A TRSpec rule equates to a BlueSpec function of state-to-
state. A BlueSpec function also supports the use of pattern matching to specify the 108

ring condition of a rule. To reduce repetition, each pattern can have multiple \righthand-side" e ect terms guarded by di erent predicate expressions. Semantically, these are di erent rules that happen to have the same left-hand-side pattern. To reduce verbosity, BlueSpec also uses a \named" record notation for product and disjunct terms. Thus, a product pattern can constrain only speci c elds by name, without mentioning the unconstrained elds. A right-hand-side e ect term can also specify changes for only selected elds without mentioning the una ected elds.
Complex Expressions: TRSpec, as presented in this thesis, only supports simple arithmetic and logical expressions. The TRAC implementation of TRSpec actually
allows more complicated expressions like if-then-else statements and switch-case statements on both scalar and algebraic types. BlueSpec further allows the full range of expression constructs from the Haskell language. In particular, BlueSpec supports the use of functions that can be compiled into pure combinational logic.
Generalized Abstract Types: Besides built-in arrays and FIFOs, BlueSpec al-
lows user-de ned abstract types with an arbitrary list of combinational and state-
transforming interfaces. External library modules can be incorporated into a TRSpec
description as a custom abstract type.
Additional I/O Types: The simple I/O mechanisms of TRSpec are not enough
to meet all design scenarios e ciently. BlueSpec supports additional I/O type classes to give users additional options over the exact implementation of I/O interfaces. The additional I/O type classes are useful when designing an I/O interface to meet a prede ned synchronous handshake. BlueSpec also provides I/O constructs for combining modular BlueSpec designs.
7.2.2 Synthesis Issues
The synthesis algorithm in this thesis always maps the entire e ect of an operation
(i.e., a TRSpec rule) into a single clock cycle. The scheduling algorithm always
attempts to maximize hardware concurrency. These two simplifying policies are necessarily accompanied by the assumption that any required hardware resources are available. In practice, TRAC's implementation of these policies, in conjunction with good register-transfer-level (RTL) logic optimizations, results in reasonably e cient implementations. Nevertheless, the synthesis and scheduling algorithms can be improved in many dimensions. Some of the optimizations outlined below are already part of the normal RTL compilation that takes place after TRAC. However, there is an advantage to incorporate these optimizations into the front-end compilation because front-end compilers like TRAC have better access to high-level semantics embedded in the source descriptions. For example, TRAC performs its own RTL optimizations before generating its output. TRAC can trace two RTL signals to their usage in the source description, and if the two signals originate from two con icting rules then TRAC can conclude the signals are never used simultaneously. The same inference
109

would be hard or impossible when the same design is passed to the back-end compiler in a distilled RTL format.
Technology Library Mapping: TRAC can instantiate library modules that have
been declared explicitly as user-de ned abstract types, but TRAC cannot map an
arbitrary TRSpec description to target a speci c set of library macros. The RTL
descriptions generated by TRAC only assume three state primitives: registers, arrays and FIFOs. The only combinational logic primitives used by TRAC are multiplexers. The remaining combinational logics are expressed as Verilog expressions. Normally, TRAC can defer technology-speci c mappings to back-end RTL compilers like Synopsys. However, unlike gate-array or standard-cell implementations, the quality of FPGA synthesis is very sensitive to the use of vender-speci c RTL coding guidelines and library macros. It is important for TRAC to generate optimized FPGA imple-
mentations because a potential application of TRAC and TRSpec is to facilitate
rapid creation of hardware prototypes using recon gurable technologies.
Mapping Operations to Multiple Clock Cycles: In many hardware applica-
tions, there are hard limits on the amount and the type of hardware resources available. In other cases, factors like power consumption and manufacturability place greater emphasis on lower hardware utilization over absolute performance. Under these assumptions, it is not optimal, sometimes even unrealistic, to require the e ect of an operation to always execute in one clock cycle. For example, an operation may perform multiple reads and writes to the same memory array whereas the implementation technology does not permit multi-ported memory. Also, it may not make sense to instantiate multiple oating-point multipliers only because one of the operations performs multiple oating-point multiplications. Finally, some operations, like divide, simply cannot be reasonably carried out in a single clock cycle. Currently, there is an e ort to develop a new method where the e ect of an operation can be executed over multiple clock cycles to meet resource requirements Ros00]. The current approach partitions a complex operation into multiple smaller operations that are suitable for single-cycle mapping. The key aspect in this transformation is to add appropriate interlocks such that the atomicity of the original operations are mimicked by the execution of the partitioned operations over multiple clock cycles. The synthesis algorithms in this thesis are directly applicable to the transformed system.
Automatic Pipelining and Superscalar Transformations: Chapter 6 of this
thesis describes manual source-to-source transformations for creating superscalar and pipelined processors. Follow-on e orts are looking into automating these transformations. The steps to automate the pipelining transformation are related to the
partitioning discussed in the previous paragraph. For pipelining, a single TRSpec
rule is partitioned for execution over multiple clock cycles. In the context of pipelining, besides maintaining the atomic semantics of the original rules, there is added attention to create sub-rules that can be executed in a pipelined fashion Ros00]. To automate superscalar transformations, sub-rules in the same pipeline stage are iden-
110

ti ed and syntactically composed to form new superscalar rules Lis00]. In a related e ort, rule transformations are applied to the veri cation of pipelined superscalar processors. For example, using rule composition, it is possible to reduce a pipelined processor to a more easily veri ed non-pipelined equivalent by eliminating pipeline bu ers one stage at a time Lis00].
Power, Area and Timing-Aware Synthesis: The current implementation of
TRAC chie y focuses on generating a correct RTL implementation for operationcentrically speci ed behaviors. The RTL implementations are optimized with a singleminded goal to maximize hardware concurrency. In many applications, it is necessary to optimize for other factors like power, area and timing. Currently, power, area and timing analyses are available during the RTL compilation phase. The designer can modify the source description according to the feedback from RTL synthesis. The three improvements to TRAC discussed above (technology library mapping, multicycle operations, and pipelining transformation) open up the possibility to automate this design re nement process. Incorporating technology-speci c library mapping into the front-end enables TRAC to estimate power, area and timing early on in the synthesis. Thus, TRAC can adjust its optimization goals accordingly. To meet a speci c power or area budget, TRAC can partition an operation over multiple clock cycles to reuse hardware resources. To meet a speci c timing requirement, pipelining transformation can be employed to break up the critical path.
7.2.3 Applications
This thesis presents several processor-related examples. Although TRSpec and
TRAC are good architectural development tools, their applications have a much larger domain. The following paragraphs point out some of the applications currently being explored.
Recon gurable Computing: Given the current pace of development in recon g-
urable computing, it is likely that some day all personal computers will be shipped with a user-recon gurable hardware alongside of the CPU. The high-level abstrac-
tion of the TRSpec framework lowers the e ort and expertise required to develop
hardware applets for the recon gurable coprocessing hardware. A programmer could retarget part of a software application for hardware implementation using the same level of time and e ort as software development. Even today, when combined with
suitable recon gurable technologies like the Annapolis Wildcard, TRSpec and TRAC
already can provide an environment where the recon gurable hardware can be used as software accelerators (see discussions in Section 5.4).
Hardware Cache Coherence: Operation-centric descriptions based on TRS have
been applied to the study of memory consistency and cache coherence. In CACHET, TRS is used to formally specify a dynamically adaptive cache coherence protocol for distributed shared-memory systems SAR99a]. The Commit-Reconcile and Fences
111

(CRF) memory model uses TRS to capture the semantics of elemental memory operations in a novel memory model designed for modern out-of-order superscalar microarchitectures SAR99b]. The properties of these formally speci ed memory consistency and coherence models can be veri ed using theorem proving techniques as well as by simulating against a reference TRS speci cation. Recent e orts have attempted to couple synthesis and formal veri cation to the same source description. The goal is
to capture architectural-level TRS models in TRSpec for both formal veri cation
and automatic synthesis into memory controllers and cache-coherence engines.
Microarchitecture Research: With TRSpec and TRAC, a high-density eld
programmable hardware platform becomes a powerful hardware prototyping testbench. Such a prototyping framework can enable VLSI-scale \bread boarding" such that even a small research group can explore a variety of architectural ideas quickly and spontaneously. Studying prototypes can expose subtle design and implementation issues that are too easily overlooked on paper or in a simulator. A prototype of a highly concurrent system also delivers much higher execution rates than simulation. A thorough investigation using a hardware prototype lends much greater credence to experimental research of revolutionary ideas.
Teaching: A high-level operation-centric framework is also a powerful teaching aide. In a lecture, an operation-centric TRSpec description gives an intuitive functional
explanation. An operation-centric description also allows digital-design issues to be separated from architectural ones. The high-level architectural prototyping environment discussed in the previous paragraph can also be integrated into a computer architecture course where an advanced hardware student can study a broad range of architectural issues in a hands-on manner. In addition to the materials presented in class, a student can acquire an even deeper understanding of a mechanism by tinkering with its high-level description and study the e ects on a simulator or a prototyping platform. This kind of independent exercise will help students build stronger intuitions for solving architectural problems. On the other hand, the course's emphasis on mechanisms rather than implementation also makes it ideal for software students who simply want to understand how to use the complex features of modern systems.
7.3 Concluding Remarks
In the short term, a high-level operation-centric hardware development framework cannot completely replace current RTL-based design practices. Clearly, there is a class of applications, such as microprocessors, that demands the highest possible performance and has the economic incentives to justify the highest level of development e ort and time. Nevertheless, a steady industry-wide move toward a higher-level design environment is inevitable. When the integrated-circuit design complexity surpassed one million gates in the early 90's, designers abandoned schematic capture in favor of textual hardware description languages. An analogous evolution to a still
112

higher-level design environment is bound to repeat when the complexity of integratedcircuit designs exceeds the capacity of current design tools.
Ultimately, the goal of a high-level description is to provide an uncluttered design representation that is easy for a designer to comprehend and reason about. Although a concise notation is helpful, the utility of a \high-level" description framework has to come from the elimination of some \lower-level" details. It is in this sense that an operation-centric framework can o er an advantage over traditional RTL design frameworks. Any non-trivial hardware design consists of multiple concurrent threads of computation in the form of concurrent nite state machines. This concurrency must be managed explicitly in traditional representations. In an operation-centric description, parallelism and concurrency are implicit in the source-level descriptions, only to be discovered and managed by an optimizing compiler.
113

114

Appendix A TRSpec Language Syntax
A.1 Keywords
A.1.1 Keywords in Type De nitions
`Type': start of an algebraic type de nition `IType': start of an input port type de nition `OType': start of an output port type de nition `TypeSyn': start of a type synonym de nition `Bit': declaring a built-in unsigned integer type `Int': declaring a built-in signed integer type `Bool': declaring a built-in Boolean type `Array': declaring a built-in abstract array type `Fifo': declaring a built-in abstract FIFO type
A.1.2 Keywords in Rule and Source Term Declarations
`Rule': start of a rule declaration `Init': start of a source term declaration `if': start of a predicate expression `where': start of a LHS or RHS where binding list
A.2 TRS
TRS :: TypeDe nitions Rules SourceTerm 115

A.3 Type De nitions
TypeDe nitions :: TypeDe nition ] TypeDe nition TypeDe nitions
TypeDe nition :: De neBuiltInType ] De neAlgebraicType ] De neAbstractType ] De neIoType ] De neTypeSynonym

A.3.1 Built-In Type
De neBuiltInType :: Type TypeName = Bit BitWidth] ] Type TypeName = Int BitWidth] ] Type TypeName = Bool

A.3.2 Algebraic Type
De neAlgebraicType :: Type TypeName = AlgebraicType
AlgebraicType :: ProductType ] SumType

Product Types

ProductType

::

ConstructorName
Note: where k 1

(TypeName

1,

...,

TypeName

k)

Sum Types

SumType :: Disjuncts

Disjuncts :: Disjunct

] Disjunct || Disjuncts

Disjunct

::

ConstructorName
Note: where k 0

(TypeName1,

...,

TypeNamek)

A.3.3 Abstract Type
De neAbstractType :: Type TypeName = AbstractType AbstractType :: Array TypeNameindex] TypeNamedata ] Fifo TypeName
116

A.3.4 I/O Type
De neIoType :: IType TypeName = TypeName ] OType TypeName = TypeName

A.3.5 Type Synonym
De neTypeSynonym :: TypeSyn TypeName = TypeName

A.3.6 Miscellaneous
BitWidth :: 1-9] 0-9]*
TypeName :: A-Z] A-Z0-9]* ConstructorName :: A-Z] a-z0-9]+

A.4 Rules
Rules :: Rule ] Rule Rules
Rule :: Rule RuleName LHS ==> RHS Note: The main pattern in LHS and the main expression in RHS
must have the same type

A.4.1

Left Hand Side
LHS :: Pattern ] Pattern PredicateClause ] Pattern LhsWhereClause ] Pattern PredicateClause LhsWhereClause

Pattern :: `-' ] VariableName ] NumericalConstant

]

ConstructorName
Note: where k 0

(Pattern

1,

...,

Patternk)

PredicateClause :: if Expression Note: Expression must have an integer type
LhsWhereClause :: where PatternMatches PatternMatches :: PatternMatch
] PatternMatch PatternMatches PatternMatch :: Pattern = Expression
Note: Pattern and Expression must have the same type

117

A.4.2 Right Hand Side
RHS :: Expression ] Expression RhsWhereClause

RhsWhereClause :: where Bindings Bindings :: Binding ] Binding Bindings Binding :: VariableName = Expression

A.4.3 Expressions

Expression :: `-' ] VariableName ] NumericalConstant

] ]

PCroinmstirtiuvcetOorpN(Eaxmpere(Essxiporne1s,s.i.o.,nE1,x.p..r,eEssxiporneks)sionk)

Note: In x representation of arithmetic and logical

operations is supported as syntactic sugar

] AbsInterface

PrimitiveOp :: Arithmetic ] Logical ] Relational Arithmetic :: Add ] Sub ] Multiply ] Divide ] Mod ] Negate Logical :: Not ] And ] Or ] ] ] ]BitwiseNegate BitwiseAnd BitwiseOr BitwiseXor Relational :: ] ] ]Equal NotEqual GreaterThan GreaterThanEqualTo ] ]LessThan LessThanEqualTo

AbsInterface :: Expressionarray.read(Expressionidx)

also as: Expressionarray Expressionidx]

] Expressionarray.write(Expressionidx, Expressiondata)

also as: Expressionarray Expressionidx:=Expressiondata]

] ]

Expression Expression

fif fif

oo..feinrqs(tE(xp)ression

)

] Expressionfifo.deq( )

] Expressionfifo.clear( )

A.4.4 Miscellaneous
RuleName :: \ A-Za-z0-9 ,.:?]+" VariableName :: a-z] a-z0-9]*
A.5 Source Term
SourceTerm :: Init Expression ] Init Expression RhsWhereClause

118

Appendix B TRSpec Description of a MIPS Processor

B.1 Type De nitions

B.1.1 Processor States

Type

PROC = Proc(PC O,RF,BD,BE,BM,BW,IPORT,DPORT R,DPORT W,SHIFTER)

User Visible Registers

OType Type Type Type
Type

PC O = PC PC = Bit 32]
RF = Array RNAME] VAL
RNAME = Reg0 || Reg1 || Reg2 || Reg3 || Reg4 || Reg5 || Reg6 || Reg7 || Reg8 || Reg9 || Reg10 || Reg11 || Reg12 || Reg13 || Reg14 || Reg15 || Reg16 || Reg17 || Reg18 || Reg19 || Reg20 || Reg21 || Reg22 || Reg23 || Reg24 || Reg25 || Reg26 || Reg27 || Reg28 || Reg29 || Reg30 || Reg31
VAL = Bit 32]

Pipeline Stage Bu ers

Type

BD = Fifo BD TEMPLATE

Type BD TEMPLATE = BdTemp(PC,INST)

TypeSyn TypeSyn TypeSyn

BE = BS BM = BS BW = BS

119

ABSType

BS = enq(BS TEMPLATE)

|| deq( ) || clear( ) || isdest(RNAME) BOOL || forward(RNAME) VAL

|| canforward(RNAME) BOOL || first( ) BS TEMPLATE || notempty( ) BOOL || notfull( ) BOOL

Type

BOOL = False || True

Note: For readability a disjunct term without any subterms,

such as True( ), can be written as the constructor name alone

without being followed by parentheses.

Type BS TEMPLATE = BsTemp(PC,I TEMPLATE)

Type I TEMPLATE = Itemp(MINOROP,WBACK,READY,RD,VAL,VAL,VAL)

Type

MINOROP = MAdd || MAddu || MAnd || MSub || MSubu || MNor || MOr || MXor || MSlt || MSll || MSra || MSrl || MLoad || MStore || MWback || MNop

|| MOnemore

TypeSyn WBACK = BOOL

TypeSyn READY = BOOL

Barrel Shifter

Type Type Type ABSType

AMOUNT = Bit 5] LEFT = Right || Left
ARITH = Logical || Arith SHIFTER = shift(AMOUNT,LEFT,ARITH,VAL) VAL

Input and Output

IType Type Type OType IType SOType IType Type SOType SOType SOType

IPORT = INST DPORT R = DportR(RADDR,RDATA) DPORT W = DportW(WADDR,WDATA,WVALID)
IADDR = PC IDATA = INST RADDR = ADDR RDATA = VAL
ADDR = Bit 32] WADDR = ADDR WDATA = VAL WVALID = BOOL

B.1.2 Instruction Set Architecture

Type

INST = Mips(OP,RD,RS,RT,SA,FUNC)

120

Type
TypeSyn TypeSyn TypeSyn Type Type
Type

OP = Special || Bcond || Jj || Jal || Beq || Bne || Blez || Bgtz || Addi || Addiu || Slti || Sltiu || Andi || Ori || Xori || Lui || Cop0 || Cop1 || Cop2 || Cop3 || Op24 || Op25 || Op26 || Op27 || Op30 || Op31 || Op32 || Op33 || Op34 || Op35 || Op36 || Op37 || Lb || Lh || Lwl || Lw || Lbu || Lhu || Lwr || Op47 || Sb || Sh || Swl || Sw || Op54 || Op55 || Swr || Op57 || Lwc0 || Lwc1 || Lwc2 || Lwc3 || Op64 || Op65 || Op66 || Op67 || Swc0 || Swc1 || Swc2 || Swc3 || Op74 || Op75 || Op76 || Op77
RD = RNAME RS = RNAME RT = RNAME
SA = Bit 5]
FUNC = Sll || Func01 || Srl || Sra || Sllv || Func05 || Srlv || Srav || Jr || Jalr || Func12 || Func13 || SysCall || Break || Func16 || Func17 || Mfhi || Mthi || Mflo || Mtlo || Func24 || Func25 || Func26 || Func27 || Mult || Multu || Div || Divu || Func34 || Func35 || Func36 || Func37 || Add || Addu || Sub || Subu || And || Or || Xor || Nor || Func50 || Func51 || Slt || Sltu || Func54 || Func55 || Func56 || Func57 || Func60 || Func61 || Func62 || Func63 || Func64 || Func65 || Func66 || Func67 || Func70 || Func71 || Func72 || Func73 || Func74 || Func75 || Func76 || Func77
BCOND = Bltz || Bgez || Bcond02 || Bcond03 || Bcond04 || Bcond05 || Bcond06 || Bcond07 || Bcond10 || Bcond11 || Bcond12 || Rim13 || Bcond14 || Bcond15 || Bcond16 || Bcond17 || Bltzal || Bgezal || Bcond22 || Bcond23 || Bcond24 || Bcond25 || Bcond26 || Bcond27 || Bcond30 || Bcond31 || Bcond32 || Rim33 || Bcond34 || Bcond35 || Bcond36 || Bcond37
121

B.2 Rules
B.2.1 M4 Macros
define(`STALL',`((be.isdest($1)&&!be.canforward($1)) || (bm.isdest($1)&&!bm.canforward($1)) || (bw.isdest($1)&&!bw.canforward($1)))')
define(`FORWARD',`(be.canforward($1)? be.forward($1): (bm.canforward($1)? bm.forward($1): (bw.canforward($1)?bw.forward($1):rf $1])))')
B.2.2 Fetch Stage Rules
Rule "Instruction Fetch and Speculate" Proc(pc,rf,bd,be,bm,bw,inst,rport,wport,shftr) if bd.notfull( )
==> Proc(pc+4,rf,bd.enq(BdTemp(pc,inst)),be,bm,bw, inst,rport,wport,shftr)
122

B.2.3 Decode Stage Rules
I-Type Instructions
Rule "Decode Immediate" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && !STALL(rs) && bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,rs,rt,immh,immm,imml) = inst
imm16 = fimmh,immm,immlg
canhandle = (op==Addi) || (op==Addiu) || (op==Slti) || (op==Sltiu) || (op==Andi) || (op==Ori) || (op==Xori)
==> Proc(pc,rf,bd.deq( ),be',bm,bw,iport,rport,wport,shftr) where be' = be.enq(BsTemp(pc',itemp)) itemp = (rt==Reg0)? Itemp(MNop,False,False,-,-,-,-): Itemp(mop,True,False,rt,vs,vimm,-) vs = FORWARD(rs)
vimm = f(immh 4:4]?16'hffff:16'h0000),imm16 15:0]g
mop = switch(op) case Addi: MAdd case Addiu: MAdd case Slti: MSlt case Sltiu: MSlt case Andi: MAnd case Ori: MOr case Xori: MXor
Rule "Instruction Decode Lui" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(Lui,rd,rs,immh,immm,imml) = inst
imm16 = fimmh,immm,immlg
==> Proc(pc,rf,bd.deq( ),be',bm,bw,iport,rport,wport,shftr) where be' = be.enq(BsTemp(pc',itemp)) itemp = (rd==Reg0)? Itemp(MNop,False,False,-,-,-,-):
Itemp(MWback,True,True,rd,fimm16 15:0],16'b0g,-,-)
123

Rule "Instruction Decode Load" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && !STALL(base) && bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,base,rt,offh,offm,offl) = inst
offset = f(offh 4:4]?(16'hffff):16'h0), offh,offm,offlg
canhandle = (op==Lw) ==> Proc(pc,rf,bd.deq( ),be',bm,bw,iport,rport,wport,shftr)
where be' = be.enq(BsTemp(pc',itemp)) itemp = (rt==Reg0)? Itemp(MNop,False,False,-,-,-,-): Itemp(mop,True,False,rt,vbase,offset,-) vbase = FORWARD(base) mop = switch(op) case Lw: MLoad
Rule "Instruction Decode Store" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && !(STALL(base) || STALL(rt)) && bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,base,rt,offh,offm,offl) = inst
offset = f(offh 4:4]?(16'hffff):16'h0),offh,offm,offlg
canhandle = (op==Sw) ==> Proc(pc,rf,bd.deq( ),be',bm,bw,iport,rport,wport,shftr)
where be' = be.enq(BsTemp(pc', Itemp(mop,False,False,-,vbase,offset,vt)))
vbase = FORWARD(base) vt = FORWARD(rt) mop = switch(op)
case Sw: MStore
Rule "Decode R-Compare Branch Taken" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && !(STALL(rs) || STALL(rt)) && taken && bd.notempty( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,rs,rt,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (op==Beq) || (op==Bne) ==> Proc(target,rf,bd.clear( ),be,bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs) vt = FORWARD(rt)
voff = f(offh 4:4]?(14'h3fff):(14'h0000)),(offset 15:0]),2'b00g
taken = switch(op) case Beq:(vs==vt) case Bne:(vs!=vt)
target = pc'+32'h4+voff
124

Rule "Decode R-Compare Branch Not-Taken" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && !(STALL(rs) || STALL(rt)) && !taken && bd.notempty( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,rs,rt,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (op==Beq) || (op==Bne) ==> Proc(pc,rf,bd.deq( ),be,bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs) vt = FORWARD(rt) taken = switch(op) case Beq:(vs==vt) case Bne:(vs!=vt)
Rule "Decode Compare-To-Zero Branch Taken" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && !STALL(rs) && taken && bd.notempty( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,rs,-,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (op==Blez) || (op==Bgtz) ==> Proc(target,rf,bd.clear( ),be,bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs)
voff = f(offh 4:4]?(14'h3fff):(14'h0000)),(offset 15:0]),2'b00g
taken = switch(op) case Blez:(vs 31:31]) || (vs==0) case Bgtz:(!vs 31:31]) || (vs!=0)
target = pc'+32'h4+voff Rule "Decode Compare-To-Zero Branch Not-Taken"
Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && !STALL(rs) && !taken && bd.notempty( )
where BdTemp(pc',inst) = bd.first( ) Mips(op,rs,-,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (op==Blez) || (op==Bgtz) ==> Proc(pc,rf,bd.deq( ),be,bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs) taken = switch(op) case Blez:(vs 31:31]) || (vs==0) case Bgtz:(!vs 31:31]) || (vs!=0)
125

Rule "Decode Bcond Branch Taken" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if !(STALL(rs)) && taken && bd.notempty( ) where BdTemp(pc',inst) = bd.first( ) Mips(Bcond,rs,type,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (type==Bltz) || (type==Bgez) ==> Proc(target,rf,bd.clear( ),be,bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs)
voff = f(offh 4:4]?(14'h3fff):(14'h0000)),(offset 15:0]),2'b00g
taken = switch(type) case Bltz:(vs 31:31]) case Bgez:(!vs 31:31])
target = pc'+32'h4+voff Rule "Decode Bcond Branch Not-Taken"
Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if !(STALL(rs)) && !taken && bd.notempty( )
where BdTemp(pc',inst) = bd.first( ) Mips(Bcond,rs,type,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (type==Bltz) || (type==Bgez) ==> Proc(pc,rf,bd.deq( ),be,bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs)
voff = f(offh 4:4]?(14'h3fff):(14'h0000)),(offset 15:0]),2'b00g
taken = switch(type) case Bltz:(vs 31:31]) case Bgez:(!vs 31:31])
Rule "Decode Bcond Branch-and-Link Taken" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if !(STALL(rs)) && taken && bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(Bcond,rs,type,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (type==Bltzal) || (type==Bgezal) ==> Proc(target,rf,bd.clear( ),be',bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs)
voff = f(offh 4:4]?(14'h3fff):(14'h0000)),(offset 15:0]),2'b00g
taken = switch(type) case Bltzal:(vs 31:31]) case Bgezal:(!vs 31:31])
be' = be.enq(BsTemp(pc', Itemp(MWback,True,True,Reg31,pc'+8,-,-)))
target = pc'+32'h4+voff
126

Rule "Decode Bcond Branch-and-Link Not-Taken" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if !(STALL(rs)) && !taken && bd.notempty( ) where BdTemp(pc',inst) = bd.first( ) Mips(Bcond,rs,type,offh,offm,offl) = inst
offset = foffh,offm,offlg
canhandle = (type==Bltzal) || (type==Bgezal) ==> Proc(pc,rf,bd.deq( ),be,bm,bw,iport,rport,wport,shftr)
where vs = FORWARD(rs)
voff = f(offh 4:4]?(14'h3fff):(14'h0000)),(offset 15:0]),2'b00g
taken = switch(type) case Bltzal:(vs 31:31]) case Bgezal:(!vs 31:31])
J-Type Instructions
Rule "Instruction Decode J" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if bd.notempty( ) where BdTemp(pc',inst) = bd.first( ) Mips(Jj,off5,off4,off3,off2,off1) = inst
offset = foff5,off4,off3,off2,off1g
==> Proc(target,rf,bd.clear( ),be,bm,bw,iport,rport,wport,shftr)
where target = fpc' 31:28],offset 25:0],2'b00g
Rule "Instruction Decode Jal" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(Jal,off5,off4,off3,off2,off1) = inst
offset = foff5,off4,off3,off2,off1g
==> Proc(target,rf,bd.clear( ),be',bm,bw,iport,rport,wport,shftr)
where target = fpc' 31:28],offset 25:0],2'b00g
be' = be.enq(BsTemp(pc', Itemp(MWback,True,True,Reg31,pc'+8,-,-)))
127

R-Type Instructions
Rule "Instruction Decode Constant Shifts" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if (op==Special) && canhandle && !STALL(rt) && bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,-,rt,rd,sa,func) = inst
imm16 = frt,sa,funcg
canhandle = (func==Sll) || (func==Sra) || (func==Srl) ==> Proc(pc,rf,bd.deq( ),be',bm,bw,iport,rport,wport,shftr)
where be' = be.enq(BsTemp(pc',itemp)) itemp = (rd==Reg0)? Itemp(MNop,False,False,-,-,-,-): Itemp(mop,True,False,rd,sa,vt,-) vt = FORWARD(rt) mop = switch(func) case Sll: MSll case Sra: MSra case Srl: MSrl
128

Rule "Instruction Decode Triadic" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if (op==Special) && canhandle && !(STALL(rs) || STALL(rt)) && bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(op,rs,rt,rd,-,func) = inst canhandle = (func==Add) || (func==Addu) || (func==Sub) || (func==Subu) || (func==And) || (func==Nor) || (func==Or) || (func==Xor) || (func==Slt) || (func==Sltu) || (func==Sllv) || (func==Srav) || (func==Srlv)
==> Proc(pc,rf,bd.deq( ),be',bm,bw,iport,rport,wport,shftr) where be' = be.enq(BsTemp(pc',itemp)) itemp = (rd==Reg0)? Itemp(MNop,False,False,-,-,-,-): Itemp(mop,True,False,rd,vs,vt,-) vs = FORWARD(rs) vt = FORWARD(rt) mop = switch(func) case Add: MAdd case Addu: MAdd case Sub: MSub case Subu: MSub case And: MAnd case Nor: MNor case Or: MOr case Xor: MXor case Slt: MSlt case Sltu: MSlt case Sllv: MSll case Srav: MSra case Srlv: MSrl
Rule "Decode Jr" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if !STALL(rs) && bd.notempty( ) where BdTemp(pc',inst) = bd.first( ) Mips(Special,rs,-,-,-,Jr) = inst
==> Proc(vs,rf,bd.clear( ),be,bm,bw,iport,rport,wport,shftr) where vs = FORWARD(rs)
129

Rule "Instruction Decode Jalr" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if !STALL(rs) && bd.notempty( ) && be.notfull( ) where BdTemp(pc',inst) = bd.first( ) Mips(Special,rs,-,rd,-,Jalr) = inst
==> Proc(vs,rf,bd.clear( ),be',bm,bw,iport,rport,wport,shftr) where be' = be.enq(BsTemp(pc',itemp)) itemp = (rd==Reg0)? Itemp(MNop,False,False,-,-,-,-): Itemp(MWback,True,True,rd,pc'+8,-,-) vs = FORWARD(rs)
B.2.4 Execute Stage Rules
Rule "Execute Stage Drop" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if be.notempty( ) && ((template==Itemp(MNop,-,-,-,-,-,-))) where BsTemp(pc',template) = be.first( )
==> Proc(pc,rf,bd,be.deq( ),bm,bw,iport,rport,wport,shftr) Rule "Execute Stage Pass"
Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if ((template==Itemp(MWback,-,-,-,-,-,-))) && be.notempty( ) && bm.notfull( )
where BsTemp(pc',template) = be.first( ) ==> Proc(pc,rf,bd,be.deq( ),bm',bw,iport,rport,wport,shftr)
where bm' = bm.enq(BsTemp(pc',template))
130

Rule "Execute 2-to-1 Function" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && be.notempty( ) && bm.notfull( ) where BsTemp(pc',template) = be.first( ) Itemp(mop,fwd,-,dest,v1,v2,-) = template canhandle = (mop==MAdd) || (mop==MSub) || (mop==MAnd) || (mop==MNor) || (mop==MOr) || (mop==MXor) || (mop==MSlt)
==> Proc(pc,rf,bd,be.deq( ),bm',bw,iport,rport,wport,shftr) where bm' = bm.enq(BsTemp(pc',template')) template' = Itemp(MWback,fwd,True,dest,result,-,-) result = switch(mop) case MAdd: v1+v2 case MSub: v1-v2 case MAnd: v1&v2 case MNor: (~(v1|v2)) case MOr: v1|v2 case MXor: v1^v2 case MSlt: (v1 31:31]==v2 31:31])?(v1<v2):v1 31:31]
Rule "Execute Shift Function" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && be.notempty( ) && bm.notfull( ) where BsTemp(pc',template) = be.first( ) Itemp(mop,fwd,-,dest,v1,v2,-) = template canhandle = (mop==MSll) || (mop==MSra) || (mop==MSrl)
==> Proc(pc,rf,bd,be.deq( ),bm',bw,iport,rport,wport,shftr) where bm' = bm.enq(BsTemp(pc',template')) template' = Itemp(MWback,fwd,True,dest,result,-,-) result = shftr.shift(v1 4:0],left,arith,v2) left = switch(mop) case MSll: Left case MSra: Left case MSrl: Right arith = switch(mop) case MSll: Logical case MSra: Arith case MSrl: Logical
131

Rule "Execute Address Calc" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if canhandle && be.notempty( ) && bm.notfull( ) where BsTemp(pc',template) = be.first( ) Itemp(mop,fwd,-,dest,base,offset,v) = template canhandle = (mop==MLoad) || (mop==MStore)
==> Proc(pc,rf,bd,be.deq( ),bm',bw,iport,rport,wport,shftr) where bm' = bm.enq(BsTemp(pc',template')) template' = Itemp(mop,fwd,False,dest,addr,-,v) addr = base+offset
B.2.5 Memory Stage Rules
Rule "Memory Stage Pass" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if (template!=Itemp(MLoad,-,-,-,-,-,-)) && (template!=Itemp(MStore,-,-,-,-,-,-)) && bm.notempty( ) && bw.notfull( ) where BsTemp(pc',template) = bm.first( )
==> Proc(pc,rf,bd,be,bm.deq( ),bw',iport,rport,wport,shftr) where bw' = bw.enq(BsTemp(pc',template))
Rule "Memory Stage Store" Proc(pc,rf,bd,be,bm,bw,iport,rport,-,shftr) if bm.notempty( ) where BsTemp(pc',template) = bm.first( ) Itemp(MStore,-,-,-,addr,-,v) = template
==> Proc(pc,rf,bd,be,bm.deq( ),bw,iport,rport,DportW(addr,v,True),shftr) Rule "Memory Stage Store Off"
Proc(pc,rf,bd,be,bm,bw,iport,rport,DportW(-,-,-),shftr) if !(bm.notempty( ) && Itemp(MStore,-,-,-,-,-,-)==template)
where BsTemp(pc',template) = bm.first( ) ==> Proc(pc,rf,bd,be,bm,bw,iport,rport,DportW(-,-,False),shftr) Rule "Memory Stage Load"
Proc(pc,rf,bd,be,bm,bw,iport,DportR(-,data),wport,shftr) if bm.notempty( ) && bw.notfull( )
where BsTemp(pc',template) = bm.first( ) Itemp(MLoad,fwd,-,rd,addr,-,-) = template
==> Proc(pc,rf,bd,be,bm.deq( ),bw',iport,DportR(addr,-),wport,shftr) where bw' = bw.enq(BsTemp(pc',template')) template' = Itemp(MWback,fwd,True,rd,data,-,-)
132

B.2.6 Write-Back Stage Rules
Rule "Write-Back Stage" Proc(pc,rf,bd,be,bm,bw,iport,rport,wport,shftr) if bw.notempty( ) && (template==Itemp(MWback,-,-,-,-,-,-)) where BsTemp(pc',template) = bw.first( ) Itemp(MWback,-,-,rd,v,-,-) = template
==> Proc(pc,rf rd:=v],bd,be,bm,bw.deq( ),iport,rport,wport,shftr)
B.3 Source Term
Init Proc(0,-,-,-,-,-,-,-,DportW(-,-,False),-)
133

134

Bibliography
Ann] Annapolis Micro Systems, Inc. A family of recon gurable computing engines. http://www.annapmicro.com.
AS99] Arvind and X. Shen. Using term rewriting systems to design and verify processors. IEEE Micro Special Issue on Modeling and Validation of Microprocessors, May 1999.
Aug00] L. Augustsson. BlueSpec language de nition. Working Draft, March 2000. Ber98] G. Berry. The foundations of Esterel. In Proof, Language and Interaction:
Essays in Honour of Robin Milner. MIT Press, 1998. BN98] F. Baader and T. Nipkow. Term Rewriting and All That. Cambridge
University Press, 1998. BRM+99] J. Babb, M. Rinard, C. A. Moritz, W. Lee, M. Frank, R. Barua, and
S. Amarasinghe. Parallelizing applications into silicon. In Proceedings of the 7th IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM'99), Napa Valley, CA, April 1999. Cad] Cadence Design Systems, Inc. A rma NC Verilog simulator. http://www.cadence.com/datasheets/a rma nc verilog sim.html. Co-] Co-Design Automation, Inc. Superlog. http://www.co-design.com/ superlog. CW91] R. Camposano and W. Wolf, editors. High-level VLSI Synthesis. Kluwer Academic Publishers, 1991. DWA+92] D. W. Dobberpuhl, R. T. Witek, R. Allmon, R. Anglin, D. Bertucci, S. Britton, L. Chao, R. A. Conrad, D. E. Dever, B. Gieseke, S. M. N. Hassoun, G. W. Hoeppner, K. Kuchler, M. Ladd, B. M. Leary, L. Madden, E. J. McLellan, D. R. Meyer, J. Montanaro, D. A. Priore, V. Rajagopalan, S. Samudrala, and S. Santhanam. A 200-MHz 64-bit dual-issue CMOS microprocessor. Digital Technical Journal, 4(4), 1992. Fey99] R. P. Feynman. The pleasure of nding things out : the best short works of Richard P. Feynman. Perseus Books, 1999.
135

FPF95] A. Fauth, J. Van Praet, and M. Freericks. Describing instruction set processors using nML. In Proceedings of European Design and Test Conference (ED&TC'95), Paris, France, March 1995.
Gal95] D. Galloway. The Transmogri er C hardware description language and compiler for FPGAs. In Proceedings of IEEE Workshop on FPGAs for Custom Computing Machines (FCCM'95), Napa Valley, CA, April 1995.
GDWL92] D. Gajski, N. Dutt, A. Wu, and S. Lin. High-level Synthesis: Introduction to Chip and System Design. Kluwer Academic Publishers, 1992.
GG97] M. Gokhale and E. Gomersall. High level compilation for ne grained FPGAs. In Proceedings of the IEEE Symposium on FPGA-based for Custom Computing Machines (FCCM'97), Napa Valley, CA, April 1997.
GM93] M. Gokhale and R. Minnich. FPGA computing in a data parallel C. In Proceedings of IEEE Workshop on FPGAs for Custom Computing Machines (FCCM'93), Napa Valley, CA, April 1993.
GZD+00] D. D. Gajski, J. Zhu, R. Domer, A. Gerslauer, and S. Zhao. SpecC Speci cation Language and Methodology. Kluwer Academic Publishers, 2000.
HHD93] I. J. Huang, B. Holmer, and A. Despain. ASIA: Automatic synthesis of instruction-set architectures. In Proceedings of the 2nd Workshop on Synthesis and System Integration of Mixed Technologies (SASIMI'93), Nara, Japan, October 1993.
HHD97] G. Hadjiyiannis, S. Hanono, and S. Devadas. ISDL: An instruction set description language for retargetbility. In Proceedings of the 34th ACM/IEEE Design Automation Conference (DAC'97), Anaheim, CA, June 1997.
Hoa85] C. A. R. Hoare. Communicating Sequential Processes. Prentice-Hall International, 1985.
HP96] J. L. Hennessy and D. A. Patterson. Computer Architecture: A Quantitative Approach. Morgan Kaufmann, 2nd edition, 1996.
Ins88] The Institute of Electrical Electronics Engineers, Inc., New York. IEEE Standard VHDL Language Reference Manual, 1988.
JHA+98] S. P. Jones, J. Hughes, L. Augustsson, D. Barton, B. Boutel, W. B, J. Fasel, K. Hammond, R. Hinze, P. Hudak, T. Johnsson, M. Jones, J. Launchbury, E. Meijer, J. Peterson, A. Reid, C. Runciman, and P. Wadler. Haskell 98: A Non-strict, Purely Functional Language, 1998. http://www.haskell.org.
Joh91] M. Johnson. Superscalar Microprocessor Design. Prentice Hall, 1991.
136

Kan87] G. Kane. MIPS R2000 RISC Architecture. Prentice Hall, 1987. Klo92] J. W. Klop. Term Rewriting System, volume 2 of Handbook of Logic in
Computer Science. Oxford University Press, 1992. Lis00] M. Lis. Superscalar processors via automatic microarchitecture transfor-
mations. Master's thesis, Massachusetts Institute of Technology, Cambridge, MA, June 2000. LS99] L. Lavagno and E. Sentovich. ECL: A speci cation environment for system-level design. In Proceedings of the 36th ACM/IEEE Design Automation Conference (DAC'99), New Orleans, LA, June 1999. LSI] LSI Logic Corporation. ASIC products. http://www.lsilogic.com/ products/asic/index.html. LTG97] S. Liao, S. Tjinag, and R. Gupta. An e cient implementation of reactivity for modeling hardware in the Scenic design environment. In Proceedings of the 34th ACM/IEEE Design Automation Conference (DAC'97), Anaheim, CA, June 1997. Mar84] P. Marwedel. The Mimola design system: Tools for the design of digital processors. In Proceedings of the 21st ACM/IEEE Design Automation Conference (DAC'84), Albuquerque, New Mexico, 1984. Mic99] G. De Micheli. Hardware synthesis from C/C++ models. In Proceedings of Design, Automation and Test in Europe (DATE'99), Munich, Germany, March 1999. ML99] J. Matthews and J. Launchbury. Elementary microarchitecture algebra. In Proceedings of Conference on Computer-Aided Veri cation, Trento, Italy, July 1999. MLC98] J. Matthews, J. Launchbury, and B. Cook. Microprocessor speci cation in Hawk. In Proceedings of the 1998 International Conference on Computer Languages, Chicago, IL, 1998. MLD92] P. Michel, U. Lauther, and P. Duzy, editors. The Synthesis Approach to Digital System Design. Kluwer Academic Publishers, 1992. MR99] M. Marinescu and M. Rinard. A synthesis algorithm for modular design of pipelined circuits. In Proceedings of X IFIP International Conference on VLSI (VLSI 99), Lisbon, Portugal, November 1999. PSH+92] I. Pyo, C. Su, I. Huang, K. Pan, Y. Koh, C. Tsui, H. Chen, G. Cheng, S. Liu, S. Wu, , and A. M. Despain. Application-driven design automation for microprocessor design. In Proceedings of the 29th ACM/IEEE Design Automation Conference (DAC'92), Anaheim, CA, June 1992.
137

Raj89] V. K. Raj. DAGAR: An automatic pipelined microarchitecture synthesis system. In Proceedings of the International Conference on Computer Design (ICCD'89), Boston, MA, October 1989.

Ros00] D. L. Rosenband. Synthesis of multi-cycle operation-centric descriptions. PhD Dissertation Proposal, Massachusetts Institute of Technology, June 2000.

SAR99a] X. Shen, Arvind, and L. Rudolph. CACHET: An adaptive cache coherence protocol for distributed shared-memory systems. In Proceedings of the 13th ACM SIGARCH International Conference on Supercomputing, Rhodes, Greece, June 1999.

SAR99b] X. Shen, Arvind, and L. Rudolph. Commit-reconcile & fences (CRF): A new memory model for architects and compiler writers. In Proceedings of the 26th International Symposium on Computer Architecture (ISCA'99), Atlanta, Georgia, May 1999.

SM98]

L. Semeria and G. De Micheli. SpC: Synthesis of pointers in C, application of pointer analysis to the behavioral synthesis from C. In Proceedings of International Conference on Computer-Aided Design (ICCAD'98), San Jose, CA, November 1998.

SRI97] SRI International, University of Cambridge. The HOL System Tutorial, Version 2, July 1997.

Sta90] Stanford University. HardwareC { A Language for Hardware Design, December 1990.

Syna] Synopsys, Inc. CBA libraries datasheet. http://www.synopsys.com/ products/siarc/cba lib one.html.

Synb] Synopsys, Inc. HDL Compiler for Verilog Reference Manual.

Sync] Synopsys, Inc. RTL Analyzer Reference Manual.

Synd] Synplicity, Inc. Synplify User's Guide, Version 3.0.

TM96] D. E. Thomas and P. R. Moorby. The Verilog Hardware Description Language. Kluwer Academic Publishers, 3rd edition, 1996.

TPPW99] D. E. Thomas, J. M. Paul, S. N. Pe ers, and S. J. Weber. Peer-based multithreaded executable co-speci cation. In Proceedings of International Workshop on Hardware/Software Co-Design, San Diego, CA, May 1999.

VBR+96] J. Vuillemin, P. Bertin, D. Roncin, M. Shand, H. Touati, and P. Boucard. Programmable active memories: Recon gurable systems come of age. IEEE Transactions on VLSI, 4(1), March 1996.

138

WC91] Win95]
Xila] Xilb]

R. A. Walker and R. Camposano, editors. A Survey of High-Level Synthesis Systems. Kluwer Academic Publishers, 1991. P. J. Windley. Verifying pipelined microprocessors. In Proceedings of the 1995 IFIP Conference on Hardware Description Languages and their Applications (CHDL'95), Tokyo, Japan, 1995. Xilinx, Inc. The Programmable Logic Data Book. Xilinx, Inc. Xilinx foundation series. http://www.xilinx.com/products/ found.htm.

139


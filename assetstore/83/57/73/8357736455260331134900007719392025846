Inductive Inference: Theory and Methods 
DANA ANGLUIN 
Department ofComputer Science, Yale University, New Haven, Connecticut 06520 
AND 
CARL H. SMITH 
Department ofComputer Science, The University of Maryland, College Park, Maryland 20742 
There has been a great deal of theoretical nd experimental work in computer science on 
inductive inference systems, that is, systems that try to infer general rules from examples. 
However, acomplete and applicable theory of such systems is still a distant goal. This 
survey highlights and explains the main ideas that have been developed in the study of 
inductive inference, with special emphasis on the relations between the general theory 
and the specific algorithms and implementations. 
Categories and Subject Descriptors: F.1.0 [Computation by Abstract Devices]: 
General; 1.2.2 [Artificial Intelligence]: Automatic Programming--program synthesis; 
1.2.6 [Artificial Intelligence]: Learning--induction 
General Terms: Algorithms, Theory 
Additional Key Words and Phrases: Inductive inference, learning by example 
INTRODUCTION 
The term "inductive inference" denotes the 
process of hypothesizing a general rule from 
examples. For example, given the strings 
011, 000011, 00111, 0001, 0011, 
one could conjecture that they are charac- 
terized by the rule 
any number of O's followed by any number 
ofl's. 
This kind of process eems to be a funda- 
mental and ubiquitous component of intel- 
ligent behavior. But a formal study of these 
processes raises a number of questions, 
among them: 
• How can we evaluate inference methods? 
• What are good criteria for comparing hy- 
potheses? 
• Can theoretical models of inference meth- 
ods be useful? 
Most of the work in inductive inference can 
be viewed as attempts to answer these ques- 
tions. 
The scope of this survey encompasses 
work done within the general paradigm of 
inductive inference stablished by Gold in 
his fundamental paper [Gold 1967], as de- 
scribed in subsequent sections. There are 
two branches in inductive inference re- 
search: one is concerned with the general 
theoretical properties of inference methods, 
and the other with finding specific, practi- 
cal inference methods. We shall describe 
and compare the major ideas and results 
from both branches. Even within this lim- 
ited scope, we are bound to omit significant 
ideas, issues, and references, for which we 
ask the reader's forbearance. 
It is useful not to confuse inductive in- 
ference with learning, although induction 
may be used in learning. Webster's Diction- 
ary indicates the spirit of this distinction 
Permission to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial dvantage, the ACM copyright notice and the title of the publication and its 
date appear, and notice is given that copying is by permission ofthe Association for Computing Machinery. To 
copy otherwise, or to republish, requires a fee and/or specific permission. 
© 1983 ACM 0360-0300/83/0900-237 $00.75 
Computing Surveys, Vol. 15, No. 3, September 1983 
238 * D. Angluin and C. H. Smith 
CONTENTS 
INTRODUCTION 
I. BASICS 
1.1 An Example 
1.2 Two Concepts 
1.3 Specifying Inference Problems 
2. DOMAINS,  GUESSES,  AND EXAMPLES 
2.1 Functions, Sequences, and Traces 
2.2 Languages and Predicates 
2.3 Stochastic Languages 
3. CRITERIA FOR EVALUATING INFERENCE 
METHODS 
3.1 What Is an Inference Method? 
3.2 Criteria of Success: Scope, Power, and Infer- 
ability 
3.3 Quality of Hypotheses 
3.4 Data Efficiency 
3.5 Numbers of Hypotheses, Mind Changes, 
Errors 
4. COMPARING INFERENCE CRITERIA 
4.1 Identification in the Limit 
4.2 Other Inference Criteria 
4.3 The Effects of Data Presentation 
4,4 Restrictions on Inference Methods 
5. COMPARING HYPOTHESES 
5.1 Simplicity of Hypotheses 
5.2 A Set-Theoretic Goodness of Fit Measure 
5.3 Mixed Measures: Derivational Complexity 
5.4 Mixed Measures: Bayesian Approaches 
5.5 Other Mixed Measures 
6. GENERAL  INFERENCE METHODS 
6.1 Identification of Resource-Bounded Classes 
6.2 Methods Uniformly Adaptable to Different 
Domains 
6.3 A Complexity Theory for Inference Methods 
7. PRACT ICAL  INFERENCE METHODS 
7.1 Search Spaces 
7.2 Pruning: Forbidden Features 
7.3 Hill Climbing 
7.4 Jumping to Conclusions: Plausible Features 
7.5 Efficient and Characterizable Methods 
7.6 LISP Program Inference 
8. SKETCH OF  SOME APPL ICAT IONS 
9. FUTURE D IRECT IONS 
ACKNOWLEDGMENTS 
REFERENCES 
A 
• T 
in its definitions. "To learn" is "to gain 
knowledge, understanding, or skill by 
study, instruction, or experience," or more 
broadly, "to come to be able to." In contrast, 
"induction" is defined as "the act, process, 
or result of an instance of reasoning from 
a part to a whole, from particulars to gen- 
erals, or from the individual to the univer- 
sal." Researchers in artificial intelligence 
are investigating many aspects of the 
broader topic of learning. The survey article 
by Dietterich et al. [1982] divides the topic 
of learning into four areas: rote learning, 
learning by being told, learning from ex- 
amples, and learning by analogy. Of these, 
learning from examples overlaps with in- 
ductive inference, but there are some dif- 
ferences in emphasis. Often the work in 
artificial intelligence is more concerned 
with cognitive modeling than the work in 
inductive inference, and less concerned 
with formal properties uch as convergence 
in the limit or computational efficiency. 
The collection of articles edited by Mich- 
alski et al. [1983] covers a number of issues 
and recent developments in machine learn- 
ing, and provides an extensive bibliography 
on the subject. 
Another area related to inductive infer- 
ence is the formal study of language acqui- 
sition within linguistics. Wexler and Culi- 
cover [1980], for example, describe a set of 
constraints on transformational grammars 
which guarantee that they may be learned 
with probability 1 from pairs consisting of 
a deep-structure parse and the surface sen- 
tence derived from it, where the degree of 
sentence embedding is at most 2. They 
emphasize use of learnability to suggest 
detailed restrictions on the possible gram- 
mars of natural anguages; most of the pay- 
off seems to be in linguistics rather than 
inductive inference. 
Other surveys covering some of the same 
material as the present one are Biermann 
and Feldman's [1972a] survey of grammat- 
ical inference, Fu and Booth's [1975] sur- 
vey of the inference of deterministic and 
stochastic grammars and its use in struc- 
tural pattern recognition, the survey by 
Klette and Wiehagen [1980] of East Ger- 
man work in inductive inference, D. 
Smith's [1982] survey of inferring LISP 
programs from examples, and Bundy and 
Silver's [1981] brief survey of rule learning 
programs. The comprehensive paper by 
Case and C. Smith [1983] gives a useful 
account of many of the theoretical results 
in inductive inference. See also the books 
by Fu [1975, 1977, 1982] and Gonzalez and 
Thomason [1978] on structural pattern rec- 
ognition and the need for grammatical in- 
ference. 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 239 
1. BASICS 
1.1 An Example 
We start with a concrete xample intended 
to help the reader interpret he abstract 
concepts described later. Consider the typ- 
ical IQ test problem of guessing the next 
term of the numerical sequence 3, 5, 7. A 
reasonable guess of the next term is 9, and 
we might be quite annoyed to find that the 
test maker had intended the sequence of 
odd prime numbers, so that the "correct" 
next term is 11 instead of 9. Yet what is 
the basis of this annoyance? There are 
infinitely many more or less plausible rules 
for generating different sequences that be- 
gin with the three terms 3, 5, 7. Is "the odd 
numbers tarting with 3" somehow prefer- 
able to all the others on grounds of simplic- 
ity? 
Suppose instead that we imagine a some- 
what different setting. Before us stands a 
black box (or computer terminal) that will 
play a game with us. It will display the first 
element of a numerical sequence, and we 
will guess the second element and type it 
in. The black box will then display YES or 
NO, according to whether our guess was 
correct, and also display the correct first 
and second elements. We then guess the 
third element and type it in, and so on. A 
possible sequence of interactions i  
It displays 2. 
We guess 3 is next. 
It displays YES:  2, 3. 
We guess 4 is next. 
It displays NO: 2, 3, 5. 
We guess 7 is next. 
It displays YES:  2, 3, 5, 7. 
We guess 11 is next. 
It displays YES:  2, 3, 5, 7, 11. 
And the interaction continues imilarly. 
A p laus ib le  c r i te r ion  of success is 
whether we eventually "catch on" to the 
rule being presented, that is, whether after 
a certain point our guesses of the next 
element are always correct. Of course, if the 
black box keeps changing the underlying 
rule, we may make infinitely many wrong 
guesses. Thus we restrict he box to select 
a rule at the start of play and not change it 
thereafter. 
Can we always beat the box in this set- 
ting? That seems to depend on the class of 
rules the box is allowed to select from. 
Suppose that the box starts with the rule 
The first term is 1 and 
each subsequent term is I greater 
than the guess of that term. 
Then every single guess will be off by pre- 
cisely -1. This may not seem like an ac- 
ceptable rule for the box to select; we shall 
clarify subsequently in what sense it may 
be considered legitimate. 
Let us now restrict he box still further 
so that it chooses a rule such that the term 
at position n of the sequence is expressible 
as a fixed polynomial in n with positive 
integer coefficients. One such sequence, 
specified by the polynomial n2 + 3, begins 
4, 7, 12, 19, 28, 39, . . . .  
Now can we beat the box? The answer is 
yes, and we shall demonstrate two different 
methods, which illustrate concepts pre- 
sented later. 
The first method is very general and 
powerful, and very crude. We begin to enu- 
merate systematically all the polynomials 
of one variable with integer coefficients. 
We could proceed by enumerating for d = 
0, 1, 2 . . . .  the finite number of (d + 1)- 
tuples of integers in the range f-d, d] and 
using them as the coefficients of 1, n, n 2, 
. . . ,  n d. One such enumeration begins 
0 ,1+n, l , l -n ,n ,  0 , -n ,  
-1 + n, -1, -1 - n, 
2 + 2n + 2n 2, 2 + 2n + n 2, . . . .  
Then, when required to guess the next term 
given the initial terms yl, y2 . . . .  , Yt, we 
find the first polynomial, say p(n), in this 
enumeration such that p(n)  = Yn for n = 
1, 2 . . . .  , t, and we guessp(t + 1). 
To see that this strategy always ucceeds, 
suppose that the box initially selects the 
polynomial q(n), where 
q(n)  = ao + aln + a2n 2 + • .. + arn r. 
When d exceeds the maximum of the ab- 
solute values of all the ai's and r, the poly- 
nomial q (n) appears in the enumeration for 
Computing Surveys, Vol. 15, No. 3, September 1983 
240 • D. Angluin and C. H. Smith 
d. Consider the first occurrence of q(n) in 
our enumeration. All of the polynomials 
preceding it are different from q(n) for 
some values of n, so when t is large enough, 
q(n) is the first polynomial in our enumer- 
ation that agrees with all the values yl, 
y2,. . . ,  yr. At this point and at every sub- 
sequent point we use q(n) to predict the 
next element, and so our guesses will all be 
correct from this point on. 
Th is  enumerat ion  method,  which 
searches ystematically through the whole 
space of possible rules eliminating those 
that disagree with the data until it finds 
one that agrees with the data, searches 
through more than d d different polynomials 
to arrive at the correct degree d explana- 
tion. A much more expeditious method is 
to take the initial segment yx, y2 . . . . .  yt of 
the sequence, interpolate a polynomial of 
degree at most t - 1 through these points, 
and use this polynomial to predict he next 
term. This approach will also succeed in 
the limit, with much less computational 
cost than the preceding method. In partic- 
ular, this method uses a number of arith- 
metic operations polynomial in d to con- 
verge to a rule of degree d, whereas the 
enumeration method uses a number of op- 
erations that are exponential in d. This 
suggests that the size of the search space 
(typically exponential) is not the only fac- 
tor determining the existence of efficient 
inference methods. 
Although the second method is much 
more efficient, it is also special to the do- 
main. If we were to add to the set of possible 
rules sequences that are sums of integer 
exponentials in n, for example, 2 n or 3 n + 
5 ~, it is immediately clear how to modify 
the first method to enumerate functions of 
this type so that it continues to beat the 
box, but it is much less straightforward to
modify the second method for this case. 
Note also that with neither method can 
we conclude that we have reached the point 
where our guesses will continue to be cor- 
rect. After seeing, say, 
0 ,0 ,0 ,0 ,0 ,0 ,  
we might be tempted to conclude that this 
polynomial sequence is always 0, but in- 
stead it might be generated by 
(n - 1)(n - 2)(n - 3) 
x (n - 4) (n  - 5 ) (n  - 6 ) ,  
so that the next term is 720 instead of 0. 
Without a bound on the degree of the 
polynomial being exemplified, there is no 
way to tell when we have successfully 
pinned it down. 
Our example illustrates several of the 
concepts formalized in inductive inference, 
and we shall refer to it in the discussion 
that follows. 
1.2 Two Concepts 
Gold's [1967] theoretical study of language 
learning introduced two fundamental con- 
cepts that have been very important in 
subsequent theoretical work in inductive 
inference: identification in the limit, and 
identification by enumeration. 
1.2.1 Identification in the Limit 
Identification in the limit views inductive 
inference as an infinite process. Then the 
eventual or limiting behavior of an induc- 
tive inference method may be used as one 
criterion of its success. In our example, the 
game of guessing the next element of the 
sequence isprolonged indefinitely, and suc- 
cess is defined as always making the correct 
guess after some finite number of errors. 
More abstractly, suppose that M is an 
inductive inference method that is attempt- 
ing to describe correctly some unknown 
rule R. If M is run repeatedly on larger and 
larger collections of examples of R, an in- 
finite sequence of M's conjectures i gen- 
erated, say, gl, g2, g3,. . . .  If there exists 
some number m such that gm is a correct 
description of R and 
gm= gm+l = gin+2 . . . .  , 
then M is said to identify R correctly in the 
limit on this sequence of examples. 
M may be viewed as learning more and 
more about the unknown rule R and suc- 
cessively modifying its conjecture about R. 
If after some finite time M stops modifying 
its conjecture and the final conjecture is a 
correct description of R, then M correctly 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive 
identifies R in the limit on this sequence of 
examples. Note that M cannot determine 
whether it has converged to a correct hy- 
pothesis, since new data may or may not 
conflict with the current conjecture. 
1.2.2 Identification by Enumeration 
Identification by enumeration is an ab- 
straction of our first method for guessing 
polynomial sequences, namely, systemati- 
cally searching the space of possible rules 
until one is found that agrees with all the 
data so far. Suppose that a particular do- 
main of rules is specified, and there is an 
enumeration of descriptions, say, dl, d2, 
d3 . . . . .  such that each rule in the domain 
has one or more descriptions in this enu- 
meration. (This corresponds to the enu- 
meration of polynomials in our example.) 
Given any collection of examples of a rule, 
the method of identification by enumera- 
tion goes down this list to find the first 
description, say di, that is compatible with 
the given examples and then conjectures 
di. (In our example, since the problem was 
to guess values, not rules, we used the 
polynomial to generate he next value.) 
This method may or may not achieve 
correct identification in the limit, and may 
or may not be computable. If the presen- 
tation of examples and the compatibility 
relation satisfy the following two condi- 
tions, then the enumeration method is 
guaranteed to identify in the limit all the 
rules in the domain: 
(1) A correct hypothesis i always compat- 
ible with the examples given. 
(2) Any incorrect hypothesis is incompat- 
ible with some sufficiently large collec- 
tion of examples and with all larger 
collections. 
In order that the enumeration method be 
computable, the enumeration dl, d2, d3 . . . .  
must be computable, and it must be possi- 
ble to compute whether a given description 
and a given collection of examples are com- 
patible. These conditions were all satisfied 
in our polynomial example, giving another 
demonstration of its correctness in the 
limit. 
The method of identification by enumer- 
ation is very general and powerful but also 
Inference: Theory and Methods * 241 
rather impractical because the size of the 
space that must be searched is typically 
exponential in the length of the description 
of the correct guess. Some improved var- 
iants of identification by enumeration are 
described in Section 7. 
1.3 Specifying Inference Problems 
Researchers have studied inductive infer- 
ence problems for a variety of types of 
objects including LISP programs, finite- 
state machines, tree grammars, Turing ma- 
chines, logical expressions, and stochastic 
context-free grammars, using various rep- 
resentations of sample inputs and inferred 
rules. To define an inductive inference 
problem, five items must be specified: 
(1) the class of rules being considered, usu- 
ally a class of functions or languages; 
(2) the hypothesis space, that is, a set of 
descriptions such that each rule in the 
class has at least one description in the 
hypothesis space; 
(3) for each rule, its set of examples, and 
the sequences of examples that consti- 
tute admissible presentations of the 
rule; 
(4) the class of inference methods under 
consideration; 
(5) the criteria for a successful inference. 
The problem in our example, the extrap- 
olation of polynomial sequences, could be 
specified as follows. The rule space is the 
set of all functions f from the positive in- 
tegers to the integers uch that f(n) is some 
fixed polynomial in n with integer coeffi- 
cients. The hypothesis pace is the set of 
polynomials in one variable with integer 
coefficients. (Since this is a problem of 
prediction rather than identification, the 
hypothesis space is immaterial, but see the 
next example.) For each rule f, its set of 
examples consists of pairs of the form 
(n , f (n ) ) ,  where n is a positive integer. 
There is only one admissible presentation 
of f, namely, the infinite sequence 
(1, f(1)), (2, f(2)), (3, f(3)), . . . ,  
which we abbreviate as 
f(1),f(2),f(3),.... 
Computing Sttrveys, Vol. 15, No. 3, September 1983 
242 • D. Anglu in and C. H. Smi th  
We were not very explicit about the class 
of inference methods in the example, but a 
reasonable restriction might be that any 
method be representable by a computer 
program taking as input a finite sequence 
of integers, always halting, and giving a 
single integer as output. The criterion we 
used was that a method M succeeds on a 
rule [, provided that there exists some in- 
teger N such that for all n _> N, 
M(/(1) , / (2)  . . . .  , [ (n))  = [ (n  + 1). 
To further illustrate these ideas, let us 
formalize a problem of guessing regular 
expressions from positive and negative 
data. Suppose we are given the facts that 
the strings 
0011, 000011, 0000, 011, 00, 000000, 
are in the regular set L, and that 
0010, 0, 00110, 111, 0001111, 00000, 
are not in L. A plausible conjecture might 
be that L consists of strings that are either 
an even number of zeros or any number of 
zeros followed by two ones, defined by the 
regular expression 
(00)* + 0"11. 
We can formally define the inference 
problem as follows. The class of rules is all 
the regular sets over the alphabet {0, 1}. 
The hypothesis pace is all regular expres- 
sions over the same alphabet. (There are 
various other hypothesis paces that we 
could use, namely, deterministic finite- 
state acceptors, nondeterministic finite- 
state acceptors, or context-free grammars. 
Note that the hypothesis pace must con- 
tain descriptions of all the rules, but it may 
contain descriptions of other things as 
well.) An example of a regular language L
is a pair (s, d> such that d is Y or N accord- 
ing to whether the string s is in L or not. 
An admissible presentation of L is an infi- 
nite sequence of examples of L such that 
every string over the alphabet {0,1} appears 
in some example in the sequence. So an 
admissible presentation of the language of 
the expression 0"11 might begin 
<00011, Y>, <00, N), (II, Y), <011, Y>, 
<I,N}, <0111, N), (1, N),.... 
(Note that repetitions are permitted.) We 
consider methods that can be represented 
by a computer program that takes a finite 
initial segment of a presentation of any 
regular language as input, always halts, and 
produces as output a regular expression. 
We choose identification in the limit as the 
criterion of success, that is, a method M 
identifies a language L if and only if for 
every admissible presentation 
(Sl, dl), (S2, d2), (s3, d3) . . . .  
of L, if En is the expression produced by M 
on the initial segment of the first n terms 
of this sequence, then there exists N such 
that En = EN for all n _> N, and moreover, 
the language of EN is L. The reader should 
easily be able to construct a variant of 
identification by enumeration that success- 
fully identifies in the limit all the regular 
sets over the alphabet {0, 1} (recalling that 
the question of whether a regular expres- 
sion generates a given string is effectively 
decidable.) It would be nice to have a more 
efficient method, by analogy with the use 
of polynomial interpolation in our first ex- 
ample, but no similarly efficient method 
has been found for this problem, and there 
is some evidence that none may exist (see 
Section 5.1). 
At this point the reader should have some 
idea of the basic Gold paradigm of inference 
in the limit, and some inkling of the variety 
of ways in which inference problems may 
be posed. In the next section we discuss the 
specification of rules, hypothesis paces, 
and data presentations. In subsequent sec- 
tions we examine various possible restric- 
tions on inference methods, criteria for 
successful inference, general and practical 
inference methods, and applications. 
2. DOMAINS, GUESSES, AND EXAMPLES 
We now consider various settings for in- 
ductive inference problems, that is, types 
of rules, along with their hypothesis spaces 
and data presentations. The types of rules 
that we consider are functions, languages 
and predicates, and stochastic languages. 
2.1 Functions, Sequences, and Traces 
A class of functions is specified together 
with two sets, the domain of argument val- 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods . 243 
ues, and the range of function values. The 
functions may be either total (defined on 
all argument values) or partial (not neces- 
sarily defined on all argument values). The 
functions considered are computable, and 
the typical hypothesis pace is a class of 
programs, such as the class of all Turing 
machines or the class of all LISP programs 
satisfying a certain syntactic restriction. 
I f / i s  a function, the graph of f  is the set 
of all ordered pairs (x, f(x)) such that f is 
defined on argument x. An example of a 
function is an element of its graph. An 
admissible presentation of a function is an 
infinite sequence containing all and only 
the elements of the graph of the function. 
For instance, if f is the function that ex- 
tracts the last element of a list, one pres- 
entation of it bezins 
((a b c), c), ((e d), d), ((a e i o), o) . . . . .  
Some of the questions in function infer- 
ence concern the effects of various assump- 
tions about admissible presentations. A
reasonable restriction is to limit presenta- 
tions to those that are computable, or com- 
putable within some complexity bound. An- 
other common restriction is to specify that 
the examples in a presentation be given in 
the sequence determined by a fixed total 
ordering of the domain. Unless we specify 
otherwise, we shall use the general defini- 
tion of admissible presentation given 
above. We discuss and compare other pres- 
entations in Section 4.3. 
An obvious way to present a total func- 
tion f defined on the natural numbers is to 
identify it with the sequence of its result 
values, 
f(0), f(1), f(2) . . . .  , 
and define this sequence to be the unique 
admissible presentation of f. The sequence 
inference problem is to infer classes of func- 
tions presented in this fashion. Another 
problem for sequences i  to predict he next 
term (rather than identifying the rule) from 
finite initial segments of the sequence. Pre- 
dicting the next value of a sequence is also 
called extrapolation and is the problem that 
we considered in the first example in Sec- 
tion 1. Some early systems for identifying 
or predicting sequences are described by 
Fredkin [1964], Persson [1966], Pivar and 
Finkelstein [1964], Pivar and Gord [1964], 
and Simon and Kotovsky [1963]. 
In the following discussion, we use the 
term "inference" to refer to both identifi- 
cation and prediction. We explore the issue 
of prediction versus identification in Sec- 
tion 4.2.1. 
The phrase "inference from input/output 
behavior" focuses attention on the pro- 
grams or automata computing a function 
rather than on the function itself. From 
this perspective, it is reasonable to consider 
inference problems that give additional in- 
formation about he programs or automata, 
usually designated as "trace information." 
An example is the problem of inferring a 
Turing machine from a sequence of snap- 
shots giving the tape contents and head 
positions before and after every step of 
execution on some input. Another type of 
trace information is the sequence of in- 
structions executed by an unknown pro- 
gram running on a given input. Biermann 
has developed a general methodology for 
using trace information that has been ap: 
plied to program synthesis in a variety of 
domains [Biermann, 1972, 1978; Biermann 
and Krishnaswamy 1976; Biermann and 
Smith 1977; Biermann et al. 1975]. Barzdin 
[1974a] has also considered the problem of 
synthesizing programs from traces. 
In the case above of Turing machine 
traces, it may initially seem that the trace 
information is just the input/output behav- 
ior of a different function, namely, the tran- 
sition function of the machine. However, it 
may happen that two instances of the same 
snapshot in the execution sequence have 
different successors because the unknown 
machine is in different control states in the 
two instances. Thus the sequence of snap- 
shots may not be argument/value pairs for 
any function at all. In fact, since sequences 
of snapshots contain no state information, 
they constitute an incomplete specification 
of the input/output behavior of the Turing 
machine's transition function. On the other 
hand, the sequences of snapshots contain 
more information about the underlying 
machine than input/output pairs. Thus 
inference from trace information is not 
reducible to inference from input/output 
behavior. 
Computing Surveys, Vol. 15, No. 3, September 1983 
244 • D. Angluin and C. H. Smith 
2.2 Languages and Predicates 
A class of languages i specified by a uni- 
versal set S and a collection of subsets of 
S. As an example, consider the class of all 
regular languages over the alphabet {a,b}; 
in this case, S is the set of all strings of 
a's and b's, that is, {a, b}*. The hypothesis 
space for a class of sets is typically a class 
of formal grammars (e.g., the right linear 
grammars) or a class of acceptors (e.g., the 
class of all deterministic finite-state auto- 
mata), although other types of descriptions, 
such as the regular expressions, are possi- 
ble. The term grammatical inference is 
often used to describe the inference of a 
class of languages described by grammars. 
The sets we consider can be enumerated by
some effective process; that is, they are 
recursively enumerable. 
In the case of inferring an unknown lan- 
guage L, there is an important distinction 
between giving only positive information 
(members of L) and giving both positive 
and negative information (both members 
and nonmembers of L). A positive presen- 
tation of L is an infinite sequence giving all 
and only the elements of L. A complete 
presentation of L is a sequence of ordered 
pairs (s, d) from S × {0, 1} such that d = 1 
if and only if s is a member of L, and such 
that every element s of S appears as the 
first component of some pair in the se- 
quence. A positive presentation eventually 
includes every member of L, whereas a com- 
plete presentation eventually classifies 
every element of S as to its membership n
L. Clearly, a complete presentation can be 
used effectively to construct a positive pres- 
entation, but not vice versa. If the language 
being presented is all strings of balanced 
parentheses, a positive presentation might 
begin 
(0)0, 00, ((0))(0), (0)((000)), 
((0)), O0 . . . .  
and a complete (positive and negative) 
presentation might begin 
(0(, 0), ((0), 1), (((((, 0), 
()))(, 0), (000 ,  1), (0, 1), . . . .  
Another method of presenting a language 
L is presentation by informant. An inform- 
ant is an oracle that the inference method 
may query concerning the language L. The 
inference method proposes ome string s, 
and the oracle answers "yes" or "no" ac- 
cording to whether s belongs to L. In a very 
abstract sense, presentation by informant 
is equivalent to complete presentation, but 
in practice algorithms using queries tend to 
be quite different from algorithms using 
given data and no queries. Gold [1967] de- 
fined positive and complete presentation 
and presentation by informant and studied 
their effects on inferability (described in 
Section 4.3). 
Pao and Cart [1978] consider a mixture 
of given data and queries in identifying 
regular sets. Their inference algorithm is 
initially given a structurally complete sam- 
ple and then makes queries to complete 
the identification. (For a regular set L, a 
structurally complete sample is one that 
exercises every transition in the minimum 
acceptor for L.) Angluin [1982a] demon- 
strates the improvement in efficiency that 
this form of mixed presentation permits. It 
would be interesting to see whether the 
ideas of mixed presentation and structur- 
ally complete samples generalize usefully to 
other settings, for example, the context- 
free grammars. 
A language L may be associated with the 
predicate P that is true for all s in L and 
false for all s not in L. Consequently, infer- 
ring a language may be thought of as infer- 
ring a single unary predicate. The problem 
of inferring several predicates of arbitrary 
arity simultaneously may be defined by a 
straightforward generalization of the no- 
tions of examples and presentations. Sha- 
piro's Model Inference system [Shapiro 
1981a, 1981b], for instance, infers Prolog 
programs corresponding to the Peano ax- 
iomatizations ofthe predicates plus(x, y ,z) 
and times(x, y, z) over the unary numbers. 
2.3 Stochastic Languages 
A stochastic language isa language together 
with a probability distribution on its ele- 
ments. The hypothesis pace is usually a 
class of stochastic grammars. A stochastic 
Computing Surveys, Voi. 15, No. 3, September 1983 
Inductive Inference: Theory andMethods • 245 
grammar has probabilities associated with 
alternative productions, which are used to 
define the probability that the grammar 
derives a given string. An example of a 
stochastic language is an element of the 
language, and a presentation is any infinite 
sequence ofexamples. The presentations of 
a given stochastic language are assumed to 
be generated by random selection from the 
language according to the probabilities as- 
sociated with its elements. Under this as- 
sumption, the frequency of a given value in 
initial segments of a presentation may be 
used to estimate its probability in the lan- 
guage. A frequently used criterion of suc- 
cess is identification i the limit with prob- 
ability 1, which filters out the effects of 
statistically aberrant presentations. Infer- 
ence of stochastic languages has been stud- 
ied by Cook et al. [1976], Gaines [1976], 
Homing [1969], Maryanski and Booth 
[1977], and Van der Mude and Walker 
[1978]. 
3. CRITERIA FOR EVALUATING 
INFERENCE METHODS 
3.1 What Is an Inference Method? 
Intuitively, an inference method is a com- 
putable process of some kind that reads in 
examples and outputs guesses from the hy- 
pothesis space. We can define inference 
methods formally by using a modification 
of some standard model of computation. 
Turing machines equipped with a special 
input instruction (used to request the next 
example) and a special output instruction 
(used to output he value of a guess) provide 
one such model. The Turing machine 
model, and others as well, may be restricted 
in a variety of natural ways, such as requir- 
ing that each guess be compatible with the 
examples read in up to the time that the 
guess is made (the consistency property). 
The effects of this and other restrictions 
have been the focus of considerable study. 
We survey some of this work in Section 4.4. 
3.2 Criteria of Success: Scope, Power, 
and Inferability 
An important component in specifying an 
inference problem is the particular c ite- 
rion of success chosen. We have discussed 
identification i the limit, but many other 
criteria have been studied. We shall now 
describe a framework for comparing var- 
ious inference criteria. 
Let C be a particular criterion of correct 
inference, D a particular type of data pres- 
entation, and I a particular class of infer- 
ence methods. For any method M from I, 
let the scope of M with respect to C and D 
be the class of rules that M correctly infers 
with respect o the criterion C, using the 
method of data presentation D. A method 
M1 is more powerful than a method M2 with 
respect to C and D if and only if the scope 
of M1 properly contains the scope of M2. 
A set U of rules is inferable with respect 
to L C, and D if and only if U is contained 
in the scope of some method M from I with 
respect to C and D. Denote the class of all 
such sets U by Inf(I, C, D). 
Note that the preceding definition per- 
mits the inference method M to succeed on 
more than just the rules in U. An alterna- 
tive definition that eliminates this possibil- 
ity is the following. A set U of rules is 
exactly inferable with respect to I, C, and D 
if and only if U is exactly equal to the scope 
of some method M from I with respect to 
C and D. Denote the class of all such sets 
Uby XInf(/, C, D). 
One way to compare various inference 
criteria, methods of data presentation, 
and types of inference methods i  to com- 
pare the corresponding classes Inf(/, C, D) 
and XInf(/, C, D), which are called identi- 
fication types. We describe some results 
on comparing identification types in Sec- 
tion 4. 
3.3 Quality of Hypotheses 
Since the inference process is potentially 
infinite, a person using a given inference 
method to identify an unknown rule usually 
cannot ell whether the method has con- 
verged. Thus correct identification or pre- 
diction in the limit appears to be a rather 
weak criterion of success in practice, and it 
may be desirable to augment i with some 
guarantee ofthe quality of hypotheses pro- 
duced at finite stages of the process. Con- 
sequently, there has been much work on 
Computing Surveys, Vol. 15, No. 3, September 1983 
246 • D. Angluin and C. H. Smith 
measures of "goodness" of hypotheses with 
respect to samples, and on methods of find- 
ing hypotheses that are optimal with re- 
spect to these measures. Such measures 
have been designed to capture the "simplic- 
ity" of the hypothesis and its "goodness of 
fit " to  the finite set of sample data seen so 
far. We examine measures of this kind in 
Section 5. 
3.4 Data Efficiency 
We can measure the efficiency of an infer- 
ence method by the quantity of data it 
requires to converge to a correct hypothesis. 
If M is an inference method and 
X -~ X l ,  X2, X3, . . .  
is an admissible presentation of a rule that 
M correctly infers, we define the conver- 
gencepoint of M on x to be the least positive 
integer n such that M outputs a correct 
hypothesis before reading Xn+l and does not 
subsequently change its guess. If M1 and 
M2 are two inference methods, than M1 is 
as data efficient as M2 if and only if for any 
admissible presentation x of any rule that 
M2 identifies, M1 also identifies the rule 
and the convergence point of M1 on x does 
not exceed the convergence point of M2 on 
x. M1 is strictly more data efficient han M2 
if and only if M1 is as data efficient as M2 
and there exists a presentation x of a rule 
that M2 identifies such that the conver- 
gence point of M1 on x is strictly less than 
the convergence point' of M2 on x. Finally, 
a method M is optimally data efficient if 
and only if no other method is strictly more 
data efficient han M. 
Gold [1967] has shown that identifica- 
tion by enumeration is an optimally data 
efficient method. Jantke and Beick [1981] 
have extended Gold's result to show that 
the sets of recursive functions identifiable 
by optimally data efficient methods are pre- 
cisely those identified by consistent and 
class preserving identification methods. 
(Definitions of these terms can be found in 
Section 4.4.) 
3.5 Numbers of Hypotheses, Mind Changes, 
Errors 
Another measure of efficiency is the num- 
ber of t imes an identi f icat ion method 
changes its hypothesis en route to a suc- 
cessful inference. Two quantities have been 
studied in this connection. If x is an admis- 
sible presentation of some rule and M 
some identification method, we may define 
DH(M, x) as the number of distinct hy- 
potheses output by M in the course of 
reading the sequence of examples x and 
MC(M, x) as the number of mind changes, 
that is, times when M outputs a hypothesis 
different from its previous one. 
Note that 1 + MC(M, x) is an upper 
bound for DH(M, x). Also, if M identifies 
in the limit the rule presented by x, then 
both MC(M, x) and DH(M, x) are finite. 
The converse is not always true, since M 
may converge to an incorrect hypothesis. 
CA "reliable" method, defined in Section 
4.4, may not converge to an incorrect hy- 
pothesis.) 
Consider the problem of identifying a 
recursively enumerable class of recursive 
functions. Identification by enumeration is
applicable; however, to identify the nth 
function in the enumeration,  distinct hy- 
potheses and n - 1 mind changes may be 
necessary. Barzdin and Freivald [1972] 
have shown that there is an identification 
method that requires only log n + o(log n) 
distinct hypotheses, and that log n - Oil) 
is a lower bound for this problem. Their 
method constructs a hypothesis that does 
not necessarily belong to the given enumer- 
ation. If a hypothesis from the enumeration 
is desired, then n is a lower bound On the 
number of distinct hypotheses, although 
there is a randomized strategy that achieves 
an expected value of O(log n) distinct hy- 
potheses. Podnieks [1975a] obtains an 
asymptotically exact estimate of the num- 
ber of mind changes for a probabilistic 
strategy inferring effectively enumerable 
classes of total recursive functions. Jantke 
[1979] proves analogous general results 
about he number of mind changes for se- 
lective and constructive inference opera- 
tors. Barzdin [1972, 1974a] has also consid- 
ered the question of the number of distinct 
hypotheses for identifying finite automata 
and programs in the presence of additional 
information about a program, such as the 
number of instructions or a complete or 
partial trace of its execution. 
Computing Surveys, Vo|. 15, No. 3, September 1983 
/ 
Inductive 
The analogous measure for the predic- 
tion problem is the number of errors of 
prediction for a given sequence. This quan- 
tity is finite if and only if the prediction 
method succeeds in the limit. Barzdin and 
Freivald [1972] obtain upper and lower 
bounds of log n + o(log n) and log n - 0(1) 
for the problem of predicting a recursively 
enumerable set of recursive functions. 
4. COMPARING INFERENCE CRITERIA 
We now turn our attention to studies that 
compare various inference criteria with one 
another. Comparisons of inference prob- 
lems with other computational problems 
and classes may be found in Adleman and 
Blum [1975], Brandt [1981], Jung [1977], 
Kugel [1977], Thiele [1973, 1975], and Wie- 
hagen [1978]. 
4.1 Identification in the Limit 
Consider first some very general definitions 
of identification in the limit for functions 
and languages. Let ¢1, ¢2, ¢3 . . . .  be any 
acceptable programming system [Machtey 
and Young 1978]. (For concreteness the 
reader may think of this simply as an enu- 
meration of all Turing machines, LISP pro- 
grams, or whatever.) Then each ~bi denotes 
a partial recursive function, and each par- 
tial recursive function is denoted by at least 
one ¢i. For each i define Wi to be the 
domain of definition of the function ~bi; W1, 
W2, W3, ... is then an enumeration of all 
the recursively enumerable sets. 
Let M be an identification method. If 
X ~ X l ,  X2, X3, . . . 
is an infinite sequence of inputs, denote by 
M[x] the empty, finite, or infinite sequence 
of outputs produced by M with input se- 
quence x. M is defined to converge to i on 
input x if and only if either M[x] is finite 
and its last element is i or M[x] is infinite 
and equal to i except at finitely many 
points. The convergence of M[x] to i is 
denoted by M[x]$i. 
Assume that a specific definition D of 
admissible presentations of functions and 
languages has been chosen. If f is a partial 
recursive function, let D(f) denote the set 
of admissible presentations of the function 
Inference: Theory and Methods • 247 
f according to the definition D. Similarly, 
if L is a recursively enumerable set, let 
D(L) denote the set of admissible presen- 
tations of L. 
If f is a partial recursive function, M 
identifies f in the limit if and only if for 
every x in D(f), there exists an i such that 
M[x]~i and f is a subfunction of ~i. (Note 
that if f is a total function, then the second 
condition reduces to [ = ¢i.) If U is a set of 
functions, then M identifies U in the limit 
if and only if M identifies in the limit every 
f in  U. 
If L is a recursively enumerable anguage, 
M identifies L in the limit if and only if for 
every presentation x from D(L), there ex- 
ists an i such that M[x],Li and L = Wi. If U 
is a set of recursively enumerable lan- 
guages, then M is said to identify U in the 
limit if and only if M identifies in the limit 
every L in U. 
Note that in these definitions the hy- 
pothesis paces are universal for the partial 
recursive functions and the recursively en- 
umerable sets. Various restrictions and 
modifications of these definitions are de- 
scribed in subsequent sections. 
Let EX denote the class of all sets U of 
total recursive functions such that some 
identification method identifies U in the 
limit. (EX abbreviates "explanatory.") A
fundamental result, due to Gold [1967], is 
that the set R of all total recursive functions 
cannot be identified in the limit by any 
identification method. This result can be 
proved by taking any sufficiently powerful 
identification method and defining from it 
a total recursive function that it does not 
identify in the limit. (The intuition behind 
the proof is similar to the faintly suspect 
rule "one more than the guess" described 
in Section 1.1.) In some sense, this means 
that there is no "most powerful" method of 
identifying the total recursive functions. 
One important focus of research as been 
the attempt o characterize just which sets 
of functions and languages are identifiable 
in the limit. 
Gold's result was claimed earlier by Put- 
nam in his 1963 Voice of America lecture 
[Putnam 1975]. However, Putnam's proof 
implicitly assumes that the inference ma- 
chines are restricted to output only pro- 
Computing Surveys, Voi. 15, No. 3, S~p~mber 1983 
248 • D. Angluin and C. H. Smith 
grams for total functions. Machines re- 
stricted in this. fashion are discussed in 
Section 4.4. 
4.2 Other Inference Cdteda 
4.2.1 Prediction versus Identification 
The sequence prediction problem is to pre- 
dict the next value in the range of a total 
recursive function of the natural numbers, 
given some initial segment of the range, or, 
in other words, to predict the nth element 
of the sequence given the first n - I terms. 
A slightly different type of inference 
method, called a prediction method, is used 
for these problems. A prediction method is 
simply an algorithmic device that accepts 
as input a finite (possibly empty) sequence 
of values and may output some value and 
halt, or may diverge. If M is defined on the 
sequence xl ,  x2 . . . .  , Xn, let M ( xl, x2, . . . ,  
x,) denote the value output. Three defini- 
tions of successful prediction have been 
considered, differing in the restrictions 
they impose on the divergence of the pre- 
diction method, M. 
The first, and most restricted, is NV- 
prediction, considered by Barzdin [1972] 
and Blum and Blum [1975]. (NV abbrevi- 
ates "next value.") M NV-predicts a total 
recursive function [ if and only if M(x l ,  x2, 
. . . .  x,) is defined for all finite sequences of 
values xl, x2,. •., Xn and 
M([(O), [(1), . . . ,  [ (n - 1)) ffi [(n) 
for all but finitely many n. (This is the type 
of convergence that we implicitly used in 
Section 1.1.) If U is a set of total recursive 
functions, then M NV-predicts U if and 
only if M NV-predicts every [ in U. Let NV 
denote the class of all sets of total recursive 
functions that are NV-predicted by some 
prediction method. 
MNV' -pred ic ts  the function ffif and only 
if M(/(0), /(1),  . . . ,  f (n  - 1)) is defined for 
all n, and 
M(f(O),  f(1), . . . ,  f (n  - 1)) = f(n) 
for all but finitely many n. The notion of 
NV'-prediction is due to Barzdin and Frei- 
vald [1972]. NV'-prediction of a set of func- 
tions and the class NV' are defined analo- 
gously. 
The third and least restricted efinition 
is due to Podnieks [1974]. M NV"-predicts 
[ if and only if for all but finitely many n, 
M([(O), [(1) . . . .  , [ (n - 1)) is defined and 
equal to [(n). NV"-prediction of a set of 
functions and the class NV"  are defined 
analogously. 
Note that NV requires that M converge 
on all finite sequences, and NV' requires 
that M converge on all initial segments of 
correctly predicted functions. However, 
NV" allows M to diverge on finitely many 
initial segments of correctly predicted 
functions. These apparent differences in 
strength of restriction are real: NV is a 
proper subclass of NV', which in turn is a 
proper subclass of NV" [Barzdin and Frei- 
vald 1972; Case and Smith 1983; Podnieks 
1974]. Also, NV and NV' are proper sub- 
classes of EX, whereas NV" is equal to BC, 
a proper superclass of EX defined in Sec- 
tion 4.2.2 below [Case and Smith 1983; 
Podnieks 1975b]. Further comparisons 
with other identification types are given in 
subsequent sections. 
Naive intuition suggests that prediction 
is possible whenever identification is, since 
the hypothesized rule may be used to pre- 
dict the next term. However, this is false 
unless all guesses are restricted to be total 
recursive functions. In a limited sense, the 
reverse is true. 
To see that a method of NV'-prediction 
may be used to give a method of identifi- 
cation, suppose that A is an algorithm that 
takes an initial segment of a sequence and 
predicts the next term. Given an initial 
segment of a sequence, 
Xl ,  X2, . . . ,  Xn, 
define a rule ff as follows: 
[ ( i )=x i  for i=1 ,2 , . . . ,n ,  
[(i + 1) ---- A ( / (1 ) , / (2 )  . . . . .  [(i)) 
for i_> n. 
Clearly, a strategy that uses ff as its new 
guess every time the current conjecture 
turns out to be false will correctly identify 
in the limit any sequence that A correctly 
predicts in the limit. 
Computing Staweya, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 249 
4.2.2 Behaviorally Correct Identification 
Behaviorally correct (BC) identification re- 
quires that the semantics (rather than the 
syntax) of the guesses converge correctly in 
the limit. An inference process is allowed 
to continue changing its guess indefinitely, 
as long as after some finite initial segment 
the guesses are all correct descriptions of 
the underlying rule. 
If M is an identification method and f is 
a total recursive function, then M BC-iden- 
tifies f if and only if for every admissible 
presentation x of f, Mix] is an infinite se- 
quence: yl, Y2, Y3 . . . .  such that for all but 
finitely many n, 
 byo =/.  
M BC-identifies a set U of total recursive 
functions if and only if M BC-identifies 
every f in U. BC denotes the class of all 
sets U such that for some inference method 
M, M BC-identifies U. 
BC-identification is a less restrictive cri- 
terion than identification in the limit; in 
fact, EX is a proper subclass of BC [Barzdin 
1974b; Case and Smith 1983]. The class BC 
is equal to the prediction class NV" [Case 
and Smith 1983; Podnieks 1974]. Feldman 
[1972] introduced the concept of behavior- 
ally correct identification of languages, 
which he termed "matching in the limit." 
He also defined notions of approaching and 
strong approaching, and compared these 
with identification in the limit. 
4.2.3 Approximate Identification and Anomalies 
We can weaken the definition of identifi- 
cation in the limit, and thus allow more 
classes of rules to be effectively identifiable, 
by permitting the final guess to be "nearly 
correct" as a representation of the rule 
being presented. It is probably rare that a 
scientific theory or large computer program 
is completely "bug free," that is, gives the 
correct intended answers on all possible 
inputs [Laudan 1977]. Thus it seems real- 
istic to consider hypotheses that are ap- 
proximately correct in some sense. 
Wharton [1974] proposes a general no- 
tion of approximation of one language by 
another one. In particular, each string s 
over the alphabet is assigned a fixed posi- 
tive weight w(s) such that the sum of the 
weights of all the strings is 1. Then the 
measure of the distance d(L1, L~) between 
two languaged L1 and//~ is the sum of the 
weights of the strings in the symmetric 
difference of L1 and L2. This defines a 
metric on the space of languages. For any 
positive number ~, an inference method r- 
identifies a language L if and only if it 
converges to a grammar G such that 
d(L(G), L) < ~. 
One result of this theory is that for any ~, 
the class of all languages over a given al- 
phabet is e-identifiable in the limit from 
positive presentations by a method that 
outputs only grammars for finite languages. 
Consequently, a-identification allows more 
classes of languages to be inferred from 
weaker types of data presentation than the 
standard notion of identification in the 
limit, but perhaps the criterion of approxi- 
mation is too liberal. 
As an alternative, we might permit the 
final guess to have some finite number of 
"bugs" or "anomalies," that is, places where 
the guess and the target rule disagree. 
These anomalies are not weighted, as in 
Wharton's scheme, but an upper bound 
may be placed on their number. 
If k is a natural number and f and g are 
partial recursive functions, define f •kg if 
and only if f(n) ffi g(n) for all but at most 
k values of n. Thus f =0 g is equivalent to 
/ = g. If / is a total recursive function and 
M is an identification method, then M iden- 
tifies [ in the limit with at most k anomalies 
if and only if for every admissible presen- 
tation x of f, M[x] converges to a value i 
k k such that ~i = f. Let EX denote the class 
of all sets U of recursive functions such 
that some identification method M identi- 
fies all f in U in the limit with at most k 
anomalies. Thus EX ° is just EX. Define 
f =* g to mean that f(n) ffi g(n) for all but 
finitely many values of n. The class EX* is 
defined analogously. EX*-identification is 
called a.e. identification by Blum and Blum 
[1975] and subidentification by Minicozzi 
[1976]. 
Case and Smith [1978, 1983] show that 
EX °, EX 1, EX 2 . . . .  is a proper hierarchy of 
Computing Su~eys, Vol. 15, No. 3, September 1983 
250 • D. Angluin and C. H. Smith 
classes, and that their union is properly 
contained in EX*. Barzdin and Podnieks 
[1973] study a relaxation of the conver- 
gence criteria that allows identification by 
a method M that ultimately outputs only 
hypotheses from some finite set of correct 
hypotheses. They show that such a relaxa- 
tion does not alter in any way the set of 
identifiable functions. Case and Smith 
[1978, 1983] extend this result to cases of 
identification with anomalies. Anomalies 
may also be permitted with respect o the 
behaviorally correct identification crite- 
rion, where the classes BC h, for k = 
0, 1, 2 , . . .  and k =* are analogously de- 
fined. Case and Smith also show that this 
is a proper hierarchy. Harrington has con- 
structed a "most powerful" inference 
method that BC*-identifies all the total 
recursive functions (see Case and Smith 
[1983]). However, it is necessarily the case 
that for Harrington's machine ach succes- 
sive correct hypothesis has more bugs than 
the previous one [Chen 1982]. 
4.2.4 Identification by Teams of Machines 
The class EX is not closed under union 
[Barzdin 1974b; Blum and Blum 1975]; that 
is, there are two classes U and V of total 
recursive functions uch that each class is 
identifiable in the limit, but their union is 
not. Blum and Blum [1975] give the follow- 
ing example. Let U contain all 0, 1-valued 
total recursive functions that are 0 on all 
but finitely many points. Let V contain all 
0, 1-valued total recursive functions f such 
that the least value of n for which [(n) = 1 
has the property that ¢, = f. This slightly 
paradoxical set is called the self-describing 
functions for obvious reasons. Clearly, each 
of U and V is separately identifiable, but it 
may be shown that their union is not. ("Re- 
liability," discussed in Section 4.4, is a re- 
striction that guarantees closure under 
union and other operations.) 
The nonunion property of identification 
leads to the definition of identification by 
"teams" of inductive inference machines. 
We consider some fixed finite collection (a 
team) of inductive inference machines 
working in parallel on some sequence of 
examples, and define the rule as correctly 
identified if one or more of the machines 
succeeds in identifying the rule according 
to some particular criterion of inference. 
Then there is a hierarchy with respect o 
the number of machines in the team for the 
basic criterion of identification in the limit, 
and others. In particular, for each n, n 
inductive inference machines can EX-iden- 
tify sets of functions that cannot be BC- 
identified by any smaller team of inference 
machines [C. Smith 1982]. 
Studies of the trade-offs between the 
number of inductive inference machines in 
a team and the accuracy of the hypothesis 
(as measured by the number of anomalies) 
indicate that this relationship is more fun- 
damental than the distinction between EX 
and BC identification. C. Smith [1982] has 
shown that m EX ~ machines can be simu- 
lated by n EX b machines if and only if 
(L J) n_<mx 1+ 
Daley [1983] has shown that the same for- 
mula characterizes the trade-off between 
accuracy of the hypothesis and the number 
of machines for BC-identification. 
4.3 The Effects of Data Presentation 
Gold [1967] considers the effects of differ- 
ent types of data presentation on the infer- 
ence of languages. He shows that no set of 
languages containing all the finite lan- 
guages and at least one infinite language 
can be identified in the limit from positive 
presentations. This result applies to many 
important classes of languages (e.g., the 
regular languages and the context-free lan- 
guages), which presents something of a 
problem to linguists attempting to account 
formally for the fact that children learn 
their native languages from what appears 
to be positive rather than positive and neg- 
ative data. Researchers have made various 
suggestions for getting around this diffi- 
culty. One is to add a semantic omponent 
to the model of the input; another is to 
consider stochastic presentations of the 
data. For example, Horning [1969] shows 
that the stochastic ontext-free languages 
can be identified in the limit with proba- 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 251 
bility 1 from stochastic presentations 
(which contain no explicit negative data). 
Both semantics and stochastic presenta- 
tion are employed by Wexler and Culicover 
[1980] to obtain their results. 
However, it is not the case that no inter- 
esting classes of languages can be identified 
from positive presentations, and it could be 
argued that classes in which any finite lan- 
guage could be the target language are in- 
appropriate for inference. Angluin gives a 
characterization of those sets of recursive 
languages that can be identified in the limit 
from positive data [Angluin 1980a], and 
two concrete examples of such sets [An- 
gluin 1980b, 1982b], described further in 
Section 7.5. Osherson and Weinstein 
[1982] consider inference from positive 
presentations with respect to EX, EX*, BC, 
BC*, and other criteria. Case and Lynes 
[1982] extend their results to include all 
the EX- and BC-type inference criteria for 
positive presentations. 
Gold also considers restricting positive 
presentations to those that may be enum- 
erated by primitive recursive programs and 
shows that there exists an identification 
method that will identify in the limit every 
recursively enumerable set from such pres- 
entations. Wiehagen [1977] describes fur- 
ther work on characterizing the effects of 
restrictions of this type. 
To infer total recursive functions, we 
may assume without loss of generality that 
the data presentation is in the order 
f(0), f(1), f (2 ) , . . . .  
The reason is that an identification method 
may simply read and store inputs until it 
has some complete initial segment, simu- 
late a method that expects values in the 
above order, and then repeat this cycle. 
However, for some types of restricted meth- 
ods, this difference in presentations makes 
a difference in identifiability, as shown by 
Jantke and Beick [1981]. 
Blum and Blum [1975] consider the ef- 
fects of restrictions of presentations on the 
identification of partial recursive functions. 
They show that identification types using 
arbitrary and effective presentations are 
equivalent, and identification methods may 
be made order independent without loss of 
generality. An order-independence result 
for positive presentations i described by 
Osherson and Weinstein [1982]. 
4.4 Restrictions on Inference Methods 
We shall now describe some of the restric- 
tions on inference methods that have been 
studied. In general, the effect of a restric- 
tion is to make fewer sets of rules identi- 
fiable, since some otherwise acceptable 
inference methods may not satisfy the re- 
striction. 
One restriction is consistency, which re- 
quires that an identification method output 
as guesses only hypotheses that are com- 
patible (or consistent) with the examples 
read in up to the time that the guess is 
made. Clearly, identification by enumera- 
tion is a consistent method. To be a little 
more precise, there is a distinction between 
requiring that the guesses of a method be 
consistent on every finite sequence of ex- 
amples or only on those finite sequences of 
examples that are initial segments of pres- 
entations of successfully identified rules. 
If CONS denotes the class of sets of 
recursive functions that are identifiable by 
a consistent inference method (in the sec- 
ond sense), then CONS is a proper subset 
of EX [Wiehagen 1976]; that is, there exist 
sets of recursive functions that are identi- 
fiable in the limit, but not by any consistent 
method. One such set is that of the self- 
describing functions, defined in Section 
4.2.4. Consistency in the first sense yields 
weaker inference methods than does con- 
sistency in the second sense [Wiehagen and 
Liepe 1976]. Additional studiesof consist- 
ency may be found in Freivald and Wie- 
hagen [1979], Jantke and Beick [1981], and 
Kugel [1977]. 
One of the salient properties of identifi- 
cation in the limit is that the inference 
method generally has no effective way of 
testing for convergence, so processing con- 
tinues indefinitely. Finite identification re- 
moves this uncertainty. One definition (es- 
sentially that of Freivald and Wiehagen 
[1979]) is that a finite or infinite sequence 
yl ,  y2, y3 , . . ,  finitely converges to a value 
t at element n if and only if y ,  = t and 
n is the least natural number such that 
Computing Surveys, Vol. 15, No. 3, September 1983 
252 • D. Angluin and C. H. Smith 
Y~ -- Y~+i. So as soon as a value is repeated 
intwo successive positions, the sequence is
defined to have converged. 
Let [ be a function and M an identifica- 
tion method. If for every admissible pres- 
entation x of f there exists an i such that 
M[x] finitely converges to i and ~bi ffi f, then 
M finitely identifies f. The definition for 
languages i analogous. 
Finite identification is harder to achieve 
than identification in the limit, and so 
fewer classes of functions are finitely iden- 
tifiable. In particular, if FIN denotes the 
class of sets of total recursive functions that 
are finitely identified by some identifica- 
tion method, then FIN is a proper subclass 
of EX [Lindner 1972]. Freivald [1979] de- 
fines finite identification by a probabilistic 
inference strategy of a class of functions 
with a certain probability; he shows that 
identification with probability greater than 
is the same as finite identification, and 
that there is a hierarchy of classes based 
on the probabilities ~, ~, ~, ~ . . . . . .  Flanagan 
[1981] describes an interesting variant of a 
finite identification problem for finite func- 
tions, in connection with inferring the se- 
mantics of a language with known syntax. 
The problem is to identify an unknown 
function [ defined on and taking values in 
the set {1, 2 , . . . ,  n}, given a collection of 
pairs of sets (X, Y) such that f(X) = Y. 
Another restriction is reliability, which 
requires that if an identification method 
converges on an input sequence, it must 
converge to a correct hypothesis. If a reli- 
able method fails to identify some rule, it 
must do so by changing its hypothesis in- 
finitely often. For a reliable method, suc- 
cessful identification occurs exactly when 
the number of mind changes, MC(M, x), is 
finite. 
To be precise, the class of inputs over 
which a method must be reliable must be 
specified. An identification method M is 
reliable on a set of functions U if and only 
if for every f in U and every presentation x 
off, if M[x] converges, then it converges to 
an index for a superfunction of f  [Blum and 
Blum 1975]. 
Minicozzi [1976] exhibits a variety of 
effective closure properties for the class of 
sets of total recursive functions identifiable 
by an identification method that is reliable 
on all the total recursive functions. (She 
uses the term "strong" instead of "relia- 
ble.") For example, this class is closed un- 
der infinite union, whereas the standard 
type EX is not even closed under finite 
union. Wiehagen and Liepe [1976] charac- 
terize when the union of two sets in EX is 
also in EX. Blum and Blum [1975] give 
complexity-theoretic characterizations of
the sets of partial recursive functions iden- 
tified by methods reliable on all the partial 
recursive functions, and just on the total 
recursive functions. (These results are de- 
scribed further in Section 6.) 
A restriction termed Popperian by Case 
and Ngo-Manguelle [1979] is that all the 
hypotheses output as guesses by an identi- 
fication method must be total recursive 
functions. (This restriction reflects Pop- 
per's methodological requirement that hy- 
potheses in science be refutable.) The 
classes of total recursive functions identi- 
fiable by Popperian machines are precisely 
the prediction type NV [Case and Smith 
1983]. Intuitively, this equivalence xists 
because the total recursive guesses of a 
Popperian identification machine may be 
used to predict without fear that it will not 
converge, and a predictive method, may 
have its answers fed back into it to define 
a total recursive function to use as a guess. 
(See the description in Section 4.2.1.) 
An identification method is conservative 
if and only if it outputs a hypothesis differ- 
ent from its previous guess only when the 
previous guess is incompatible with the ex- 
amples read in so far. A conservative 
method will stick to a hypothesis as long as 
it is consistent with the data read in so far. 
Clearly, identification by enumeration may 
be implemented asa conservative method. 
The effects of restricting identification 
methods to be conservative are considered 
by Angluin [1980a] and Kugel [1977]. 
Given a class U of total recursive func- 
tions and an identification method M, 
Jantke and Beick [1981] define M to be 
class preserving on U if and only if for every 
presentation x of a function from U, every 
element of the sequence M[x] is from U. 
Intuitively, M is forced to make its guesses 
from the class U. Identification by enumer- 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 253 
ation is a class-preserving method on the 
class of functions in the given enumeration. 
For an arbitrary inference method, the 
class U is usually taken to be the class of 
functions correctly identified by the 
method. Jantke and Beick study the effect 
of this restriction, as well as a related re- 
striction called "weakly class preserving." 
The class-preserving property is related to 
what Jantke calls selective in his study of 
inference operators [Jantke 1979], which is 
discussed further in Section 6. Jantke and 
Beick [1981] also study the effects of var- 
ious combinations of these restrictions and 
others. 
Another restriction requires that an 
identification method produce the "small- 
est" possible description as its ultimate 
guess of a rule. For a hypothesis pace 
consisting of the programs from some ac- 
ceptable programming system, "smallest" 
is defined with respect o an abstract mea- 
sure of program size, defined by Blum 
[1967a]. Freivald [1975a] considers the 
identification in the limit of absolutely 
minimal programs. He shows that whether 
or not minimal programs for some sets of 
functions can be identified in the limit de- 
pends on the choice of an acceptable pro- 
gramming system used as the hypothesis 
space. In fact, the only classes of functions 
that can be inferred in all such hypothesis 
spaces are finite [Freivald 1974]. In con- 
trast, Chen [1982] shows that the inference 
of nearly minimal-size programs (programs 
minimal with respect o a recursive factor) 
is independent of the hypothesis pace. He 
also considers variations of identification 
in the limit, including identification with 
anomalies and identification by Popperian 
inference machines, and shows that iden- 
tification of nearly minimal-size programs 
is independent of the hypothesis pace for 
these variations. However, there is a loss of 
inferring power if a nearly minimal pro- 
gram is required, unless a finite unbounded 
number of anomalies is tolerated [Chen 
1982]. 
In their study of the inference of mini- 
mal-size programs, Freivald et al. [1982] 
consider inference performed with respect 
to a particular class of restricted hypothesis 
spaced called Friedberg numberings. A 
Friedberg numbering is a list of all and only 
the partial recursive functions 4~1, ¢2, 
¢8 . . . .  such that each function appears only 
once in the list. Friedberg [1958] has shown 
that such enumerations exist. Freivald et 
al. show that for every class of recursive 
functions U in EX, there is a Friedberg 
numbering that can be used as a hypothesis 
space to identify in the limit all the func- 
tions in U. They also prove that if U is in 
FIN, then U is finitely identifiable within 
the constructed Friedberg numbering. 
5. COMPARING HYPOTHESES 
There are at least two obvious criteria to 
consider in evaluating hypotheses: the sim- 
plicity of the hypothesis and its relation- 
ship to the sample data. These may be given 
formal significance in several ways, some 
of which are considered below. 
Each of the measures weconsider can be 
represented abstractly as a partial ordering 
of the hypotheses compatible with a given 
sample. The ordering is permitted to de- 
pend on the sample under consideration. 
The relation g is less than h with respect to 
S is the definition of what it means for g to 
be a "better" explanation of the sample S 
than h. When two guesses are incomparable 
in the ordering, the interpretation is that 
neither is a better explanation of the sam- 
ple than the other. The set of best guesses 
for a given sample are the minimal ele- 
ments in this ordering. We call an ordering 
of this type a goodness ordering. We now 
consider some general properties of such 
orderings, and describe particular orderings 
that have been studied. 
A goodness ordering is computable if
there is an algorithm that takes ~any finite 
sample and returns an element of the set 
of best guesses for the sample. It is sample 
independent if and only if whenever g is 
less than h with respect to some sample S, 
then g is less than h with respect o every 
sample S with which they are both com- 
patible. Orderings based only on some 
property of the hypotheses {e.g., size) are 
sample independent. 
Consider any computable sample-inde- 
pendent goodness ordering, and let A de- 
note an algorithm for finding an element of 
Computing Survey~ Vol. 15, No. 3, September 1983 
254 • D. Angluin and C. H. Smith 
the set of best guesses for a given sample. 
The algorithm A may be used in the follow- 
ing scheme P to produce an identification 
method: 
(1) A is used to generate a best guess for 
the initial sample. Then the method 
continues to read data and check the 
current guess for compatibility with it. 
(2) As long as the current guess remains 
compatible, it is not changed, but if it 
becomes incompatible with the data, A 
is used to generate a new best guess for 
the current sample set. 
The method obtained from P is guaran- 
teed to output a best guess for the given 
sample at every finite stage, and it is con- 
servative in the sense that it does not 
change a previous hypothesis unless it is 
incompatible with the data. Whether this 
process correctly identifies in the limit all 
the rules in the domain depends on prop- 
erties of the specific domain and ordering. 
The rest of Section 5 describes pecific 
orderings based on simplicity of hy- 
potheses, goodness of fit to the data, and 
mixtures of these. 
5.1 Simplicity of Hypotheses 
If there is some measure of "simplicity" on 
the set of all possible hypotheses, we may 
consider the ordering in which g is less than 
h if g is simpler than h. This is a sample 
independent goodness ordering. The set of 
best guesses for a sample is the set of sim- 
plest hypotheses that are compatible with 
the sample. To illustrate, define the com- 
plexity of a polynomial to be its degree. 
Then the classical interpolation algorithm 
produces the least complex polynomial 
compatible with each initial segment of a 
given sequence. 
Consider also the problem of inferring 
finite-state acceptors from complete pres- 
entations, and define the complexity of a 
finite-state acceptor to be the number of 
states that it contains. The problem of com- 
puting the best guesses for a given sample 
is as follows. Given two disjoint finite sets 
of strings, So and $1, find a finite-state 
acceptor with the fewest possible states 
that accepts all the strings in $1 and none 
of the strings in So. It is easy to see that 
this is computable: just enumerate accep- 
tots in order of increasing numbers of states 
and take the first compatible one. More- 
over, if this algorithm is used in the infer- 
ence scheme P, the resulting method cor- 
rectly identifies in the limit all the regular 
sets. 
However, Gold [1978l has shown that the 
computational problem of finding a mini- 
mum finite-state acceptor compatible with 
given data is NP-hard. Consequently, all 
known algorithms for this problem require 
exponential time on some inputs. An anal- 
ogous result holds also for regular expres- 
sions. Define the size of a regular expres- 
sion to be the number of occurrences of 
alphabet symbols it contains, so that the 
size of the expression (00)* + 0"11 is 5. 
Then the computational problem of finding 
a regular expression of smallest size com- 
patible with given positive and negative 
examples i  NP-hard [Angluin 1978]. 
In general, identification by enumeration 
produces the "simplest" compatible hy- 
pothesis if the enumeration order is con- 
sistent with the intended simplicity order- 
ing of hypotheses. However, the complexity 
results uggest that there may be no analog 
of polynomial interpolation i general; that 
is, in some settings there may be no effi- 
cient algorithm for the computational prob- 
lem of finding the simplest compatible hy- 
pothesis. 
5.2 A Set-Theoretic Goodness of Fit Measure 
When languages are inferred from positive 
data, one way to define the goodness of fit 
of hypotheses to samples is to use the or- 
dering of set inclusion among all the can- 
didate languages that contain the given 
positive examples. Let H be the hypothesis 
space and L(h) the language generated by 
hypothesis h. Consider the ordering in 
which g is less than or equal to h if and 
only if L(g) is a subset of L(h). Then given 
a sample S, the set of best guesses for S are 
the minimal elements in this ordering that 
are compatible with S, that is, such that S 
is a subset of L(h). Roughly speaking, a 
best guess for S generates the sample S and 
as few additional strings as possible. This 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 255 
ordering is sample independent, and so the 
inference scheme P is applicable. 
Despite the difficulties of identification 
from positive data, there are efficient al- 
gorithms for this criterion that lead to cor- 
rect identification in the limit in several 
different domains [Angluin 1980b, 1982b; 
Crespi-Reghizzi 1972; Crespi-Reghizzi et al. 
1978; Shinohara 1982a, 1982b]. These re- 
sults are described in Section 7. 
5.3 Mixed Measures: Derivational Complexity 
Feldman [1972] gives a reasonable set of 
general axioms for orderings that combine 
grammatical complexity with derivational 
complexity for inferring grammars. Any 
such ordering is called a grammatical com- 
plexity measure. He proves that every 
ordering that satisfies the axioms is com- 
putable; that is, there is an algorithm for 
finding a grammar compatible with given 
data and minimal in the ordering. (The 
Bayesian measure of Horning, discussed 
below, is one instance of a grammatical 
complexity measure.) Some results are 
given relating such measures to various cri- 
teria of inference in the limit. Analogous 
results for measures that combine program 
size and resource complexity for inference 
of programs are given by Feldman and 
Shields [1977]. 
5.4 Mixed Measures: Bayesian Approaches 
One approach to combining the measures 
of simplicity of the hypothesis and good- 
ness of fit to the data, based on Bayes' 
Theorem, defines certain probability meas- 
ures and then looks for a hypothesis that 
maximizes the probability of the hypothe- 
sis, given that a particular sample is ob- 
served. We define a probability measure 
Pr(h) on the hypothesis space, aprobability 
measure Pr(S) on the sample space, and 
Pr(S] h), the probability of a given hypoth- 
esis h generating a given sample S. Then 
we define the ordering to be g is less than 
h with respect o S if and only if Pr(glS) 
is greater than Pr(hiS). 
For a given sample S, the best guesses 
are those h that maximize Pr(h[S). Since 
by Bayes' Theorem, 
Pr(hlS) = Pr(h)Pr(Si h) 
Pr(S) ' 
it suffices to maximize Pr(h)Pr(SIh). 
If we think of Pr(h) as a measure of the 
simplicity of the hypothesis h (higher prob- 
ability assigned to simpler hypotheses) and 
Pr(SI h) as a measure of the goodness of fit 
of h to the sample S (higher probability 
assigned to a better fit), then this measure 
does trade off simplicity against goodness 
of fit. Note that this ordering is usually not 
sample independent, and so the scheme P
described above is not applicable. 
To use this approach, we must decide 
how to assign probabilities to the elements 
of the hypothesis space. This represents an 
opportunity to incorporate prior knowledge 
of the domain into the inference strategy. 
The approach is practically useful only in- 
sofar as the probability measures are com- 
putable or approximately computable in 
the appropriate sense. Watanabe [1960] 
gives arguments for the use of Bayesian 
methods in inductive inference and inves- 
tigates the rate at which evidence dimin- 
ishes the effect of the prior probabilities in 
the limit. 
An early and important paper by Solo- 
monoff [1964] advocates use of the Baye- 
sian approach with a priori probabilities 
based on the theory of program size com- 
plexity. Work on the general theory of pro- 
gram size complexity and its relation to 
randomness and information content of 
specific objects has been done by Kolmo- 
gorov, Chaitin, and Martin-Lof, among 
others [Chaitin 1974, 1975, 1976; Daley 
1973, 1977; Gacs 1983; Kolmogorov 1965; 
Martin-Lof 1966, 1971; Zvonkin and Levin 
1970]. More recent work by Solomonoff on 
this topic may be found in Solomonoff 
[1975, 1978]. 
Horning [1969] has used this approach 
in the context of inferring stochastic on- 
text-free grammars. The probability of a 
sample given a grammar is defined using 
the production probabilities of the gram- 
mar, and the probability measure on gram- 
mars is defined by a stochastic grammar 
generator. Horning describes an enumera- 
Computing Surveys, Voi. 15, No. 3, September 1983 
256 • D. Angluin and C. H. Smith 
tive algorithm for finding h to maximize 
Pr(h)Pr(S]h), given a sample S of strings 
as input. He proves that if the input is 
stochastically generated, this algorithm 
leads to correct identification in the limit 
with probability 1. Note that even though 
the context-free languages are not identi- 
fiable in the limit from positive presenta- 
tions, they are identifiable in the limit with 
probability 1 in this stochastic setting, 
which includes no explicit negative xam- 
ples. Van der Mude and Walker [1978] have 
also used this approach for the inference of 
stochastic regular grammars. 
Cook et al. [1976] define a measure that 
is rather similar to Horning's for the prob- 
lem of inferring stochastic ontext free 
grammars from stochastically generated 
positive samples. They define a measure of 
grammatical complexity, C(G), and a mea- 
sure of the discrepancy between the lan- 
guage of a grammar and given sample, 
D(L(G), S). Their combined measure is a 
positive linear combination of these two 
measures. One interesting property of their 
measure of grammatical complexity is that 
it assigns a lower complexity to the gram- 
mar 
{X ~ aaaaa, X ~ bbbbb} 
than to the grammar 
{X ~ aabab, X ~ babba}, 
thus taking into account somewhat theran- 
domness of the strings composing the 
grammar. This may be viewed as an ap- 
proximation to a priori probabilities de- 
fined using program size complexity. They 
describe an algorithm that takes a sample 
and finds a grammar whose measure is a 
local minimum with respect to certain 
transformation steps. (Their algorithm is 
described in Section 7.3.) 
5.5 Other Mixed Measures 
Maryanski and Booth [1977] consider the 
problem of inferring stochastic finite auto- 
mata from stochastically generated ata. 
The measure of goodness of fit of a given 
stochastic finite automaton to a given set 
of strings is defined using a chi-square test 
of the expected frequencies of the sample 
strings against their actual frequencies in 
the sample. The measure of simplicity of 
an automaton is the number of states that 
it contains. The user specifies a threshold 
for the chi-square test, and then the desired 
answer is an automaton with the minimum 
possible number of states that meets the 
threshold value for goodness of fit. Maryan- 
ski and Booth describe an enumerative al- 
gorithm for solving this problem and study 
its implementation. (See also Gaines [1978l 
for certain corrections.) 
Gaines [1976] also considers the problem 
of mixed measures in the context of infer- 
ring stochastic finite automata from sto- 
chastically generated ata. His suggestion 
is not to trade off simplicity and goodness 
of fit, but  to consider them jointly and 
independently. The measure of simplicity 
is the number of states in an automaton, 
and the measure of goodness of fit is de- 
fined using the transition probabilities of 
the automaton. The ordering that he con- 
siders is simply the product of these two 
total orders; that is, an automaton is a best 
guess for a given sample if and only if no 
other automaton has both fewer states and 
a better fit to the sample. An algorithm is 
described that enumerates the best guesses 
for a given sample. Examples are given to 
illustrate the properties of this ordering. 
Gaines also discusses the use of stochastic 
methods in deterministic cases to filter out 
errors in the data. 
6. GENERAL INFERENCE METHODS 
Identification by enumeration, sketched in 
the introductory section of this paper, is a 
consistent, class preserving, conservative, 
and optimally data efficient method. We 
may think of it as a general scheme pa- 
rameterized by an enumeration of a class 
of recursive languages or total recursive 
functions to yield an identification method 
for the enumerated class, or, equivalently, 
as a general search method that can be 
adapted to different domains by giving it 
different generators for the spaces to be 
searched. We shall now consider general 
inference methods related to identification 
by enumeration. Particular inference meth- 
ods, some based on enumeration and others 
not, are described in the next section. 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive 
6.1 identification of Resource-Bounded 
Classes 
Blum and Blum [1975] describe three dif- 
ferent types of inference methods thatuse 
resource bounds as part of the criterion for 
deciding when to "give up" on the current 
hypothesis. For concreteness, imagine that 
pl, p2, p3 . . . .  is an enumeration ofprograms 
in some acceptable programming system. 
As a resource, consider the number of steps 
executed by a program, although any com- 
plexity measure would do [Blum 1967b]. 
Let h be a total recursive function of one 
argument. A function [ is called h-easy if 
and only if there exists a program Pi that 
computes f such that for all but finitely 
many inputs x, pi on input x runs for at 
most h(x) steps. Given h, the following 
method correctly predicts all the h-easy 
functions in the limit. On input Yl, Y2 . . . . .  
yn, search in some fixed order through all 
pairs (i, k) until the first one is found such 
that 
pi(x) = Yx, 
for all x _< n, and pi runs for at most 
max{k, h(x)} steps on input x, for x _< n + 
1. When the first such pair (i, k) is found, 
then output the guess pi(n + 1). This 
method gives up on a hypothesis when it 
runs too slowly (as measured by h), al- 
though it may come back to the hypothesis 
later when k is sufficiently increased. Adle- 
man (see Blum and Blum [1975]) and Barz- 
din and Freivald [1972] have shown inde- 
pendently that the sets of functions in the 
class NV are all the subsets of the h- asy 
sets of functions, as h ranges over all recur- 
sive functions. 
Blum and Blum also consider two iden- 
tification methods that are, respectively, 
reliable on all the partial recursive func- 
tions (P) and reliable on the total recursive 
functions (R). The first of these uses an a 
priori bound h(x, y) that is a total recursive 
function of two arguments. A function f is 
said to be h-honest if and only if there exists 
a program pi to compute f such that for all 
but finitely many inputs x, i fy  =/(x),  then 
Pi runs for at most h(x, y) steps on input x. 
They show that the sets of partial recursive 
functions identifiable by methods that are 
Inference: Theory and Methods • 257 
reliable on P are all subsets of h-honest 
sets of functions, as h ranges over all total 
recursive functions of two variables. The 
method constructed using a given h is sim- 
ilar to the prediction method described 
above, except hat for the input/output pair 
(x, y), the bound max{k, h(x, y)} is used as 
the upper bound on the running time of a 
viable hypothesis when checking consist- 
ency with the data. 
The last type of inference method that 
Blum and Blum consider is parameterized 
by a recursive operator instead of a recur- 
sive function and is called an a posteriori 
method. We shall not give formal defini- 
tions for this case, but intuitively the 
method gives up on the current hypothesis 
only when it has found another hypothesis 
that correctly computes the exemplified 
function on "many more inputs" and "much 
faster" than the current hypothesis. The 
quantif ication of "more inputs" and 
"faster" is given by the particular ecursive 
operator used. Blum and Blum characterize 
the sets of partial recursive functions iden- 
tified by methods that are reliable on R as 
all the subsets of sets identified by this 
general scheme for all possible recursive 
operators. 
6.2 Methods Uniformly Adaptable 
to Different Domains 
We can also generalize identification by 
enumeration by studying properties of what 
might be termed effectively adaptable in- 
ference methods. Consider, for example, a
general inference scheme that is given a 
program p to enumerate a class U of func- 
tions, which then "adapts itself" to be an 
identification method for the class U. Iden- 
tification by enumeration is one such 
scheme. Jantke [1979] studies general 
properties of such schemes, which he terms 
strategic operators. He defines a selective 
strategic operator as one that uses as hy- 
potheses only elements of the class U 
enumerated by the given program p. A con- 
structive strategic operator constructs hy- 
potheses by a recursive selection f ele- 
ments of U depending on the input. Jantke 
shows that constructive strategic operators 
may be more powerful than selective ones. 
Computing Surveys, Vol. 15, No. 3, September 1983 
258 • D. Angluin and C. H. Smith 
Even when there is no difference in power, 
constructive strategic operators may be ex- 
ponentially more efficient in the number of 
mind changes than selective ones for par- 
ticular enumerations. 
6.3 A Complexity Theory 
for Inference Methods 
Daley and Smith [1983], building on the 
preliminary work of Freivald [1975b], have 
developed an axiomatic treatment of the 
complexity of inference in the spirit of 
Blum's axiomatic treatment of the com- 
plexity of computation [Blum 1967b]. An 
inference complexity measure for a collec- 
tion of inference procedures i a sequence 
of functionals, one for each procedure, sat- 
isfying the following conditions: 
* The complexity of an inference is defined 
if and only if the inference procedure 
converges. 
• The complexity functional is defined pre- 
cisely on the input sequences on which 
the inference procedure is defined. 
• There is a limiting recursive procedure 
for determining whether a given infer- 
ence procedure has a particular complex- 
ity on a certain input. 
• The complexity functional requires pre- 
cisely the same input to converge as the 
inference procedure. 
• The number of mind changes is an ab- 
solute lower bound on the complexity of 
any inference. 
Using a rigorous version of the above 
axioms, Daley and Smith [1983] show the 
existence of arbitrarily difficult to infer sets 
of functions and sets of inferable functions 
for which there is no most efficient infer- 
ence procedure. A trade-off between com- 
plexity and accuracy (as in the number of 
anomalies) is also exhibited. 
7. PRACTICAL INFERENCE METHODS 
The general inference methods just de- 
scribed are not directly feasible in practical 
domains. Our goal now is to describe some 
of the ideas used in the large variety of 
specific inference methods that have been 
proposed and studied. 
7.1 Search Spaces 
A useful approach is to organize the space 
of hypotheses into something more com- 
plex~han the simple nonadaptive linear list 
used in identification by enumeration. A
richer organization of the hypothesis space 
may allow the elimination of more hy- 
potheses than just the current one when an 
incompatibility with the examples is de- 
tected. Furthermore, the structure itself 
may suggest a plausible replacement for a 
failed hypothesis. 
To illustrate this approach, consider the 
problem of finding a deterministic finite- 
state acceptor of minimum size compatible 
with a given positive sample $1 and nega- 
tive sample So. Although this problem is 
NP-hard [Angluin 1978; Gold 1978], the 
hypothesis space has a useful structure. In 
particular, if M is the canonical tree accep- 
tot of the set $1, then every reduced eter- 
ministic acceptor compatible with $1 and 
So may be obtained by partitioning the 
states of M and merging the states within 
each block of the partition. Consequently, 
it is sufficient o search the space of parti- 
tions of the states of M. Moreover, if M1 
and M2 are obtained from the two parti- 
tions rl  and 7r2 of the states of M, and r l  is 
a refinement of ~r2, then L(Mi) is contained 
in L(M2). Thus, if M~ incorrectly accepts 
some element of So, then M2 does also and 
need not be examined. Also, any partition 
of the states of M that does not correspond 
to a deterministic acceptor may be col- 
lapsed until it does. This structure and 
related ones have been used in many con- 
texts [Angluin 1982a, 1982b; Biermann and 
Feldman 1972b; Feldman et al. 1969; Gold 
1972, 1978; Miclet 1980; Pao and Carr 1978; 
Veelenturf 1978]. 
Another useful structure is provided by 
the subsumption relation in predicate logic. 
If A and B are conjunctions of atoms, A 
subsumes B if and only if there exists 
a substitution a such that the atoms of 
a(A) are a subset of those of B. For exam- 
ple, U(x) & E(x) & T(y) subsumes U(a) & 
E(a) & T(f(a)) & I([(a)). I fA subsumes B, 
then A is more general than B. 
Consider as the set of hypotheses all fi- 
nite conjunctions of atoms from some log- 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 259 
ical language. A hypothesis denotes the set 
of ground atoms that it subsumes. Inferring 
such hypotheses from positive or complete 
presentations of conjunctions of ground at- 
oms is a language inference problem. Sub- 
sumption gives a partial ordering of the 
hypothesis pace according to generality/ 
specificity. If a hypothesis A fails to sub- 
sume some positive example, then no hy- 
pothesis more specific than A need be con- 
sidered, and, conversely, when A subsumes 
some negative xample, no hypothesis more 
general than A need be considered. The 
field generally called concept learning in- 
cludes many elaborations and extensions of 
this simple setting {see, e.g., Dietterich and 
Michalski [1979], Hayes-Roth and Mc- 
Dermott [1978], Michalski [1980], Vere 
[1980, 1981], and Winston [1975]). 
Mitchell [1982] describes an abstraction 
of this setting for concept learning, and 
gives an algorithm called version spaces 
that maintains the boundaries of the sub- 
space of compatible hypotheses. That is, it 
maintains the set G of most general hy- 
potheses that do not subsume any negative 
example, as well as the set S of most specific 
hypotheses that subsume very positive ex- 
ample. As positive and negative xamples 
accumulate, the subspace of compatible hy- 
potheses hrinks accordingly, and if it is 
reduced to one hypothesis, the unknown 
concept is said to be unambiguously 
learned. 
The subsumption relation and the com- 
putability of the least general generaliza- 
tion have been formally studied by Rey- 
nolds [1970] and Plotkin [1970, 1971a, 
1971b]. Shapiro [1981a, 1981b] has made 
use of subsumption to order search spaces 
of axioms in Horn form. When an axiom is 
found to be false, its immediate descend- 
ants in the subsumption relation are taken 
to be plausible replacements. 
7.2 Pruning: Forbidden Features 
A more complex hypothesis pace may be 
used to eliminate a whole group of hy- 
potheses when the current hypothesis is 
found to be incorrect. One approach uses a 
diagnosis routine that takes a hypothesis 
and a collection of examples with which it 
is incompatible and attempts to find the 
reason for the incompatibility. The reason 
is often a forbidden feature, that is, some 
part of the discredited hypothesis that will 
cause any hypothesis in which it is included 
to be incompatible with the data. Subse- 
quent searching is structured to avoid ex- 
amining hypotheses that incorporate the 
forbidden feature. 
Wharton [1977] describes one V rsion of 
this idea in his work on speeding up the 
enumeration of grammars. His method, 
which is a form of backtracking, enumer- 
ates context-free grammars in increasing 
order of complexity and tests them against 
a given sample. If there is an incompatibil- 
ity, a diagnostic routine returns the first 
production in the grammar that must be 
changed to avoid the incompatibility de- 
tected. (All the productions up through the 
indicated one constitute a forbidden feature 
of the grammar.) The enumerator then 
skips over the intervening rammars until 
it finds the first one for which the indicated 
production is different. Unfortunately, this 
method does not avoid a forbidden set of 
productions that occurs in a different po- 
sition or order in a grammar. 
Shapiro [1981a] describes a method of 
searching the hypothesis pace of all finite 
sets of axioms of the form H--~ P, where H 
is a conjunction of atoms and P is a single 
atom. The data consist of a set of ground 
atoms marked as 'true and another set 
marked as false in an unknown model. 
When the current hypothesis-is found to be 
incompatible with the data, the diagnostic 
procedure queries an oracle (the user) about 
certain other ground atoms and returns an 
axiom H ~ P of the current hypothesis 
that is false in the unknown model. The 
offending axiom is then discarded and 
never again appears in any hypothesis. This 
approach is more accurate in pinpointing 
forbidden features, and more successful in 
avoiding them, than a straightforward 
backtracking approach. 
7.3 Hill Climbing 
The hill climbing method finds a locally 
optimal hypothesis by exploring a neigh- 
borhood of the current hypothesis and mov- 
Computing Surveys, Vol. 15, No. 3, September 1983 
260 • D. Angluin and C. H. Smith 
ing if a better hypothesis is found. The 
exploration is iterated until the current hy- 
pothesis is optimal within its neighbor- 
hood. 
Cook et al. [1976] propose ahill climbing 
method for inferring stochastic ontext- 
free grammars from stochastically gener- 
ated positive data. They define a particular 
measure M(G, S) that combines the com- 
plexity of the grammar G with its discrep- 
ancy from the sample S. They also define 
several grammar transformations ( ubsti- 
tution, disjunction, simplification) de- 
signed to reduce the complexity of a gram- 
mar without changing the language that it 
generates by very much. Their procedure 
begins with a grammar that generates pre- 
cisely the observed sample (high complex- 
ity and low discrepancy) and searches the 
neighborhood that consists of all the gram- 
mars obtained by applying a single trans- 
formation to the current grammar. If any 
of these transformed grammars is better 
(according to the measure M) than the 
current one, the procedure moves to the 
best grammar in the neighborhood and it- 
erates the search. Otherwise, it stops and 
outputs the locally optimal grammar that 
it has found. The behavior of their method 
on several inference tasks suggests that 
methods of this kind deserve further study. 
7.4 Jumping to Conclusions: 
Plausible Features 
Methods based on systematic search and 
pruning of hypothesis spaces are essentially 
cautious. They are designed not to skip over 
any compatible hypothesis, and so it is 
usually easy to prove that they produce a 
simplest compatible hypothesis if the sim- 
plicity ordering is consistent with the order 
of search. For the same reason, they tend 
to be very time consuming, with the search 
time growing exponentially in the size of 
the hypothesis to be discovered. 
Methods of another type, called "jumping 
to conclusions," are generally based on a 
set of criteria that suggest, on the basis of 
the data, features that are probably or plau- 
sibly present in the desired hypothesis.By 
composing such features into a full or par- 
tial hypothesis, such methods greatly re- 
duce the number of hypotheses examined, 
sometimes to just one. We use the term 
constructive for methods that jump to con- 
clusions. Their efficiency is often obtained 
at the expense of any simple characteriza- 
tion of what hypotheses will be produced in 
a given situation. There has been some 
success in finding efficient constructive in- 
ference algorithms with simply character- 
ized results; these are described in the next 
section. Many other methods hould be de- 
scribed as heuristic; we consider these first. 
Biermann and Feldman [1972b] describe 
one heuristic method, called the method of 
k-tails, for inferring nondeterministic fi-
nite-state transducers from input/output 
data. The method requires a user-supplied 
parameter k, a positive integer that controls 
when similar states will be identified. 
States are identified if they have the same 
behavior on strings of length at most k, 
that is, their k-tails are the same. 
The method starts with a tree automaton 
M for the given sample and forms a parti- 
tion ~rk of its states. Two states are in the 
same block of ~rh if and only if they are not 
distinguishable by strings derived from the 
sample of length less than or equal to k. 
The method then identifies the states 
within each block of ~rk and ouputs the 
resulting (not necessarily deterministic) 
transducer. In the space of all possible par- 
titions of the states of M, indistinguisha- 
bility by short strings is taken as evidence 
that two states should be identified in the 
final result. If k is chosen sufficiently large 
and there are enough data, this method 
produces a correct deterministic trans- 
ducer. The user's control of k allows tuning 
of the method to some extent. 
The method of k-tails has been general- 
ized for use in inferring tree grammars by 
Brayer and Fu [1977] and Levine [1981, 
1982]. Another heuristic method for infer- 
ring tree grammars i described by Gonza- 
lez et al. [1976]. 
Miclet [1980] describes another ap- 
proach based on state partitioning, but with 
a more flexible criterion of similarity than 
identity of k-tails. He defines a distance 
measure between the behaviors associated 
with two states. His method looks for a pair 
of states that are at most some fixed dis- 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 261 
tance apart, identifies the closest such pair, 
recomputes the distance measures, and it- 
erates until no pair of states is sufficiently 
close to be identified. 
7.5 Efficient and Characterizable Methods 
We shall now briefly describe several con- 
structive inference methods that are prov- 
ably efficient and have simply characteriz- 
able results. These methods all correctly 
identify in the limit a nontrivial class of 
formal languages from positive data. Each 
uses a polynomial-time algorithm to pro- 
duce a smallest (in the sense of set contain- 
ment) language in the class containing a
given positive sample. 
Crespi-Reghizzi [1972] describes a 
method for inferring context-free gram- 
mars from bracketed samples (i.e., positive 
samples from a bracketed grammar). Given 
a bracketed sample, in effect one has a set 
of parse trees from the unknown grammar 
with all internal labels erased, and the 
problem is to reconstruct the internal a- 
bels. CrespioReghizzi's approach is to apply 
a fixed function which can depend on the 
subtree rooted at a node and/or the envi- 
ronment of the subtree in the parse tree, 
and to use the result of this function to 
label the node. A general theory of func- 
tions of this type, called "abstract profiles," 
is developed by Crespi-Reghizzi and Man- 
drioli [1980a, 1980b]. One type of unction 
yields a method for the free operator prec- 
edence grammars [Crespi-Reghizzi 1972], 
another, the free homogeneous k-profile 
grammars [Crespi-Reghizzi et al. 1978]. 
Berger and Pair [1978] have studied this 
general approach in an abstract setting us- 
ing bimorphisms. The method may be 
viewed as a clustering method similar to 
the heuristics described in the preceding 
section. 
A method escribed by Angluin [1980b] 
infers the pattern languages. A pattern is a 
concatenation of variables and constants, 
such as 30xx or 14xylx55y. The language 
L(p) generated by a pattern p is the set of 
strings obtainable by substituting constant 
strings for the variables of the pattern, and 
so L(3Oxx4) contains the strings 30114, 
30004, 301231234, but not he strings 30564 
or 114. She presents an lgorithm that finds 
a smallest pattern language containing a
given positive sample. "Pattern automata" 
are used to represent concisely the set of 
all the patterns that could have generated 
a given string or set of strings. For the 
special case of patterns that contain occur- 
rences of only one variable, the algorithm 
is shown to run in polynomial time. 
Shinohara [1982a, 1982b] has considered 
other types of pattern languages, including 
the extended regular pattern languages, in 
which all the variables occurring in a pat- 
tern are distinct and the null string may be 
substituted for any variable, and the non- 
cross-pattern languages, in which all of the 
occurrences of each variable are either to 
the left or to the right of all occurrences 
of any other variable. He gives polyno- 
mial-time algorithms for finding minimal 
extended regular and non-cross-pattern 
languages from positive data. These algo- 
rithms start with a "most general" pattern 
and specialize it as much as possible using 
local optimization. They lead to correct 
identification i  the limit of the extended 
regular and non-cross-pattern languages. 
Shinohara's application of the regular pat- 
tern languages to a data entry system is 
described in Section 8. 
Another method described by Angluin 
[1982b] infers the k-reversible languages, 
which form a proper subclass of the regular 
languages. Let k be a nonnegative integer 
and L a regular language. Then L is k- 
reversible if and only if whenever ulvw and 
u2vw are in L and Iv I = k, then for every z, 
ulvz is in L if and only if u2vz is in L. An 
example of a zero-reversible language is the 
set of strings of O's and rs  with an even 
number of O's and an even number of l's. 
For any fixed k there is a polynomial-time 
algorithm for finding the smallest k-revers- 
ible language containing a given positive 
sample. The algorithm, which is based on 
a heuristic originally proposed by Feldman 
[1967], constructs the canonical tree accep- 
tot for the sample and then successively 
merges any pair of states that satisfy a 
particular criterion of similarity {depend- 
ent on k). The method is thus another 
clustering method. 
Computing Surveys, Vol. 15, No..3, September 1983 
262 • D. Angluin and C. H. Smith 
7.6 LISP Program Inference 
A number of researchers have studied the 
inference of LISP programs from their in- 
put/output behavior, which constitutes a 
significant subfield of inductive inference. 
D. R. Smith [1982] gives a survey of this 
work. We shall present some of the main 
ideas. 
The systems of Hardy [1975] and Shaw 
et al. [1975] are heuristic; they are based 
on matching a schema for a LISP program 
to the given data and filling in the schema 
with concrete values in a plausible way. The 
system of Siklossy and Sykes [1975] has a 
similar general approach for inferring a 
LISP-like language. Biermann [1978] ap- 
plies his general methodology of synthesiz- 
ing programs from traces to a class that he 
calls the regular LISP programs. Biermann 
and Smith [1977] combine this method 
with a hierarchical decomposition scheme 
to achieve greater efficiency in the synthe- 
sis of "scanning" LISP programs. 
Summers [1976, 1977] describes a gen- 
eral approach to LISP program inference 
based on simple recursive schemata and an 
algorithmic matching procedure. Each out- 
put y is expressed as a term composed from 
the base functions (car, cdr, and cons) ap- 
plied to the corresponding input x, and then 
a recurrence r lation is sought among these 
terms and used to synthesize a recursive 
program. For example, the input/output 
pairs 
<(A), A>, ((A B), B), ((A B C), C> 
give rise to the terms 
ts = car(x), t2 = car(cdr(x)), 
t3 = car(cdr(cdr(x))), 
and the recurrence 
ti+s = ti(cdr(x)/x) 
is detected. A separate, somewhat similar 
procedure is used to discover the predicates 
and synthesize the recursive function 
[(x) = if atom(cdr(x)) 
then car(x) else [(cdr(x)), 
which extracts the last element of an input 
list. 
Summers also describes a method of in- 
troducing auxiliary variables to synthesize 
more complex functions. Considerable 
work exploring and expanding his approach 
has subsequently been done by Guiho, 
Jouannaud, Kodratoff, and others [Jouan- 
naud and Guiho 1979; Jouannaud and Ko- 
dratoff 1979, 1980; Jouannaud et al. 1977; 
Kodratoff 1979; Kodratoff and Fargues 
1978]. One promising feature of this work 
is that the underlying matching algorithms 
are quite efficient. Another is that there 
has been reasonable success in character- 
izing the inferences made by this and re- 
lated methods. 
8. SKETCH OF SOME APPLICATIONS 
Inductive inference may be used to pro- 
vide abstract models of the process of sci- 
entific inquiry or the process by which a 
child acquires its native language [Gold 
1967; Putnam 1975; Wexler and Culicover 
1980]. In the area of L-systems, which uses 
tools from formal language theory to de- 
scribe biological organisms as they change 
in time, the problem of identifying an or- 
ganism, given a sequence of descriptions of
it, has been addressed by Doucet [1974] 
and Feliciangeli and Herman [1977]. There 
have also been a number of proposals for 
applying inductive inference in practical 
systems, which we now sketch. 
In structural pattern recognition, a gram- 
mar or other formal device describing all 
the patterns assigned to a particular class 
is specified. To decide whether a given in- 
put pattern belongs to the specified class, 
it is parsed according to the given grammar. 
Stochastic grammars are particularly suit- 
able for this application. It is often quite 
difficult to devise grammars to fit large 
bodies of data, and it would be useful to 
automate the task at least partially. For an 
excellent introduction to the area of struc- 
tural pattern recognition, including its re- 
lationship to inference, see the books by Fu 
[1975, 1977, 1982] and Gonzalez and 
Thomason [1978]. 
In the Meta-Dendral project at Stanford 
[Buchanan and Feigenbaum 1978], induc- 
tive inference of "breakage rules" for chem- 
ical molecules from mass spectrograph data 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 263 
has been very successfully automated. This 
project was undertaken partly to alleviate 
the bottleneck of acquiring a large number 
of such rules in an explicit form from chem- 
ical experts. 
Inductive inference is also potentially ap- 
plicable in automatic program synthesis. 
Biermann and Krishnaswamy [1976] de- 
scribe a synthesis ystem that uses trace 
information provided by the user. Shaw 
et al. [1975] present an interactive system 
for synthesizing LISP programs. Shapiro 
[1982a, 1982b] suggests that examples and 
specifications may be combined to provide 
a useful, partially automated debugging tool 
for Prolog programs. 
A slightly different emphasis is given by 
Siklossy and Sykes [1975], who suggest 
that an induction mechanism ay be useful 
to a robot in generalizing solutions it has 
found for specific problems. Stolfo and 
Harrison [Stolfo 1979; Stolfo and Harrison 
1979] describe a system that infers heuris- 
tics for puzzle solving from examples of 
solutions. 
Crespi-Reghizzi et al. [1973], as well as 
Knobe and Knobe [1976], suggest that in- 
ference may be useful in specifying pro- 
gramming languages. Certainly no expert 
language designer would trust any existing 
inference system to construct a reasonable 
grammar for a realistic language solely 
from examples, but inference may be of use 
in some stages of the process of language 
design. Coulon and Kayser [1979] describe 
a system that infers a limited database 
query language from examples. 
Another application allows nonexpert 
users to specify what they want a computer 
to do at least partly by examples. Systems 
based on this idea include Zloof's [1977] 
Query by Example system and the RITA 
system of Waterman et al. [1980]. Neither 
of these systems has a true induction com- 
ponent; instead, both use examples to spec- 
ify variables or parameters for procedures 
and to provide an informal user interface. 
Shinohara [1982b] describes an applica- 
tion of the regular pattern languages (see 
Section 7.5) to a system for data entry. For 
example, if the user is entering a series of 
bibliography entries with common key- 
words: Author:, Title:, Journal:, and Year:, 
then the system infers the regular pattern 
Author: u Title: v Journal: w Year: x, 
where the u, v, w, x represent variables in 
the pattern, and the rest are constants. The 
system then prints out the next constant 
portion of the pattern (e.g., Author:) and 
waits for the user to type in the next vari- 
able portion, and so on, effectively prompt- 
ing the user for the fields of the record, and 
removing the burden of typing the key- 
words. 
The Editing by Example system of Nix 
[1983, 1984] also contains a nontrivial ap- 
plication of inductive inference. In this sys- 
tem, which is implemented in the context 
of a general-purpose screen editor, the user 
may specify input/output pairs exemplify- 
ing a text transformation, and the system 
will attempt o infer the pattern common 
to the pairs and synthesize a program to 
perform the transformation in general. The 
user may then execute this program to per- 
form further transformations, or may give 
further examples to the system. A liberal 
"undo" facility allows graceful recovery 
from errors arising from the inference pro- 
cess. 
The text transformations synthesizable 
by the Editing by Example system are gap 
programs, which consist of a gap pattern 
that matches a portion of the text and 
parses it into fields, and a gap replacement 
that copies, rearranges, ordeletes the fields 
(and may introduce new constant fields). 
The resulting string replaces the matched 
string, and the matching and replacement 
process continues with the remainder of the 
file. Nix's thesis contains many results re- 
lated to the inference of gap programs. 
To see how the system operates, uppose 
that the user specifies the following two 
input/output examples: 
George Washington, (202) 357-1243 
=> George: 357-1243 
Abe Lincoln, (202) 356-6636 
=> Abe: 356-6636 
The EBE system synthesizes a gap pro- 
gram that has a gap pattern 
-1- " -2- ',(202)' -3- eol 
Computing Surveys, Vol. 15, No. 3, September 1983 
264 • D. Angluin and C. H. Smith 
and a gap rearrangement 
-1- ':' -3- eol 
This gap program matches any line that 
consists of two words separated by a blank, 
followed by a comma and the area code 
(202), followed by a telephone number, and 
replaces each such line with the first of the 
two words, followed by a colon, followed by 
the telephone number (minus the area 
code). 
The ideas and systems we have described 
represent just the beginning of the appli- 
cation of inductive inference. We expect 
that inductive inference will play a signifi- 
cant role in practical computing in the fu- 
ture. 
9. FUTURE DIRECTIONS 
As this survey documents, there has been 
considerable progress in inductive infer- 
ence research under Gold's paradigm. How- 
ever, as the reader is surely aware, we have 
not provided efinitive answers to the most 
basic questions in inductive inference: 
Given a particular domain, what is the 
"best" possible inference method in that 
domain? What are the relevant measures 
of behavior? What are the trade-offs among 
them (memory, time, background knowl- 
edge, accuracy, errors, data, etc.)? 
The most significant open problem in the 
field is perhaps not any specific technical 
question, but the gap between abstract and 
concrete results. It would be unfortunate if 
the abstract results proliferated fruitlessly, 
while the concrete results produced little or 
nothing of significance beyond their very 
narrow domains. 
Paradoxically, part of the problem might 
be the original Gold paradigm itself. While 
it captures well certain gross features of the 
problem of natural anguage learning--es- 
sentially, one "lifelong" problem with data 
exclusively in the form of examples--it may 
not be wise to stretch it to try to include 
more and more of the "microstructure" of 
inference problems, or domains that consist 
of several related problems or that present 
data in a variety of forms. As useful as the 
Gold paradigm has been, we should not let 
it blind us to other important questions 
about the phenomena of inductive infer- 
ence. 
Computing Surveys, VoL 15, No. 3, September 1983 
Another difficulty is the paucity of prac- 
tical applications to guide the formulation 
of appropriate theoretical questions. Appli- 
cations do not simply occur; they must be 
invented and developed, sometimes in ad- 
vance of the theory that may later explain 
them. 
Notwithstanding these problems, we are 
confident that the basic questions in induc- 
tive inference are sufficiently interesting 
and important that good work will continue 
to unravel them, and future generations 
will have a good laugh at our present igno- 
rance. If this survey has stimulated any 
part of that future good work, it will have 
served its purpose. 
ACKNOWLEDGMENTS 
The initial stages of this project were completed while 
the second author was on the faculty of Purdue Uni- 
versity. The Yale, Purdue, and University ofMaryland 
Computer Science Departments provided computer 
services. The writing of this paper was greatly facili- 
tated by CSNET. We wish to thank the National 
Science Foundation for its direct and indirect support 
of this project. We also thank all the people who read 
and commented onearlier drafts of this paper; their 
comments substantially improved its content and 
readability. 
This work was supported by the National Science 
Foundation under Grants MCS-8301536, MCS- 
8105214, MCS-8002447, and MCS-7903912. 
REFERENCES 
ADELMAN, L., AND BLUM, M. 1975. Inductive infer- 
ence and unsolvability. Tech. Rep., Electrical En- 
gineering and Computer Science Dept., Univ. of 
Calif., Berkeley. 
ANGLUIN, D. 1978. On the complexity of minimum 
inference of regular sets. Inf. Control 39, 337-350. 
ANGLUIN, D. 1980a. Inductive inference of formal 
languages from positive data. Inf. Control45, 117- 
135. 
ANGLUIN, D. 1980b. Finding patterns common to a 
set of strings. J. Comput. Syst. Sci. 21, 46-62. 
ANGLUIN, D. 1982a. A note on the number of queries 
needed to identify regular languages. Inf. Control 
51, 76-87. 
ANGLUIN, D. 1982b. Inference of reversible lan- 
guages. J ACM 29, 3 (July), 741-765. 
BARZDIN, J. M. 1972. Prognostication of automata 
and functions. In Information Processing 71, B. 
Gilchrist, Ed. Elsevier North-Holland, New York, 
pp. 81-84. 
BARZDIN, J. M. 1974a. On synthesizing programs 
given by examples. In Lecture Notes in Com- 
puter Science, vol. 5. Springer-Verlag, New York, 
pp. 53-63. 
Inductive Inference: Theory and Methods * 265 
BARZDIN, J. M. 1974b. Two theorems on the limiting 
synthesis offunctions. Latv. Gos. Univ. Uch. Zap- 
iski 210, 82-88 (in Russian). 
BARZDIN, J. M., AND FREIVALD, R. V. 1972. On the 
prediction of general recursive functions. Soy. 
Math. Dokl. 13, 1224-1228. 
BARZDIN, J. M., AND PODNIEKS, K. M. 1973. The 
theory of inductive inference (in Russian). In 
Proceedings of the 1st Conference on the Mathe- 
matical Foundations of Computer Science (High 
Tetras, Czechoslovakia). Math. Inst. of the Slo- 
vak Academy of Sciences, pp. 9-15. 
BERGER, J., AND PAIR, C. 1978. Inference for regular 
bilanguages. J. Comput. Sys. Sci. 16, 100-122. 
BIERMANN, A. W. 1972. On the inference of Turing 
machines from sample computations. Artif Intell. 
3, 181-198. 
BIERMANN, A. W. 1978. The inference of regular 
LISP programs from examples. IEEE Trans. 
Syst. Man Cybern. SMC-8, 585-600. 
BIERMANN, A. W., AND FELDMAN, J. A. 1972a. A 
survey of results in grammatical inference. In 
Frontiers of Pattern Recognition. Academic Press, 
New York. 
BIERMANN, A. W., AND FELDMAN, J. A. 1972b. On  
the synthesis of finite-state machines from sam- 
ples of their behavior. IEEE Trans. Comput. C- 
21, 592-597. 
BIERMANN, A. W., AND KRISHNASWAMY, R. 1976. 
Constructing programs from example computa- 
tions. IEEE Trans. So#w. Eng. SE-2, 141-153. 
BIERMANN , A. W., AND SMITH, D. R. 1977. The 
hierarchical synthesis of LISP scanning func- 
tions. In Information Processing 77, B. Gilchrist, 
Ed. Elsevier North-Holland, New York, pp. 41- 
45. 
BIERMANN, A. W., BAUM, R. I., AND PETRY, F. E. 
1975. Speeding up the synthesis of programs 
from traces. IEEE Trans. Comput. C-24, 122-136. 
BLUM, L., AND BLUM, M. 1975. Toward a mathe- 
matical theory of inductive inf rence. Inf. Control 
28, 125-155. 
BLUM, M. 1967a. On the size of machines. Inf. Con- 
trol 11, 257-265. 
BLUM, M. 1967b. A machine-independent theory of 
the complexity of recursive functions. J. ACM 14, 
2 (Apr.) 322-336. 
BRANDT, U. 1981. A characterization f identifiable 
sets. Preprint, Technische Hochschule Darms- 
tadt, F.R.G. 
BRAYER, J. M., AND FU, K. S. 1977. A note on the 
k-tall method of tree grammar inference. IEEE 
Trans. Syst. Man Cybern. SMC-7, 293-300. 
BUCHANAN, B. G., AND FEIGENBAUM, E. A. 1978. 
Dendral and Meta-Dendral: Their applications 
dimension. Arti[. Intell. 11, 5-24. 
BUNDY, A., AND SILVER, B. 1981. A critical survey 
of rule learning programs. Tech. Rep. 169, Dept. 
of Artificial Intelligence, Edinburgh Univ. 
CASE, J., AND LYNES, C. 1982. Inductive inference 
and language identification. In Proceedings of the 
International Colloquium on Algorithms, Lan- 
guages, and Programming (ICALP) 82, (Barce- 
lona, Spain; June). Springer-Verlag, New York, 
pp. 107-115. 
CASE, J., AND NGo-MANGUELLE, S. 1979. Refine- 
ments of inductive inference by Popperian ma- 
chines. Tech. Rep., Dept. of Computer Science, 
State Univ. of New York, Buffalo. 
CASE, J., AND SMITH, C. 1978. Anomaly hierarchies 
of mechanized inductive inference. In Proceed- 
ings of the lOth ACM Symposium on Theory of 
Computing (San Diego, Calif., May 1-3). ACM, 
New York, pp. 314-319. 
CASE, J., AND SMITH, C. 1983. Comparison of iden- 
tification criteria for machine inductive infer- 
ence. Theor. Comput. Sci. 25,193-220. 
CHAITIN, G. J. 1974. Information-theoretic limita- 
tions of formal systems. J. ACM 21, 3 (July), 403- 
424. 
CHAITIN, G. J. 1975. A theory of program size for- 
mally identical to information theory. J. ACM 22, 
3 (July) 329-340. 
CHAITIN, G. J. 1976. Information-theoretic charac- 
terizations of recursive infinite strings. Theor. 
Comput. Sci. 2,45-48. 
CHEN, K. J. 1982. Tradeoffs in the inductive infer- 
ence of nearly minimal size programs. Inf. Control 
52, 68-86. 
COOK, C. M., ROSENFELD, A., AND ARONSON, A. R. 
1976. Grammatical inference by hill-climbing. 
Inf. Sci. 10, 59-80. 
COULON, D., AND KAYSER, D. 1979. Construction f 
natural language sentence accepters by a super- 
vised learning technique. IEEE Trans. Pattern 
Anal. Mach. InteU. PAMI-1, 94-99. 
CRESPI-REGHIZZI, S. 1972. An effective model for 
grammar inference. In Information Processing 71, 
B. Gilchrist, Ed. Elsevier North-Holland, New 
York, pp. 524-529. 
CRESPI-REGHIZZI, S., AND MANDRIOLI, D. 1980a. 
Abstract profiles for context-free languages. 
Tech. Rep. 80-6, Ist. di Elettrotechnica edElet- 
tronica del Politechnico di Milano, Milan, Italy. 
CRESPI-REGHIZZI, S., AND MANDRIOLI, D. 1980b. 
Inferring rammars by means of profiles: a unify- 
ing view. Internal report, Ist. di Elettrotechnica 
ed Elettronica del Politechnico diMilano, Milan, 
Italy. 
CRESPI-REGHIZZI, S., MELKANOFF, M. A., AND LICH- 
TEN, L. 1973. The use of grain-'L.~tical inference 
for designing programming la uages. Commun. 
ACM 16, 2 (Feb.), 83-90. 
CRESPI-REGHIZZI, S., GUIDA, G., AND MANDRIOLI, D. 
1978. Noncounting context-free languages. J. 
ACM 25, 4 (Oct.), 571-580. 
DALEY, R. 1973. An example of information and 
computation resource trade-off. J. ACM 20, 4 
(Oct.), 687-695. 
DALEY, R. 1977. On  the inference of optimal descrip- 
tions. Theor. Comput. Sci. 4, 301-319. 
DALEY, R. 1983. On the error correcting power of 
pluralism in inductive inference. Theor. Comput. 
Sci. 24, 95-104. 
Computing Surveys, Vol. 15, No. 3, September 1983 
266 • D. Angluin and C. H. Smith 
DALEY, R., AND SMITH, C. 1983. On the complexity 
of inductive inference. Tech. Rep. 83-4, Dept. of 
Computer Science, Univ. of Pittsburgh. 
DIETTERICH, T. G., AND MICHALSKI, R. S. 
1979. Learning and generalization f character- 
istic descriptions: Evaluation criteria and com- 
parative review of selected methods. In Proceed- 
ings of the 6th International Joint Conference on 
Artificial Intelligence (Tokyo, Aug.). Interna- 
tional Joint Council on Artificial Intelligence, pp. 
223-231. 
DIETTERICH, T. G., LONDON, R., CLARKSON, K., AND 
DROMEY, R. 1982. Learning and inductive in- 
ference. In The Handbook of Artificial Intelligence, 
P. Cohen and E. Feigenbaum Eds. Kaufman, Los 
Altos, Calif., pp. 323-512. 
DOUCET, P. G. 1974. The syntactic inference prob- 
lem for D0L sequences. In L-Systems, G. Rozen- 
berg and A. Salomaa, Eds. Springer Lecture 
Notes on Computer Science, vol. 15. Springer- 
Verlag, New York, pp. 146-161. 
FELDMAN, J. A. 1967. First thoughts on grammatical 
inference. Tech. Rep., Computer Science Dept., 
Artificial Intelligence Memo 55, Stanford Uni- 
versity, Stanford, Calif. 
FELDMAN, J. A. 1972. Some decidability results in 
grammatical inference. Inf. Control 20, 244-262. 
FELDMAN, J. A., AND SHIELDS, P. 1977. Total com- 
plexity and the inference of best programs. Math. 
Syst. Theory. 10, 181-191. 
FELDMAN, J. A., GIPS, J., HORNING, J. J., AND REDER, 
S. 1969. Grammatical complexity and inference. 
Tech. Rep., Computer Science Dept., Stanford 
Univ., Stanford, Calif. 
FELICIANGELI, H., AND HERMAN, G. 1977. Algo- 
rithms for producing rammars from sample der- 
ivations: A common problem of formal language 
theory and developmental biology. J. Comput. 
Syst. Sci. 7, 97-118. 
FLANAGAN, P. A. 1981. A discrete model of semantic 
learning. Tech. Rep. CS-79, Computer Science 
Dept., Brown University, Providence, R. I. 
FREDKIN, E. 1964. Techniques using LISP for au- 
tomatically discovering interesting relations in 
data. In The Programming Language LISP, Ber- 
key, Ed. M.I.T. Press, Cambridge, Mass., pp. 108- 
124. 
FREIVALD, R. V. 1974. On the limit synthesis of 
numbers of general recursive functions in various 
computable numerations. Soy. Math. Dokl. 15, 
1681-1683. 
FREIVALD, R. V. 1975a. Minimal Godel numbers and 
their identification i  the limit. In Lecture Notes 
in Computer Science, vol. 32. Springer-Verlag, 
New York, pp. 219-225. 
FREIVALD, R. V. 1975b. On the complexity and op- 
timality of computation i the limit. Latv. Gos. 
Univ. Uch. Zapiski 233, 155-173 (in Russian). 
FREIVALD, R. V. 1979. Finite identification of gen- 
eral recursive functions by probabilistic strate- 
gies. In Proceedings of the Conference on Alge- 
braic, Arithmetic, and Categorial Methods in Corn- 
putation Theory (Berlin, Sept. 1979). L. Bndach, 
Ed. Akademie-Verlag, Berlin, DDR, pp. 138-145. 
FREIVALD, R. V., AND WlEHAGEN, R. 1979. Induc- 
tive inference with additional information. E/ec- 
tron. Informationsverarb. Kybern. (EIK) 15, 179- 
185. 
FREIVALD, R. V., KINBER, E. B., AND WIEHAGEN, R. 
1982. Inductive inference and computable one- 
one numberings. Z Logik Grundlag. Math. 23, 
463-479. 
FRIEDBERG, R. 1958. Three theorems on recursive 
enumeration. J. Symb. Logic 23, 309-316. 
Fu, K. S. 1975. Syntactic Methods in Pattern Rec- 
ognition. Academic Press, New York. 
Fu, K. S. 1977. Syntactic Pattern Recognition, Ap- 
plications. Springer-Verlag, New York. 
Fu, K. S. 1982. Syntactic Pattern Recognition and 
Applications. Prentice-Hall, New York. 
Fu, K. S., AND BOOTH, T. L. 1975. Grammatical 
inference: introduction a d survey, parts 1 and 2. 
IEEE Trans. Syst. Man Cybern. SMC-5, 95-111, 
409-423. 
GACS, P. 1983. On the relationship between descrip- 
tional complexity and algorithmic probability. 
Theor. Comput. Sci., 22, 1, 2, 71-93. 
GAINES, B. R. 1976. Behavior/structure t ansfor- 
mations under uncertainty. Int. J. Man-Mach. 
Stud. 8, 337-365. 
GAINES, B. R. 1978. Maryanski's grammatical infer- 
encer. IEEE Trans. Comput. C-28, 62-64. 
GOLD, E. M. 1967. Language identification i the 
limit. Inf. Control 10, 447-474. 
GOLD, E. M. 1972. System identification via state 
characterization. Autornatica 8, 621-636. 
GOLD, E. M. 1978. Complexity of automaton iden- 
tification from given data. Inf. Control. 37, 302- 
320. 
GONZALEZ, R. C., AND THOMASON, M. G. 1978. 
Syntactic Pattern Recognition, An Introduction. 
Addison-Wesley, Reading, Mass. 
GONZALEZ, R. C., EDWARDS, J. J., AND THOMASON, 
M. G. 1976. An algorithm for the inference of 
tree grammars. Int. J. Comput. Inf. Sci. 5, 145- 
164. 
HARDY, S. 1975. Synthesis of LISP programs from 
examples. In Proceedings of the 4th International 
Joint Conference on Artificial Intelligence (Tibilsi, 
Georgia, USSR, Sept.). International Joint Coun- 
cil on Artificial Intelligence, pp. 268--273. 
HAYES-ROTH, F., AND MCDERMOTT, J. 1978. An 
interference matching technique for inducing ab- 
stractions. Commun. ACM 21, 5 (May), 401-411. 
HORNING, 3. J. 1969. A study of grammatical infer- 
ence. Ph.D. dissertation, Computer Science 
Dept., Stanford Univ., Stanford, Calif. 
JANTKE, K. P. 1979. Natural properties of strategies 
identifying recursive functions. Elektron. Infor- 
mationsverarb. Kybern. (EIK) 15, 487-496. 
JANTKE, K. P., AND BEICK, H. R. 1981. Combining 
postulates of naturalness in inductive inference. 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods * 267 
Elektron. Informationsverarb. Kybern. (EIK) 17, 
465-484. 
JOUANNAUD, J. P., AND GUIHO, G. 1979. Inference 
of functions with an interactive system. In Ma- 
chine Intelligence 9, J. E. Hayes, D. Michie, and 
L. I. Mikulich, Eds. Wiley, New York, pp. 227- 
250. 
JOUANNAUD, J. P., AND KODRATOFF, Y. 1979. 
Characterization f a class of functions ynthe- 
sized by a Summers-like method using a B.M.W. 
matching technique. In Proceedings of the 6th 
International Joint Conference on Artificial Intel- 
ligence (Tokyo, Aug.). International Joint Council 
on Artificial Intelligence, pp. 440-447. 
JOUANNAUD, J. P., AND KODRATOFF, Y. 1980. An 
automatic onstruction of LISP programs by 
transformations of functions synthesized from 
their input-output behavior. Int. J. Policy Analy. 
In[. Syst. 4, 331-358. 
JOUANNAUD, J. P., GUIHO, G., AND TREUIL, T. P. 
1977. SISP/1, an interactive system able to syn- 
thesize functions from examples. In Proceedings 
of the 5th International Joint Conference on Ar- 
tificial Intelligence (Cambridge, Mass., Aug.). In- 
ternational Joint Council on Artificial Intelli- 
gence, pp. 412-418. 
JUNG, H. 1977. Zur Untersuchung von abstrakten 
interaktiven Erkennungssystemen. Ph.D. disser- 
tation, Humbolt University, Berlin, GDR. 
KLETTE, R., AND WIEHAGEN, R. 1980. Research in 
the theory of inductive inference by GDR math- 
ematicians--A survey. Inf. Sci. 22, 149-169. 
KNOBE, B., AND KNOBE, K. 1976. A method for 
inferring context-free grammars. Inf. Control 31, 
129-146. 
KODRATOFF, Y. 1979. A class of functions synthe- 
sized from a finite number of examples and a 
LISP program scheme. Int. J. Comput. Inf. Sci. 
8, 489-521. 
KODRATOFF, Y., AND FARGUES, J. 1978. A sane al- 
gorithm for the synthesis of LISP functions from 
example problems. In Proceedings of the AISB/  
GI Conference on Artificial Intelligence (Hamburg, 
July). Society for the Study of Artificial Intelli- 
gence and Simulation of Behavior/Gesellschaft 
fur Informatik, pp. 169-175. 
KOLMOGOROV, A. N. 1965. Three approaches to the 
quantitative definition of information. Probl. Inf. 
Transm. 1, 1-7. 
KUGEL, P. 1977. Induction, pure and simple. Inf. 
Control 35, 276-336. 
LAUDAN, L. 1977. Progress and Its Problems. Uni- 
versity of California Press, Berkeley, Calif. 
LEVlNE, B. 1981. Derivatives of tree sets with appli- 
cations to grammatical inference. IEEE Trans. 
Pattern Anal. Mach. InteU. PAMI-3, 285-293. 
LEVINE, B. 1982. The use of tree derivatives and a 
sample support parameter for inferring tree sys- 
tems. IEEE Trans. Pattern Anal. Mach. InteU. 
PAMI-4, 25-34. 
LINDNER, R. 1972. Algorithmische Erkennung. 
Ph.D. dissertation, Friedrich-Schiller-Universi- 
t~it, Jena, G.D.R. 
MACHTEY, M., AND YOUNG, P. 1978. An Introduc- 
tion to the General Theory of Algorithms. Elsevier 
North-Holland, Hew York. 
MARTIN-LOF, P. 1966. The definition of random se- 
quences. Inf. Control 9, 602-619. 
MARTIN-LOF, P. 1971. Complexity oscillations in in- 
finite binary strings. Z. Wahrscheinlichkeitstheor. 
Verw. Geb. 19, 225-230. 
MARYANSKI, F. J., AND BOOTH, T. L. 1977. In- 
ference of finite-state probabilistic grammars. 
IEEE Trans. Comput. C-26, 521-536. 
MICHALSKI, R. 1980. Pattern recognition as rule 
guided inductive inference. IEEE Trans. Pattern 
Anal. Mach. Inteli. PAMI-2, 349-361. 
MICHALSKI, R., CARBONELL, J., AND MITCHELL, T., 
Eds. 1983. Machine Learning. Tioga Publ., Palo 
Alto, Calif. 
MICLET, L. 1980. Regular inference with a tail-clus- 
tering method. IEEE Trans. Syst. Man Cybern. 
SMC-IO, 737-743. 
MINICOZZl, E. 1976. Some natural properties of 
strong identification in inductive inference. 
Theor. Comput. Sci. 2, 345-360. 
MITCHELL, T. M. 1982. Generalization as search. 
Artif. Intell. 18, 203-226. 
NIX, R. 1983. Editing by example. Ph.D. disserta- 
tion, Computer Science Dept., Yale University, 
New Haven, Conn. 
Nix, R. 1984. Editing by example. In Proceedings of 
the 11th ACM Symposium on Princip~s of Pro- 
gramming Languages, (Salt Lake City, Utah, Jan. 
15-18). ACM, New York, pp. 186-195. 
OSHERSON, D. N., AND WEINSTEIN, S. 1982. Criteria 
of language learning. Inf. Control 52, 123-138. 
PAO, T. W., AND CARR, J. W., III. 1978. A solution 
of the syntactical induction-inference problem for 
regular languages. Comput. Lang. 3, 53-64. 
PERSSON, S. 1966. Some sequence extrapolating 
programs: A study of representation a d model- 
ing in inquiring systems. Ph.D. dissertation, 
Computer Science Dept., Stanford University, 
Stanford, Calif. 
PIVAR, M., AND FINKELSTEIN, M. 1964. Automation, 
using LISP, of inductive inference on sequences. 
In The Programming Language LISP, Berkey, 
Ed. M.I.T. Press, Cambridge, Mass., pp. 125-136. 
PIVAR, M., AND GORD, E. 1964. The LISP program 
for inductive inference on sequences. In The Pro- 
gramming Language LISP, Berkey, Ed. M.I.T. 
Press, Cambridge, Mass., pp. 260-289. 
PLOTKIN, G. D. 1970. A note on inductive generali- 
zation. In Machine Intelligence 5. Elsevier North- 
Holland, New York, pp. 153-163. 
PLOTKIN, G. D. 1971a. Automatic methods of induc- 
tive inference. Ph.D. dissertation, Computer Sci- 
ence Dept., Edinburgh University. 
PLOTKIN, G. D. 1971b. A further note on inductive 
generalization. InMachine Intelligence 6. Elsevier 
North-Holland, New York, pp. 101-124. 
Computing Surveys, Vol. 15, No. 3, September 1983 
268 • D. Angluin and C. H. Smith 
PODNIEKS, K. M. 1974. Comparing various concepts 
of function prediction, Part I. Latv. Gosudarst. 
Univ. Uch. Zapiski 210, 68-81 (in Russian). 
PODNIEKS, K. M. 1975a. Probabilistic synthesis of 
enumerated classes of functions. Soy. Math. Dokl. 
16, 1042-1045. 
PODNIEKS, K. M. 1975b. Comparing various con- 
cepts of function prediction, Part II. Law. Gosu- 
darst. Univ. Uch. Zapiski 233, 33-44. (in Rus- 
sian). 
PUTNAM, H. 1975. Probability and confirmation. In
Mathematics, Matter and Method. Cambridge 
Univ. Press, Cambridge, England. 
REYNOLDS, J. C. 1970. Transformational systems 
and the algebraic structure of atomic formulas. 
In Machine Intelligence 5. Elsevier North-Hol- 
land, New York, pp. 135-151. 
SHAPIRO, E. 1981a. Inductive inference of theories 
from facts. Tech. Rep. 192, Dept. of Computer 
Science, Yale University, New Haven, Conn. 
SHAPIRO, E. 1981b. A general incremental algorithm 
that infers theories from facts. In Proceedings of 
the 7th International Joint Conference on Artifi- 
cial Intelligence (Vancouver, B.C., Canada, Aug.). 
International Joint Council on Artificial Intelli- 
gence, pp. 446-451. 
SHAPIRO, E. Y. 1982a. Algorithmic program diag- 
nosis. In Proceedings of the 9th ACM Symposium 
on Principles of Programming Langtmges (Albu- 
querque, N. Mex.). ACM, New York, pp. 299- 
308. 
SHAPIRO, E. 1982b. Algorithmic program debugging. 
Ph.D. dissertation, Computer Science Dept., Yale 
University, 1982. Published by M.I.T. Press, 
1983. 
SHAW, D., SWARVOUT, W., AND GREEN, C. 1975. In- 
ferring LISP programs from example problems. 
In Proceedings of the 4th International Joint Con- 
ference on Artificial Intelligence (Tibilsi, Georgia, 
USSR, Sept.). International Joint Council on Ar- 
tificial Intelligence, pp. 260-267. 
SHINOHARA, T. 1982a. Polynomial time inference of 
extended regular pattern languages. In Proceed- 
ings, Software Science and Engineering, Kyoto, 
Japan. 
SHINOHARA, T. 1982b. Polynomial time inference of 
pattern languages and its applications. In Pro- 
ceedings of the 7th IBM Symposium on Mathe- 
matical Foundations of Computer Science. 
SIKLOSSY, L., AND SYKES, D. 1975. Automatic pro- 
gram synthesis for example problems. In Proceed- 
ings of the 4th International Joint Conference on 
Artificial Intelligence (Tibilsi, Georgia, USSR, 
Sept.). International Joint Council on Artificial 
Intelligence, pp. 268--273. 
SIMON, H. A., AND KOTOVSKY, K. 1963. Human 
acquisition of concepts for sequential patterns. 
Psych. Rev. 70, 534-546. 
SMITH, C. H. 1982. The power of pluralism for au- 
tomatic program synthesis. J. ACM 29, 4 (Oct.), 
1144-1165. 
SMITH, D. R. 1982. A survey of the synthesis ofLISP 
programs from examples. In Automatic Program 
Construction Techinques, A. W. Biermann, G. 
Guiho, and Y. Kodratoff, Eds. Macmillan, New 
York. 
SOLOMONOFF, R. J. 1964. A formal theory of induc- 
tive inference. Inf. Control 7, 1-22, 224-254. 
SOLOMONOFF, R. J. 1975. Inductive inferende the- 
ory--A unified approach to problems in pattern 
recognition and artificial intelligence. In Proceed- 
ings of the 4th International Conference Artificial 
Intelligence (Tibilsi, Georgia, USSR, Sept.). In- 
ternational Joint Council on Artificial Intelli- 
gence, pp. 274-280. 
SOLOMONOFF, R. J. 1978. Complexity-based induc- 
tion systems: comparisons and convergence theo- 
rems. IEEE Trans. Inf. Theor. IT-24, 422-432. 
STOLFO, S. J. 1979. Automatic discovery heuristics 
for nondeterministic programs from sample xe- 
cution traces. Ph.D. dissertation, Computer Sci- 
ence Dept., New York University. 
STOLFO, S. J., AND HARRISON, M. C. 1979. Au- 
tomatic discovery of heuristics for nondetermin- 
istic programs. Tech. Rep. 7, Computer Science 
Dept., New York University. 
SUMMERS, P. D. 1976. Program construction from 
examples. Ph.D. dissertation, Computer Science 
Dept., Yale University, New Haven, Conn. 
SUMMERS, P. D. 1977. A methodology for LISP pro- 
gram construction from examples. J. ACM 24, 1 
(Jan.), 161-175. 
THIELE, H. 1973. Lernverfahren zur Erkennung for- 
maler Sprachen. Kybern. Forsch. 3, 11-93. 
THIELE, H. 1975. Zur Charakterisierung yon Erken- 
nungssytemen mit einbettendem Konvergenzbe- 
griff. Kompliziertheit Lern Erkennungsprozessen 
2, 188-207. Jena, G.D.R. 
VAN DER MUDE, A., AND WALKER, A. 1978. On the 
inference of stochastic regular grammars. Inf. 
Control 38, 310-329. 
VEELENTURF, L. P. J. 1978. Inference of sequential 
machines from sample computations. IEEE 
Trans. Comput. C-27, 167-170. 
VERE, S. A. 1980. Multilevel counterfactuals for gen- 
eralizations of relational concepts and produc- 
tions. Artif. InteU. 14, 139-164. 
VERE, S. A. 1981. Constrained N-to-1 generaliza- 
tion. Preprint, Jet Propulsion Laboratory, Pasa- 
dena, Calif. 
WATANABE, S. 1960. Information-theoretical aspects 
of inductive and deductive inference. IBM J. Res. 
Devel. 4, 208-231. 
WATERMAN, D. A., FAUGHT, W. S., KLAHR, P., RO- 
SENSCHEIN, S. J., AND WESSON, R. 1980. De- 
sign issues for exemplary programming. Tech. 
Rep. N-1484-RC, Rand Corporation, Santa Mon- 
iea, Calif. 
WEXLER, K., AND CULICOVER, P. 1980. Formal 
Principles of Language Acquisition. M.I.T. Press, 
Cambridge, Mass. 
Computing Surveys, Vol. 15, No. 3, September 1983 
Inductive Inference: Theory and Methods • 269 
WHARTON, R. M. 1974. Approximate language iden- 
tification. Inf. Control 26, 236-255. 
WHARTON, R. M. 1977. Grammar enumeration and 
inference. Inf. Control 33, 253-272. 
WIEHAGEN, R. 1976. Limes-erkennung rekursiver 
Funktionen dutch spezielle Stratigen. Elektron. 
Informationsverarbeit. Kybern. (EIK) 12, 93-99. 
WIEHAGEN, R. 1977. Identification of formal lan- 
guages. In Lecture Notes in Computer Science, 
vol. 53. Springer-Verlag, New York, pp. 571-579. 
WIEHAGEN, R. 1978. Characterization problems in 
the theory of inductive inference. In Proceedings 
of the 5th Colloquium on Automata, Languages, 
and Programming (Udine, Italy, July), Lecture 
Notes on Computer Science, vol. 62. Springer- 
Verlag, New York, pp. 494-508. 
WIEHAGEN, R., AND LIEPE, W. 1976. Charak- 
teristche Eigenschaften yon erkennbaren Klas- 
sen rekursiver Funktionen. Elektron. Informa- 
tionsverarbeit. Kybern. (EIK) 12, 421-438. 
WINSTON, P. 1975. Learning StructuralDescriptions 
#ore Examples. McGraw-Hill, New York. 
ZLOOF, M. 1977. Query-by-Example: A data base 
language. IBM Syst. J. 16, 324-343. 
ZVONKIN, A. K., AND LEV[N, L. A. 1970. The com- 
plexity of finite objects and the development of
the concepts of information and randomness by 
means of the theory of algorithms. Russian Math. 
Rev. 25, 83-124. 
Received November 1982; revised October 1983; final revision accepted February 1984. 
Computing Surveys, Vol. 15, No. 3, September 1983 

AAECC manuscript No. (will be inserted by the editor)
Lambda-Calculus with Director Strings
Maribel FernaÂ´ndez1, Ian Mackie1, FrancÂ¸ois-ReÂ´gis Sinot2, 1 Department of Computer Science, Kingâ€™s College London
Strand, London WC2R 2LS, UK e-mail: {maribel,ian}@dcs.kcl.ac.uk 2 LIX, EÂ´ cole Polytechnique, 91128 Palaiseau, France e-mail: frs@lix.polytechnique.fr
Received: date / Revised version: date
Abstract We present a name free Î»-calculus with explicit substitutions, based on a generalised notion of director strings. Terms are annotated with information â€“ directors â€“ that indicate how substitutions should be propagated. We ï¬rst present a calculus where we can simulate arbitrary Î²-reduction steps, and then simplify the rules to model the evaluation of functional programs (reduction to weak head normal form). We also show that we can deï¬ne the closed reduction strategy. This is a weak strategy which, in contrast with standard weak strategies, allows certain reductions to take place inside Î»-abstractions thus offering more sharing. Our experimental results conï¬rm that, for large combinator-based terms, our weak evaluation strategies out-perform standard evaluators. Moreover, we derive two abstract machines for strong reduction which inherit the efï¬ciency of the weak evaluators.
Key words Î»-calculus â€“ explicit substitutions â€“ director strings â€“ strategies
1 Introduction
In the Î»-calculus, the operation of substitution used in the Î²-reduction rule is deï¬ned outside the system: it is a meta-operation (see [5]). In contrast, explicit substitution calculi deï¬ne substitution with reduction rules at the same level as Î²reduction. Over the last years a whole range of explicit substitution calculi have been proposed, starting with the work of de Bruijn [14] and the Î»Ïƒ-calculus [1]. Although there are many different applications of such calculi, one of the main advantages that we see in describing the process of propagation of substitution at the same level as Î²-reduction is that it allows us to control the substitution process, with an emphasis on implementation.
Projet Logical, PoË†le Commun de Recherche en Informatique du plateau de Saclay, CNRS, EÂ´ cole Polytechnique, INRIA, UniversiteÂ´ Paris-Sud.

2 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot
There are different notations for substitution, or more precisely, for variables in the Î»-calculus. Explicit substitution calculi can be classiï¬ed as:
â€“ named, when variables are denoted by names such as x, y, . . .; and â€“ unnamed, for instance when numbers (also called indices) are used.
During the process of propagation of substitution, it may be necessary to perform Î±-conversion (i.e. variable renaming) to avoid variable capture or clash. Unnamed explicit substitution calculi are thus often preferred for implementation purposes (although there are some exceptions, see for instance [33, 17]). De Bruijn notation [13] has, without doubt, become the standard name-free syntax for explicit substitution calculi. The purpose of this paper is to deï¬ne an alternative notation for unnamed explicit substitution calculi, based on director strings.
Director strings were introduced by Kennaway and Sleep [22] for combinator reduction, which translated to the Î»-calculus gives a system where no reduction can be performed under abstractions. They were generalised in [17, 18] in order to deï¬ne closed reduction for the Î»-calculus (a weak reduction strategy which in contrast with standard weak strategies allows certain reductions to take place inside Î»-abstractions, thus offering more sharing). A further generalisation of director strings was used in [35] to deï¬ne strong reduction strategies for the Î»-calculus, and to derive abstract machines suitable for reduction to weak head normal form and to full normal form. In this present paper we deï¬ne a calculus of explicit substitutions with director strings in which any strategy of reduction in the Î»-calculus can be simulated, and we explore the properties of this general director string calculus as a rewrite system and also as a means to express efï¬cient (weak and strong) reduction strategies for the Î»-calculus. We consider this important for several reasons:
â€“ Director strings offer an alternative to de Bruijn notation [13] for unnamed calculi. However, as for de Bruijn notation, the syntax is not as readable as the corresponding named version. We will show that the general notation can be simpliï¬ed in some cases, for instance, closed reduction turns out to be a natural restriction leading to a very simple rewrite system for weak reduction.
â€“ Director strings are a natural notation for explicit substitutions from an operational point of view: terms are annotated to indicate what they should do with a substitution. Substitutions are only propagated to places where they are needed, thus these calculi preserve strong normalisation (i.e. if a Î»-term is strongly normalisable, so is its compilation). Other calculi preserving strong normalisation are presented in [27,12] (see [29, 9] for counterexamples in Î»Ïƒ).
â€“ We provide a generalisation of the director strings introduced by Kennaway and Sleep [22] for combinator reduction. With our generalised director strings we can simulate arbitrary Î²-reductions.
We thus see the calculi presented in this paper both as an alternative syntax for explicit substitutions and as a basis for more efï¬cient implementations of the Î»calculus. We present three calculi based on director strings. The ï¬rst one, which we call Î»o, is a general system where any Î²-reduction in the Î»-calculus can be simulated. From a theoretical point of view, Î»o has the desired properties (it is conï¬‚uent, preserves strong normalisation, fully simulates the Î»-calculus), however, from an

Lambda-Calculus with Director Strings

3

implementation perspective, its generality is a drawback rather than an advantage. In the other two calculi, which we call Î»l and Î»c, reduction is restricted so that only some evaluation strategies (which are efï¬cient) can be simulated. In this sense, Î»l and Î»c are weak, but not as weak as standard weak calculi. It is well-known that standard weak explicit substitution calculi avoid Î±-conversion by allowing neither reduction under abstraction nor propagation of substitution through an abstraction (see for instance [11]). In contrast, our weak calculi allow certain reductions under, and propagation of substitutions through, abstractions. In this way more reductions can be shared. Moreover we may use the explicit information given by directors to avoid copying a substitution which contains a free variable, avoiding the duplication of potential redexes.
We have implemented a family of abstract machines for weak and strong reduction based on the director strings calculi, and the benchmarks (given in Section 8) show that the level of sharing obtained is close to optimal reduction [23, 19, 3] with considerably less overhead in many cases. Immediate applications of this work include, on one hand Î»-calculus/functional language evaluators (where weak reduction is needed), and on the other hand, partial evaluation (also called program specialisation) and proof assistants based on powerful type theories (where strong reduction is needed).

1.1 Related Work
Our work is clearly related to the general work on explicit substitutions, starting from de Bruijnâ€™s seminal Î»CÎ¾Ï† [14] (see [7] for a modern presentation) and the Î»Ïƒ-calculus [1]. However, it is much more in line with the use of explicit substitutions for controlling the substitution process in implementations of the Î»calculus [2, 36,21,24,31]. Our calculi are closer to Î»Ï… [26, 27] than to Î»Ïƒ [1] in the sense that we do not have a syntactic construct for concatenation (also sometimes referred to as composition) of independent substitutions. Nadathurâ€™s work [30, 31] is also concerned with efï¬ciency and has some common points with ours (although in a quite different framework): for instance, a variant of his calculus has conditions of closedness on certain terms.
Efï¬ciency and sharing in the Î»-calculus have been important topics in the last twenty years. There is, in the literature, a wide range of mechanisms used for sharing: environments [36], sharing graphs [4], calculi with explicit addressing [8, 24]. We use director strings and the mechanism of explicit substitution itself for the purpose of sharing, i.e. we do not use any external machinery. While optimal sharing [28] means optimal number of Î²-reductions, its implementation relies on sharing graphs [23,19,3,4] in which a wealth of costly book-keeping rules are necessary (see [25] for instance). Hence we will in this paper give to efï¬ciency a rather algorithmic meaning, or more pragmatically, we will count the total number of reduction steps necessary to reach a normal form, provided these steps are elementary in some sense.
Director strings were introduced in [22] for combinatory reduction. A ï¬rst generalisation was used in [18] for closed reduction, which was the starting point of [35]. This present work is a substantially revised and extended version of [35].

4 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot
1.2 Overview
The rest of this paper is structured as follows. In the following section we provide the background material and deï¬ne the syntax of director strings. In Section 3 we present a general calculus where we can simulate arbitrary Î²-reduction steps. Section 4 presents the simpliï¬ed local open calculus, and the closed reduction system. A type system for these calculi is presented in Section 5. We then use these calculi to deï¬ne several strategies: weak (Section 6) and strong ones (Section 7), which we experimentally compare (Section 8). Finally, we conclude the paper in Section 9.
2 Director Strings
2.1 Background
We brieï¬‚y recall the basic ideas of director strings [22]. As a motivating example, consider a term with two free variables f, x and substitutions for both of them: ((f (f x))[F/f ])[X/x]. The best way to perform these substitutions is to propagate them only to the places in the syntactic tree where they are required. Figure 1(a) shows the paths which the substitutions should follow in the tree, where the solid line corresponds to the substitution for f , and the dotted line for x.

@

Â d

fÂ 

d @

Â d fÂ  dx

@

Â d

fÂ 

d @

Â d fÂ  dx

(a) Paths

(b) Annotated term

Fig. 1 Substitution paths and director strings

A natural way to guide the substitutions to their correct destination is given in Figure 1(b) by director strings, which annotate each node in the graph with information about where the substitution must go (on both application nodes the ï¬rst arrow-like symbol or director corresponds to f and the second to x). When the substitution for f passes the root of this term, a copy of F is sent to both subterms, and the director is erased. The second substitution can then pass the root, where it is directed uniquely to the right branch by the director . Note that substitutions

Lambda-Calculus with Director Strings

5

are copied only when they need to be: if there is just one occurrence of a variable in a term, then no duplication is performed.
This simple idea works well when the substitution is closed (does not contain free variables). Otherwise, as each open substitution passes a given node we must add the additional directors for each free variable in the substitution.
We end this section by brieï¬‚y recalling the analogy between director strings and combinator reduction, as presented in [22]. The reduction rules for the S, B, C, K combinators are the following:
S x y z â†’ x z (y z) B x y z â†’ x (y z) Cxyz â†’ xzy Kxy â†’ x
where S x y takes an argument and directs it to both x and y; B x y takes an argument and directs it just to y; C x y takes an argument and directs it just to x; and ï¬nally K x takes an argument and discards it. Thus we can annotate the application x y with the combinators, which correspond exactly to the directors: S is , B is
, C is and K is âˆ’ (see below).

2.2 Syntax
We assume the reader is familiar with the Î»-calculus [5] and rewrite systems [15]. We recall that a reduction relation â†’ is terminating (or strongly normalising) if all reduction sequences are ï¬nite. It is locally conï¬‚uent if t â†’ u and t â†’ v implies that there exists w such that u â†’âˆ— w and v â†’âˆ— w; we say that u and v are joinable in this case1. If u and v are joinable with one-step reductions, the relation is strongly conï¬‚uent. It is conï¬‚uent if t â†’âˆ— u and t â†’âˆ— v implies that there exists w such that u â†’âˆ— w and v â†’âˆ— w.
We now introduce more formally the syntax of annotated terms. This syntax is common to the different rewrite systems that will be described later.
Deï¬nition 1 (Î»-calculus with Director Strings) We deï¬ne four syntactic categories:
Directors: We use ï¬ve special symbols, called directors, ranged over by Î±,Î³,Î´: 1. â€˜ â€™ indicates that the substitution should be propagated only to the right branch of a binary construct (application or substitution, as given below). 2. â€˜ â€™ indicates that the substitution should be propagated only to the left branch of a binary construct. 3. â€˜ â€™ indicates that the substitution should be propagated to both branches of a binary construct. 4. â€˜â†“â€™ indicates that the substitution should traverse a unary construct (abstraction and variables, see below).
1 â†’âˆ— denotes the reï¬‚exive and transitive closure of â†’.

6 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

5. â€˜âˆ’â€™ indicates that the substitution should be discarded (when the variable concerned does not occur in a term).
Strings: A director string is either empty, denoted by , or built from the above symbols (so is of the form Î±1Î±2 . . . Î±n where the Î±iâ€™s are directors). We use Greek letters such as Ï,Ïƒ. . . to range over strings. The length of a string Ïƒ is denoted by |Ïƒ|. If Î± is a director, then Î±n denotes a string of Î±â€™s of length n. If Ïƒ is a director string of length n and 1 â‰¤ i â‰¤ j â‰¤ n, Ïƒi denotes the ith director of Ïƒ and Ïƒ\i = Ïƒ1 . . . Ïƒiâˆ’1Ïƒi+1 . . . Ïƒn is Ïƒ where the ith director has been removed. Ïƒi..j = Ïƒi . . . Ïƒj is our notation for substrings. |Ïƒ|l denotes the number of and occurring in Ïƒ, |Ïƒ|r the number of and , and |Ïƒ|+ the number of directors that are not âˆ’.
Preterms: Let Ïƒ range over strings, k be a natural number and t, u range over preterms, which are deï¬ned by the following grammar:
t ::= Ïƒ | (Î»t)Ïƒ | (t u)Ïƒ | (t[k/u])Ïƒ
We denote preterms by t, or tÏƒ if we want to emphasise the director string Ïƒ. Terms: Well-formed terms are preterms that recursively satisfy the following con-
ditions, where U = (â†“ |âˆ’)âˆ— and B = ( | | |âˆ’)âˆ—:

Name

Term

Constraints

Variable

Ïƒ

Ïƒ âˆˆ U , |Ïƒ|+ = 1

Abstraction (Î»tÏ)Ïƒ

Ïƒ âˆˆ U , |Ï| = |Ïƒ|+ + 1

Application (tÏ uÎ½ )Ïƒ Ïƒ âˆˆ B, |Ï| = |Ïƒ|l, |Î½| = |Ïƒ|r

Substitution (tÏ[k/uÎ½])Ïƒ Ïƒ âˆˆ B, |Ï| = |Ïƒ|l + 1, |Î½| = |Ïƒ|r, 1 â‰¤ k â‰¤ |Ï|

Remark 1 For terms we use the same convention as for preterms, writing t or tÏƒ depending on whether or not we need to mention speciï¬cally the director string of an annotated term, as in the deï¬nition above. To sum up the naming conventions in this paper, boldface lower-case letters designate preterms in general (hence also well-formed terms), while normal lower-case letters designate preterms (or terms) with their root director string removed, hence explicitly needing a director string to form a preterm or term. Upper-case letters are reserved for terms of the usual Î»-calculus.

We have a variety of different term constructs:
â€“ represents variables (a place holder), â€“ (Î»t)Ïƒ is an abstraction, â€“ (t u)Ïƒ is an application, â€“ ï¬nally (t[k/u])Ïƒ is our notation for explicit substitution, meaning that the vari-
able corresponding to the kth director in tâ€™s string is to be replaced by u. We will often write (t[u])Ïƒ instead of (t[1/u])Ïƒ when the substitution binds the ï¬rst variable.
The name of the variable is of no interest since the director strings give the path that the substitution must follow through the term to ensure that it gets to the right place; all we need is a place holder.

Lambda-Calculus with Director Strings

7

In contrast with other explicit substitutions syntax, ours has explicit information about duplication ( ) and erasing (âˆ’). This is inspired by calculi for linear logic, and will allow us a ï¬ner control on substitutions: we may reduce a subterm more when encountering a , thus taking advantage of the mechanism of explicit substitution to share some reductions. There is an alternative presentation which combines the director â€˜âˆ’â€™ with abstraction, using the notation (Î»âˆ’t)Ïƒ to indicate that the bound variable does not occur in the term t. The resulting syntax is simpler, and it allows us to erase terms as soon as possible. However, it does not allow to deï¬ne Î²-reduction in full generality. We will discuss this choice again in Section 4.
As with most Î»-calculi, we will adopt several syntactic conventions: we will drop parentheses whenever we can, and omit the empty string unless it is essential.

2.3 Compilation and Readback

We use Î»-terms with director strings as an intermediate language. We thus need to provide a function to compile usual Î»-terms into this syntax and another to read them back. Notice that, as usual, we consider terms of the Î»-calculus modulo Î±-conversion (renaming of bound variables).
The following deï¬nition of a compilation from the usual Î»-calculus into director strings syntax indicates precisely how the strings and terms are built. We use an auxiliary ordered list [x1, . . . , xn] in the compilation function to keep track of the variable names corresponding to each director in the strings. Each step of the compilation function goes down one node in the syntax tree of the term, and computes the corresponding string using auxiliary functions Î¾ and Î¸. We denote by fv(M ) the set of free variables of the Î»-term M . We use the standard notations for the empty list and the cons and append operations ([ ], x :: and Â· respectively) and abbreviate x1 :: . . . :: xn :: [ ] to [x1, . . . , xn] or x. We denote i the ith element of a list .

Deï¬nition 2 (Compilation) Let M be a Î»-term with fv(M ) âŠ† {x1, . . . , xn}, its compilation M x is deï¬ned as follows:

xx = Ïƒ

where ([x], Ïƒ) = Î¾x(x)

Î»x.M x = (Î» M Â·[x])Ïƒ where ( , Ïƒ) = Î¾(Î»x.M)(x)

(M N ) x = ( M N )Ïƒ where ( , , Ïƒ) = Î¸M,N (x)

Î¾M ([ ]) = ([ ], )

Î¾M (x :: ) =

(x :: , â†“ Ïƒ) if x âˆˆ fv(M ) ( , âˆ’Ïƒ) if x âˆˆ fv(M )

where( , Ïƒ) = Î¾M ( )

Î¸M,N ([ ]) = ([ ], [ ], )

ï£± (x :: , , Ïƒ) if x âˆˆ fv(M ) \ fv(N ) ï£¼

ï£´ï£´

Î¸M,N (x ::

)

=

ï£´ ï£²

( (x

,x ::

:: ,

x

, ::

ï£´

Ïƒ)

if

x âˆˆ fv(N

)

\

fv(M )

ï£´ ï£½

where

,

Ïƒ) if x âˆˆ fv(M ) âˆ© fv(N ) ( , ï£´

, Ïƒ) = Î¸M,N ( )

ï£´ ï£³

(

,

, âˆ’Ïƒ)

if

x

âˆˆ

fv(M

)

âˆª

fv(N

)

ï£´ ï£¾

8 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

When there is no ambiguity, we use the notation M x with the implicit assumption that fv(M ) âŠ† x, and write M when the list is empty.

Remark 2 (Order of directors) In an abstraction (Î»tÏ)Ïƒ, the last director in the string Ï corresponds to the bound variable. This is reminiscent of CreÂ´gutâ€™s reversed de Bruijnâ€™s indexing [10].

Example 1 We show the compilation of some Î»-terms:

I = Î»x.x

= (Î» â†“)

K = Î»x.Î»y.x

= (Î»(Î» â†“âˆ’)â†“)

S = Î»x.Î»y.Î»z.(xz)(yz) = (Î»(Î»(Î»(( â†“ â†“) ( â†“ â†“)

2 = Î»f.Î»x.f (f x)

= (Î»(Î»( â†“( â†“ â†“) ) )â†“)

)

)â†“â†“)â†“)

In order to show that the result of the compilation is a well-formed term, we need two auxiliary lemmas.

Lemma 1 If (M N ) is an application with fv(M N ) âŠ† {x1, . . . , xn}, then Î¸M,N ([x1, . . . , xn]) = ( 1, 2, Ïƒ) where 1 = fv(M ), 2 = fv(N ), and |Ïƒ| = n.

Proof Straightforward induction on n.

Lemma 2 (Length of Strings) Let M be a Î»-term and fv(M ) âŠ† {x1, . . . , xn}, then:
M x = uÏƒ where |Ïƒ| = n
In particular, if M is closed then M has an empty director string ( ).

Proof By induction on n.
For n = 0, M is either an abstraction, in which case the compiled term has director string â†“0 = as required, or an application and (M N ) = ( M N ) since Î¸M,N ([ ]) = ([ ], [ ], ) by deï¬nition.
For n > 0 we distinguish cases according to M . The cases of variable and abstraction are trivial. The interesting case is application. In this case, the property is a direct consequence of Lemma 1.

Proposition 1 (Consistency of Compilation) If M is a Î»-term with fv(M ) âŠ† x then M x is a well-formed term.

Proof By induction on the structure of Î»-terms. If M is a variable then the result holds trivially. If M is an abstraction, then the result holds by induction. If M is an application, it is a consequence of Lemma 1, the induction hypothesis, and the construction of Ïƒ in the deï¬nition of Î¸.

Since we prefer to think of this calculus as some form of intermediate language, we also provide a notion of readback, which simply puts names back in.

Lambda-Calculus with Director Strings

9

Deï¬nition 3 (Readback) Let t = tÏƒ be a term where |Ïƒ| = n, and let x1, . . . , xn be n fresh variables. We deï¬ne the readback of t as t [x1,...,xn], where the read-
back function (which uses an auxiliary list M of Î»-terms) is deï¬ned as follows:

Ïƒ M

=M

(Î»t)Ïƒ M

= Î»x. t ÎºÏƒ(MÂ·[x])

(t u)Ïƒ M = t u

(t[k/u])Ïƒ M = t [ 1,..., kâˆ’1, u

where [M ] = ÎºÏƒ(M ) where x is fresh
where ( , ) = Î³Ïƒ(M ) , k,..., m] where ( , ) = Î³Ïƒ(M )

Îº ([ ])

=[]

Îºâ†“Ïƒ(M :: ) = M :: ÎºÏƒ( ) Îºâˆ’Ïƒ(M :: ) = ÎºÏƒ( )

Î³ ([ ])

= ([ ], [ ])

Î³ Ïƒ(M :: ) = (M :: , ) Î³ Ïƒ(M :: ) = ( , M :: ) Î³ Ïƒ(M :: ) = (M :: , M ::

Î³âˆ’Ïƒ(M :: ) = ( , )

ï£¼

ï£´ ï£´ ï£´ ï£´ ï£½
where ( ,

)

ï£´ ï£´

ï£´

ï£´

ï£¾

) = Î³Ïƒ( )

Notice that in the case of a variable Ïƒ we must have |Ïƒ|+ = 1 since the term is
well-formed, hence ÎºÏƒ(M ) is a singleton. Also note that the auxiliary list M may contain arbitrary Î»-terms, and not necessarily only variables as in the compilation. This makes it possible to complete substitutions in a thorough and elegant way: we only put them in the list, and the terms will be guided to the right places, thanks to the directors. More precisely:

t [x1,...,xn]{xi â†’ Mi} = t [x1,...,xiâˆ’1,Mi,xi+1,...,xn]
where M {x â†’ N } is our notation for implicit substitution (the meta-operation of substitution in the Î»-calculus).

Example 2 We give two small examples of the readback procedure:

(Î» â†“)

= Î»x. â†“ [x] = Î»x.x

(Î»(Î»( â†“( â†“ â†“) ) )â†“) = Î»x.Î»y. ( â†“( â†“ â†“) )

= Î»x.Î»y.x (x y)

[x,y]

To prove that the readback function is well deï¬ned we use the following lemma:

Lemma 3 If |Ïƒ| = n and M1, . . . , Mn are Î»-terms then â€“ length(ÎºÏƒ([M1, . . . , Mn])) = |Ïƒ|+ if Ïƒ âˆˆ U ; â€“ Î³Ïƒ([M1, . . . , Mn]) = ( , ) where length( ) = |Ïƒ|l and length( ) = |Ïƒ|r.
Proof Straightforward induction on n.

Lemma 4 During the readback of a well-formed term, the readback procedure is only called under the form tÏ [M1,...,Mn] with |Ï| = n.

10 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Proof By induction and Lemma 3. Each step of the procedure adjusts the length of the auxiliary list according to the constraints on well-formed terms (see Deï¬nitions 1 and 3).

Proposition 2 (Consistency of Readback) If tÏƒ is a well-formed term with |Ïƒ| = n and x1, . . . , xn are n fresh variables then tÏƒ [x1,...,xn] is a Î»-term.
Proof We prove the more general property: If tÏƒ is a well-formed term, |Ïƒ| = n and M1, . . . , Mn are Î»-terms, then
tÏƒ [M1,...,Mn] is well-formed. This is proved by induction on t, using Lemma 4. The case of a variable Ïƒ
is trivial since |Ïƒ|+ = 1 by well-formedness. For an abstraction, application or substitution the result follows directly by induction and Lemma 4.

The compilation and the readback function are inverses modulo Î±-conversion. To prove it we use the following auxiliary lemma:

Lemma 5 â€“ Î¾M ([x1, . . . , xn]) = ( , Ïƒ) implies ÎºÏƒ([x1, . . . , xn]) = ; â€“ Î¸M,N ([x1, . . . , xn]) = ( 1, 2, Ïƒ) implies Î³Ïƒ([x1, . . . , xn]) = ( 1, 2).
Proof Straightforward induction on n.

Proposition 3 (Inverses) If M is a Î»-term with fv(M ) âŠ† x, then M x x =Î± M. In particular, if M is a closed Î»-term, M =Î± M .

Proof By induction on M .

â€“ Variable:
x x x = x as required. â€“ Abstraction:
(Î»x.M ) x x = (Î» M Â·[x])Ïƒ x = Î»y. M Â·[x] Â·[y], using Lemma 5, where ( , Ïƒ) = Î¾Î»x.M (x). By induction, this is Î±-equivalent to Î»x.M . â€“ Application:
M N x x = ( M N )Ïƒ x, where ( , , Ïƒ) = Î¸M,N (x). By Lemma 5 and the deï¬nition of readback:

( M N )Ïƒ x = ( M

N ).

The result then follows directly by induction.

3 The Open Calculus
We will now give the reduction rules that will allow us to fully simulate the Î»calculus. This calculus is called open (Î»o) in contrast with the calculus for closed reduction (Î»c) deï¬ned in Section 4.3, which is simpler but does not fully simulate Î²-reduction.

Lambda-Calculus with Director Strings

11

3.1 The Beta Rule
We need a Beta rule to eliminate Î²-redexes and introduce an explicit substitution instead. In a compiled term (Î»tÎ½)Ï the variable bound by the abstraction is determined by the last director in Î½ (see Remark 2), and |Î½| = |Ï|+ + 1 according to the constraints in Deï¬nition 1. Since Ï may contain erasing directors (âˆ’) which we have to preserve, we move them up to the director string of the substitution:

Beta ((Î»t)Ï u)Ïƒ o (t[|Ï|+ +1 / u])Ï„

where Ï„ = Ïˆb(Ïƒ, Ï)

with:

Ïˆb( , ) =
Ïˆb( Ïƒ, Ï) = Ïˆb(Ïƒ, Ï) Ïˆb( Ïƒ, â†“ Ï) = Ïˆb(Ïƒ, Ï) Ïˆb( Ïƒ, â†“ Ï) = Ïˆb(Ïƒ, Ï) Ïˆb(âˆ’ Ïƒ, Ï) = âˆ’ Ïˆb(Ïƒ, Ï) Ïˆb( Ïƒ, âˆ’ Ï) = âˆ’ Ïˆb(Ïƒ, Ï) Ïˆb( Ïƒ, âˆ’ Ï) = Ïˆb(Ïƒ, Ï)

Remark 3 If the function is closed (i.e. has an empty director string), then the substitution binds the ï¬rst (and only) variable of t, and we simply have:

((Î»t) u)Ïƒ o (t[u])Ïƒ.

Based on this idea we will deï¬ne a simpliï¬ed system for the evaluation of closed terms in Section 4.

3.2 Propagation Rules

We need rules to propagate the substitutions created by the Beta rule deï¬ned above in Section 3.1. The directors indicate the path that the substitution should follow: we will need a rule per term construct and possible director.
To understand how the rules for the propagation of substitutions are deï¬ned, consider a simple case: an application (t u) Ï with a substitution concerning the ï¬rst variable, i.e. we have ((t u) Ï[v])Ïƒ. The substitution should be propagated to the left branch of the application node as the director indicates. Therefore we need a rule of the form:

(App1)

((t u) Ï[v])Ïƒ ((t[v])Ï u)Ïƒ

Letâ€™s try to ï¬nd Ï and Ïƒ . Suppose that a (closed) substitution is applied to the left and right hand-sides of the above equation. If, for example, Ïƒ = Ï = , then the substitution is for t on the left, so we must have Ïƒ = Ï = to ensure that the substitution is also guided towards t on the right. If Ïƒ = and Ï = , then it is to be directed towards u, and we must have Ïƒ = and Ï is not concerned (say Ï = here). Finally, if Ïƒ = , the substitution is for v, and we write Ïƒ = and Ï=.

12 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Name

Reduction

Cond.

Var ( Ï[i/vÎ½ ])Ïƒ

o vÏ„

where Ï„ = Ï€ (Ïƒ, Î½) Ïi =â†“

App1 ((t u)Ï[i/v])Ïƒ

o ((t[j/v])Ï… u)Ï„

Ïi =

where Ï… = Ï†l(Ïƒ, Ï\i), Ï„ = Ïˆ1(Ïƒ, Ï\i), j = |Ï1..i|l

App2 ((t u)Ï[i/v])Ïƒ

o (t (u[k/v])Ï‰)Ï„

Ïi =

where Ï‰ = Ï†r(Ïƒ, Ï\i), Ï„ = Ïˆ2(Ïƒ, Ï\i), k = |Ï1..i|r

App3 ((t u)Ï[i/v])Ïƒ

o ((t[j/v])Ï… (u[k/v])Ï‰)Ï„

Ïi =

where Ï… = Ï†l(Ïƒ, Ï\i), Ï‰ = Ï†r(Ïƒ, Ï\i), Ï„ = Ïˆ3(Ïƒ, Ï\i), j = |Ï1..i|l, k = |Ï1..i|r

Lam ((Î»t)Ï[i/v])Ïƒ

o `Î»(t[i/v])Ï…Â· Â´Ï„

Ïi =â†“

Comp ((t[j/u])Ï[i/v])Ïƒ

where Ï… = Ï†d(Ïƒ, Ï\i), Ï„ = Ïˆd(Ïƒ, Ï\i)

o (t[j/(u[k/v])Ï‰])Ï„

Ïi =

Erase (tÏ[i/v])Ïƒ

where Ï‰ = Ï†r(Ïƒ, Ï\i), Ï„ = Ïˆ2(Ïƒ, Ï\i), k = |Ï1..i|r

o tÏ„

where Ï„ = Ï€(Ïƒ, Ï\i) Ïi = âˆ’

Fig. 2 Propagation Rules: P
Ïƒ1 Ï1 Ï†l Ï†r Ïˆ1 Ïˆ2 Ïˆ3 Ï€ âˆ’

âˆ’ âˆ’ âˆ’

âˆ’ âˆ’ âˆ’âˆ’ âˆ’ âˆ’ âˆ’âˆ’
âˆ’

Fig. 3 Functions used in the Propagation Rules

Ïƒ1 Ï1 Ïˆd Ï†d Ïˆb â†“
â†“â†“ â†“â†“ âˆ’ âˆ’âˆ’ âˆ’âˆ’ âˆ’ âˆ’â†“

We obtain most of the propagation rules in the same way. Notice that not every

combination of directors is to be considered, as some of them do not correspond to well-formed terms. Figure 2 shows the set P of propagation rules.

The various functions Ï† and Ïˆ used in the propagation rules just compute the

ad hoc director strings. They are generated recursively in the same way as above

from the tables in Figure 3.

For example:

Ï†l( , Ï†l( Ïƒ, Ï†l( Ïƒ, Ï†l( Ïƒ,

)= Ï) = Ï) = Ï) =

Ï†l(Ïƒ, Ï) Ï†l(Ïƒ, Ï) Ï†l(Ïƒ, Ï), etc.

The function Ï€ used only in the Erase rule adds a new director âˆ’ for every free

variable of an erased term, and can be paraphrased from Figure 3 in the following

way (Î± is any director):

Lambda-Calculus with Director Strings

13

Ï€( , ) = Ï€( Ïƒ, Ï) = âˆ’ Ï€(Ïƒ, Ï) Ï€( Ïƒ, Î± Ï) = Î± Ï€(Ïƒ, Ï) Ï€( Ïƒ, Î± Ï) = Î± Ï€(Ïƒ, Ï) Ï€(âˆ’ Ïƒ, Ï) = âˆ’ Ï€(Ïƒ, Ï)
The function Ï€ used for the Var rule performs a similar job (Ï€ is not in Figure 3 because it does not follow exactly the same pattern):
Ï€( , )= Ï€ ( Ïƒ, Î± Î½) = Î± Ï€ (Ïƒ, Î½) Ï€ ( Ïƒ, Î½) = âˆ’ Ï€ (Ïƒ, Î½) Ï€ ( Ïƒ, Î± Î½) = Î± Ï€ (Ïƒ, Î½) Ï€ (âˆ’ Ïƒ, Î½) = âˆ’ Ï€ (Ïƒ, Î½)
These rules deserve some explanations:
â€“ The Var rule is the simplest. When the substitution reaches such a place holder with a corresponding director â†“, we know that it is indeed the right variable (because the substitution has been guided there earlier and is not erased). We know that Ï\i = âˆ’n if the term is well-formed so that both â€˜âˆ’â€™ and in Ïƒ should result in a â€˜âˆ’â€™ in Ï„ to ensure that the erased variables are preserved. Moreover we do not need to inspect Ï in the computation of Ï„ .
â€“ The rules for application are the main rules here. Depending on Ïi, the substitution is guided to the left or right, or copied in App3 only when there is more than one occurrence of the given variable. The new director strings are computed by ad hoc functions from Ïƒ and Ï (omitting the ith director of the last one).
â€“ Surprisingly, the rule that allows an open substitution to pass through an abstraction (Lam) is simple. This is quite remarkable, as this is especially difï¬cult in usual calculi. For example, it requires Î±-conversion in a calculus with names.
â€“ The Comp rule is the counterpart of App2 in terms of substitution instead of application. We could have written composition rules for substitutions similar to App1 and App3, but the substitutions would then be allowed to overtake (i.e. their order would not be preserved), which means that the system would trivially fail to preserve strong normalisation.
â€“ The Erase rule applies to a substitution in any term construct, provided the director corresponding to the substitution is â€˜âˆ’â€™. Then the substitution is simply discarded and new â€˜âˆ’â€™ directors are added to take into account possible free variables of the discarded term.
We have a small number of propagation rules in comparison with standard explicit substitution calculi. However, our rules require non-trivial syntactical computations on director strings when we consider arbitrary substitutions. The system can be drastically simpliï¬ed if we impose some restrictions on the substitutions, as we will see in the next section. Note that the condition on Ïi in the rules has been externalised only to improve readability and is of course a simple pattern-matching.

14 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot
Example 3 We show a reduction sequence using this calculus. Consider the Î»-term Î»x.(Î»y.y)x which contains a single redex:
Î»x.(Î»y.y)x = (Î»((Î» â†“) â†“) ) o (Î»( â†“[ â†“]) ) o (Î» â†“) = Î»x.x
Note that an encoding into combinators, using director strings as presented in [22], would not allow this redex to be contracted, and thus if used as an argument could potentially be duplicated. In this sense, our calculus offers a generalisation of the director strings of [22].

3.3 Properties

Lemma 6 (Preservation of Well-Formedness) If tÏƒ is a well-formed term and tÏƒ o uÏ„ , then uÏ„ is a well-formed term, moreover |Ïƒ| = |Ï„ |.
Proof We have to prove that each rule produces a well-formed term with a director string of the same size. The proof is laborious but not difï¬cult, and we omit the details.

The process of propagating a substitution to the corresponding leaves in the tree associated to the term, using the rules in P, is terminating: each rule either erases the substitution or moves it down towards the leaves. Formally, we prove the termination of P by using an interpretation function. This function computes the lengths of the paths to be traversed by a substitution to reach the corresponding leaves. We call it the distance of a substitution.

Deï¬nition 4 (Distance) The distance associated to the substitution [i/v] in the term (t[i/v])Ï is computed by the function |t[i/v]| deï¬ned by induction on t as
follows:

| Ïƒ[i/v]| = 1

|(Î»t)Ïƒ[i/v]| = 1

(if Ïƒi = âˆ’)

|(Î»t)Ïƒ[i/v]| = 1 + |t[i/v]|

(if Ïƒi =â†“)

|(t u)Ïƒ[i/v]| = |(t[j/u])Ïƒ[i/v]| = 1 + |t[k/v]|

(if Ïƒi = ,

k computed as in Fig. 2)

|(t u)Ïƒ[i/v]| = |(t[j/u])Ïƒ[i/v]| = 1 + |u[k/v]|

(if Ïƒi = ,

k computed as in Fig. 2)

|(t u)Ïƒ[i/v]| = |(t[j/u])Ïƒ[i/v]| = 1 + |t[k/v]| + |u[k /v]| (if Ïƒi = ,

k, k computed as in Fig. 2)

|(t u)Ïƒ[i/v]| = |(t[j/u])Ïƒ[i/v]| = 1

(if Ïƒi = âˆ’)

Proposition 4 (Termination of Propagation) The set P of propagation rules is terminating.

Proof We deï¬ne an interpretation that associates to each term t a multiset with an element |u[i/v]| for each subterm (u[i/v])Ï occurring in t. Each application of a propagation rule decreases the interpretation of the term: rules Var and Erase erase one element of the multiset, and in the other rules one element is replaced by one or two strictly smaller elements.

Lambda-Calculus with Director Strings

15

It is easy to see that using the propagation rules we eventually reach a pure term (i.e. a term without substitutions):
Proposition 5 (Completion of Substitutions) Every term has a P-normal form which is a pure term.
Proof We prove by contradiction that any term of the form (u[i/v])Ï can be reduced by a rule in P. Assume there is a counterexample, and take one of minimal size: (t[i/w])Ïƒ. Consider all the cases for t. If t is a variable, application or abstraction obviously we can apply a rule from P. If it is a substitution (t [j/u ])Ï…, then it is reducible (by the minimality of the counterexample).
Lemma 7 (Local Conï¬‚uence of Propagation) The set P of propagation rules is locally conï¬‚uent.
Proof P has seven critical pairs, all with Comp. In the version of the calculus without the director for erasing (Section 4), they all easily converge. But here, the pairs reduce to terms where the erasing may occur at two different levels, and we need to go one step further with the corresponding rule to ï¬‚atten these two strings to one, and obtain syntactically equal terms. Of course, this makes the proof rather tiresome, due to the deï¬nition by cases of the functions involved in the rules, so we only give the details for the Var/Comp pair, as the other critical pairs can be dealt with in a similar way.

(( Ï‡[j/uÎ½ ])Ï[i/v])Ïƒ
Var
? (uÎ³ [i/v])Ïƒ

Comp - ( Ï‡[j/(uÎ½ [k/v])Ï‰])Ï„
Var
? (uÎ½ [k/v])Âµ

?

with the conditions Ï‡j =â†“, Ïi = and the intermediate results: k = |Ï1..i|r, Î³ = Ï€ (Ï, Î½), Ï„ = Ïˆ2(Ïƒ, Ï\i), Ï‰ = Ï†r(Ïƒ, Ï\i), Âµ = Ï€ (Ï„, Ï‰).
The two terms may be syntactically different, the only difference being that

directors â€˜âˆ’â€™ may have been added in Î³ in the left term and in Âµ in the right term,

so at different levels. Fortunately, these terms are not in P-normal form (thanks

to Proposition 5), and we know that another P-rule may be applied at the root

of both terms. (There is one case where we canâ€™t, but then we have uÎ½

âˆ— o

uÎ½

where we can reduce at the root and uÎ³ = uÏ€ (Ï,Î½)

âˆ— o

u

Ï€

(Ï,Î½

),

and

the

situation

is similar). It is indeed the same rule for both terms, because the unlabelled term

to the left of the substitution is the same and Î³i = Î½k, which comes from the

following observations:

(i) Ï€ (Ï, Î½) is well-deï¬ned if and only if |Î½| = |Ï|r; (ii) if |Î½| = |Ï|r, 1 â‰¤ i â‰¤ |Ï| and k = |Ï1..i|r, Ï€ (Ï1..i, Î½1..k) is a preï¬x of Ï€ (Ï, Î½); (iii) if |Î½| = |Ï|r, |Ï€ (Ï, Î½)| = |Ï|;

16 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

(iv) if |Î½| = |Ï|r, 1 â‰¤ i â‰¤ |Ï| and k = |Ï1..i|r, (Ï€ (Ï, Î½))1..i = Ï€ (Ï1..i, Î½1..k).

(i), (ii), (iii) are clear from the deï¬nition of Ï€ and (iv) is a direct consequence

of (ii) and (iii). As a corollary, we have indeed from (iv), back to our case, that

Î³i

=

Î½k

and

(Ï€

(Ï,

Î½))
\i

=

Ï€

(Ï\i,

Î½\k ).

(âˆ—)

Now, to prove that the terms are equal after this (unknown) reduction, we just

need to show that f (Ïƒ, Î³\i) = f (Âµ, Î½\k) for f âˆˆ {Ï†l, Ï†r, Ï†d, Ïˆ1, Ïˆ2, Ïˆ3, Ïˆd, Ï€, Ï€ } and |Î³1..i|r = |Î½1..k|r, |Î³1..i|l = |Î½1..k|l.

This job is of course awfully tiresome and we will omit most of it. We will

however deal completely with one case, say Ïˆ1, which means that our unknown

rule was in fact App1, and we want to compare the outermost director strings of
both terms. We thus have to compare Ïˆ1(Ïƒ, Î³\i) and Ïˆ1(Âµ, Î½\k) for every valid case, i.e. for every case that corresponds to initially well-formed terms:

Ïƒ Ï\i Î½\k Î³\i Ïˆ1(Ïƒ, Î³\i) Ï„ Ï‰ Âµ Ïˆ1(Âµ, Î½\k)

âˆ’âˆ’ âˆ’

âˆ’

âˆ’âˆ’

âˆ’âˆ’

âˆ’ âˆ’âˆ’

âˆ’âˆ’

âˆ’ âˆ’ âˆ’âˆ’ âˆ’

Note that we compute Î³\i from Ï\i and Î½\k thanks to (âˆ—). The table above shows that if Ïƒ, Ï, Î½ are as deï¬ned in a well-formed term,
Ïˆ1(Ïƒ, Î³\i) = Ïˆ1(Âµ, Î½\k) (cases with have been omitted but are similar), hence, after the application of an App1 rule to both terms of the critical pair, their outermost director strings are the same. A similar proof with Ï†l would allow to conclude that these terms are indeed the same, which concludes this case. All the other cases
are similar.

Local conï¬‚uence and termination imply conï¬‚uence, using Newmanâ€™s Lemma [32].

Corollary 1 (Conï¬‚uence of Propagation) The set P of propagation rules is conï¬‚uent on Î»o-terms.

As a consequence of conï¬‚uence (Corollary 1) and termination (Proposition 4), the set P of propagation rules deï¬nes a function from a term t to its unique normal form, denoted P(t). The following lemma shows that P implements the metaoperation of substitution in the Î»-calculus: to compute M {x â†’ N } we compile M , compile N , create an explicit substitution, and normalise it with P.

Lemma 8 (Substitution) Let M and N be Î»-terms. Let M x = tÏ, N y = u. Let Ï = Ïâˆ’ and i = |Ï| + 1 if x âˆˆ x, otherwise Ï = Ï and i is the position of x in x. Then P((tÏ [i/u])Ïƒ) z = M {x â†’ N } (where x, y, z, Ïƒ are adequately
chosen).

Lambda-Calculus with Director Strings

17

Proof By induction on M . If M is a variable we distinguish two cases: if M = y = x we obtain the required result using the rule Erase, otherwise we use Var. The case of an application follows directly by induction using rules App1, App2, App3 or Erase. For an abstraction we use the induction hypothesis and rules Lam or Erase.
Reduction in Î»o is sound with respect to the Î»-calculus, in the following sense.
Proposition 6 (Soundness of Î»o) If t o u then t x â†’âˆ—Î² u x. In particular:
1. if t P u then t x = u x, and 2. if t is a P-normal form then t x â†’Î² u x.
Proof The result is trivial if the reduction step uses a rule in P, since the readback function performs the substitution. Note that
t =[x1,...,xiâˆ’1,Mi,xi+1,...,xn] t [x1,...,xn]{xi â†’ Mi}.
Assume we use the Beta rule. We proceed by induction on t. The only interesting case is when the reduction takes place at the root of t. In this case t = ((Î»w)Ï v)Ïƒ and u = (w[|Ï|+ + 1/v])Ï„ . By deï¬nition of readback, t x = (Î»x. w y) v z, where x, y, z are the corresponding lists of free variables. There is therefore a Î²reduction step: t x â†’Î² w y{x â†’ v z} = u x by deï¬nition of readback. Note that in the general case this Beta-redex might occur in a substitution and it might be copied or erased by the readback function, thus t x â†’âˆ—Î² u x. In particular, if t is a P-normal form then t x â†’Î² u x since t does not contain substitutions.

We also have a Simulation Theorem which shows the completeness of Î»o with respect to Î²-reduction.

Theorem 1 (Simulation) Let M be a Î»-term with fv(M ) âŠ† x. If M â†’Î² N , then

there exists u such that M x

âˆ— o

u

and

u x = N.

Â·

M.........?

â†’Î² -

N..........6Â·

t -u

âˆ—

o

Proof Since the compilation function (Deï¬nition 2) transforms applications into applications and abstractions into abstractions, it is clear that if M has a Î²-redex then M x has a Beta-redex. We proceed by induction on M . The only interesting case is when the Î²-reduction takes place at the root of M . In this case, M = (Î»x.M ) N and N = M {x â†’ N }. Using the compilation function, we get:

â€“ Î»x.M z = (Î»r)Î½ where r = M z x â€“ N y=w
â€“ M x Beta (r[i/w])Ïƒ

18 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Using the Substitution Lemma (Lemma 8) we obtain the required result:

(r[i/w])Ïƒ

âˆ— o

u

with

u x = N.

The reduction steps in the latter reduction are all in P.

Remark 4 In general, we do not have u = N x (which would be stronger). For

instance, Î»xy.((Î»z.y) x) = (Î»(Î»((Î» â†“âˆ’)â†“ â†“) )â†“)

âˆ— o

(Î»(Î»

âˆ’â†“)â†“) , and

(Î»(Î» âˆ’â†“)â†“) = Î»xy.y, while Î»xy.y = (Î»(Î» â†“)âˆ’) . Intuitively, there are

two different levels where we may say that x should be erased. The compilation

function makes the canonical choice to put this information at the highest possible

level, i.e. right below the corresponding Î», but it may also happen that a variable

disappears â€œat runtimeâ€, as in the example above. Then we could of course add a

rule to make the erasing director go up until it reaches its canonical position, so that

we would indeed have u = N x. But this would be useless, and this is exactly

what the simulation theorem says: we may have different internal representations

corresponding to the same term in the course of reduction, but we do not care as

long as we can still correctly read back a Î»-term at the end of the reduction. Hence

although seemingly weaker than one could expect, Theorem 1 is exactly the right

property for an intermediate language that cleanly separates the readback from the

reduction rules.

In order to prove conï¬‚uence of Î»o, we need some extra lemmas. Letâ€™s deï¬ne

t â†’B u if t, u are in P-normal form and t

Beta

âˆ— P

u.

Lemma 9 t o u â‡’ P(t) â†’âˆ—B P(u).

Proof Obvious if t P u. If t Beta u, the interesting case is when reduction is at the root. Then ((Î»t)Ï v)Ïƒ Beta (t[|Ï|+ +1/v])Ï„ and ((Î»P(t))Ï P(v))Ïƒ â†’B P((P(t)[|Ï|+ +1/P(v)])Ï„ ) = P(u).

Lemma 10 If t â†’B u then t x â†’Î² u x (one step).

Proof Direct consequence of Proposition 6.

According to the relation â†’B on P-normal forms of Î»o-terms, we may deï¬ne the notions of residuals and developments as in the Î»-calculus (see [5]). Then clearly by Lemma 10, to a residual of t according to â†’B corresponds a residual of t x using â†’Î². We hence have a Finite Developments Theorem for â†’B:
Lemma 11 (FD for â†’B) If t is a P-normal form, all developments of t according to the reduction â†’B are ï¬nite.
Proof Assume an inï¬nite development of (t, F) where t is a Î»o-term in P-normal form and F a set of â†’B-redex occurrences of t:
t â†’B t1 â†’B Â· Â· Â· â†’B tn â†’B Â· Â· Â·

Then we have by Lemma 10:

t x â†’Î² t1 x â†’Î² Â· Â· Â· â†’Î² tn x â†’Î² Â· Â· Â·
where every step is a residual of a redex corresponding to F. Since the Î»-calculus satisï¬es the Finite Developments Theorem [5], this is impossible.

Lambda-Calculus with Director Strings

19

A complete development of (t, F) is a development of (t, F) such that there is no residual of F after it.

Lemma 12 All complete developments of (t, F) with â†’B end with the same term.

Proof Assume that t â†’B u and t â†’B v are the ï¬rst steps in two complete de-

velopments of (t, F), obtained by reducing redexes f1 and f2 respectively. Then,

by deï¬nition of â†’B, u and v are P-normal forms, and there are terms u and v

such that t Beta u

âˆ— P

u

â‰¡

P (u

)

and

t

Beta v

âˆ— P

v

â‰¡

P (v

).

If the Beta-redex reduced is the same in both cases, then obviously u â‰¡ v be-

cause P is conï¬‚uent. Letâ€™s assume that f1, f2 âˆˆ F are different Beta-redexes.

Then u and v can be joined using Lemma 9, as shown in the following diagram,

where

fi /fj Beta

denotes

the

Beta -reduction

of

the

residuals

of

fi

relative

to

fj

(see [5]):

f1

t

Beta
-

u

âˆ—

P
-

u

f2 Beta
? v
âˆ— P
v?

f2 /f1 Beta
- s?
f1 /f2 Beta

â†’âˆ—B

âˆ—P -

? - P(s) â†’âˆ—B

Note that the rule Beta is left-linear and only superposes trivially with itself, which guarantees the existence of the terms s and P(s) obtained by developing (t, {f1, f2}).
Lemma 11 and Newmanâ€™s Lemma [32] complete the proof.

Lemma 13 â†’B is conï¬‚uent.

Proof Deï¬ne t B u if u is a complete development of (t, F ) according to â†’B for some F. Then:

â€“ â†’âˆ—B=

âˆ— B

â€“ B is strongly conï¬‚uent: if t B t1 (with F1) and t B t2 (with F2),

then let t3 be the complete development of t with F1 âˆª F2. t B t1 results

from a partial development of (t, F1 âˆª F2), hence by completely developing

the residuals of F1 âˆª F2 in t1, one obtains by Lemma 12 t1 B t3. Similarly

t2 B t3.

Theorem 2 (Conï¬‚uence) The calculus Î»o is conï¬‚uent.

20 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Proof The situation is the following: t

âˆ—

o
-

u

âˆ—âˆ— oo

v? - w?
âˆ— o

We use the interpretation method. By Lemma 9, P(t) â†’âˆ—B P(u) and P(t) â†’âˆ—B

P(v). By Lemma 13, there exists w such that P(u) â†’âˆ—B w and P(v) â†’âˆ—B w.

And since â†’âˆ—BâŠ‚

âˆ— o

,

the

derivations

u

âˆ— o

P (u)

âˆ— o

w

and

v

âˆ— o

P (v)

âˆ— o

w

conclude the proof.

The conï¬‚uence property above is sometimes called ground conï¬‚uence in the terminology of explicit substitution calculi, since it applies to terms in Î»o rather than terms with metavariables of the rewrite system. Another property that has been extensively studied for explicit substitution calculi is the preservation of strong normalisation (PSN): a calculus of explicit substitutions preserves strong normalisation if the compilation of a strongly normalisable Î»-term is strongly normalisable (see [29] for a counterexample for Î»Ïƒ, and [27, 12] for calculi satisfying PSN). Thanks to our limited form of composition (the rule Comp only allows to move a substitution inside another where the substituted variable does occur) Î»o preserves strong normalisation. Our proof of PSN is inspired from that of Î»Ï… [27]. We ï¬rst deï¬ne a notion of minimal inï¬nite derivation and external reduction. Intuitively, a derivation is minimal if we always reduce a lowest possible redex to keep non termination, and a reduction step is external if it does not take place inside a substitution. We denote by Beta,p a Beta reduction at position p, and by o,p an arbitrary Î»o reduction at position p.

Deï¬nition 5 (Minimal Derivation) An inï¬nite Î»o sequence of reductions

t1

Beta,p1 t1

âˆ— P

Â· Â· Â· ti

Beta,pi ti

âˆ— P

...

is minimal if for any other inï¬nite derivation

t1

Beta,p1 t1

âˆ— P

Â· Â· Â· ti

Beta,q u

âˆ— P

Â·Â·Â·

we have q = pip for every p .

In other words, in any other inï¬nite derivation, pi and q are disjoint or q is above pi, which means that the Beta-redex we reduce is a lowest one.

Deï¬nition 6 (External Reduction) The set Ext(t) of external positions in the term t is deï¬ned as follows, where Î› denotes the root position and x X denotes {x Â· y | y âˆˆ X} if X is a set of positions:

Ext( Ïƒ)

= {Î›}

Ext((Î»t)Ïƒ) = 1Ext(t) âˆª {Î›}

Ext((t u)Ïƒ) = 1Ext(t) âˆª 2Ext(u) âˆª {Î›}

Ext((t[k/u])Ïƒ) = 1Ext(t) âˆª {Î›}

Lambda-Calculus with Director Strings

21

A reduction step t o,p u is external if p âˆˆ Ext(t), otherwise it is internal. We

write t

ext o

u (resp. t

int o

u) to emphasise that a rewrite is external (resp.

internal).

Lemma 14 If t Beta,p u is an external reduction step then t x â†’+Î² u x. In particular t x = u x if t x is strongly normalisable.

Proof The term t contains a Beta redex, and since it is external it is obviously not erased in the readback (see Proposition 6). Hence t x â†’+Î² u x.
Lemma 15 If there is an inï¬nite derivation

t o t1 o t2 o t3 o t4 Â· Â· Â·

in which all the Beta steps are internal, then there exists another inï¬nite derivation

t

âˆ— P

u1

o u2

o u3

o u4 Â· Â· Â·

in which the rewrites from t to u1 are the only external ones (i.e. all steps are internal from u1). Moreover, the transformation preserves minimality.

Proof It is sufï¬cient to prove that if u

int o

v

ext o

w

in a

minimal

derivation

then u

ext o

v

int âˆ— o

w

preserving

minimality.

The

proof

is

by

induction

on

the structure of u. If the external rewrite does not take place at the root position

then the result follows by induction. If the external rewrite takes place at the root

of u, then the rule applied is in P by assumption, and therefore u is a term of the

form (t[i/v])Ïƒ. We distinguish cases according to the rule applied. If the rule is

Var, App1, App2, Lam, or Comp the rewrite steps commute. If the rule is App3 the

internal rewrite is replaced by two internal rewrites. If the rule is Erase the internal

rewrite is not needed.

The termination of P (Proposition 4) ensures that there are a ï¬nite number of

external steps t

âˆ— P

u1,

completing

the

proof.

We will prove a result which is slightly more general than preservation of strong normalisation and will also be used to prove the termination of typeable terms in Section 5.

Proposition 7 If t x is a strongly normalisable Î»-term then t is strongly normalisable in Î»o.

Proof For a contradiction, assume that there is an inï¬nite reduction sequence starting from t. We show that there is an inï¬nite derivation for t x. For this we consider an inï¬nite minimal reduction sequence, which contains an inï¬nite number of applications of Beta since the propagation rules are terminating:

t

âˆ— P

t1

Beta t2

âˆ— P

t3

Beta t4 Â· Â· Â·

By Proposition 6:

t x = t1 x â†’âˆ—Î² t2 x = t3 x â†’âˆ—Î² t4 x Â· Â· Â·

22 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot
and since t x is strongly normalisable by assumption, there exists k such that for all i â‰¥ k, ti x = ti+1 x. Hence, by Lemma 14, all the Beta reduction steps after that point are internal, and by Lemma 15 we can assume that all the reduction steps after tk are internal. There is therefore a subterm u[i/v] of tk with an inï¬nite reduction sequence out of v. But this substitution was created by a previous Beta step, which contradicts the minimality assumption.
Theorem 3 (PSN) If M is a strongly normalisable Î»-term with fv(M ) âŠ† x then M x is strongly normalisable in Î»o.
Proof Direct consequence of Propositions 7 and 3.

4 Simpliï¬ed Calculi

We now have a general framework to simulate the Î»-calculus with a director strings notation. However, our aim is to search for new efï¬cient reduction strategies, so we may give up completeness if we can gain some efï¬ciency and simplicity, provided that we are still able at least to reduce closed terms to weak head normal form2, which is the widely accepted minimal requirement for a Î»-evaluator, such as found in functional compilers and interpreters.
We will thus introduce two new calculi Î»l and Î»c (with reduction l and c respectively) on the same terms, which will be simpliï¬cations (specialisations) of Î»o. By deï¬nition, the following relation will hold:

câŠ‚

lâŠ‚

âˆ— o

so that, for instance, Î»l and Î»c will inherit properties of termination of Î»o. From an algorithmic point of view, the rewrite rules of Î»o cannot be consid-
ered as constant time operations, because we have to access directors at arbitrary positions in strings, and the computation of the new director strings a priori seems to require a time linear in the size of the original ones. In contrast with Î»o, the calculus Î»l performs only local reduction steps. In other words, Î»l is a restriction of Î»o where the rewrite rules make only local modiï¬cations in the director strings. It is the most general sub-calculus of Î»o with such a property.
The calculus Î»c is a restriction of Î»l which implements a closed reduction strategy. This is the most interesting calculus from an implementation perspective, as our benchmarks show (see Section 8).
We will ï¬rst deï¬ne the common syntax for Î»l and Î»c, then we will give the two sets of rewrite rules deï¬ning these calculi, focusing on the properties of Î»c (the proofs for Î»l are similar).
2 A Î»-term is in weak head normal form if it is either an abstraction or a nested application ultimately beginning by a variable, i.e. any term not beginning by a redex.

Lambda-Calculus with Director Strings

23

4.1 Syntax

Deï¬nition 7 (Simpliï¬ed Terms) In this section, the syntax is slightly different:
â€“ The directors are , , and â†“ only (âˆ’ is no longer a director). â€“ Preterms are deï¬ned according to the following grammar:
t ::= | (Î»t)Ïƒ | (Î»âˆ’t)Ïƒ | (t u)Ïƒ | (t[u])Ïƒ
where (Î»âˆ’t)Ïƒ is an abstraction where the bound variable does not occur in t and (t[u])Ïƒ is a substitution for the ï¬rst variable of t only. â€“ Terms are preterms satisfying the constraints given in Deï¬nition 1 for Î»o, taking U =â†“âˆ— and B = ( | | )âˆ—. Note that now variables do not have a director string. Moreover, abstractions have the following additional constraint:
(Î»âˆ’tÏ)Ïƒ is well-formed if Ïƒ âˆˆ U and |Ï| = |Ïƒ|
The new set of terms can be seen as a subset of Î»o-terms using some abbreviations:
Proposition 8 (Syntactic Equivalence) The following mapping from simpliï¬ed terms to Î»o-terms is injective.
â€“ â€«â†“ = ) (×’â€¬ â€“ â€«((×’â€¬Î»t)Ïƒ) = (Î»â€«(×’â€¬t))Ïƒ â€“ â€«((×’â€¬Î»âˆ’tÏ)Ïƒ) = (Î»â€«(×’â€¬tÏâˆ’))Ïƒ â€“ â€«((×’â€¬t u)Ïƒ) = (â€«(×’â€¬t) â€«(×’â€¬u))Ïƒ â€“ â€«((×’â€¬t[u])Ïƒ) = (â€«(×’â€¬t)[1/â€«(×’â€¬u)])Ïƒ
We will hence sometimes feel free to identify simpliï¬ed terms with Î»o-terms (i.e. to omit â€«)×’â€¬.

4.2 The Local Open Calculus
Taking a closer look at the Beta rule, we notice that for a redex where the function is closed, we generate a substitution for the ï¬rst (and only) variable of the function body (which was bound by the abstraction). Moreover, we know from [17] that restricting Î²-reduction to closed functions still allows to reach weak head normal form, for closed terms. In this section we will thus describe the calculus resulting from this restriction which greatly simpliï¬es the rules. This calculus will be called local open (Î»l) because it still allows open substitutions to propagate, even inside abstractions (in contrast with the system of closed reduction [17]). This does not need global rewrite steps if we restrict the syntax to allow substitutions for the ï¬rst director only.

24 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Deï¬nition 8 (Î»l-calculus) Below are shown the reduction rules for the local open calculus.

Name

Reduction

Beta ((Î»t) u)Ïƒ
BetaE ((Î»âˆ’t) u )
Var ( [v])Ïƒ App1 ((t u) Ï[v]) mÂ· n App2 ((t u) Ï[v]) mÂ· n App3 ((t u) Ï[v]) mÂ· n Lam ((Î»t)â†“Ï[v])Ïƒ
LamE ((Î»âˆ’t)â†“Ï[v])Ïƒ Comp ((t[w]) n+1Â· m [v]) pÂ· q

l (t[u])Ïƒ
lt
lv l ((t[v]) mÂ· |Ï|l u) mÂ·Ï l (t (u[v]) mÂ· )|Ï|r mÂ·Ï l ((t[v]) mÂ· |Ï|l (u[v]) mÂ· )|Ï|r mÂ·Ï l (Î»(t[v])ÏƒÂ· )â†“|Ïƒ| l (Î»âˆ’(t[v])Ïƒ)â†“|Ïƒ| l (t[(w[v]) pÂ· n ]) p+nÂ· m

Even though the rules in this system apply to terms with director strings of a particular pattern, the system is still suitable to reduce general terms, thanks to the following property.

Lemma 16 (Completeness of the Reduction) In any reduct of a closed compiled term, any subterm of the form (t[v])Ïƒ has a director string Ïƒ = m Â· n for some
natural numbers m, n.

Proof It is sufï¬cient to notice that the rules for propagation only generate substitutions of this form, and that the Beta rule does as well: if the term ((Î»t) u)Ïƒ is well-formed (and it is by induction) then Ïƒ is of the form n.

We remark that, in this calculus, we allow even open substitutions to pass through abstractions, without any global reduction step. This is of course one of the greatest strengths of this calculus, compared to those based on names or de Bruijn indices.

Lemma 17 (Preservation of Well-Formedness) If t is a well-formed term in Î»l and t l u then u is a well-formed term in Î»l.

Proof Straightforward inspection of the rewrite rules.

Proposition 9

lâŠ‚

âˆ— o

Proof We can easily check that all the rules given in Deï¬nition 8, except BetaE, are particular cases of the rules for Î»o taking into account the simpliï¬ed syntax of terms. The rule BetaE can be simulated in Î»o with two reductions: Beta followed by Erase.

Theorem 4 below summarises the properties of Î»l. The calculus Î»l is conï¬‚uent and preserves strong normalisation. However, it does not fully simulate Î²reduction. Instead we can show that it can be used to evaluate closed Î»-terms. This is a consequence of the fact that Î»c, which will be shown to be a restriction of Î»l, can compute weak head normal forms of closed terms (see next section).

Lambda-Calculus with Director Strings

25

Theorem 4 (Properties of Î»l)
Conï¬‚uence: Î»l is conï¬‚uent. PSN: Î»l preserves strong normalisation. Adequacy: Î»l can evaluate closed terms.
Proof Conï¬‚uence is shown by easily adapting the proof of conï¬‚uence of Î»c, which is given in the next section. PSN comes from Theorem 3 (PSN for Î»o) and Proposition 9. Adequacy will be obvious from Theorems 7 and 8 (adequacy for Î»c) and Proposition 10 (see next section).

4.3 The Closed Calculus

The notion of closed reduction was introduced in [17] using a calculus of explicit substitutions with names where Î²-reductions are performed when the function part is closed (as above) and substitutions are propagated through abstractions only if they are closed, which is crucial to avoid Î±-conversion in a named setting. These restrictions were expressed by rewrite rules with external conditions on free variables, and the internalisation of these conditions was the main motivation for the introduction of director strings in [18].
We can easily derive a calculus for closed reduction (Î»c) by adding restrictions to the rules of Î»l (see Deï¬nition 8) as follows: Beta, BetaE, Var are the same; in every other rule we force the substitution v to be closed, that is, to have an empty string ( ). Moreover, in App1,2,3, m = 0, and in Comp, m = p = 0 and n = q. This thus leads to the very simple following rewrite system.

Deï¬nition 9 (Î»c-calculus) The reduction rules for the closed calculus are:

Name

Reduction

n
Beta ((Î»t) u)

n
c (t[u])

BetaE ((Î»âˆ’t) u )
Var ( [v]) n
App1 ((t u) Ï[v ]) n App2 ((t u) Ï[v ]) n App3 ((t u) Ï[v ]) n Lam ((Î»t)â†“Ï[v ]) n
LamE ((Î»âˆ’t)â†“Ï[v ]) n
Comp ((t[w]) n+1 [v ]) n

ct
cv c ((t[v ]) |Ï|l u)Ï c (t (u[v ]) )|Ï|r Ï c ((t[v ]) |Ï|l (u[v ]) )|Ï|r Ï c (Î»(t[v ]) )n+1 â†“n c (Î»âˆ’(t[v ]) n )â†“n c (t[(w[v ]) n ]) n

This system is complete because a property similar to Lemma 16 still holds here:

Lemma 18 (Completeness of the Reduction) In any reduct of a closed compiled term, if a subterm is of the form (t[v])Ïƒ then either Ïƒ = n or Ïƒ = n for some
natural number n.

26 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Proof Beta creates a substitution annotated by n, the other rules only generate substitutions with a n string.

Note that the choice between n and n is always obvious from the context

(by well-formedness). For instance, in rule Comp, we reduce a term of the form

((t[w])

n+1 [v

])

m
,

then

it

is

easy

to

see

that

m

=

n.

Lemma 19 (Preservation of Well-Formedness) If tÏƒ is a well-formed term in Î»c and tÏƒ c uÏ„ then uÏ„ is a well-formed term in Î»c. Moreover, |Ïƒ| = |Ï„ |.

Proof Straightforward inspection of the rewrite rules.

Proposition 10

câŠ‚

lâŠ‚

âˆ— o

Proof Since the rewrite rules in Deï¬nition 9 are particular instances of the rules of Î»l (Deï¬nition 8), the rewrite relation c is included in l. The latter inclusion was established in Proposition 9.

We will show that c is conï¬‚uent on Î»c-terms, preserves strong normalisation, and implements call-by-value and call-by-name evaluation strategies for closed Î»terms. For this, we ï¬rst show that the propagation rules (i.e. all the rules except Beta and BetaE) are terminating and conï¬‚uent, and are sufï¬cient to implement the metaoperation of substitution in the Î»-calculus, provided substitutions are closed. The set of propagation rules will be denoted Pc.

Proposition 11 (Termination and Conï¬‚uence of Pc) All the reduction sequences on Î»c using only rules in Pc are ï¬nite. Moreover, Pc is locally conï¬‚uent (hence conï¬‚uent).

Proof Termination is a direct consequence of Proposition 4 since

âˆ— Pc

âŠ‚

âˆ— P

.

Local conï¬‚uence is proved by showing that the critical pairs are joinable. There

is only one, between Comp and Var (the conditions on the director strings prevent

superpositions between Comp and the other propagation rules).

n+1

n

(( [v]) [u ])

Comp

nn

- ( [(v[u ]) ])

Var
?
n
(v[u ])

Var

?

â‰¡

n
(v[u ])

Since the propagation rules are terminating, the system is conï¬‚uent by Newmanâ€™s Lemma [32].

Since Pc is terminating and conï¬‚uent, it deï¬nes a function associating to each Î»c-term t its unique normal form, denoted Pc(t).

Lemma 20 (Closed Substitutions) Let M and N be Î»-terms such that x âˆˆ fv(M )
âŠ† x and fv(N ) = âˆ…. Let M x = t, N = u , M {x â†’ N } y = vÏƒ, where y = x\{x}. Then Pc((t[u ])Ïƒ) = vÏƒ.

Lambda-Calculus with Director Strings

27

Proof By induction on M . If M is a variable (in this case it must be x) then its compilation is simply and using the rule Var, [u ] c u = M {x â†’ N } y as required. If M is an application, then one of the rules App1, App2, App3 applies, and the result follows directly by induction. If M is an abstraction then either Lam
or LamE applies and again the result follows by induction.

As a consequence, we deduce that even with the restrictions imposed by Î»c rules, closed substitutions do not remain blocked: if a closed substitution is created, it will be fully propagated.
The following lemma shows that the rules Beta and BetaE also generate a conï¬‚uent relation, which we denote B. In the proof we use an auxiliary relation, â‡’, which makes B steps in parallel. It is deï¬ned by induction.

Deï¬nition 10 (Parallel Beta) Let â‡’ be the rewrite relation on Î»c-terms inductively deï¬ned as follows.
1. t â‡’ t, 2. ((Î»t) u) n â‡’ (t [u ]) n if t â‡’ t and u â‡’ u , 3. ((Î»âˆ’t) u ) â‡’ t if t â‡’ t , 4. (Î»t)Ïƒ â‡’ (Î»t )Ïƒ if t â‡’ t , 5. (t u)Ïƒ â‡’ (t u )Ïƒ if t â‡’ t and u â‡’ u , 6. (t[v])Ïƒ â‡’ (t [v ])Ïƒ if t â‡’ t and v â‡’ v .

Lemma 21 (Conï¬‚uence of Beta reductions) B is conï¬‚uent.

Proof We show that â‡’ is strongly conï¬‚uent (i.e. has the diamond property), and

since obviously

BâŠ†â‡’ and â‡’âˆ—=

âˆ— B

,

we

obtain

conï¬‚uence

of

B.

Assume t â‡’ u and t â‡’ v (u = v). We show that âˆƒw such that u â‡’ w and

v â‡’ w by induction on t â‡’ u. The proof is standard. The case t â‡’ t is trivial,

taking w

â‰¡

v. We give the details for ((Î»t)

u)

n

â‡’

(t [u ])

n
, where t

â‡’

t

and u â‡’ u . In this case, the only alternatives are:

â€“

v â‰¡ ((Î»t)

u)

n
,

in

which

case

we

take

w

â‰¡

(t

[u

])

n
, or

â€“ v â‰¡ ((Î»t ) u ) n , where t â‡’ t and u â‡’ u . By induction, there exists w

and w such that t â‡’ w , t â‡’ w , u â‡’ w , u â‡’ w . Hence we take

w â‰¡ (w [w

])

n
.

The case ((Î»âˆ’t) u ) â‡’ t where t â‡’ t is similar. All the other cases follow directly by induction.

Next we prove the commutation of B and Pc . Rosenâ€™s Lemma [34] states that if two conï¬‚uent rewrite relations are independent (commute) then their union
is conï¬‚uent. Since c= B âˆª Pc and we have just shown that both relations are conï¬‚uent, commutation implies conï¬‚uence of c.

Lemma 22 (Commutation) If t

a

âˆ— B

c

and

b

âˆ— Pc

c.

âˆ— Pc

a

and

t

âˆ— B

b,

then

there

exists

c

such

that

28 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Proof Again we use the parallel reduction relation â‡’ (Deï¬nition 10). Since â‡’âˆ—

coincides with

âˆ— B

,

it

is

sufï¬cient

to

prove

that

if

b

â‡

t

Pc a then there exists

a term c such that b

and t

âˆ— B

b

(which

is

of the derivation t â‡’âˆ—

Pc c â‡ a. equivalent
b.

In to

this way b âˆ—â‡ t

we can close the diagram: t

âˆ— Pc

a),

by

induction

on

the

âˆ— Pc

a

length

We proceed by induction on t â‡’ b. We distinguish the following cases:

1. t â‰¡ b: Then we take c â‰¡ a. 2. ((Î»t) u) n â‡’ b â‰¡ (t [u ]) n where t â‡’ t and u â‡’ u : Since no rule

from Pc applies at the root, it must be either t Pc t or u Pc u . In

the ï¬rst case, by induction, there exists a term t t â‡’ t , therefore we can take c â‰¡ (t [u ])

such n . In

that the

t Pc second

t and case, by

induction, there exists u such that u can take c â‰¡ (t [u ]) n .

Pc u and u â‡’ u , therefore we

3. The case ((Î»âˆ’t) u ) â‡’ b â‰¡ t where t â‡’ t is similar to the above.

4. (t[v])Ïƒ â‡’ (t [v ])Ïƒ where t â‡’ t and v â‡’ v : We distinguish two cases

according to the position of the Pc-reduction step (t[v])Ïƒ Pc a. â€“ If it does not apply at the root then the property holds by induction.

â€“ If it applies at the root: then there is a substitution Î¸ and a rule l â†’ r in Pc such that (t[v])Ïƒ = lÎ¸ and a â‰¡ rÎ¸.

â€“ If all the B-redexes that are contracted in the reduction (t[v])Ïƒ â‡’ (t [v ])Ïƒ â‰¡ b are under variables in l (that is, they are in Î¸) then

these variables are uniquely identiï¬ed (since l is left-linear) and we

can therefore deï¬ne a substitution Î¸ such that Î¸ â‡’ Î¸ and the diagram

commutes: (t[v])Ïƒ â‰¡ lÎ¸ Pc rÎ¸ â‰¡ a â‡’ rÎ¸ â‰¡ c and (t[v])Ïƒ â‰¡
lÎ¸ â‡’ lÎ¸ â‰¡ b Pc rÎ¸ â‰¡ c. â€“ If there is a B-redex at the root of t in (t[v])Ïƒ then we have a critical
pair, between the Pc-rule applied at the root of (t[v])Ïƒ and the Beta
or BetaE rule applied at the root of t. In that case the Pc-rule applied

must be App2 (it cannot be App1 or App3 because of the restrictions:

the function has a director ). Then the diagram commutes as follows:

we have

(t[v])Ïƒ â‰¡ (((Î»w)

u)

n+1
[v

])

n

App2

((Î»w)

(u[v

])

n
)

n

â‰¡a

and

(t[v])Ïƒ â‰¡ (((Î»w)

u)

n+1
[v

])

n

â‡’ ((w [u ])

n+1
[v ])

n

â‰¡b

where w â‡’ w , u â‡’ u , v â‡’ v , hence

a â‡’ (w [(u [v ])

n
])

n

â‰¡c

and

b

Comp

(w [(u [v ])

n
])

n

â‰¡ c.

5. In the other cases the property follows directly by induction.

This concludes the proof.

Lambda-Calculus with Director Strings

29

Theorem 5 (Conï¬‚uence) Î»c is conï¬‚uent.
Proof Consequence of Proposition 11, Lemma 21, and Lemma 22, using Rosenâ€™s Lemma [34].
Theorem 6 (PSN) Î»c preserves strong normalisation.
Proof Consequence of Theorem 3 (PSN for Î»o) and Proposition 10.
The restrictions imposed on the rewrite rules of Î»c still allow us to simulate the usual evaluation strategies, for closed Î»-terms. We will show that call-by-value and call-by-name reductions to weak head normal form can be implemented in Î»c.

Call-by-name. We recall the evaluation rules for weak reduction in the call-byname Î»-calculus:

M â‡“ Î»x.M M {x â†’ N } â‡“ V

x â‡“ x Î»x.M â‡“ Î»x.M

(M N ) â‡“ V

where M â‡“ V means that M evaluates to the (principal) weak head normal form V using the call-by-name strategy.

Theorem 7 (Call-by-name Evaluation) If M is a closed Î»-term and M â‡“ V by

the call-by-name strategy, then M

âˆ— c

V

.

Proof By induction on the derivation of M â‡“ V .

If M is a weak head normal form then the theorem holds trivially. Otherwise,

it is an application (M N ) and (M N ) = ( M N ) (since fv(M N ) = âˆ…).

By induction, since M â‡“ Î»x.M , we know that M

âˆ— c

Î»x.M

. Now there are

two alternatives.

â€“ Î»x.M = (Î» M [x]) if x âˆˆ fv(M ).

Then ((Î» M [x]) N ) c ( M [x][ N ]) . By Lemma 20, ( M [x][ N ])

âˆ— c

M {x â†’ N }

and by induction the latter reduces to

V

as required.

â€“ Î»x.M = (Î»âˆ’ M ) if x âˆˆ fv(M ).

Then ((Î»âˆ’ M ) N ) c M = M {x â†’ N } and by induction the

latter reduces to V as required.

Call-by-value. We recall the evaluation rules for weak reduction in the call-byvalue Î»-calculus:

M â‡“ Î»x.M N â‡“ V M {x â†’ V } â‡“ V

x â‡“ x Î»x.M â‡“ Î»x.M

(M N ) â‡“ V

where V is a weak head normal form.

Theorem 8 (Call-by-value Evaluation) If M is a closed Î»-term and M â‡“ V by

the call-by-value strategy, then M

âˆ— c

V

.

30 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Proof Similar to Theorem 7. In the case of an application (M N ) where M â‡“

Î»x.M and N â‡“ V , we get: (M N ) = ( M N ) (since fv(M N ) = âˆ…), and

by induction, M

âˆ— c

Î»x.M

,

N

âˆ— c

V

. Again there are two cases to

consider, which are identical to those in the proof of Theorem 7 above.

This system has several advantages as a basis for an implementation of a Î»calculus evaluator:
â€“ closed substitutions can be propagated through abstractions, which permits more sharing of work than in standard weak calculi,
â€“ it forbids copying open terms, which ensures that we never duplicate a potential redex.
Moreover, the shape of the rewrite rules shows that in most cases we do not need to represent director strings as complex structures. For example, a director string of a substitution can be represented as a simple relative integer, because of Lemma 18. In the same spirit, the information on |Ï|l and |Ï|r can be maintained at each step. The system can thus be implemented in a realistic and efï¬cient way, giving to each rewrite step a constant algorithmic cost. We will come back to this point in Section 6.

5 A Type System for Director Strings Calculi

For completeness, we present in this section a type system common to the above
deï¬ned calculi and which enjoys the same kind of properties as the usual simply typed Î»-calculus. We present the system with the syntax of Î»o, but it is of course also valid for Î»l and Î»c thanks to Proposition 8. The types are the simple types of the Î»-calculus: type variables A, B . . . and function types A â†’ B. We also add a generic constant Ïƒ of type (well-formed iff Ïƒ = âˆ’n for some n). Typing judgements will be of the form Î“ t : A where the context Î“ is an ordered list of
types.

Deï¬nition 11 (Typed Terms) Each term t is assigned a type in a given context: Î“ t : A, as given by the following set of rules. A term is typeable if there exists a context such that it is typeable in that context. The typing rules make use of two auxiliary relations deï¬ned below.

Î“ âˆ¼Ïƒ âˆ… (Cst )
Î“ Ïƒ:

Î“ âˆ¼Ïƒ A (Ax )
Î“ Ïƒ:A

Î“ , A t : B Î“ âˆ¼Ïƒ Î“ (Abs )
Î“ (Î»t)Ïƒ : A â†’ B

Î“1 t : A â†’ B Î“2 u : A Î“ (t u)Ïƒ : B

Î“Ïƒ

Î“1 Î“2 (App)

Lambda-Calculus with Director Strings

31

Î“1 i/A t : B Î“2 u : A Î“ (t[i/u])Ïƒ : B

Î“Ïƒ

Î“1 Î“2 (Sub)

where Î“ i/A = A1, . . . , Aiâˆ’1, A, Ai, . . . , An if Î“ = A1, . . . , An.

Note that there are no structural rules, as the type of a given variable is always

at a known location in the context. In the typing rules we used two auxiliary relations. The ï¬rst one (Î“ âˆ¼Ïƒ Î“ )

ensures that the right types are erased from Î“ depending on Ïƒ. The second one

ensures that a context adequately splits according to a director string, which is

shown as Î“ Ïƒ Î“Î“12.

Î“ âˆ¼Ïƒ Î“

Î“ âˆ¼Ïƒ Î“

âˆ… âˆ¼ âˆ… A, Î“ â†“âˆ¼Ïƒ A, Î“

A, Î“ âˆ’âˆ¼Ïƒ Î“

Î“ Ïƒ Î“1 Î“2

Î“Ïƒ

Î“1 Î“2

âˆ…

âˆ… âˆ…

A, Î“

Ïƒ A, Î“1 A, Î“2

A, Î“ âˆ’Ïƒ

Î“1 Î“2

Î“Ïƒ

Î“1 Î“2

Î“Ïƒ

Î“1 Î“2

A, Î“

Ïƒ Î“1 A, Î“2

A, Î“ Ïƒ A, Î“1 Î“2

Remark 5 If a term t is typeable in a context Î“ , then the length of Î“ is exactly the length of tâ€™s string. In particular, a term with empty director string (e.g. the compilation of a closed Î»-term) is typeable if and only if it is typeable in the empty context.

Example 4 We give two small examples of type derivations in this system. 1. (Î» â†“) : A â†’ A

âˆ… 2. (Î»(Î» â†“âˆ’)â†“) : A â†’ B â†’ A

A â†“:A (Î» â†“) : A â†’ A

A, B â†“âˆ’ : A A (Î» â†“âˆ’)â†“ : B â†’ A âˆ… (Î»(Î» â†“âˆ’)â†“) : A â†’ B â†’ A
A ï¬rst useful property is the following:
Proposition 12 (Well-Formedness) Typeable preterms are well-formed terms.

32 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Proof The functions âˆ¼Â· and Â· clearly enforce the constraints on well-formed terms (Deï¬nition 1).

This type system corresponds to the Î»-calculus simple type system (see for instance [6]), whose judgements we denote Î“ Î» M : A, in the following sense:

Lemma 23
1. x1 : A1, . . . , xn : An Î» M : A =â‡’ A1, . . . , An 2. A1, . . . , An t : A =â‡’ x1 : A1, . . . , xn : An Î»

M [x1,...,xn] : A t [x1,...,xn] : A

Proof By straightforward induction on the type derivation.

We also have the basic results expected from a simply typed calculus:

Theorem 9 (Subject Reduction) If Î“ t : A and t u then Î“ u : A (for âˆˆ { o, l, c}).

Proof By Proposition 10, the cases for Î»l and Î»c follow from the general case
of Î»o, which is proved by checking that each rule preserves type. We will only
illustrate the proof on the case of App1, the others being similar. The situation is the following, where Ïi = , j = |Ï1..i|l, Ï… = Ï†l(Ïƒ, Ï\i),
Ï„ = Ïˆ1(Ïƒ, Ï\i) and Î“ = A1, . . . , An:

âˆ†1

t : A â†’ B âˆ†2 âˆ† = Î“1 i/C

u:A âˆ† Ï (t u)Ï : B

âˆ†1 âˆ†2
Î“2

v:C

Î“Ïƒ

Î“1 Î“2

Î“ ((t u)Ï[i/v])Ïƒ : B

â†“

â„¦1 j/C

t:Aâ†’B

â„¦2

v:C

Î˜1 Ï…

â„¦1 â„¦2

Î˜1 (t[j/v])Ï… : A â†’ B

Î˜2

u:A Î“ Ï„

Î˜1 Î˜2

Î“ ((t[j/v])Ï… u)Ï„ : B

Then subject reduction is veriï¬ed for this rule, provided that

ï£± ï£²

â„¦1

j/C

= âˆ†1

â„¦2 = Î“2

ï£³ Î˜2 = âˆ†2

Letâ€™s write LÏƒ = {i | Ïƒi = or }, RÏƒ = {i | Ïƒi = or } and [A]I = Ai1 , . . . , Aim if I = {i1, . . . , im} and (ij)j is increasing, so that Î“ = [A]{1...n}. Then we have:

Î“1 = [A]LÏƒ Î“2 = [A]RÏƒ âˆ†1 = [A]LÏƒâˆ©LÏ j/C âˆ†2 = [A]LÏƒâˆ©RÏ

since Ïi =

and j = |Ï1..i|l

Lambda-Calculus with Director Strings

33

and on the other hand:
Î˜1 = [A]LÏ„ Î˜2 = [A]RÏ„ = [A]LÏƒâˆ©RÏ by deï¬nition of Ïˆ1 â„¦1 = [A]LÏ„ âˆ©LÏ… = [A]LÏƒâˆ©LÏ by deï¬nition of Ï†l â„¦2 = [A]LÏ„ âˆ©RÏ… = [A]RÏƒ again by deï¬nition of Ï†l
Hence subject reduction holds for App1. The other cases are similar.
Theorem 10 (Termination) If Î“ t : A then t is strongly normalisable (in Î»o, Î»l, Î»c).
Proof Since typeable Î»-terms are strongly normalisable, this is a direct consequence of Lemma 23 and PSN for the corresponding calculus.

6 An Abstract Machine for Evaluation
In this section we will exhibit a strategy which makes use of the explicit information carried by director strings to efï¬ciently reduce closed terms to weak head normal form. Efï¬ciency is measured with respect to the total number of rewriting steps (not just Î²-steps) and we will give experimental comparisons in Section 8.
Notice that the syntax of director strings allows us to identify the moment when we have to copy a term, and we can reduce it before copying. In particular, we may want to use the most general rules, in order to be able to reduce a term to be copied to its full normal form, thus avoiding copying any redex. However, if we do so, then open substitutions are allowed in App3 as well, which means that terms with free variables, i.e. potential redexes, might be copied. Our experimental tests conï¬rmed that restricting just that rule to the closed case, we obtain a strategy very similar to closed reduction. This is because the propagation of an open substitution is very likely to be blocked by this restriction. Thus, the best strategy we found is based on the closed calculus, which is good news since it is also the simplest.
We cannot expect to reduce to full normal form with the closed rules, but some open terms can still be reduced. Thus, our strategy to compute the weak head normal form of a term t can be summarised as follows: we use the closed calculus, which allows some extra reductions under abstractions, but we stop the reduction as soon as we reach a weak head normal form of t. The extra reductions are done only when we reduce a subterm to be copied, to share more work than in usual strategies.
To formally specify this strategy we will interleave one strategy which reduces under Î»â€™s and one which does not. We thus deï¬ne three mutually recursive relations: â†’w, â†’f and â†’. The last one will be the strategy we want to exhibit. The relation â†’w is deï¬ned in Figure 4, which, together with the other two relations below deï¬ne the big-step operational semantics which the closed abstract machine implements.
The reduction relation â†’w is used as a tool to deï¬ne the other two and should not be interpreted on its own, as it does not treat the case of an abstraction. Notice

34 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

t â†’ (Î»r) (r[u])Ïƒ â†’ v

(t u)Ïƒ â†’w v

(Beta )

t â†’ (Î»âˆ’v)
(BetaE ) (t u) â†’w v

(Ax ) â†’w

t â†’ v v = (Î»r) âˆ§ (v = (Î»âˆ’r) âˆ¨ Ï = )

(t uÏ)Ïƒ â†’w (v uÏ)Ïƒ

(Arg )

((t[v ]) |Ï|l u)Ï â†’ w (App1 )
((t u) Ï[v ])Ïƒ â†’w w

(t (u[v ]) )|Ï|r Ï â†’ w (App2 )
((t u) Ï[v ])Ïƒ â†’w w

v â†’f v ((t[v ]) |Ï|l (u[v ]) )|Ï|r Ï â†’ w (App3 )
((t u) Ï[v ])Ïƒ â†’w w

(Î»(t[u ]) )|Ï|+1 Ï â†’ v (Lam )
((Î»t)â†“Ï[u ])Ïƒ â†’w v

(Î»âˆ’(t[u ]) )|Ï| Ï â†’ v (LamE )
((Î»âˆ’t)â†“Ï[u ])Ïƒ â†’w v

vâ†’w (Var )
( [v])Ïƒ â†’w w

(t[(u[v ]) |Ï| ])Ï â†’ w (Comp)
((t[u]) Ï[v ])Ïƒ â†’w w

t â†’ u (u[vÏ])Ïƒ â†’ w Ï =

(t[vÏ])Ïƒ â†’w w

(Subst )

Fig. 4 The relation â†’w

that the (App3) rule calls the stronger reduction â†’f , which is deï¬ned by:

t â†’w v t â†’f v

t â†’f v (Î»t)Ïƒ â†’f (Î»v)Ïƒ

t â†’f v (Î»âˆ’t)Ïƒ â†’f (Î»âˆ’v)Ïƒ

â†’f is the relation which reduces under Î»â€™s (but not to full normal form). Finally, â†’ is the combination of the two other relations: we reduce to weak
head normal form, but we reduce more the subterms that will be copied.

t â†’w v tâ†’v

(Î»t)Ïƒ â†’ (Î»t)Ïƒ

(Î»âˆ’t)Ïƒ â†’ (Î»âˆ’t)Ïƒ

It may seem that the machine returns terms which are not weak head normal forms (cf. (Arg) rule). In fact, the theory ensures that this is not the case: starting from a closed term, the closed rules allow us to reach a weak head normal form (see Theorem 7). Nevertheless, the (Arg) rule may be applied in a reduction of a term to be copied, so it is indispensable.
The (Subst) and (Comp) rules call for a comment: the restriction on (Subst) (vÏ open) forces (Comp) to be used as much as possible before reducing to the left of a closed substitution. Both intuition and experimentation conï¬rm that this is indeed the right choice.
Notice that each rule either corresponds to a closed rule (see Deï¬nition 9) or focuses reduction on a subterm (corresponding to stack manipulations) and can indeed be implemented in constant time.

Lambda-Calculus with Director Strings

35

7 Reduction to Full Normal Form
We have presented so far a rather complex system to fully simulate Î²-reduction and simpler systems to reach only weak head normal form. If we were however interested in computing full normal forms, which is the case for many applications (e.g. partial evaluation or proof assistants), then we could of course use the general setting. But this is not really satisfactory (it does not provide any guidance towards an efï¬cient strategy). On the other hand, we have an efï¬cient strategy to reduce closed terms to weak head normal form. The idea then naturally arises to use our efï¬cient weak evaluator to reach full normal form, in a way similar to [10].
The idea is to reduce a closed term to weak head normal form, then to distinguish the variable bound by the outermost abstraction in some way (to â€œfreezeâ€ it), so that we can still consider the term under the Î» as closed, and recursively apply the same process to this subterm. There are several ways to distinguish those variables in the syntax. Below we present two natural alternatives.

7.1 With Names

If we choose to represent the frozen variables with names, we can avoid any complex manipulation of the director strings to keep track of the paths to these variables. As a result, we obtain a rather simple system because we can use the usual rules (for example the closed ones), where the frozen variables are just considered as constants and do not need any extra rule. Moreover readback into named Î»-calculus is then performed at the same time.
Formally, we extend the syntax of terms in the following way, where Ïƒ ranges over strings, and x ranges over variable names:

t, u ::= | (Î»t)Ïƒ | (Î»âˆ’t)Ïƒ | (t u)Ïƒ | (t[u])Ïƒ | x | Î» x.t

that is, we add named variables, whose implicit director string is , and named abstraction binding written as Î» x.t. We do not write any director string for this
abstraction, since we will always consider closed terms of this form. Using a weak evaluation relation â‡“w, we can then deï¬ne reduction to full nor-
mal form â‡“f .

t â‡“w (Î»t )

(t [x]) â‡“f t

x fresh

t â‡“f Î» x.t

t â‡“w (Î»âˆ’t )

t â‡“f t

x fresh

t â‡“f Î» x.t

t â‡“w x (x variable)

t â‡“f x

t â‡“w (u v)

u â‡“f u

v â‡“f v

t â‡“f (u v )

36 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot

Notice that the last rule is used since we are now in a calculus with constants (the named variables), and the weak head normal form of a term may be an application (e.g. (x t) where x is a named variable).

Proposition 13 (Correctness) Letâ€™s write uËœ for the readback of a reduced term u, so ËœÂ· just transforms Î» to Î» and erases the remaining director strings. Then, if M is a closed Î»-term:
M â‡“f u â‡â‡’ M â†’âˆ—Î² uËœ irreducible.
Proof Assuming correctness of â‡“w, the proof is easily adapted from [10] or the more recent [20].

If we want to reduce an open term t = tÏƒ with |Ïƒ| = n, we ï¬rst take n fresh variable names x1, . . . , xn and start the reduction from:

nâˆ’1

nâˆ’2

((. . . ((t[x1]) [x2]) . . . [xnâˆ’1]) [xn])

The reduction to normal form follows exactly the same strategy as the corresponding weak reduction. Thus, for terms for which full and weak head normal forms are the same, the two processes need the same numbers of Î² and total steps. In particular, this strategy is much more efï¬cient than the usual naive one.
Although we now have to deal with names and fresh variables during reduction (which was not the case for reduction to weak head normal form), we still do not have to deal with name capture and Î±-conversion. Also, the readback is now simpliï¬ed.

7.2 With Directors
The previous strategy performs readback at the same time as computation of the normal form, which may, or may not, be wished. We can however implement a similar idea using only directors, obtaining a result in this syntax. We just need a way to distinguish between usual variables, and frozen ones, which correspond to an abstraction outside of the term we actually want to reduce to weak head normal form. This can be done in a quite obvious way: by introducing a new kind of directors corresponding to these frozen variables. However, the frozen part of director strings may be of any form, so we need to use the general rules on this part. From an algorithmic point of view, this means that the cost of a reduction step may be at most linear in the depth of Î»-abstractions in the resulting normal form, which still seems reasonable. We do not go further on this point as the interesting ideas have already been exposed in the previous section.

8 Experimental Results
One of the main motivations for this work is the desire to ï¬nd more efï¬cient implementations of the Î»-calculus. This interest is not simply to ï¬nd minor tweaks

Lambda-Calculus with Director Strings

37

(or optimisations) of existing systems, but rather to attempt a fresh start to uncover new strategies, and new machinery, which give asymptotic improvements over standard techniques in use in implementations of functional languages. Consequently it is essential that our strategies are implemented so that the ideas of this paper can be backed up with some experimental evidence and compared to existing evaluators.
It is difï¬cult, if not impossible, to ï¬nd a relevant measure to compare different implementations. This point is even more pertinent when we are comparing prototype implementations. Nevertheless, what we are able to do is to identify atomic reduction steps for several evaluators. Although the algorithmic cost of a single step may vary for each evaluator, the respective speed-up can still be examined. The following benchmarks suggest that the strategies developed in the paper behave better than standard reduction strategies, and moreover offer some surprising statistics with respect to some of the very best evaluators known to date.
We have two set of examples: one that belongs to Î»I in order to avoid the problem of erasing, and a small set of Î»K terms to illustrate some speciï¬c behaviours. The Church numerals are an excellent means to produce a panel of large Î»-terms. We recall that Church numerals are of the form n = Î»f.Î»x.f n x and that application corresponds to exponentiation: n m â‰¡ mn. We apply Church numerals in our examples to I I, where I = Î»x.x, which is sufï¬cient to force reduction to full normal form, and allows us to compare weak and full reducers. We also use the combinators K = Î»x.Î»y.x and M = Î»x.Î»y.(K I x)(K I x y).
We compare our machines with standard call-by-value and call-by-name evaluators, and in addition to the optimal interpreter of Asperti et al. (BOHM [3]). The latter result provides a comparison with the best known evaluator for such terms. We show the total number of steps of these evaluators (including stack manipulations). We show the number of Î²-reductions between round brackets, thus the number shown for BOHM is the minimum number of Î²-reductions possible. The results for the machines that reduce to full normal form are not shown, as they are the same as those of the underlying weak strategies on these examples.

Term

closed

CBV

CBN

BOHM

22II

61(9)

78(11)

76(12)

40(9)

2 2 2 I I 140(19)

362(42)

471(60)

93(16)

55II

217(33)

29 723(3 913) 31 250(4 689) 208(33)

5 2 2 I I 832(109)

-

- 847(31)

2 2 2 2 2 I I 1 507 714(196 655) -

- 1 074 037 060(61)

M (5 5 I I) I 266(42)

41(8)

31(8)

22(8)

K I (5 5 I I) 7(2)

29731(3915) 7(2)

4(2)

To put some of these results into perspective, we remark that the actual time taken to compute, for example, 5 2 2 I I using OCaml is around 5 minutes, and around 3 minutes using Standard ML (both implementing a variant of call-byvalue). The results for both closed reduction and BOHM are essentially instantaneous.

38 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot
The main point that we want to make with the above table is that closed reduction, a simple implementation of the Î»-calculus, clearly out performs traditional strategies, such as call-by-value, and moreover is a serious competitor to highly sophisticated implementations, such as BOHM. The comparison with call-by-value and call-by-name shows that allowing reductions under abstractions, which is especially easy with director strings compared to usual calculi, is crucial for both sharing and efï¬ciency.
The interesting point is the comparison on large terms. The results show that our machine is able to reduce larger terms than the other machines, and the larger the term, the better is our machine compared to the others. This hints that it allows for a high degree of sharing (because the larger the term, the more possible sharing). The last example of the ï¬rst set shows that our machine eventually explodes in terms of number of Î²-reductions compared to the optimal one but outperforms BOHM in total number of steps, which is our notion of efï¬ciency.
Besides this classical benchmark, we give two Î»K terms to illustrate the following points: â€“ The closed strategy may perform more work than necessary, when every copy
of a term will be erased. The occurrence of such terms in practical situations is however questionable. One could also combine the closed strategy with a call-by-need strategy similarly to [16] to avoid this problem. â€“ Except for the previous situation, we may avoid doing useless work, as opposed to call-by-value for instance, i.e. we also have some of the advantages of call-by-name.
9 Conclusion
We have presented a name-free syntax to represent terms of the Î»-calculus with explicit substitutions, in a way that follows the usual intuitions about the operational semantics of the propagation of substitutions. We have given a general calculus on director strings which can fully simulate the Î»-calculus, with rather complicated rules. We then described an intermediate calculus, the local open calculus, with very simple rules and still allowing open substitutions to traverse abstractions without global rewriting. Finally, we derived the closed reduction calculus of [18], which internalises the conditions on the original system [17].
These calculi were used as a basis to describe and implement abstract machines for weak and strong reduction (the latter was an open problem for director strings). Efï¬ciency was our main motivation, and we found in practice that these machines are quite efï¬cient on large terms and allow for a high degree of sharing. In particular, they quite favourably compare to standard evaluators, which suggests that more efï¬cient implementations of functional languages and Î»-calculus based proof assistants are still possible.
References
1. M. Abadi, L. Cardelli, P.-L. Curien, and J.-J. LeÂ´vy. Explicit substitutions. Journal of Functional Programming, 1(4):375â€“416, 1991.

Lambda-Calculus with Director Strings

39

2. Z. M. Ariola, M. Felleisen, J. Maraist, M. Odersky, and P. Wadler. A call-by-need lambda calculus. In Proceedings of the 22nd ACM Symposium on Principles of Programming Languages (POPLâ€™95), pages 233â€“246. ACM Press, 1995.
3. A. Asperti, C. Giovanetti, and A. Naletto. The Bologna optimal higher-order machine. Journal of Functional Programming, 6(6):763â€“810, 1996.
4. A. Asperti and S. Guerrini. The Optimal Implementation of Functional Programming Languages. Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1998.
5. H. P. Barendregt. The Lambda Calculus: Its Syntax and Semantics. North-Holland, revised edition, 1984.
6. H. P. Barendregt. Lambda calculi with types. In S. Abramsky, D. Gabbay, and T. S. E. Maibaum, editors, Handbook of Logic in Computer Science. Oxford University Press, 1992.
7. Z. Benaissa, D. Briaud, P. Lescanne, and J. Rouyer-Degli. Lambda-upsilon, a calculus of explicit substitutions which preserves strong normalisation. Journal of Functional Programming, 6(5):699â€“722, 1996.
8. Z. Benaissa, K. H. Rose, and P. Lescanne. Modeling sharing and recursion for weak reduction strategies using explicit substitution. In H. Kuchen and D. Swierstra, editors, 8th PLILPâ€”Symposium on Programming Language Implementation and Logic Programming, pages 393â€“407, Aachen, Germany, 1996.
9. R. Bloo and H. Geuvers. Explicit substitution: on the edge of strong normalization. Theoretical Computer Science, 211(1):375â€“395, 1999.
10. P. CreÂ´gut. An abstract machine for lambda-terms normalization. In Lisp and Functional Programming 1990, pages 333â€“340. ACM Press, 1990.
11. P.-L. Curien, T. Hardin, and J.-J. LeÂ´vy. Conï¬‚uence properties of weak and strong calculi of explicit substitutions. Journal of the ACM, 43(2):362â€“397, 1996.
12. R. David and B. Guillaume. A Î»-calculus with explicit weakening and explicit substitution. Mathematical Structures in Computer Science, 11(1):169â€“206, 2001.
13. N. G. de Bruijn. Lambda calculus notation with nameless dummies. Indagationes Mathematicae, 34:381â€“392, 1972.
14. N. G. de Bruijn. A namefree lambda calculus with facilities for internal deï¬nition of expressions and segments. Technical Report T.H.-Report 78-WSK-03, Department of Mathematics, Eindhoven University of Technology, 1978.
15. N. Dershowitz and J.-P. Jouannaud. Rewrite Systems. In J. van Leeuwen, editor, Handbook of Theoretical Computer Science: Formal Methods and Semantics, volume B. North-Holland, 1989.
16. R. Ennals and S. P. Jones. Optimistic evaluation: an adaptive evaluation strategy for non-strict programs. In C. Norris and J. James B. Fenwick, editors, Proceedings of the 8th ACM SIGPLAN International Conference on Functional Programming (ICFP-03), volume 38, 9 of ACM SIGPLAN Notices, pages 287â€“298. ACM Press, 2003.
17. M. FernaÂ´ndez and I. Mackie. Closed reductions in the Î»-calculus. In J. Flum and M. RodrÂ´Ä±guez-Artalejo, editors, Proceedings of Computer Science Logic (CSLâ€™99), number 1683 in Lecture Notes in Computer Sciences, pages 220â€“234. Springer-Verlag, 1999.
18. M. FernaÂ´ndez and I. Mackie. Director strings and explicit substitutions. WESTAPPâ€™01, Utrecht, 2001.
19. G. Gonthier, M. Abadi, and J.-J. LeÂ´vy. The geometry of optimal lambda reduction. In Proceedings of the 19th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 15â€“26, Albequerque, New Mexico, 1992.
20. B. GreÂ´goire and X. Leroy. A compiled implementation of strong reduction. In Proceedings of ICFPâ€™02, Pittsburgh, Pennsylvania, USA, 2002.

40 M. FernaÂ´ndez, I. Mackie and F.-R. Sinot
21. T. Hardin, L. Maranget, and B. Pagano. Functional runtime systems within the lambdasigma calculus. Journal of Functional Programming, 8(2):131â€“176, 1998.
22. J. Kennaway and M. Sleep. Director strings as combinators. ACM Transactions on Programming Languages and Systems, 10(4):602â€“626, 1988.
23. J. Lamping. An algorithm for optimal lambda calculus reductions. In Proceedings 17 th ACM Symposium on Principles of Programming Languages, pages 16â€“30, 1990.
24. F. Lang. Mode`les de la Î²-reÂ´duction pour les implantations. PhD thesis, EÂ´ cole Normale SupeÂ´rieure de Lyon, 1998.
25. J. L. Lawall and H. G. Mairson. Optimality and inefï¬ciency: What isnâ€™t a cost model of the lambda calculus? In International Conference on Functional Programming, pages 92â€“101, 1996.
26. P. Lescanne. From Î»Ïƒ to Î»Ï…: a journey through calculi of explicit substitutions. In Proceedings of the 21st ACM Symposium on Principles of Programming Languages (POPLâ€™94). ACM Press, 1994.
27. P. Lescanne and J. Rouyer-Degli. The calculus of explicit substitutions lambda-upsilon. Technical Report RR-2222, INRIA, 1995.
28. J.-J. LeÂ´vy. Optimal reductions in the lambda-calculus. In J. P. Seldin and J. R. Hindley, editors, To H. B. Curry: Essays in Combinatory Logic, Lambda Calculus and Formalism, pages 159â€“191. Academic Press, 1980.
29. P.-A. Mellie`s. Typed lambda-calculi with explicit substitutions may not terminate. In Proceedings of the 2nd International Conference on Typed Lambda Calculi and Applications, number 902 in Lecture Notes in Computer Science, pages 328â€“334. SpringerVerlag, 1995.
30. G. Nadathur. A ï¬ne-grained notation for lambda terms and its use in intensional operations. Journal of Functional and Logic Programming, 1999(2), 1999.
31. G. Nadathur. The suspension notation for lambda terms and its use in metalanguage implementations. In R. de Queiroz, L. C. Pereira, and E. H. Haeusler, editors, Electronic Notes in Theoretical Computer Science, volume 67. Elsevier, 2002.
32. M. Newman. On theories with a combinatorial deï¬nition of â€œequivalenceâ€. Annals of Mathematics, 43(2):223â€“243, 1942.
33. K. Rose. Explicit substitution - tutorial and survey, 1996. Lecture Series LS-96-3, BRICS, Dept. of Computer Science, University of Aarhus, Denmark.
34. B. Rosen. Tree-manipulating systems and Church-Rosser theorems. Journal of the ACM, 20(1):160â€“187, 1973.
35. F.-R. Sinot, M. FernaÂ´ndez, and I. Mackie. Efï¬cient reductions with director strings. In R. Nieuwenhuis, editor, Proceedings of Rewriting Techniques and Applications (RTAâ€™03), volume 2706 of Lecture Notes in Computer Science, pages 46â€“60. SpringerVerlag, 2003.
36. N. Yoshida. Optimal reduction in weak lambda-calculus with shared environments. Journal of Computer Software, 11(6):3â€“18, 1994.


String Searh
Graham A Stephen
1
Otober 1992
Tehnial Report TR-92-gas-01
Shool of Eletroni Engineering Siene
University College of North Wales
Dean Street, Bangor, Gwynedd, UK LL57 1UT.
(email: grahamsees.bangor.a.uk)
Copyright

Graham A Stephen 1992
All rights reserved. This doument may be reprodued freely for non-ommerial purposes
on the ondition that this opyright notie is not removed. The author asserts his moral
right to be identied as the originator of this work.
1
Supported by SERC researh grant GR/G51565
Contents
1 Introdution 1
2 The Problem 3
3 Overview of the String Searhing Algorithms 7
3.1 String Mathing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 String Distane Metris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.3 Approximate String Mathing . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.4 Longest Repeated Substring . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4 Desription of the Algorithms 23
4.1 String Mathing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.1.1 Brute Fore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.1.2 Knuth-Morris-Pratt . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.1.3 Boyer-Moore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.1.4 Boyer-Moore-Horspool . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.1.5 Sunday | Quik Searh, Maximal Shift, and Optimal Mismath . . . 35
4.1.6 Hume and Sunday | Tuned Boyer-Moore and Least Cost . . . . . . . 37
4.1.7 Harrison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.1.8 Karp-Rabin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2 String Distane and Longest Common Subsequene . . . . . . . . . . . . . . . 41
4.2.1 Wagner-Fisher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.2 Hirshberg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.2.3 Hunt-Szymanski . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.2.4 Masek-Paterson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.2.5 Ukkonen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.6 Heaviest Common Subsequene . . . . . . . . . . . . . . . . . . . . . . 62
4.3 Approximate String Mathing . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
4.3.1 Landau-Vishkin k-mismath . . . . . . . . . . . . . . . . . . . . . . . . 70
4.3.2 Landau-Vishkin k-dierene . . . . . . . . . . . . . . . . . . . . . . . . 75
4.4 Longest Repeated Substring . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.4.1 Brute Fore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.4.2 SuÆx Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5 Bak to the Original Problem 91
5.1 The System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
5.2 The Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
i
A String Nomenlature 97
B Asymptoti Notation 99
ii
List of Figures
4.1 Brute Fore string mathing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.2 Knuth-Morris-Pratt string mathing . . . . . . . . . . . . . . . . . . . . . . . 25
4.3 Initialisation of the next table (simplied) . . . . . . . . . . . . . . . . . . . . 27
4.4 Initialisation of the next table . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.5 Boyer-Moore string mathing . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.6 Initialisation of the shift table . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4.7 Boyer-Moore-Horspool string mathing . . . . . . . . . . . . . . . . . . . . . . 34
4.8 Sunday Quik Searh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.9 Karp-Rabin string mathing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.10 Wagner and Fisher string distane alulation . . . . . . . . . . . . . . . . . 43
4.11 Wagner and Fisher least ost trae omputation . . . . . . . . . . . . . . . . 44
4.12 Calulation of j ls(x; y) j . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.13 Redued spae alulation of j ls(x; y) j . . . . . . . . . . . . . . . . . . . . . . 46
4.14 Hirshberg alulation of ls(x; y) . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.15 Hunt-Szymanski alulation of j ls(x; y) j . . . . . . . . . . . . . . . . . . . . . 51
4.16 Hunt-Szymanski alulation of ls(x; y) . . . . . . . . . . . . . . . . . . . . . . 52
4.17 Masek-Paterson submatrix preproessing . . . . . . . . . . . . . . . . . . . . . 56
4.18 Masek-Paterson string distane alulation . . . . . . . . . . . . . . . . . . . . 57
4.19 Ukkonen string distane threshold test . . . . . . . . . . . . . . . . . . . . . . 61
4.20 Ukkonen string distane alulation . . . . . . . . . . . . . . . . . . . . . . . . 62
4.21 Robinson-Shensted lis alulation . . . . . . . . . . . . . . . . . . . . . . . . 64
4.22 Jaobson-Vo his alulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.23 Jaobson-Vo hs alulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.24 Jaobson-Vo hs alulation for position-independent weights . . . . . . . . . 69
4.25 Landau-Vishkin k-mismath string mathing . . . . . . . . . . . . . . . . . . 71
4.26 Proedure extend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.27 Proedure merge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.28 Landau-Vishkin pattern preproessing . . . . . . . . . . . . . . . . . . . . . . 75
4.29 Dynami programming alulation of r
p;q
values . . . . . . . . . . . . . . . . . 77
4.30 Stages in the onstrution of the suÆx tree for EWEW$ . . . . . . . . . . . . . . 83
4.31 Node implementation for EWEW$ suÆx tree . . . . . . . . . . . . . . . . . . . . 84
4.32 Node implementation for EWEW$ suÆx tree showing the suÆx links . . . . . . 85
4.33 Y
13
for string BBBBBABABBBAABBBBB$ . . . . . . . . . . . . . . . . . . . . . . . 87
4.34 SuÆx tree for string PABCQRABCSABTU$ . . . . . . . . . . . . . . . . . . . . . . 88
iii
iv
Chapter 1
Introdution
It ould be said of me that in this book I have only made a bunh of other men's owers,
providing of my own only the string that ties them together.
| Mihel de Montaigne (1533-1592), Essais.
This report is onerned with a string searhing problem whih has arisen in the development
of textual searh methods for a proposed information proessing system. The aim of the
report is not to attempt to provide a denitive solution for the problem, although some ideas
towards this end are presented, but rather to review existing string searhing algorithms
germane to the proessing system in general and, in partiular, to the spei problem. The
term string searhing is used here in a generi sense to enompass tehniques used to loate
partiular substrings, evaluate longest ommon subsequenes between two strings, ompute
the edit distane between two strings, and so on.
First of all, the problem itself is desribed | initially in a rather loose, informal manner, and
then a striter denition is onsidered.
A disussion of the development of string searhing algorithms is presented, giving an overview
of their apabilities and performane. This is then followed by a more detailed examination
of the prinipal algorithms themselves.
Finally, the impliations of these algorithms on the original problem are onsidered.
Denitions of some of the terms and notation used in this area may be found in the appendies.
1
2
Chapter 2
The Problem
Every string whih has one end also has another end.
| Finagle's First Fundamental Finding
(Robert Anton Wilson, Shrodinger's Cat Trilogy).
Stated rather loosely, the problem is this | nd the patterns whih our more than one in
a given sequene of symbols. A bit vague perhaps, so some further qualiation would appear
to be in order.
First of all, the sequene is textual and one-dimensional, but what about its omponent
symbols? Limiting ourselves to the printable ASCII haraters exluding whitespaes (ASCII
33-126), we ould take strings of these haraters to be the unitary symbols, and a whitespae-
separated list of these atomi symbols to be the sequene. This, however, leads to an innite
alphabet of symbols. It might therefore be simpler | it ertainly seems more intuitive | to
use a symbol alphabet omprising the previously mentioned ASCII haraters together with
the spae harater. The sequene of the problem now beomes a string over this (nite)
alphabet.
The next point to onsider is what exatly is meant by pattern. If the objetive is to identify
repeating patterns so that any inherent redundany in the sequene may be exploited via some
suitable oding sheme, in order to inrease the representational eÆieny, then it might be
appropriate to interpret pattern as substring. If, on the other hand, however, the intention is
to analyse the input sequene for syntatial strutures, suh as disontinuous dependenies,
then it might be more appropriate to treat patterns as subsequenes. In any event, sine a
substring is a speial ase of a subsequene, the latter shall, for generality, be adopted as the
pattern of the problem in this ontext.
It might, at this stage, be informative to pause to onsider the total number of patterns in
a sequene of length n, say. Firstly, onsider the ase of substrings. There are (n   k + 1)
substrings of length k from the sequene. The total number is therefore given by summing
3
over k, yielding
n
X
k=1
(n   k + 1) =
n
X
k=1
k =
(n+ 1)n
2
=
 
n
2
+ n

2
(2.1)
This shows that the total number of substrings is O
 
n
2

.
Turning our attention to the ase of subsequenes, we nd that the upper bound is exponential.
This is simply derived as follows. For a given subsequene, eah of the n symbols of the original
sequene may be in either of two states | namely inluded in the subsequene, or exluded
from it. It then follows that there will be a total of 2
n
subsequenes, inluding the empty
string, .
The next point to onsider is what exatly is meant by reurrene of a subsequene in the
sequene. Taking the approah that the reurring instanes of the pattern should be totally
distint, it shall be stipulated that only disjoint repeating patterns are to be onsidered. For
example, in the sequene A A B, the subsequene A B would not be ounted as reurring,
sine its instanes are not disjoint | they both share the same `B'.
Finally, having shown that the number of subsequenes in a sequene is O (2
n
), it is a simple
matter to show that, in the worst ase, there an be an exponential number of disjoint reur-
ring subsequenes. Consider, for example, the square, of length 2n, x
2
, where x omprises n
distint symbols. (For nite alphabets, n will be bounded by the ardinality of the alphabet.)
Sequene x
2
will then have 2
n
  1 reurring subsequenes, not ounting . There is, how-
ever, only one maximal reurring subsequene, namely x. This is illustrated in the following
example.
x = A B C x
2
= A B C A B C
The disjoint reurring subsequenes are A, B, C, A B, A C, B C, and A B C, whih are all
themselves subsequenes of the maximal reurring subsequene, A B C.
It would therefore seem sensible to impose the restrition of searhing only for maximal
disjoint reurring subsequenes.
Having ome thus far in the examination of the requirements, we are now in a position to
put forward a striter formulation of the problem. Before doing so, however, some of the
notational onventions employed in this doument shall be explained.
Firstly, a string x, of length, jx j = m, is given by x
1
x
2
: : : x
m
, where x
i
represents the i
th
omponent symbol of x.
The substring of x, x
i
x
i+1
: : :x
j
, where i; j 6 m and j > i, shall be denoted by x(i; j). For
the ase where i > j, the reversed substring shall be denoted expliitly by x
R
(i; j).
Example
x = trismegistus
jx j = 12
4
x(7; 10) = gist
x
R
(7; 4) = gems
Normally, x shall be used to denote a pattern, and y a text string of length n, where m 6 n.
Problem Denition
A = f ! " # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8
9 : ; < = > ?  A B C D E F G H I J K L M N O P
Q R S T U V W X Y Z [ \ ℄ ^ _ ` a b  d e f g h
i j k l m n o p q r s t u v w x y z { | } ~ g
C = A [ f` 'g
Alphabet Variant #1
Eah symbol is a non-empty string, or word, over A. The symbol alphabet, W , is
therefore innite and is given by the positive losure of A,
W = A
+
(2.2)
A whitespae harater shall be used as a separator for the omponent symbols
of a sequene, or sentene, of words. The set of all sentenes shall be denoted by
S
W
. This would be equal to W
+
(i.e. W itself) were it not for the requirement
that the words be separated by a symbol 62 A. In fat, S
W
is given by the positive
losure of C.
Alphabet Variant #2
Eah symbol is a harater from C. The symbol alphabet is therefore equal to C,
and a sequene shall be a non-empty string over C. The set of all sequenes, S
C
,
will be given by
S
C
= C
+
(2.3)
Problem
Given the sequene x 2 S, identify and loate maximal disjoint reurring subse-
quenes of x.
5
6
Chapter 3
Overview of the String Searhing
Algorithms
Attempt the end, and never stand to doubt;
Nothing's so hard, but searh will nd it out.
| Robert Herrik (1591-1674), Seek and Find.
The major string searhing problems whih have been addressed in the literature are the
string mathing problem, the string distane problem, the longest ommon subsequene prob-
lem, the approximate string mathing problems, and the longest repeated substring problem.
Denitions of these problems together with overviews of the various approahes developed to
solve them are presented below.
3.1 String Mathing
The String Mathing Problem is usually formulated as follows.
Given pattern string x, with j x j = m, and text string y, with j y j = n,
where m;n > 0 and m 6 n, if x ours as a substring of y then determine the
position within y of the rst ourrene of x, i.e. return the least value of i suh
that y(i; i + m   1) = x(1; m). The problem is sometimes extended suh that
the positions of all the ourrenes of x within y are to be found. Algorithms
satisfying the former requirement may be extended easily to satisfy the latter.
A reent survey and omparison of string mathing algorithms, inluding the reent on-
tributions by Sunday (1990), has been onduted by Pirklbauer (1992). A omprehensive
7
omparison of the time and spae omplexities of various algorithms is also provided by
Gonnet and Baeza-Yates (1991). Not to be negleted are the reviews by Aho (1990, 1980)
and Sedgewik (1983).
The intuitive, or Brute Fore, approah to the problem is simply to attempt to math x with
substrings from y at suessive positions in the text string. This requires the input text string
to be buered sine baktraking in the text is required in the event of an unsuessful math.
In the worst ase, this method runs in O(mn) time, but for pratial appliations, suh as
searhing English text, the expeted performane is O(m+ n).
A theorem about two-way deterministi pushdown automata, derived by Cook (1972), led to
the result that there exists an algorithm solving the string mathing problem in O(m + n)
time in the worst ase. In fat, Rivest (1977) demonstrated that, in the worst ase, any string
mathing algorithmmust examine at least n m+1 symbols from the text string. This shows
that no solution to the string mathing problem may have behaviour sublinear in n in the
worst ase.
In 1970 Knuth learned of Cook's theorem and from it derived a string mathing algorithm.
This was then modied by Pratt so that its running time was independent of the size of
the symbol alphabet. The resulting algorithm was also independently arrived at, without
knowledge of Cook's theorem, by Morris in 1969 for appliation in a text editor so as to avoid
having to baktrak in the text string. The operation of their algorithm (Knuth, Morris and
Pratt, 1977) is desribed briey below.
The basi idea behind the algorithm is to avoid baktraking in the text string in the event of a
mismath by taking advantage of known information. The text string is proessed sequentially
from left to right. When a substring math attempt fails at symbol x
j
, say, the previous j 1
text symbols will be known sine they will be given by x(1; j 1). This fat an be exploited to
determine how far to shift the pattern to the right for the next math attempt. An auxiliary
next table ontaining this shift information is omputed in O(m) time from the pattern before
searhing the string.
The worst ase performane ours for Fibonai pattern strings, for whih the algorithm
runs in O(m+n) time. Although the method performs well for highly self-repetitive patterns
and text, this situation ours only rarely in atual appliations. Thus, in pratie the
Knuth-Morris-Pratt algorithm is not likely to be signiantly faster than the Brute Fore
approah. It does, however, have the advantage of never having to baktrak in the input
text, obviating the need for involved buering shemes when reading from sequential text
streams for example.
Pirklbauer (1992) has disussed a variation of the Knuth-Morris-Pratt algorithm using right
to left omparison of the pattern and text, as in the Boyer-Moore approah desribed below.
An improved average ase performane was reported, but at the ost of sariing the stritly
sequential nature of proessing the input text.
A muh faster string searhing method was disovered in 1974 by Boyer and Moore, and also
independently by Gosper. The algorithm was later presented in a revised form (Boyer and
Moore, 1977) taking into aount suggestions from B. Kuiper, D.E. Knuth and R.W. Floyd.
8
The speed of the Boyer-Moore algorithm is ahieved by disregarding portions of the text
string whih annot possibly partiipate in a suessful math. For a large alphabet and
small pattern, the expeted number of symbol omparisons is about n=m, and is O(m+n) in
the worst ase. In this method, the pattern is sanned aross the text string from left to right,
but for eah trial position the symbol omparisons between pattern and text are performed
from right to left. Thus, the rst omparison is between x
m
and y
m
. If y
m
does not our
anywhere in x, then there an be no math for x starting anywhere in the rst m symbols of
y. The next potential pattern position would therefore be m + 1, and the next omparison
would be between x
m
and y
2m
. The atual pattern shift is determined by taking the larger of
the values from two preomputed auxiliary tables. One of these, the skip table, is indexed by
the oending text symbol ausing the mismath (the ourrene heuristi), whilst the other
is based on the next table of the Knuth-Morris-Pratt algorithm (the math heuristi). The
laried pattern preproessing for the Boyer-Moore algorithm desribed in the postsript of
Knuth, Morris and Pratt (1977) is atually inorret, but has been retied by Rytter (1980).
The worst ase number of symbol omparisons neessary to determine all the ourrenes
of the pattern in the text has been a matter of some interest. In these irumstanes, the
Boyer-Moore algorithm requires O(n + rm) time, where r is the number of mathes found.
Knuth, Morris and Pratt (1977) presented a modied version of the Boyer-Moore algorithm
requiring at most 6n omparisons for the ase where the pattern does not our in the text.
Guibas and Odlyzko (1977) improved this gure to 4n, and onjetured a tighter bound of
2n. Galil (1979) also put forward a variant of the algorithm, whih was improved upon by
Apostolio and Gianarlo (1986). Their version requires at most 2n symbol omparisons,
regardless of the number of ourrenes of the pattern in the text. This is at the ost of only
slightly inreased pattern preproessing overheads, whih are still linear in m. These methods
do not, however, provide any signiant improvement in the average ase.
Horspool (1980) has shown that a simplied form of the Boyer-Moore algorithm, employ-
ing only a single auxiliary table | indexed by the mismathing text symbols, results in a
performane omparable to that of the original version. In fat, Baeza-Yates (1989a) has
observed that, based on both analytial and empirial studies, of the known algorithms the
Boyer-Moore-Horspool method is the best in terms of average ase performane for nearly all
pattern lengths and alphabet sizes.
The average ase performane of the Boyer-Moore-Horspool and the Knuth-Morris-Pratt al-
gorithms, together with the Brute Fore approah, has been analysed by Baeza-Yates (1989b).
His analysis was for the ase of nding all the pattern ourrenes, and was based on random
strings (i.e. onatenations of symbols hosen independently from the alphabet with a uni-
form probability distribution) for the pattern and text. Empirially validated tight bounds
were derived for the expeted number of symbol omparisons involved in the searh methods.
A hybrid algorithm, ombining the Boyer-Moore and Knuth-Morris-Pratt approahes, was
proposed. In pratie, the hybrid performed slightly better than the Boyer-Moore-Horspool
method for searhes of English text.
For the Boyer-Moore approah, the average running time improves with larger alphabets or
longer patterns. Improved performane an therefore be obtained by inreasing the size of
the alphabet. This tehnique has been onsidered by Knuth, Morris and Pratt (1977) and
by Shabak (1988). Baeza-Yates (1989a) has proposed a simple alphabet transformation
9
whih leads to a pratial implementation of a Boyer-Moore-Horspool variant. Essentially,
this involves grouping k symbols of the pattern together as a supersymbol. This redues the
pattern size, jx j, to m  k + 1 and inreases the size of the alphabet to jC j
k
, where C is the
original alphabet. This approah has been shown to be of pratial use for small alphabets
or large patterns, and experimental inreases in speed of 50% have been reported (e.g. for
k = 2, jC j = 8 and random text).
Baeza-Yates (1989a) also put forward suggestions for improving the eÆieny of searhing
non-uniformly distributed texts, suh as English. He onsidered the optimal ordering of
pattern symbol omparisons, based on their probability distribution within the text. This
may be obtained either by preproessing the text or from a priori information about the text
soure. Rarer symbols ould, for example, be ompared before more frequent ones, thereby
inreasing the probability of an early detetion of a mismath.
Sunday (1990) has suggested using the symbol in the text immediately following the one that
aused a mismath to address the ourrene heuristi table of the Boyer-Moore algorithm.
Using this approah, he developed three variations, eah diering in the manner in whih
the order of the symbol omparisons between the pattern and the urrent text substring is
determined. In the Quik Searh algorithm, the omparisons are performed from left to right.
TheMaximal Shift algorithmorders the omparisons suh that the distane to the next pattern
position in the event of a mismath is maximised. Finally, the Optimal Mismath algorithm
uses the idea, disussed by Beaza-Yates, as mentioned above, to ompare statistially rarer
symbols rst. Smith (1991) has developed an adaptive version of Sunday's Optimal Mismath
tehnique with the advantage that it assumes no prior knowledge of the given text.
A family of algorithms based on the general struture of an optimised implementation of
Boyer and Moore's (1977) method, involving a fast skip loop, has been investigated by Hume
and Sunday (1991). Two new variants onforming to this struture have been developed |
the Tuned Boyer-Moore and the Least Cost algorithms.
An alternative approah to the string mathing problem involves the use of hashing funtions.
Suh a method to test for the ourrene of a pattern within a text string, but not to nd
its loation, was proposed by Harrison (1971). This tehnique is based on the omparison of
string signatures and requires the text to be preproessed. It would therefore be appliable
to the searhing of large stati text les, for example.
Karp and Rabin (1987) also put forward an algorithm involving the use of hashing, but one
whih does not require preproessing of the text to be searhed. Their approah is similar to
that of Brute Fore searhing, but rather than diretly omparing the m-symbol strings at
suessive text positions, their respetive signatures are ompared. This redues the task of
omparing two strings to the simpler one of omparing two integers. A arefully hosen hash
funtion allows the text substring signatures to be easily alulated in an inremental manner.
The algorithm is not restrited to string mathing and may be extended to multi-dimensional
pattern mathing, in image proessing appliations for example. In the worst ase, whih
is extremely unlikely, the algorithm takes O(mn) time, but the expeted performane is
O(m+ n).
10
The omparative performane of the various algorithms desribed above has been studied
both theoretially and empirially by many investigators (e.g. Horspool, 1980; Smit, 1982;
Baeza-Yates, 1989a; Davies and Bowsher, 1986; Pirklbauer, 1992). The overall onlusion of
these studies is that, for general appliations, the Boyer-Moore-Horspool is the best available
algorithm. As noted earlier, this is the reommendation given for almost all pattern lengths
and alphabet sizes by Baeza-Yates (1989a). In the extreme ases of very small patterns (i.e.
m 2 [1; 3℄) and pattern lengths ommensurate with the text length (i.e. m  n), the Brute
Fore approah is preferable. Horspool (1980) observed that his simplied variant gave an
average performane omparable to that of the Boyer-Moore algorithm, and outperformed a
Brute Fore method oded using speial purpose mahine instrutions for symbol searhing
when m > 6.
Despite its theoretial elegane, the non-baktraking Knuth-Morris-Pratt algorithm provides
no signiant speed advantage over the Brute Fore method in pratie. It may, however, be
a good hoie when jC j  n (Baeza-Yates, 1989a), or when dealing with binary strings, i.e.
when jC j = 2 (Davies and Bowsher, 1986). Although it may inspet text symbols more
than one and may baktrak in the input string, the Boyer-Moore approah provides, on the
whole, a signiantly faster searh method.
In terms of exeution time, the Karp-Rabin hashing tehnique fares badly in ompetition
with the other methods, but is extendible to higher dimensional problems. It may therefore
be used for pattern mathing in image proessing appliations, for example.
With regards to Sunday's variations of the Boyer-Moore approah, Pirklbauer (1992) reom-
mends the Sunday Quik Searh in addition to a simplied Boyer-Moore variant. However,
he advises against the use of the Maximal Shift and Optimal Mismath algorithms, on the
grounds of the resulting length of ode and exessive preproessing loads, espeially signif-
iant when searhing for only the rst ourrene of a pattern. Hume and Sunday (1991)
report a signiant improvement in performane over the original Boyer-Moore algorithm for
their new methods. Again, however, preproessing loads were not taken into aount in the
measurements.
Although providing a high performane, the degree to whih the Boyer-Moore approah has
been put into pratie may have been urbed to a ertain extent by oneptual diÆulties
in the preproessing, partiularly with the math heuristi. Horspool (1980) observed that
\many programmers may not believe that the Boyer and Moore algorithm (if they have heard
of it) is a truly pratial approah." Sedgewik (1983) also noted that \both the Knuth-
Morris-Pratt and the Boyer-Moore algorithms require some ompliated preproessing on the
pattern that is diÆult to understand and has limited the extent to whih they are used." In
fat, Morris disovered that after a few months his initial searh implementation in a text
editor, being too diÆult to understand by other systems programmers, had been ruined
by various gratuitous `xes' (Knuth, Morris and Pratt, 1977). Hume and Sunday (1991) add
that \partially beause the best algorithms presented in the literature are diÆult to understand
and to implement, knowledge of fast and pratial algorithms is not ommonplae." Arguing
for a more onsistent approah to algorithm development, and against \widespread haoti
algorithm presentation," Woude (1989) asserts that the Knuth-Morris-Pratt preproessing
should be no more diÆult to understand than the atual searh proedure itself. There
have also been some attempts of late to popularise the Boyer-Moore approah, partiularly
11
in its simplied form (e.g. Menio, 1989). As a step towards eluidating this eld, Hume and
Sunday (1991) have presented a means of lassifying the various string mathing algorithms
based on the Knuth-Morris-Pratt and Boyer-Moore approahes.
3.2 String Distane Metris
The notion of a distane funtion, or metri, rops up in many diverse elds, and is often
employed as a measure of the similarity between two vetors, or n-tuples. This allows the
degree of orrelation between two entities to be assessed, and, for example, has uses in pattern
reognition appliations suh as template mathing in image proessing. Formally, a distane
metri, d, is a funtion satisfying the following axioms.
Nonnegative Property d(x; y) > 0 8x; y (3.1)
Zero Property d(x; y) = 0 , x = y (3.2)
Symmetry d(x; y) = d(y; x) 8x; y (3.3)
Triangle Inequality d(x; z) 6 d(x; y) + d(y; z) 8x; y; z (3.4)
In order to evaluate the distanes between sequenes, a sequene of length n may be regarded
as a vetor in R
n
. For two vetors of the same dimensionality (sequenes of the same length),
with omponents of either disrete or ontinuous numerial values, ommon metris are the
hessboard distane (max
i
j x
i
  y
i
j), ity-blok distane (
P
i
j x
i
  y
i
j), and the familiar
Eulidean distane (

P
i
(x
i
  y
i
)
2

1=2
).
In string proessing appliations, however, it is often not appropriate to interpret the om-
ponent symbols as numerial values, although they are normally assigned suh for the on-
veniene of oding, and strings to be ompared are often of disparate length. The distane
metris employed in this area therefore tend to quantify the minimum ost of transforming
one string into the other. In general, ost weights may be assigned to the individual edit-
ing operations involved in suh a transformation, namely symbol substitution, insertion, and
deletion. Note that the latter two operations are sometimes referred to olletively as indels.
For two strings of equal length, the Hamming distane (Hamming, 1982) between them is
dened as the number of symbol positions at whih dier. This is equivalent to the minimum
ost of transforming the rst string into the seond, when only substitutions are permitted and
are given a unity weighting. When not restrited to omparing equal length strings, insertions
and deletions will also generally be required. Giving these the same weight as a substitution,
the minimum total transformation ost is equal to one of the two distane metris proposed
by Levenshtein (1965). His other metri is equal to the minimum transformation ost when
only indels are permitted. This is equivalent to assigning a ost of 1 to indels and 2 to
substitutions, as the latter may be implemented by an insertion-deletion pair. The former
metri shall heneforth be referred to as the Levenshtein distane, and the latter as the edit
distane.
12
The use of suh metris as similarity measures between strings nds appliation in many
dierent areas, suh as quantifying degrees of homology in geneti sequenes, for example.
Note, however, that not all string distane funtions are neessarily metris. For example,
when the ost of editing operations is a funtion of the partiular symbols involved, the
triangle inequality may not neessarily be satised. A omprehensive disussion of a host of
appliation milieux, from moleular biology to speeh proessing, employing string distane
funtions is given by Sanko and Kruskall (1983).
The remainder of this setion gives an overview of the major algorithms developed to solve
both the string distane problem, sometimes given under the guise of the le dierene prob-
lem or the string-to-string orretion problem, and also the related longest ommon subse-
quene problem. A reent review of these problems may be found in Aho (1990), and an
overview of omplexity results for ommon subsequene problems is presented by Hirshberg
(1983). Again, a more detailed examination of the prinipal algorithms has been deferred
until the next hapter. Before going any further, however, denitions of the problems shall
rst be given.
String Distane Problem
Given strings x and y, with j x j = m; j y j = n, where m;n > 0, and a
string distane metri, d, nd d(x; y). When given as the le dierene problem,
x and y represent two les, where x
i
is the i
th
line of x and y
j
is the j
th
line of y.
The requirement is then to determine a minimal sequene of editing operations
neessary to transform x to y.
Longest Common Subsequene Problem
A subsequene of a string is obtained be eliminating zero or more symbols, not
neessarily ontiguous, from the string. A ommon subsequene of two strings
is a string whih ours as a subsequene of both strings. A longest ommon
subsequene of two strings is a ommon subsequene of the strings having maximal
length, i.e. it is at least as long as any other ommon subsequene of the strings.
Given two strings x and y, with jx j = m; jy j = n andm;n > 0, nd j ls(x; y) j,
where ls(x; y) is a longest ommon subsequene of x and y. It is sometimes also
required to determine an ls(x; y) in addition to its length.
One method of evaluating the distane between two strings, based on a dynami programming
(Bellman and Dreyfus, 1962) tehnique, has been disovered independently by many dierent
investigators. Sanko and Kruskall (1983) redit the following authors with proedures all
based on the same underlying priniple | Vintsyuk (1968); Needleman and Wunsh (1970);
Velihko and Zagoruyko (1970); Sakoe and Chiba (1970, 1971); Sanko (1972); Reihert,
Cohen and Wong (1973); Haton (1973); Wagner and Fisher (1974); and Hirshberg (1975).
The basi idea of the dynami programming method is to evaluate suessively the distane
between longer and longer prexes of the two strings until the nal result is obtained. These
partial results are omputed iteratively and are entered in an (m+1) (n+1) array, leading
13
to O(mn) time and spae omplexities. In Wagner and Fisher's (1974) method, an ls may
then be derived in a straightforwardmanner from the ompleted distane array via a struture
known as a trae.
Hirshberg (1975) modied Wagner and Fisher's tehnique, preserving the O(mn) time om-
plexity but reduing the spae requirement from a quadrati to a linear harateristi. In this
method it is the length of the ls of progressively longer string prexes, rather than the dis-
tane between them, that is omputed iteratively. The saving in spae is eeted by holding
only two rows of the dynami programming array in memory at any given time. This loss
of the full array, however, leads to a more ompliated method of extrating an ls of the
two strings. The tehnique employed is to biset reursively the rst string and obtain an
ls between eah half and an appropriate substring of the seond string. The reursion is
performed until this ls extration beomes trivial. The nal ls of the omplete strings is
then obtained by onatenating those obtained for the reursively slied string setions.
Hunt and MIlroy (1976) implemented Hirshberg's ls algorithm for the unix diff ommand.
This was found to work well for short inputs, but its quadrati time omplexity resulted in
signiant degradation in exeution speed as the lengths of the input les were inreased.
However, MIlroy later obtained a dramati improvement in performane by employing the
ls algorithm developed by Hunt and Szymanski (1977).
Hunt and Szymanski's approah to extrating an ls from two strings is equivalent to deter-
mining the longest monotonially inreasing path in the graph omposed of nodes (i; j) suh
that x
i
= y
j
. Whereas previous methods required quadrati time in all ases, their algorithm
requires O((r+n) logn) time for equal length strings, where r is the total number of ordered
pairs of positions at whih the two strings math. In the worst ase, every element of x ould
math every symbol in y, resulting in n
2
suh ordered pairs and a omplexity of O(n
2
logn).
For many appliations, however, suh as the le dierene problem where eah line of a le
is taken as an atomi symbol, r an be expeted to be lose to n, giving a performane of
O(n logn) | a signiant improvement over the quadrati time proedures.
Masek and Paterson (1980, 1983) developed a string distane algorithm based on Wagner
and Fisher's dynami programming approah whih has a sub-quadrati time omplexity,
even in the worst ase. The method does, however, require the alphabet to be nite and the
ost weights to be integral multiples of a xed real number. Their algorithm uses the `Four
Russians' tehnique (Arlazarov, Dini, Kronod and Faradzev, 1970), and has a worst ase
exeution time of O(n
2
= logn) for equal length input strings. The basi idea of this method
is to partition the distane matrix into a number of smaller submatries. The lower and
rightmost edges of eah submatrix may then be alulated from those of adjaent submatries,
above and to the left of the urrent one, and from the appropriate substrings of x and y using a
preomputed lookup table. Although asymptotially faster than the straightforward dynami
programming method, Masek and Paterson (1983) point out that their algorithm should only
be used for long strings, and, for example, is atually faster to ompute the edit distane
between two binary strings only one the string lengths exeed 262418.
Szymanski has shown that alphabets of unrestrited size may be aommodated by a modied
version of Masek and Paterson's algorithm, taking timeO(n
2
(log logn)
2
= logn) | a behaviour
still asymptotially better than quadrati (Hirshberg, 1983).
14
An alternative ls algorithm, having a favourable performane for the ase of similar strings,
has been proposed independently by Ukkonen (1983, 1985a) and Myers (1986). The underly-
ing priniple of their methods is to searh for a minimum-ost path in a graph. This graph is
omposed of the verties of the distane matrix together with horizontal, vertial and diagonal
edges orresponding to the dependenies between adjaent nodes. The required path in the
graph starts at (0; 0) and ends on (m;n).
In Ukkonen's method, unneessary alulations in the distane matrix are avoided in a thresh-
old test of the distane between the two strings. This proess is repeated with suessively
larger threshold values until the test is suessful, yielding the required string distane in
O(md) time. An atual editing sequene may then be reovered from the omputed distane
matrix values as in the Wagner-Fisher algorithm.
Myers' approah is to nd the endpoints of suessively longer furthest-reahing paths in
the graphs until point (m;n) is reahed. In the worst ase, this takes O((m+ n)d) time, but
Myers has shown that the expeted ase temporal omplexity is O(m+n+d
2
). A linear spae
version of the algorithm and a variation with a worst ase time of O((m+n) log(m+n) + d
2
)
have also been developed. The latter is based on suÆx tree tehniques (MCreight, 1976)
and its improved asymptoti behaviour is of theoretial interest. It is not, however, a viable
tehnique owing to the omplex methods involved in its implementation.
Apostolio and Guerra (1987) have analysed both the Hirshberg and the Hunt-Szymanski
methods of nding a longest ommon subsequene, and have devised ognate algorithms with
ertain improvements. For ases where m  n, their version of Hirshberg's algorithm will
be faster than the original, as their version has a time omplexity of O(m
2
 minflog j C j;
logm; log(2n=m)g), where C is the alphabet. As noted above, Hunt and Szymanski's algo-
rithm beomes worse than quadrati in the pessimal ase. However, Apostolio and Guerra
have limited the worst ase behaviour to a quadrati harateristi. The atual time bound of
their revised version of the algorithm is O(m logn+ t log(2mn=t)), where t 6 r is the number
of dominant mathes between the two strings.
The omplexity of the ls problem, for partiular omputational models, has been the subjet
of some investigation. Wong and Chandra (1976) have derived bounds for the alulation
of string distane in terms of the editing ost funtion when the symbol omparisons are
restrited to only tests of equality. They have also shown that 
(mn) symbol omparisons
are required to nd an ls under this restrition.
Using a similar `equal-unequal' deision tree model of omputation, Aho, Hirshberg and
Ullman (1976) derived bounds for the ls problem in terms of the size of the symbol alphabet.
They found that, in general, the worst ase minimum number of omparisons required is
O(n
2
) for equal length strings, and is O(n  jC j) for n > jC j, where C is the alphabet.
When a lexiographi symbol ordering is employed, allowing `less than-equal to-greater than'
symbol omparisons to be performed, Hirshberg (1978) has shown that the minimumnumber
of suh omparisons required is O(n logn).
To sum up, the salient points onerning the performane of the various approahes to the
related string distane and ls problems are as follows. The dynami programming method
15
of Wagner and Fisher is simple to implement, but is quadrati in both time and spae.
For large appliations, Hirshberg's linear spae version is therefore preferable. However, the
Hunt-Szymanski algorithm has a signiantly better performane for many appliations, at
the ost of a worse than quadrati temporal omplexity in the worst ase. The only known
algorithm requiring sub-quadrati time in the worst ase is that of Masek and Paterson. The
additional omputational overheads of this method, however, make it more eÆient than the
straightforward dynami programming tehnique only for very long strings. For the ase
where the two strings are similar, i.e. when d is small, the methods of Ukkonen or Myers
beome attrative.
Extended Problems
A related problem, often arising in moleular biology, is that of determining whih portions
of two sequenes are similar, or homologous. This problem is disussed by Sanko and
Kruskall (1983), who expand upon Smith and Waterman's (1981) method of nding suh loal
alignments. In addition to minimising the distane between portions of the sequenes, some
means of maximising the portion length is also required, sine a minimumdistane of zero will
always be possible for trivial portions. A variation of the dynami programming method is
therefore used in whih mathes are rewarded and substitutions and indels penalised. This is
ahieved by assigning a positive ost to the former and negative ones to the latter operations.
However, the quadrati nature of the dynami programming method makes it impratial for
use with very large sequene databases. Consequently, various heuristi methods have been
developed for this problem, suh as those of Lipman and Pearson (1985) and Altshul et al.
(1990).
The problem of determining the distane between two strings when transpositions, or swaps,
of adjaent symbols are permitted as editing operations in addition to indels and substitutions
has also been studied. This ase, for example, allows transposition errors ourring during
manual keyboard operation to be taken into aount when alulating string distanes. De-
spite the signiant inrease in the diÆulty of analysing this situation, the problem is still
soluble in quadrati time using an extension of the dynami programming method (Lowrane
and Wagner, 1975; Wagner, 1975, 1983).
A generalisation of the ls problem is the N -ls problem, where it is required to determine a
longest subsequene ommon to a set of N strings. Denoting the fat that x is a subsequene
of y by x < y, the problem may be stated formally as follows. Given a set of N strings,
X = fx
1
; x
2
; : : : ; x
N
g, nd N -ls(X), where N -ls(X) is a maximal length sequene suh
that N -ls(X) < x
i
for integer i 2 [1; N ℄. Under this onvention, the ls problem disussed
previously would be known as the 2-ls problem.
For arbitrary N , Maier (1978) has shown that the problem of determining jN -lsj is NP-
omplete (Karp, 1972) for alphabets of size two and above. The extration of anN -ls for suh
purposes as data ompression is therefore not viable, as any means for so doing will inevitably
be intratable (at least as long as there remains no known polynomial-time solution to the
lass of NP-omplete problems). For example, applying the dynami programming approah
to the N -ls problem would require O(n
N
) time and spae for strings of length n.
16
For ases where it is required to solve the N -ls problem for onstant N and strings of length
n, Baeza-Yates (1991) has proposed a method whih still requires O(n
N
) worst ase time and
spae, but has a more favourable average ase behaviour. His approah is to build a partial
deterministi nite automaton, known as a Direted Ayli Subsequene Graph (DASG),
whih an reognise all the subsequenes of the strings in the set X . As the graph is being
onstruted, its edges are labelled with the number of strings sharing that transition. The
ls's of X may then be reovered by searhing the graph for maximal sequenes of edges
labelled with N .
The DASG an be onstruted in O(N jC j n logn) time and O((N + jC j)n) spae. We shall
denote the number of mathed points between the N strings by r, whih is pessimally equal
to, but on average small in omparison to, n
N
. Traversing the graph may be aomplished in
O(r) time, and at most O(r) spae is required to obtain the set of ls's of the strings. The
temporal and spatial omplexities of the overall proedure are thus O(r+N jC j n logn) and
O(r + (N + jC j)n), respetively. This time bound improves over that of a method due to
Hsu and Du (1984) whih has similar omplexities.
Maier (1978) has also addressed the N -ss problem. This is the problem of nding the
shortest ommon supersequene of a set of N strings. If x < y, then y is said to be a
supersequene of x. The N -ss problem may thus be stated as follows. Given a set of N
strings, X = fx
1
; x
2
; : : : ; x
N
g, nd N -ss(X), where N -ss(X) is a minimal length sequene
suh that N -ss(X) > x
i
for integer i 2 [1; N ℄. Maier found that, for alphabet sizes > 5, the
problem of nding jN -ssj is also NP-omplete, and has onjetured that this result may also
hold for alphabet sizes > 3.
The problem of obtaining a heaviest ommon subsequene (hs) of two strings has been
examined by Jaobson and Vo (1992). An hs(x; y) is a subsequene ommon to x and y
having a maximal sum of weights for its omponent symbols for a given weight funtion. In
general, this funtion may depend on both the symbols themselves and also on their positions
in the original strings.
The determination of an hs nds appliation in, for example, the updating of CRT sreens.
The line indels required to transform one sreen into another may be derived from an ls of
the two sreens. In the interests of minimising sreen disruption, however, it is preferable
to give preedene to losely aligned pairs of mathed lines. A minimal distane weight
funtion, whih diminishes as the distane between the positions in their respetive sequenes
of mathing symbols inreases, may therefore be employed to obtain an appropriate hs. The
omputation of an hs may be performed using an adaptation of the dynami programming ls
algorithm, whih has been used, for example, in the urses sreen update library distributed
with System V unix (Vo, 1986).
Jaobson and Vo (1992) present an hs algorithm derived from one to ompute a heaviest
inreasing subsequene (his). This, in turn, was derived from the Robinson-Shensted (Shen-
sted, 1961) method of omputing a longest inreasing subsequene (lis). An lis is a maximal
string subsequene whose omponents are stritly inreasing, given some linear ordering of
the alphabet, and an his is an inreasing subsequene with maximal weight. Their algorithm
is a generalisation of the Hunt-Szymanski method, and has the same temporal omplexity.
For position-independent weight funtions, a speialised form of the hs proedure beomes
17
a generalisation of the Apostolio-Guerra ls algorithm.
3.3 Approximate String Mathing
A generalisation of the string mathing problem, whih involves nding substrings of a text
string lose to a given pattern string, is the Approximate String Mathing Problem. This
variation of the problem is important when errors are being taken into onsideration, and, for
example, nds appliation in the eld of moleular biology (see Landau, Vishkin and Nussinov
(1985) for a disussion of some suh appliations). The approximate string mathing problem
may be stated as follows.
Given a pattern string x, with jx j= m, text string y, with jy j= n, where m;n > 0
and m 6 n, an integer k > 0 and a distane funtion d, nd all the substrings, s,
of y suh that d(x; s) 6 k.
The task is thus to determine all the text substrings having a distane of at most k from the
pattern, for the given distane funtion. When d is the Hamming distane the problem is
known as string mathing with k mismathes, and when d is the Levenshtein distane it is
known as string mathing with k dierenes (or sometimes errors).
A omprehensive overview of approximate string mathing algorithms is presented by Galil
and Gianarlo (1988), and a omparison of the omplexities of ertain algorithms is also
provided by Gonnet and Baeza-Yates (1991).
The O(mn) time brute fore string mathing algorithmmay be adapted easily to ater for the
k-mismath problem by allowing up to k mismathes in the omparisons at eah substring
position within the text. However, as with the string mathing problem, various more eÆient
approahes to both the k-mismath and the k-dierene problems have been devised.
Landau and Vishkin (1985, 1986a) have developed an algorithm for the k-mismath problem
running in O(k(m logm+n)) time. Their approah is similar to the Knuth-Morris-Pratt string
mathing tehnique in that a table derived from preproessing of the pattern is employed as
the text string is examined from left to right, and known information is exploited in order to
redue the number of symbol omparisons required.
Landau and Vishkin (1985, 1986a) also remark that a k-mismath algorithm due to Ivanov
(1984), whih has a temporal omplexity of O(f(k)(m + n)), is faster than their algorithm
for the ase of very small k and omparable values of m and n. However, they highlight the
abstruse nature of the funtion f and have established that it is bounded from below by 2
k
.
Exepting the above ase, their algorithm is thus faster than Ivanov's, and furthermore has
the appeal of its simpliity | they state that Ivanov's algorithm required in exess of forty
journal pages to desribe.
A variant of Landau and Vishkin's k-mismath algorithmwith improved pattern preproessing
has been developed by Galil and Gianarlo (1986, 1988). The overall time omplexity of their
version is O(m logm+ kn).
18
Sellers (1980) has shown how the dynami programming method of omputing the distane
between two strings may be adapted to solve the k-dierene problem. One a variation of
the distane array has been evaluated, if, and only if, there is some value in the last row, d
m;j
say, suh that d
m;j
6 k, then there is a substring, s, of y suh that d(x; s) 6 k. The atual
substring may then be reovered by traing bak through the dependeny graph. The time
required for this method is thus O(mn).
A k-dierene algorithm proposed by Ukkonen (1985b) omprises a preproessing and a text
analysis stage. A deterministi nite-state automaton is onstruted in the former stage,
whih is then used to san the input text in the latter in O(n) time. However, this algorithm
is not, in general, pratiable owing to its exponential requirements for preproessing time
and spae. (For alphabet C, the pattern analysis requires O(m jC j minf3
m
; 2
k
jC j
k
m
k+1
g)
time and O((jC j + m)minf3
m
; 2
k
jC j
k
m
k+1
g) spae.)
Landau and Vishkin have developed several k-dierene algorithms based on Ukkonen's (1983,
1985a) eÆient, O(md) time method of omputing inter-string distanes. Preproessing of
the pattern in these algorithms permits known information to be exploited in the subsequent
analysis of the text. Their (1985) algorithm suessively tests text positions for the ourrene
| with up to k dierenes | of the pattern, and runs in O(m
2
+ k
2
n) time and requires
O(m
2
) spae. The performane of this method was later (1988) improved so as to require
O(m) spae and O(k
2
n) time for a xed size alphabet and O(m logm + k
2
n) time for un-
bounded alphabets. The improved pattern preproessing of this method involves a suÆx tree
(onstruted using Weiner's (1973) algorithm), and the text analysis stage involves nding
lowest ommon anestors (LCA) (Harel and Tarjan, 1984; Shieber and Vishkin, 1988) in the
suÆx tree. Landau and Vishkin (1986b, 1989) also disovered another k-dierene algorithm
involving the use of a suÆx tree as a by-produt of developing a parallel algorithm. The
basis of this approah is to ompute eÆiently the dynami programming string distane ma-
trix, and the algorithm loates text positions at whih k-dierene ourrenes of the pattern
end. This method takes O(kn) time for a xed size alphabet and O(n logm + kn) time for
unbounded alphabets.
Galil and Gianarlo (1988) disuss how Masek and Paterson's (1980, 1983) string distane al-
gorithmmay be adapted in order to solve the k-dierene problem, resulting in anO(mn= logn)
time omputation. Reall that this approah requires the alphabet to be nite.
Taking stok of the k-dierene algorithms mentioned above leads to the following obser-
vations. Although the exponential requirements of Ukkonen's method prelude its general
appliation, it ould oneivably be used for very small patterns or for small alphabets and
very small values of k. For xed size alphabets, it may be seen that if k is 
(m= logn), then
the variation of Masek and Paterson's algorithm is asymptotially the fastest of the above
methods. (The omment made previously onerning the performane of the Masek-Paterson
algorithm | namely that it is atually faster than the straightforward dynami program-
ming method only for very large strings | should, however, be borne in mind.) If, on the
other hand, k is O(m= logn), then Landau and Vishkin's (1986b,1989) algorithm beomes
the method of hoie. For the ase of unbounded alphabets, their (1988) algorithm beomes
faster if k is O(log
1
2
m). However, the (1986b, 1989) method does have the advantage of being
a signiantly simpler algorithm, and is, of ourse, superior if k is 
(log
1
2
m). Note, how-
19
ever, that for large k, i.e. when k is (m), the performane beomes no better than Sellers'
adaptation of the straightforward, quadrati time, dynami programming approah.
One area in whih approximate mathing is relevant is the modelling of geneti sequenes.
DNA sequenes are, for example, sometimes modelled as sequenes of independent and iden-
tially distributed (i.i.d.) symbols from the alphabet fA, C, G, Tg. The omponents of this
alphabet represent ourrenes of the nuleotides adenine, ytosine, guanine and thymine, re-
spetively. Similarly, proteins are sometimes modelled as i.i.d. sequenes of amino aids (e.g.
Sibbald and White, 1987). The distribution of the length of the longest ommon substring of
two independent random sequenes has been examined by relating it to that of the longest
run of heads in a sequene of tosses of a oin. Theoretial results have been obtained for
exat mathing (Arratia and Waterman, 1985a) and also for the ases of k-mismathes and
k-dierenes (Arratia and Waterman, 1985b; Arratia, Gordon and Waterman, 1986).
A related string mathing problem is that involving don't are, or wild ard, symbols whih
math any single symbol, inluding another don't are. Pinter (1985) has addressed this
problem for the ase where only the pattern string ontains don't ares. His algorithm is an
extension of the Aho-Corasik (1975) string mathing proedure. The latter is a generalisation
of the Knuth-Morris-Pratt algorithm, and searhes for multiple patterns in the text string in
O(m+ n) time, where m is the sum of the lengths of the pattern strings. The preproessing
stage of this algorithm onstruts in O(m) time a pattern mathing automaton, similar to a
deterministi nite automaton but having two transition funtions, from the set of patterns.
This is then used to san the text and searh simultaneously for ourrenes of the pattern
strings in O(n) time. The linearity of the method is preserved in Pinter's algorithm provided
that the elements of an array of aumulators may be inremented in parallel; otherwise the
temporal omplexity beomes quadrati.
The ase where both the pattern and text strings may ontain don't are symbols has been
examined by Fisher and Paterson (1974). For strings over a nite alphabet, C, they have
demonstrated that this problem is equivalent to integer multipliation, and using Shonhage-
Strassen (1971; see also Aho, Hoproft and Ullman, 1974) multipliation have derived an
indiret algorithm running in O(n log
2
m log logm log j C j) time. The onstant term of
this omplexity expression is not, however, inonsiderable, and the algorithm is thus only of
pratial import for very long strings. For the ase of unbounded alphabets, there is as yet
no known algorithm with performane superior to that of the quadrati brute fore approah
to string mathing.
Speial-purpose hardware
Various speial-purpose hardware designs for string mathing appliations, suh as text re-
trieval from large, unformatted doument sets, have been developed. These have employed
suh tehniques as those involving ellular arrays (Hollaar, 1979; Foster and Kung, 1980;
Mukhopadhyay, 1980; Curry and Mukhopadhyay, 1983; Halaas, 1983; Lee and Mak, 1989),
assoiative memories (Burkowski, 1982; Lee and Lohovsky, 1985), nite state automata (Hol-
laar, 1979; Haskin, 1981; Robert, 1982; Haskin and Hollaar, 1983), and dynami programming
(Hall and Dowling, 1980; Salton, 1980; Yianilos, 1983).
20
One partiular CMOS VLSI devie, developed fairly reently by NEC (Yamada et al., 1987;
Hirata et al., 1988), ombines nite state automaton (FSA) logi with a ontent addressable
memory (CAM), and allows approximate string mathing operations to be performed. A 512
harater (16 bit) CAM may be ongured to ontain from 64 8-harater pattern strings to
1 512-harater pattern string. Serial input data, at a rate of 10 million haraters/s, is fed
through the CAM, whih performs parallel mathing and passes math signals to the FSA
logi. In addition to exat string mathing, an approximatemathingmode allows ourrenes
of patterns with up to 1 dierene to be found. There is also a variable length don't are
mode, whih searhes for strings starting with a given prex, ending with a given suÆx, and
having variable length intermediate setions. Finally, a mask mode permits wild ards in the
input stream to be supported, whih would, for example, allow unreognised haraters from
an OCR unit to be oded and to math any symbol in a pattern string.
3.4 Longest Repeated Substring
The problem of nding the longest substring whih ours more than one in a given string
is known as the Longest Repeated Substring problem. This may be formulated as follows.
Given string y, with j y j= n, where n > 0, identify and loate the longest
substring, x, ourring more than one in y. A longest repeated substring is thus
the longest maximal length string, x, suh that y = uxvxw for some strings u, v
and w with ju j> 0, jv j> 0 and jw j> 0.
A maximal repeated substring is one having at least two distint mathing ourrenes in the
given string, where the math may be extended no further in either diretion. As an example,
onsider the string PABCQRABCSABTU. Substrings A, B, C, AB, BC and ABC are all repeated in
the original string. Strings AB and ABC are maximal repeated substrings, and ABC is thus the
longest repeated substring.
A brute fore approah to the problem, requiring quadrati time, is as follows. An n  n
math matrix,M , is onstruted suh that M
i;j
= 1 if y
i
= y
j
, otherwise M
i;j
= 0. With the
exeption of the main diagonal, ontiguous diagonals of `1's in the matrix represent maximal
reurring substrings, the longest of whih is thus the longest repeated substring. Note that
the matrix is symmetrial about the main diagonal, and thus only the upper half, say, (i.e.
elements M
1:::n 1;i+1:::n
) need be evaluated and proessed.
However, the longest repeated substring may be found in linear time using one of several
variations of the data struture proposed by Weiner (1973), whih omprises a ompat index
to all distint substrings of a string. This index has been studied widely under the following
guises:
Patriia trees (Morrison, 1968)
Position trees (Aho, Hoproft and Ullman, 1974; Majster and Reiser, 1980)
21
SuÆx trees (MCreight, 1976; Rodeh, Pratt and Even, 1981; Crohemore, 1986)
Complete inverted les (Blumer et al., 1984a, 1984b, 1985, 1987)
Subword trees (Apostolio, 1985)
An early, impliit manifestation of the index is to be found in Morrison's Patriia tree stru-
ture. It was, however, Weiner who pioneered the expliit index, for whih a lean onstrution
is presented by Chen and Seiferas (1985). Majster and Reiser's (1980) on-line onstrution
allows the index to be built as the input string is read in a symbol at a time from left to
right. Apostolio (1985) details several appliations of the index (in the form of MCreight's
suÆx tree), suh as nding longest ommon substrings and algorithms to perform linear-time
sequential data ompression. The basi idea of the latter is to interleave the onstrution of
a suÆx tree with the parsing of the text string into phrases. Rodeh, Pratt and Even (1981)
have, for example, implemented the optimal universal data ompression methods of Ziv and
Lempel (1977) using suÆx trees.
The onstrution of suÆx trees and their appliation to this problem are desribed in more
detail in the following hapter.
Finally, a variation of the problem involves nding all the maximal repeated substrings of
a given string. Again, the quadrati time, brute fore approah may be brought to bear on
this problem. The use of suÆx trees in solving this variation has, however, been investigated
by Baker (1992). She has developed a software visualisation tool for nding ourrenes of
dupliated or related setions of ode in large software systems. Lines of ode, stripped of
omments and white spae, are taken to be the omponent string symbols, and a threshold on
the minimumlength of repeated substrings may be set in order to lter out trivial dupliations.
The method is based on MCreight's suÆx tree onstrution, and the overall algorithm runs
in O(n+r) time, where r is the number of pairs of maximal repeated substrings found (whih
is normally small in omparison with n).
22
Chapter 4
Desription of the Algorithms
Errors, like straws, upon the surfae ow;
He would searh for pearls must dive below.
| John Dryden (1631-1700), All for Love, Prologue.
This hapter is devoted to a more in-depth disussion of the major algorithms outlined in the
overview of the previous hapter.
4.1 String Mathing
4.1.1 Brute Fore
In the Brute Fore approah, x is ompared with substrings y(i; i + m   1) at suessive
positions, i, in the text string (for 1 6 i 6 n  m + 1), until a math is found or the end of
the text is reahed.
As a maximum of m symbol omparisons may be performed for eah position i in y, a total of
m(n m+ 1) omparisons are required in the worst ase. For this maximum to be inurred,
the substring omparison for eah position i must either rst fail on the last (m
th
) symbol
omparison, or in the ase of the nal position (i = n m+1) alternatively provide a suessful
math. This situation arises in the following example.
x = EEEEW
y = EEEEEEEEEEEEEEEEEEEEEEW
However, this type of ase is likely to our but seldom in pratial appliations of text
searhing. The expeted performane is therefore about O(m + n), sine the majority of
23
i = 1 | initialise y index
j = 1 | initialise x index
while (i 6 n) and (j 6 m)
if x
j
= y
i
i = i+ 1
j = j + 1
else
i = i  j + 2
j = 1
if j > m | hek for suessful math
i = i m
else
i = 0
Figure 4.1: Brute Fore string mathing
the unsuessful substring mathing attempts are likely to fail very early on. The atual
performane will, of ourse, depend on the statistial parameters of the partiular pattern
and text involved.
An implementation of the Brute Fore string mathing algorithm is given in the pseudoode
of Figure 4.1. The pattern and text indies are initially both set to point to the start of
their respetive strings, and are both inremented as long as the symbols to whih they point
math. In the event of a symbol mismath, j is reset to point to the start of the pattern,
and i to the next potential pattern position in the text. If a math is found, then i is set to
the starting position in y of the pattern, otherwise it is reset to zero to indiate that the text
searh has been unsuessful.
4.1.2 Knuth-Morris-Pratt
The basi method of the Knuth-Morris-Pratt algorithm is similar to that of the Brute Fore
approah, exept that the text index, i, is never deremented. This is ahieved by shifting
pattern x forward relative to text y, in the event of a mismath, by an amount dependent on
both the struture of the pattern and the position, j, within x at whih the mismath ourred.
This shift information is obtained from an m-entry auxiliary table, next, preomputed from
the pattern before searhing the text.
The searh algorithm is given in Figure 4.2. In the event of a mismath between the text and
the pattern at positions i and j respetively, then it is known that the previous j   1 text
symbols, i.e. substring y(i   j + 1; i  1), math the rst j   1 pattern symbols, i.e. prex
x(1; j   1). This information is exploited by the use of the next table, whih ontains values
for the next pattern position, next
j
, to be used after a mismath at pattern position j.
In the inner loop j is set to next
j
, whih is equivalent to shifting the pattern j   next
j
24
i = 1 | initialise y index
j = 1 | initialise x index
while (i 6 n) and (j 6 m)
while (j > 0) and (x
j
6= y
i
)
j = next
j
i = i+ 1
j = j + 1
if j > m
i = i m
else
i = 0
NOTE that the and in the inner loop is a `onditional and,' whih only evaluates its right
hand operand if its left hand one evaluates to true.
Figure 4.2: Knuth-Morris-Pratt string mathing
symbol positions to the right relative to the text. This is performed repeatedly until either
j = 0 or x
j
= y
i
. In the former ase, none of x mathes the urrent text substring; in the
latter, the prex x(1; j) mathes the substring y(i  j + 1; i). The outer loop then resumes
by inrementing the text and pattern indies. If a omplete math is found, then i is set to
the starting position of the ourrene of x in y, otherwise it is reset to 0.
The ruial part of the algorithm is the next table. Before onsidering it in its nal form,
a simplied version of the next table shall be examined. This is illustrated in the following
example.
j 1 2 3 4 5 6 7 8
x
j
A B C D A B C E
next
j
0 1 1 1 1 2 3 4
This shows that if, for example, mathing between the above pattern and a given text string
fails at j = 8, then the next pattern symbol to be ompared with the urrent text symbol is
x
4
. This situation is shown below.
Math attempt fails at omparison of y
i
and x
8
j = 8
pattern A B C D A B C E
text ... A B C D A B C D ...
i
25
Next omparison is between y
i
and x
4
j = 4
pattern A B C D A B C E
text ... A B C D A B C D ...
i
The entries of the next table may be derived from x as follows. For a mismath between x
j
and y
i
, it is known that x(1; j   1) mathes the previous j   1 symbols of y. If there exists
a proper prex of substring x(1; j   1), of maximal length k   1 say, i.e. x(1; k   1), equal
to a suÆx of the same substring, i.e. x(j   k + 1; j   1), then the next pattern symbol to
try is that obtained by shifting the pattern relative to the text until the prex oupies the
spae previously oupied by the suÆx. The next symbol to be ompared with y
i
is therefore
the one immediately following the prex, namely x
k
. For the ase of a mismath at the rst
pattern symbol, next
1
is given the value of 0 so that the entire pattern is shifted past y
i
.
To reiterate, the value of next
j
is given by the maximum k, less than j, suh that x(1; k  1)
is a suÆx of x(1; j   1), i.e. x(1; k   1) = x(j   k + 1; j   1) (note that for the ase of the
prex x(1; k   1) = , k is taken to be 1). This may be found in the following manner.
Compare x(1; j   1) with itself by sliding one opy over the other from left to right. Start
in the position suh that the rst omparison is between x
1
and x
2
, and stop when either all
the overlapping symbols math or there is none left. The overlapping haraters then dene
the required prex, and k is given by the size of this prex plus 1.
Returning to the previous example pattern, the situation for next
8
is shown below.
A B C D A B C
x(1; 7) A B C D A B C
From the above, it may be seen that there are 3 overlapping symbols. In this ase, the value
of next
8
is therefore equal to 4.
Note that in the searh proedure, just after it has been inremented, the value of j is the
largest integer, less than or equal to i, suh that x(1; j   1) = y(i   j + 1; i   1). This is
exatly the requirement desribed earlier for the next values, when y is replaed by x. The
method to generate the next table is therefore similar to the atual searh proedure, with
the exeption that the pattern is being mathed against itself. The pseudoode for this is
given in Figure 4.3.
It was mentioned previously that the above formulation of the next table aters for a simplied
ase. This does not take into aount all the available information, and advantage may still be
taken of the atual symbol ausing a mismath. Consider the previous example. If there is a
mismath between y
i
and x
7
(the seond C), say, then the next omparison would be between
26
j = 1
k = 0
next
1
= 0 | speial value for mismath at x
1
while j < m
while (k > 0) and (x
j
6= x
k
)
k = next
k
j = j + 1
k = k + 1
next
j
= k
NOTE that the and in the inner loop is a `onditional and.'
Figure 4.3: Initialisation of the next table (simplied)
y
i
and x
next
7
, i.e. x
3
(the rst C). However, this omparison will also fail and its next value
will shift the omparison to x
1
. An improved next table would take the omparison diretly
to x
1
in the event of a mismath at x
7
. The modied requirement for next
j
is therefore that
it is now equal to the greatest k less than j suh that x(1; k   1) = x(j   k + 1; j   1) and
x
j
6= x
k
, and is equal to 0 if there is no suh k. Using this denition, the new next table is
given below.
j 1 2 3 4 5 6 7 8
x
j
A B C D A B C E
next
j
0 1 1 1 0 1 1 4
This revised requirement is easily aommodated by modifying the next
j
assignment in the
next table reation algorithm, giving the nal version of Figure 4.4. The value of next
k
is
now assigned to next
j
if x
j
= x
k
. This ensures that on a mismath, a math is not next
attempted between the urrent text symbol and another pattern symbol equal to the one just
tried.
The searh algorithm given in Figure 4.2 has the unusual feature that the total number of
times that the assignment \j = next
j
" in the inner loop is exeuted never exeeds that of
the inrement operation of i in the outer loop. The pattern is therefore shifted to the right
a total of at most n times by the inner loop, giving a mathing time of O(n). Similarly, it
may be shown that the next table initialisation stage is performed in O(m) time. This gives
an overall worst ase time of O(m+ n), whih inidentally is independent of the size of the
symbol alphabet.
The worst ase time is inurred for the task of mathing a Fibonai string pattern. Also, the
maximum number of times that the inner loop is exeuted whilst still at the same position in
the text has been shown to be 1+ log

m, where  is the golden ratio, equal to (1+
p
5)=2 
1:618 (Knuth, Morris and Pratt, 1977).
27
j = 1
k = 0
next
1
= 0 | speial value for mismath at x
1
while j < m
while (k > 0) and (x
j
6= x
k
)
k = next
k
j = j + 1
k = k + 1
if x
j
= x
k
next
j
= next
k
else
next
j
= k
NOTE that the and in the inner loop is a `onditional and.'
Figure 4.4: Initialisation of the next table
The average ase performane of the algorithm for random strings has been analysed by
Baeza-Yates (1989b). An upper bound for the expeted number of symbol omparisons, A
n
,
was derived whih improves upon an earlier result (Regnier, 1988). This is given below.
A
n
n
6 1 +
1
jC j
+

log

m

  1
jC j 3
+ O

log
2
m
jC j
4

(4.1)
for j C j >

log

m

, where C is the alphabet. To give a numerial value to this bound,
onsider the ase of a pattern length of 10 and an alphabet size of 256. This gives an upper
bound for A
n
of  1:004n. Note that, with the same parameters and for large n, a virtually
idential result is obtained for the average ase of the Brute Fore algorithm, for whih the
expeted value of A
n
for random strings is as follows (Baeza-Yates, 1989b).
A
n
=
jC j
jC j   1

1 
1
jC j
m

(n m+ 1) + O(1) (4.2)
Various implementation eÆieny onsiderations were addressed by Knuth, Morris and Pratt
(1977). For example, sentinels may be appended to the ends of the strings in order to redue
the requirement for index bound heking. Also, the next table may be used to ompile a
nite-state mahine mather for the pattern. Extensions to the problem, suh as nding all
the ourrenes of the pattern in the text were also onsidered.
4.1.3 Boyer-Moore
When searhing using the Boyer-Moore algorithm, the pattern is sanned aross the text
string from left to right, but the atual symbol omparisons between the pattern and the text
are performed from right to left. The rst omparison is therefore between x
m
and y
m
. In
28
the event of a mismath, if y
m
does not our at all in x, then the pattern may be shifted
safely m plaes to the right. This is due to the fat that the possibility of x ourring in y
starting at any of the rst m positions has been ruled out. The next omparison would then
be between x
m
and y
2m
.
Consider the ase of a mismath ourring between x
m
and y
i
, say. As mentioned above,
if y
i
does not appear anywhere in the pattern, then a shift of the pattern m plaes to the
right may be eeted. If, on the other hand, y
i
does appear in the pattern and its rightmost
ourrene is x
m s
, then the pattern may be shifted s plaes to the right, aligning x
m s
with
y
i
. Testing may be resumed by omparing x
m
with y
i+s
, i.e. the text index is inremented by
s.
If a math is found between symbols x
m
and y
i
, then the orresponding preeding symbols
from the pattern and the text are ompared until either a omplete math is obtained or a
mismath ours. If a mismath ours at x
j
, say, then the suÆx, of length m  j, x(j+1; m)
will be equal to text substring y(i   m + j + 1; i), and x
j
6= y
i m+j
. If the rightmost
ourrene in x of y
i m+j
is again x
m s
, say, then the pattern may be shifted j   m + s
plaes to the right, lining up x
m s
with y
i m+j
. One more, the proedure may be ontinued
by omparing x
m
with its orresponding text symbol, in this ase y
i+j m+s
. The inrement
in the text index from the mismath position to that of the next omparison is therefore
(i + j   m + s)   (i  m + j) = s. Note that this inrement is the same as that for the
previously onsidered ase.
If x
m s
happens to be to the right of x
j
, giving a negative value for j   m + s, there is
nothing to be gained by aligning x
m s
with y
i m+j
, as this would involve a retrograde step.
In these irumstanes, therefore, the pattern may be shifted one plae to the right, with the
omparisons being resumed with symbols x
m
and y
i+1
. This requires a text index inrement
of (i+ 1)   (i m+ j) = m+ 1  j.
The inrements, s, obtained from this ourrene heuristi are preomputed and stored in a
skip table (denoted by delta
1
by Boyer and Moore (1977)). During searhing, the table is
indexed by the text symbol ausing a mismath, so a table the size of the symbol alphabet in
use is required. The entry for symbol w is equal tom j, where x
j
is the rightmost ourrene
of w in x, and equal to m if w does not our at all in x, i.e.
skip[w℄ = minfs : s = m or (0 6 s < m and x
m s
= w)g (4.3)
The skip table is simply initialised as follows for alphabet C.
for w = 1 to jC j
skip[w℄ = m
for j = 1 to m
skip[x
j
℄ = m  j
From the above, it may be seen that the initialisation of the skip table is aomplished in
O(m + jC j) time.
The following example illustrates the operation of the skip table for the pattern ABCDB.
29
w A B C D E F G H . . .
skip[w℄ 4 0 2 1 5 5 5 5 . . .
Mismath ours at y
i
= A
pattern A B C D B
text . . . L M N A B C D B . . .
i
Next omparison is between x
m
and y
i+skip[A℄
, i.e. y
i+4
pattern A B C D B
text . . . L M N A B C D B . . .
i i + 4
For the ase desribed earlier of a mismath ourring after a partial math had been obtained,
it is possible that a greater shift of the pattern than that given by the ourrene heuristi
may be feasible. If this is so, then it is preferable to use the larger shift, whih is obtained
via the math heuristi. This is based on a table similar to that of the Knuth-Morris-Pratt
algorithm. The idea behind this is that when the pattern is shifted, it must math all the
symbols previously mathed, and there must be a hange in the pattern symbol at the text
position that aused the mismath. The latter ondition was proposed independently by
Kuipers (redited in Boyer and Moore (1977)) and by Knuth (in the postsript of Knuth,
Morris and Pratt (1977)). It improves over Boyer and Moore's original heuristi in a manner
analogous to that of the improvement of the next table of the Knuth-Morris-Pratt algorithm.
As mentioned earlier, the partiular ase under examination is when x(j + 1; m) = y(i  
m + j + 1; i) and x
j
6= y
i m+j
. If the suÆx x(j + 1; m) also ours in x as the substring
x(j + 1  t;m  t), with x
j t
6= x
j
, and it is the rightmost suh ourrene, then the pattern
may be shifted t plaes to the right. This brings x(j + 1   t;m   t) into alignment with
y(i   m + j + 1; i), and the searh proess may be resumed by omparing x
m
with y
i+t
. A
shift table (denoted by delta
2
by Boyer and Moore (1977)) for the t shift information may
be preomputed from the pattern and is indexed by the position in the pattern at whih
mismath ours. The value of shift[j℄ is equal to a pattern shift, t, plus the extra shift
required to restart the omparisons at the extreme right of the pattern. The value of t is the
minimum suh that when x
m
is aligned with y
i+t
, then assuming that x
j
6= y
i m+j
, pattern
substring x(j + 1   t;m   t) will math text substring y(i   m + j + 1; i). Thus, the shift
table entries are dened as follows.
shift[j℄ = minft+m  j : t > 1 and (t > j or x
j t
6= x
j
) and (4.4)
((t > k or x
k t
= x
k
) for j < k 6 m)g
The shift[j℄ value is therefore equal to the required inrement for the text index from the
urrent position, y
i m+j
, to the next omparison position, y
i+t
, i.e. (i+t)   (i m+j) = t+
m   j. The following example illustrates the shift values for the pattern ABCDABC.
x
j
A B C D A B C
30
i = m
j = m
while (j > 0) and (i 6 n)
if y
i
= x
j
i = i  1
j = j   1
else
| a mismath
i = i+maxfskip[y
i
℄; shift[j℄g
j = m
if j < 1
i = i+ 1
else
i = 0
Figure 4.5: Boyer-Moore string mathing
j 1 2 3 4 5 6 7
m  j 6 5 4 3 2 1 0
t 4 4 4 4 7 7 1
shift[j℄ 10 9 8 7 9 8 1
From the denition of shift[j℄, it may be seen that shift[j℄ > m + 1  j. Thus, in the ase
desribed earlier requiring a text index inrement of m+ 1  j rather than the relevant skip
value, as the latter would involve a bakwards pattern shift, it is suÆient to use the relevant
shift table value. In the event of a mismath while searhing, the maximum of the text
inrements from the two heuristis desribed above should therefore be used. The algorithm
for this method is given in Figure 4.5. The nal value of i gives the position of the pattern
in the text, with 0 indiating that the pattern was not found.
The shift table may be preomputed from the pattern as shown in Figure 4.6. This is the
method presented by Aho (1990), originated by Knuth (Knuth, Morris and Pratt, 1977)
and modied by Mehlhorn (Smit, 1982). The intermediate funtion, f , alulated in the
algorithm is dened as follows: f [m℄ = m + 1, and for 1 6 j < m, f [j℄ = minfi : j < i 6
m and x(i+ 1; m) = x(j + 1; j +m  i)g.
Knuth demonstrated that to loate all the ourrenes of the pattern, the Boyer-Moore algo-
rithm performs O(n+ rm) symbol omparisons, where r is the number of times the pattern
appears in the text (Knuth, Morris and Pratt, 1977). He also showed that when the pattern
is absent from the text, a total of 6n symbol omparisons is required. For large alphabets,
the expeted performane of the Boyer-Moore algorithm is sublinear, requiring about n=m
symbol omparisons on average.
A variant of the Boyer-Moore algorithm developed by Apostolio and Gianarlo (1986) nds
all of the ourrenes of the pattern, performing at most 2n   m + 1 symbol omparisons.
31
for i = 1 to m
shift[i℄ = 2 m  i
j = m
k = m+ 1
while j > 0
f [j℄ = k
while (k 6 m) and (x
j
6= x
k
)
shift[k℄ = minfshift[k℄; m  jg
k = f [k℄
j = j   1
k = k   1
for i = 1 to k
shift[i℄ = minfshift[i℄; m+ k   ig
j = f [k℄
while k 6 m
while k 6 j
shift[k℄ = minfshift[k℄; j   k +mg
k = k + 1
j = f [j℄
NOTE that the and in the seond while loop is a `onditional and.'
Figure 4.6: Initialisation of the shift table
32
The attendant preproessing requirements are slightly more onerous, but are still linear in m.
The method employed exploits knowledge of whih pattern substrings mathed whih text
substrings at previous trial positions, and uses the simpler form of the shift table denition.
There are many other variations of the Boyer-Moore approah to string mathing. One of
the more notable of these, the Boyer-Moore-Horspool algorithm, whih involves simplied
preproessing, is desribed in the next setion.
4.1.4 Boyer-Moore-Horspool
In pratie, the math heuristi of the Boyer-Moore algorithmmakes only a small ontribution
to the overall speed of the searh. It does provide benets for highly self repetitive patterns,
but these seldom rop up in atual appliations. The inlusion of the heuristi prevents
the theoretial worst ase performane from being O(mn), as opposed to the atual linear
behaviour.
Horspool (1980) developed a simplied form of the Boyer-Moore algorithm whih dispenses
with the math heuristi and relies solely on the ourrene heuristi. This redues the
preproessing eort required, and also uts down the spae requirements from O(m + jC j)
to O(jC j), for alphabet C. As the alphabet is usually xed, the storage requirements will
therefore normally be onstant. Although the worst ase performane is now O(mn), this
atually ours only for rare pathologial ases, and the expeted performane is omparable
to that of the full Boyer-Moore algorithm.
The skip table used in the Horspool variant is slightly dierent from the Boyer-Moore version,
in that skip[x
m
℄ is assigned the value of m, and thus only x
1
to x
m 1
need be used in the
initialisation. In the Boyer-Moore table, the skip value for x
m
is equal to 0, and a non-zero
shift is obtained from the omparison to nd the maximum of the math and ourrene
heuristi values.
Horspool also noted that, in the event of a mismath at x
j
, say, the next pattern position to
try may be that suh that any of the m  j mathing text symbols is brought into alignment
with the rightmost ourrene in the pattern of that partiular symbol. By always using
the text symbol orresponding to the last pattern symbol (x
m
) to index the skip table,
the situation desribed previously involving a bakwards pattern shift may be avoided. This
situation would otherwise have to be expliitly tested for. Reall that that situation is averted
in the Boyer-Moore algorithm by virtue of the math heuristi. The Boyer-Moore-Horspool
algorithm is depited in Figure 4.7. Again, the pattern position is given by the nal value of
i, with 0 indiating that no math was found.
Baeza-Yates (1989a) has shown that the expeted number of symbol omparisons, A
n
, per-
formed by the Boyer-Moore-Horspool algorithm for random strings is bounded from below as
follows.
A
n
>
1 
1
jCj
m
(jC j  1)
h
1 

1 
1
jCj

m
i
(n m + 1) (4.5)
For a pattern of length 10, the above relation gives lower bounds of 0.17, 0.11, and 0.10 times
33
| initialise skip table
for w = 1 to jC j
skip[w℄ = m
for j = 1 to m  1
skip[x
j
℄ = m  j
| searh
i = m
j = m
while (j > 0) and (i 6 n)
k = i
while (j > 0) and (y
k
= x
j
)
k = k   1
j = j   1
if j > 0
| a mismath
i = i+ skip[y
i
℄
j = m
if j < 1
i = k + 1
else
i = 0
NOTE that the and in the inner while loop is a `onditional and.'
Figure 4.7: Boyer-Moore-Horspool string mathing
34
(n   m + 1) for alphabet sizes of 10, 100, and 256 respetively. This shows that, for large
alphabets, the lower bound is lose to (n m+ 1)=m in the average ase, i.e. about n=m for
nm.
4.1.5 Sunday | Quik Searh, Maximal Shift, and Optimal Mismath
Sunday (1990) observed that in the Boyer-Moore algorithm, for a mismath ourring when
x
m
is aligned with text symbol y
i
, say, then the next text symbol, y
i+1
, ould be used rather
than y
i
to address the ourrene heuristi table. This is due to the fat that for the minimum
pattern shift, i.e. one plae to the right, y
i+1
is part of the next text substring to be examined.
Sunday states that in pratie this should result in a shift greater than or equal to that of the
Boyer-Moore skip value plus 1, on average. The speed advantage oered by this phenomenon,
however, diminishes as the length of the pattern inreases.
Smith (1991) has pointed out that there are irumstanes under whih the use of y
i
rather
than y
i+1
is advantageous, as demonstrated in the example below for the pattern ABCDE. He
therefore proposed using the maximum of the ourrene heuristi values for text symbols y
i
and y
i+1
.
Boyer-Moore skip table for the pattern ABCDE
w A B C D E F G H . . .
skip[w℄ 4 3 2 1 0 5 5 5 . . .
Mismath ours at y
i
= T
pattern A B C D E
text . . . P Q R S T D U V W X . . .
i
Next pattern position based on skip[y
i
℄, i.e. i+ skip[T℄
pattern A B C D E
text . . . P Q R S T D U V W X . . .
i + 5
Next pattern position based on skip[y
i+1
℄, i.e. i+ skip[D℄ + 1
pattern A B C D E
text . . . P Q R S T D U V W X . . .
i + 2
Sunday also noted that the symbol omparisons between the pattern and the urrent text
substring may be performed in an arbitrary order, and used this fat in an eort to improve
35
| initialise skip table
for w = 1 to jC j
skip[w℄ = m+ 1
for j = 1 to m
skip[x
j
℄ = m+ 1  j
| searh
i = 1
j = 1
while (j 6 m) and (i 6 n m+ 1)
k = i
while (j 6 m) and (y
k
= x
j
)
k = k + 1
j = j + 1
if j 6 m
| a mismath
i = i+ skip[y
i+m
℄
j = 1
if j 6 m
i = 0
NOTE that the and in the inner while loop is a `onditional and.'
Figure 4.8: Sunday Quik Searh
the overall mathing eÆieny. He proposed three dierent variations of the Boyer-Moore
searh, all of whih use y
i+1
in the ourrene heuristi, but dier in the way in whih the
order of the symbol omparisons is determined. Sunday has onjetured that the linear worst
ase behaviour of the Boyer-Moore algorithm is preserved when the substring symbols are
ompared in an arbitrary order.
The simplest of the three is the Quik Searh algorithm, in whih the omparisons are per-
formed stritly from left to right and only the ourrene heuristi is employed. Pseudoode
for this method is given in Figure 4.8. Note that the skip table values are initialised to be
1 greater than those of the Boyer-Moore table to take into aount the extra plae in the
pattern shifts.
In the other two methods | the Maximal Shift and the Optimal Mismath algorithms | an
additional array is used to store the pattern indies in the order that the symbol omparisons
are performed at eah trial pattern position. During searhing, this array is therefore aessed
sequentially so as to provide the desired order of symbol positions within the pattern.
In the Maximal Shift algorithm, the omparison order is hosen suh that the math heuristi
shift values are maximised. This is done by sorting the pattern symbols in desending order
of their distane to their next leftward ourrene in the pattern, or, if there is none, to the
start of the pattern. Thus, for a pattern ontaining no repeated symbols, the omparisons
36
are performed in sequential order from right to left.
In the nal algorithm, the Optimal Mismath searh, the probability of an early detetion of
a mismath is maximised by omparing rarer symbols rst. This requires the pattern symbols
to be sorted in desending order of their frequeny of ourrene in the text to be searhed,
neessitating an a priori knowledge of its statistis. Full implementations of the Maximal
Shift and Optimal Mismath algorithms, oded in C, may be found in Sunday (1990).
Sunday found the Optimal Mismath version to be the fastest of his methods, and reported
all three to be superior in average ase performane to the Boyer-Moore algorithm | a
speed advantage of 10-20% is reported for the Optimal Mismath algorithm. The speed
dierentials, however, dereased with an inrease in the pattern length, onsistent with the
behaviour obtained by using the text symbol following that ausing a mismath.
Sunday's algorithms have also been empirially ompared with a simplied Boyer-Moore
variant by Pirklbauer (1992). In this study, it was found that for long strings and for the ase
where preproessing times were taken into aount, the Maximal Shift and Optimal Mismath
proedures fared rather badly. Their preproessing overheads were found to be signiant,
espeially when searhing for only the rst ourrene of the pattern. The Quik Searh
algorithm was, however, reommended on the grounds of its performane, similar to that of
the Boyer-Moore variant, and its ease of implementation.
The disadvantage of the OptimalMismathmethod, namely its requirement for foreknowledge
of the input text statistis, has been addressed by Smith (1991). His adaptive approah starts
with an arbitrary symbol omparison order, whih is modied as the searh proeeds. The
algorithm dispenses with the math heuristi and relies solely on the ourrene heuristi.
This language independent sheme was found to perform only slightly worse on average than
the equivalent stati language based searh, i.e. a version of the Optimal Mismath algorithm
employing only the ourrene heuristi. The method used to adapt the ordering is to move
the pattern symbol position that auses a mismath either up one position or to the top of the
ordering list, thereby giving it a higher preedene in future trials. As noted earlier, Smith's
method also involves seleting the maximum of two ourrene heuristi shift values | one
for the text symbol aligned with the end of the pattern, and the other for the following text
symbol.
4.1.6 Hume and Sunday | Tuned Boyer-Moore and Least Cost
In disussing eÆieny onsiderations, Boyer and Moore (1977) presented an implementation
of their algorithm inorporating a `fast' skip loop involving only the ourrene heuristi.
This is used to swiftly disount non-math positions, and only after a suessful math has
been found for the rst pattern symbol to be tested, i.e. x
m
, is a `slow' loop entered in whih
the other pattern symbols are ompared with the text. If a mismath ours at this stage,
then the maximum of the two heuristis is used to determine the next trial pattern position,
and searhing is resumed using the fast loop.
Hume and Sunday (1991) have presented a taxonomy of various ognate algorithms based on
the struture of this form of the Boyer-Moore searh method. Three distint omponents of
37
the proedure were onsidered, namely the fast skip loop, the slow mathing loop, and the
alulation of the pattern shift to the next trial position in the event of a mismath whilst in
the slow loop.
For eah of these three omponents, various dierent tehniques were onsidered. For exam-
ple, initially testing for x
1
, x
m
, or the least frequent, in terms of the text symbol statistis,
pattern symbol were examined for the skip loop. Various omparison orders were investi-
gated for the mathing loop | e.g. left to right, right to left, and Sunday's Maximal Shift
and Optimal Mismath orderings. With the exeption of the Optimal Mismath, these were
optionally guarded by preeding the full math attempt with a single omparison involving
a low frequeny pattern symbol. The shift alulation stage tests involved the use of the
Knuth-Morris-Pratt table, the Boyer-Moore heuristis, Sunday's shift tables, and ombina-
tions thereof. It was found, however, that the advantages of ombiningmore than one heuristi
were outweighed by the overhead inurred by omputing the maximumof the dierent values.
From empirial tests, based on searhes of a bible text, the most eetive methods for eah of
the three stages were identied, and two new proedures were proposed, namely the Tuned
Boyer-Moore and the Least Cost algorithms. These both use a guarded left to right ompar-
ison for the math loop and Sunday's version of the math heuristi for the shift alulation
for a mismath in the math loop. However, they dier in the methods used in the skip loop
| the Tuned Boyer-Moore algorithm uses an optimised version of Boyer and Moore's original
`fast' loop, whereas the Least Cost algorithm initially seeks the pattern symbol inurring the
lowest searh ost. This ost is based on both the text symbol frequeny distribution and the
position of the symbol in the pattern, whih inuenes the size of the pattern shift possible.
Implementations of the two algorithms in C are presented in Hume and Sunday (1991), where
details of how full opies of the programs and the datasets employed may be obtained via
internet ftp or email are also given.
A speed improvement by a fator of 4.5 over the original Boyer-Moore algorithm (not in its
`fast-loop' implementation) has been reported for the Tuned Boyer-Moore and Least Cost
searh proedures. Note that the latter method relies on a priori information of the text
symbol statistis. Furthermore, it should be pointed out that, in ommon with Sunday
(1990), the algorithms were ompared for the ase of searhing for all ourrenes of the
pattern in the text, the preproessing osts were not inluded in the measurements, and the
tests were onduted using relatively small patterns (m 6 16).
4.1.7 Harrison
Although not stritly solving the string mathing problem as dened earlier, Harrison's ap-
proah is inluded here sine it an provide an eÆient method of eliminating text strings,
with low probabilities of ontaining a given pattern, when there is no atual math. Also,
note that this tehnique requires the text to be preproessed.
The basi idea of this substring deision proedure involves representing strings with simpler
data objets, whih preserve some of their inherent properties. In this method, a string is
represented by the set of its substrings of a xed length k, say. A suitable hashing funtion
38
is employed that maps the j C j
k
possible distint substrings (where C is the alphabet) to
integers in the interval [1; p℄. A binary string of length p, known as a hashed k-signature, may
then be used to represent this set of substrings | the i
th
bit of the signature is set to `1' only
if i is the hashed value of some member of the set of substrings.
A neessary, but not suÆient, ondition for a pattern to our as a substring of a text is
that the pattern's signature, x
sig
, is a subset of that of the text, y
sig
. If a pattern substring
appears nowhere in the text, then the pattern itself annot be a substring of the text. This
means that if x
sig
has a `1' in a position where y
sig
does not, then it an be onluded that
x is not a substring of y. This test may be performed eÆiently using logial operators on
the two signatures as follows. A non-zero result from the bitwise and of x
sig
and not y
sig
indiates that there an be no possible math. On the other hand, however, in the event of a
zero result, a rigorous searh is still required to establish whether or not x is present in the
text.
The method may therefore be used to rapidly disount text strings where there an be no
math with the pattern. Eah line of a large text le ould, for example, be stored together
with a hashed k-signature to failitate later pattern searhes.
4.1.8 Karp-Rabin
In the Karp-Rabin algorithm, omparison of m-symbol text substrings, y(i; i + m   1), at
suessive positions, 1 6 i 6 n  m+ 1, with the pattern x, proeeds in a manner similar to
that of the Brute Fore approah. The dierene being that rather than diretly omparing
x(1; m) with y(i; i+m  1), it is a single integer omparison that is performed. The integers
involved are the hash values for the orresponding m-symbol strings.
The signatures for x(1; m) and y(1; m) are preomputed, and the use of a arefully seleted
hash funtion permits the signature for y(i; i + m   1) to be derived easily from that for
y(i   1; i+m   2) as the san of the text proeeds. The atual hash funtion used is given
below.
h(k) = k mod q (4.6)
where q is a large prime. For the alulation, the m-symbol strings are treated as m-digit
integers in base , where  = jC j for alphabet C. The omponent symbols are therefore the
(base ) digits of the integer. Adopting a big-endian onvention, the most signiant digit
omes rst. Thus, the number, k
i
, orresponding to the text substring at position i is given
by
k
i
= y
i

m 1
+ y
i+1

m 2
+ : : : + y
i+m 1
(4.7)
The number for the next text substring, y(i+ 1; i+m), k
i+1
is derived from k
i
as follows
k
i+1
= (k
i
  y
i

m 1
) + y
i+m
(4.8)
The assoiative property of the mod funtion, i.e. (a+b) mod  = (a mod  + b mod ) mod
, allows the remainders to be taken after eah stage of the alulation. This keeps the partial
results small, and yields the same result as that that would be obtained by performing the
mod operation only at the nal stage of the alulation.
39
| initialisation
i = 1
found = false
d = 
m 1
mod q
h
x
= (x
1
 
m 1
+ x
2
 
m 2
+ : : : + x
m
) mod q
h
y
= (y
1
 
m 1
+ y
2
 
m 2
+ : : : + y
m
) mod q
| searh
while (i 6 n m+ 1) and (not found)
if (h
x
= h
y
) and (x(1; m) = y(i; i+m  1))
| a suessful math
found = true
else
| alulate next signature value
h
y
= (h
y
+   q   y
i
 d) mod q
h
y
= (h
y
  + y
i+m
) mod q
i = i+ 1
if (i > n m+ 1)
i = 0
NOTE that the and in the if statement is a `onditional and.'
Figure 4.9: Karp-Rabin string mathing
As the hash funtion values are not unique, the equality of two signatures is a neessary, but
not suÆient, ondition for the equality of their orresponding strings. Therefore, when a
potential math is deteted, the two strings must then be ompared diretly, to hek for a
signature ollision. For a large hash table, i.e. a large value of q, the hanes of suh ollisions
are slight. One of the algorithms presented by Karp and Rabin (1987) further redues the
probability of a large number of ollisions by randomly reseleting the prime q should a
ollision our, reinitialising and then ontinuing with the searh. The theoretial worst ase
involvesm symbol omparisons being fored at eah stage, giving an overall exeution time of
O(mn). In pratie, however, the signatures at eah stage would be expeted, on the whole,
to be unequal. In these irumstanes, eah text position is proessed in onstant time, giving
an O(m+n) expeted performane. The algorithm is given in Figure 4.9. Note that the initial
alulation of h
x
and h
y
may be performed in the following manner.
h
x
= 0
for j = 1 to m
h
x
= (h
x
 + x
j
) mod q
The value of d, used in the inremental signature alulation, may also be initially omputed
iteratively, again exploiting the assoiativity of mod. In the alulation of the next signature
value,   q is added to ensure that the partial result remains positive. On ompletion of the
40
searh proedure, i gives the position of the pattern in the text, or is equal to 0 when no
math has been found.
The prime number q should be as large as possible, onsistent with ( + 1)q not ausing
arithmeti overow (this is the maximum partial result in the next signature omputation).
Sedgewik (1983) uses a value of 33554393 (for  = 32), and Pirklbauer (1992) suggests
8355967 (for  = 256). Gonnet and Baeza-Yates (1991) present an implementation of the
algorithm obviating the mod operations by virtue of the impliit modular arithmeti of the
target hardware.
4.2 String Distane and Longest Common Subsequene
4.2.1 Wagner-Fisher
In the dynami programming method of omputing the distane between two strings, the
distanes between longer and longer prexes of the strings are suessively evaluated from
previous values until the nal result is obtained. The preise details of this proedure are
given below.
Let the distane between the prexes of strings x and y, of lengths i and j respetively, be
denoted by d
i;j
, i.e.
d
i;j
= d(x(1; i); y(1; j)) (4.9)
Further, let the weighting for the ost of transforming symbol a into symbol b be denoted
by w(a; b). Thus, w(a; b) is the ost of a symbol substitution if a 6= b, w(a; ) is the ost of
deleting a, and w(; b) is the ost of inserting b. Note that when d is the Levenshtein distane,
we have the following.
w(a; ) = 1 (4.10)
w(; b) = 1
w(a; b) = 1 if a 6= b
= 0 otherwise
During the alulation proedure, the values of d
i;j
are reorded in an (m+1) (n+1) array,
and are omputed using the following reurrene relation.
d
i;j
= minfd
i 1;j
+ w(x
i
; ); d
i;j 1
+ w(; y
j
); d
i 1;j 1
+ w(x
i
; y
j
)g (4.11)
This relationship is derived as follows. If the ost of transforming x(1; i  1) into y(1; j) is
known, then that of transforming x(1; i) into y(1; j) may be obtained by adding the ost of
deleting x
i
. Similarly, if the ost of transforming x(1; i) into y(1; j  1) is known, then that of
transforming x(1; i) into y(1; j) may be obtained by adding the ost of inserting y
j
. Finally,
if the ost of transforming x(1; i   1) into y(1; j   1) is known, then that of transforming
x(1; i) into y(1; j) may be obtained by adding the ost of replaing x
i
by y
j
. Realling that
the distane d
i;j
is the minimum ost of transforming x(1; i) into y(1; j), it is neessary to
selet the heapest of the three operations desribed above in order to arrive at the orret
value for d
i;j
.
41
Before performing the d
i;j
alulations, the array boundary onditions must be set up. For
the rst olumn of the array, the value of d
i;0
is equal to the sum of the osts of deleting the
rst i symbols of x. Similarly, for the rst row, the value of d
0;j
is given by the sum of the
osts of inserting the rst j symbols of y. Thus, we have the following:
d
0;0
= 0 (4.12)
d
i;0
=
i
X
k=1
w(x
i
; ) for 1 6 i 6 m
d
0;j
=
j
X
k=1
w(; j
k
) for 1 6 j 6 n
For the ase of the Levenshtein distane, it therefore follows that d
i;0
= i and d
0;j
= j.
The alulation proedure is demonstrated in the following example whih shows the array
produed in the evaluation of the Levenshtein distane between the strings preterit and
zeitgeist. From this, it may be seen that the distane between these strings, i.e. d
8;9
, is
equal to 6.
j 0 1 2 3 4 5 6 7 8 9
i z e i t g e i s t
0 0 1 2 3 4 5 6 7 8 9
1 p 1 1 2 3 4 5 6 7 8 9
2 r 2 2 2 3 4 5 6 7 8 9
3 e 3 3 2 3 4 5 5 6 7 8
4 t 4 4 3 3 3 4 5 6 7 7
5 e 5 5 4 4 4 4 4 5 6 7
6 r 6 6 5 5 5 5 5 5 6 7
7 i 7 7 6 5 6 6 6 5 6 7
8 t 8 8 7 6 5 6 7 6 6 6
The algorithm to ompute the distane array, as developed by Wagner and Fisher, is given
in Figure 4.10. It may be seen that the border initialisation stage omprises 1 + m + n
assignments, and that the main loop is iterated mn times. The time omplexity of the
method is therefore O(mn).
The editing sequene taking x to y may be obtained by way of a struture known as a trae.
A trae from x to y may be visualised as omprising string x plaed above string y, with
edges joining symbols in x to symbols in y, suh that any symbol is touhed by at most one
edge and no two edges ross eah other. Representing the edge from x
i
to y
j
by the ordered
pair of integers (i; j), a trae from x to y may be dened formally as a set of suh ordered
pairs satisfying the following onditions.
(a) 1 6 i 6 m; 1 6 j 6 n (4.13)
(b) for distint edges (i
1
; j
1
); (i
2
; j
2
)
i
1
6= i
2
and j
1
6= j
2
; i
1
< i
2
, j
1
< j
2
42
| initialise array borders
d
0;0
= 0
for i = 1 to m
d
i;0
= d
i 1;0
+ w(x
i
; )
for j = 1 to n
d
0;j
= d
0;j 1
+ w(; y
j
)
| alulate d
i;j
for i = 1 to m
for j = 1 to n
d
i;j
= minfd
i 1;j
+ w(x
i
; ); d
i;j 1
+ w(; y
j
); d
i 1;j 1
+ w(x
i
; y
j
)g
Figure 4.10: Wagner and Fisher string distane alulation
An editing sequene may be obtained from a trae as follows. Symbols in x untouhed by
edges are deleted, whereas those untouhed in y are inserted. For an edge in the trae, (i; j),
if x
i
6= y
j
, then x
i
is replaed by y
j
, otherwise no edit is required. Returning to the previous
example, a least ost trae from preterit to zeitgeist is given below.
i 1 2 3 4 5 6 7 8
x
i
p r e t e r i t
| / | \ | \
y
j
z e i t g e i s t
j 1 2 3 4 5 6 7 8 9
A minimal ost trae, and hene a least ost editing sequene, from x to y may be obtained
from the ompleted distane array. An algorithm to print out the ordered pairs of the trae
is given in Figure 4.11. The proedure starts with d
m;n
and works bak until either i or j
equals 0. The method is to determine from whih of its neighbours d
i;j
was derived, and to
move to that position in the array. If d
i;j
was derived from d
i 1;j 1
, then the ordered pair
(i; j) is output | orresponding to an edge in the trae, i.e. either a substitution or a math
depending on whether x
i
6= y
j
or not respetively. No output is produed for the ase where
d
i;j
was derived from either d
i 1;j
or d
i;j 1
, as deletions and insertions orrespond to symbols
untouhed by edges in the trae. Sine either i or j, or both, is deremented on eah pass of
the loop, the maximum number of iterations is m+n. The least ost trae is thus determined
in O(m+ n) time.
Returning one more to the previous example, the above proedure yields the following trae,
T (in reverse order from that output by the algorithm).
T = f(1; 1); (3; 2); (4; 4); (5; 6); (7; 7); (8; 9)g
Note that it is now a fairly straightforward matter to obtain an ls(x; y) from the least ost
trae from x to y. The omponents of ls(x; y) are the x
i
symbols, or equivalently the y
j
symbols, suh that (i; j) 2 T and x
i
= y
j
. So, for our example, we have:
ls(x; y) = x
3
x
4
x
5
x
7
x
8
= y
2
y
4
y
6
y
7
y
9
= eteit
43
i = m
j = n
while (i > 0) and (j > 0)
if d
i;j
= d
i 1;j
+ w(x
i
; )
i = i  1
else if d
i;j
= d
i;j 1
+ w(; y
j
)
j = j   1
else
print(`(i; j)')
i = i  1
j = j   1
Figure 4.11: Wagner and Fisher least ost trae omputation
Finally, a partiular point made by Wagner and Fisher worth noting is that for spelling
orretion appliations, it might be useful to have a ost weighting funtion for the edit
a ! b, w(a; b), dependent on the partiular symbols a and b. For the standard qwerty
keyboard, for example, it is muh more likely that an e would be miskeyed as a w than, say,
a p. Using this riterion, wast would, for example, be deemed to be loser to east than to
past.
4.2.2 Hirshberg
Considering the distane array of the above method, it is apparent that the spae requirement
of the Wagner-Fisher approah is O(mn). In order to avoid potential storage problems when
dealing with long strings, Hirshberg (1975) developed a linear spae version of the algorithm.
Hirshberg's objetive was to determine an ls(x; y) of two strings x and y. Rather than
evaluate string distanes in the dynami programming array, it is therefore the lengths of
longest ommon subsequenes of progressively longer string prexes that are determined. Let
this quantity be denoted by l
i;j
, i.e.
l
i;j
= j ls(x(1; i); y(1; j)) j (4.14)
For a given metri, there is a xed relationship between l
i;j
and d
i;j
. Consider, for example,
the edit distane, dened by the following ost weightings.
w(a; ) = 1 (4.15)
w(; b) = 1
w(a; b) = 2 if a 6= b
= 0 otherwise
From a minimal ost trae from x to y, it is apparent that the edit distane d(x; y), is related
to j ls(x; y) j as derived below, where del, ins and sub are the number of symbol deletions,
insertions and substitutions respetively.
d(x; y) = del + ins + 2sub
44
| initialise array borders
l
0;0
= 0
for i = 1 to m
l
i;0
= 0
for j = 1 to n
l
0;j
= 0
| alulate l
i;j
for i = 1 to m
for j = 1 to n
if x
i
= y
j
l
i;j
= l
i 1;j 1
+ 1
else
l
i;j
= maxfl
i 1;j
; l
i;j 1
g
Figure 4.12: Calulation of j ls(x; y) j
= m  (j ls(x; y) j + sub) + n  (j ls(x; y) j + sub) + 2sub
= m+ n  2 j ls(x; y) j (4.16)
In this ase, the relationship between l
i;j
and d
i;j
is therefore as follows.
l
i;j
= (i+ j   d
i;j
)=2 (4.17)
Employing this transformation, the dynami programming proedure for l
i;j
may be derived
from that for d
i;j
, and is given in Figure 4.12. Sine the length of an ls of any string and
the empty string is 0, the array border values are given by l
i;0
= l
0;j
= 0. As with the d
i;j
alulations, the values of l
i;j
are built up from previous results. When at position (i; j), i.e.
when onsidering prexes x(1; i) and y(1; j), if x
i
= y
j
then we obtain a new ls by appending
this symbol to the urrent ls of x(1; i 1) and y(1; j 1), and thus l
i;j
= l
i 1;j 1
+1. Otherwise
the length of the urrent partial ls is simply the maximum of the previous neighbouring
values.
The storage requirements of the above algorithmmay be redued by noting that only row i 1
of the matrix is required in the evaluation of row i. This fat is exploited in the algorithm
given in Figure 4.13. This returns vetor ll as output, where ll
j
= l
m;j
. A 2 (n + 1) array,
h, is employed, whose rows 0 and 1 at as rows i   1 and i of array l, respetively. Row 1 is
therefore shifted up into row 0 prior to eah new `row i' alulation.
As before, the if statement is exeuted exatly mn times, giving a time omplexity of O(mn).
The input and output arrays require m+n+(n+1) loations, with 2(n+1) storage loations
alloated to array h. The spae omplexity is therefore linear in m and n, i.e. O(m + n).
This method may be used in an algorithm to determine an atual ls(x; y) in linear spae, as
desribed below.
The general idea is to divide reursively string x in half, and for eah half, x1 and x2, nd a
orresponding prex and suÆx, y1 and y2, of y suh that the ls of x1 and y1 onatenated
with that of x2 and y2 is equal to an ls of the omplete strings x and y, i.e.
ls(x1; y1)  ls(x2; y2) = ls(x; y) (4.18)
45
ls length(m; n; x; y; ll)
| initialise
for j = 0 to n
h
1;j
= 0
| alulate h
1;j
for i = 1 to m
| shift row up
for j = 0 to n
h
0;j
= h
1;j
for j = 1 to n
if x
i
= y
j
h
1;j
= h
0;j 1
+ 1
else
h
1;j
= maxfh
1;j 1
; h
0;j
g
| opy result to output vetor
for j = 0 to n
ll
j
= h
1;j
Figure 4.13: Redued spae alulation of j ls(x; y) j
The problem may thus be deomposed reursively until trivial subproblems are obtained.
Let the length of an ls of the suÆxes x(i+ 1; m) and y(j + 1; n) be denoted by l

i;j
, i.e.
l

i;j
= j ls(x(i+ 1; m); y(j+ 1; n)) j (4.19)
For 0 6 j 6 n, values of l
i;j
are the lengths of ls's of the prex x(1; i) and various prexes
of y. Similarly, for 0 6 j 6 n, values of l

i;j
are the lengths of ls's of the reversed suÆx
x
R
(m; i+ 1) and various prexes of the reversed string y
R
. The following theorem permits
the appropriate prex and suÆx of y to be found when x is biseted, i.e. when i is taken to
be bm=2. The theorem states that when x is divided into two parts at any point, then the
maximum value, over all divisions of y into two setions, of the sum of the lengths of an ls
of the rst setions of x and y and that of the latter setions is equal to the length of an ls
of the omplete strings x and y.
46
Theorem
Dening M
i
as
M
i
= max
06j6n
fl
i;j
+ l

i;j
g (4.20)
then,
M
i
= l
m;n
for 0 6 i 6 m (4.21)
Proof
Let
M
i
= l
i;j
+ l

i;j
for j = j
0
; say (4.22)
s
i;j
0
be any ls(x(1; i); y(1; j
0
)) (4.23)
s

i;j
0
be any ls(x(i+ 1; m); y(j
0
+ 1; n)) (4.24)
then  = s
i;j
0
 s

i;j
0
is a ommon subsequene of x and y, and has length M
i
. Thus,
l
m;n
>M
i
(4.25)
Let s
m;n
be any ls(x; y), then s
m;n
is a subsequene of y that is equal to s1  s2, where
s1 is a subsequene of x(1; i) and s2 is a subsequene of x(i+ 1; m). So, there exists a
value of j, j
1
say, suh that s1 is a subsequene of y(1; j
1
) and s2 is a subsequene of
y(j
1
+ 1; n). The lengths of s1 and s2 satisfy the following.
js1 j 6 j ls(x(1; i); y(1; j
1
)) j
6 l
i;j
1
from (4:14) (4.26)
js2 j 6 j ls(x(i+ 1; m); y(j
1
+ 1; n)) j
6 l

i;j
1
from (4:19) (4.27)
Thus we have,
l
m;n
= js
m;n
j
= js1 j + js2 j
6 l
i;j
1
+ l

i;j
1
from (4:26) and (4:27)
6 M
i
from (4:20) (4.28)
From inequalities (4.25) and (4.28), we have
M
i
= l
m;n
 (4.29)
Figure 4.14 gives the overall algorithm, inorporating the above theorem, to determine ls(x; y).
String  is returned as output, and is equal to an ls of input strings x and y. The proedure
returns with either the empty string or a single-symbol ls for a trivial problem. Otherwise,
string x is slied in half and the lengths of the ls of its lower half and varying length prexes
of y, and that of its upper half and varying length suÆxes of y are found. The theorem is
then employed in order to nd the rst position in y, denoted here by k, suh that the ls of
47
ls(m; n; x; y; )
| trivial ase
if n = 0
 = 
else if m = 1
if 9j; 0 6 j 6 n, suh that x
1
= y
j
 = x
1
else
 = 
| non-trivial ase, split the problem
else
i = bm=2
| alulate l
i;j
and l

i;j
for 0 6 j 6 n
ls length(i; n; x(1; i); y(1; n); l1)
ls length(m  i; n; x
R
(m; i+ 1); y
R
(n; 1); l2)
| nd j suh that l
i;j
+ l

i;j
= l
m;n
M = max
06j6n
fl1
j
+ l2
n j
g
k = min j suh that l1
j
+ l2
n j
=M
| solve simpler problems
ls(i; k; x(1; i); y(1; k); 1)
ls(m  i; n  k; x(i+ 1; m); y(k + 1; n); 2)
| onatenate results to produe output
 = 1  2
Figure 4.14: Hirshberg alulation of ls(x; y)
48
the rst half of x and y(1; k) onatenated with that of the seond half of x and y(k + 1; n)
is equal to the desired ls(x; y). It thus only remains to use this value of k in two reursive
alls to the proedure to obtain the required subproblem ls's, and then to onatenate them
to provide the output.
The linear spae harateristi of this algorithm may be demonstrated as follows. Strings x
and y may be held in global storage, and substring parameters may be passed by transferring
as arguments pointers to the start and end of the relevant substrings. As shown earlier, alls
to ls length require temporary storage proportional to m and n. Not ounting reursive
alls, the storage requirement of proedure ls is onstant. It an be shown that during
exeution, there is a total of 2m 1 all to ls. The overall storage requirements are therefore
linear in m and n, i.e. the spae omplexity of this method of determining an ls is O(m+n).
Hirshberg (1975) also provided a temporal analysis of the exeution of the algorithm and
has shown its time omplexity to be O(mn).
4.2.3 Hunt-Szymanski
Hunt and Szymanski's method of extrating an ls from two input strings, x and y, is equiv-
alent to nding a maximal monotonially inreasing path in the graph omposed of points
(i; j) suh that x
i
= y
j
. This will be illustrated by an example one the atual method has
been desribed.
For onveniene, it shall be assumed that the lengths of the input strings are equal, i.e.
j x j = j y j = n. The nal algorithm may, however, be modied easily in order to
aommodate unequal length strings. The number of ordered pairs of positions within x and
y at whih the respetive symbols math, that is the ardinality of the set f(i; j) : x
i
= y
j
g
shall be denoted by r.
Cruial to the method is an array of k
i;l
values, dened as follows
k
i;l
= min j suh that l
i;j
= l (4.30)
where l
i;j
is dened in (4.14).
The value of k
i;l
thus gives the shortest length of prex of y having a subsequene of length l
in ommon with the prex of x of length i. This is illustrated by onsidering the ase where
x = preterit and y = zeitgeist, for example, for whih k
5;1
= 2, k
5;2
= 4, k
5;3
= 6, and
k
5;4
and k
5;5
are not dened.
Sine, by denition, k
i;l
is a minimal value, it follows that the last symbol of a subsequene of
length l ommon to the prexes x(1; i) and y(1; k
i;l
) is equal to y
k
i;l
. Thus, the length of the
ls of x(1; i) and y(1; k
i;l
  1) is equal to l  1. Consequently, k
i;l 1
6 k
i;l
  1, i.e. k
i;l 1
< k
i;l
.
This shows that the values of k
i;l
in eah row of the array must inrease monotonially.
The values of k may be omputed iteratively from previous values in the array. In partiular,
the value of k
i+1;l
may be derived from those of k
i;l 1
and k
i;l
. But before showing how this
is done, the range of values within whih k
i+1;l
must fall shall rst be examined.
49
If x(1; i) and y(1; k
i;l
) have a ommon subsequene of length l, then so too will x(1; i+1) and
y(1; k
i;l
). Thus
k
i+1;l
6 k
i;l
(4.31)
By denition, x(1; i+1) and y(1; k
i+1;l
) have an ls of length l. The ls of prexes one symbol
shorter, namely x(1; i) and y(1; k
i+1;l
  1), will therefore be shorter than l by at most one
symbol. Thus, k
i;l 1
6 k
i+1;l
  1, or
k
i;l 1
< k
i+1;l
(4.32)
Combining (4.31) and (4.32) gives the following bounds for k
i+1;l
k
i;l 1
< k
i+1;l
6 k
i;l
(4.33)
The atual rule for alulating k
i+1;l
from k
i;l 1
and k
i;l
is given below.
k
i+1;l
=

min j suh that x
i+1
= y
j
and k
i;l 1
< j 6 k
i;l
k
i;l
if no suh j exists
(4.34)
This may be shown to be orret as follows. Firstly, onsider the ase for whih no suitable
j exists. Sine k
i+1;l
is a minimal value, the last omponent of any length-l subsequene
ommon to x(1; i+ 1) and y(1; k
i+1;l
) must be equal to y
k
i+1;l
. Realling that k
i+1;l
must lie
within the bounds given in (4.33), it follows that y
k
i+1;l
6= x
i+1
, otherwise k
i+1;l
would be the
required value of j. It then follows that the same length-l subsequene is ommon to x(1; i)
and y(1; k
i+1;l
), implying that k
i;l
6 k
i+1;l
. Combining this with (4.33) gives the result that,
when no suitable j exists, k
i+1;l
= k
i;l
.
Seondly, onsider the ase where there is a j suh that x
i+1
= y
j
and k
i;l 1
< j 6 k
i;l
.
String prexes x(1; i+ 1) and y(1; j) will have a ommon subsequene of length l equal to
the one of length l  1, ommon to x(1; i) and y(1; k
i;l 1
), with symbol x
i+1
appended to the
end (sine x
i+1
= y
j
). Thus, k
i+1;l
6 j. It now remains to determine whether, as is desired,
the equality holds, or whether the value of k
i+1;l
is atually less than our value of j. By
assuming the latter and obtaining a ontradition, the former may be proven. We proeed as
follows. Assume that k
i+1;l
< j. Again, the last symbol of the length-l subsequene ommon
to x(1; i+ 1) and y(1; k
i+1;l
) must be y
k
i+1;l
. Sine both k
i+1;l
and j are onstrained to lie in
the same interval (from (4.33) and (4.34)), we know that y
k
i+1;l
6= x
i+1
as j is the minimum
value in the permitted range suh that y
j
= x
i+1
and we are assuming that k
i+1;l
< j. It then
follows that x(1; i) and y(1; k
i+1;l
) also have a ommon subsequene of length l, implying that
k
i;l
6 k
i+1;l
. Taken in onjuntion with (4.33), this means that k
i;l
= k
i+1;l
. Our assumption
was that k
i+1;l
< j. Thus, from (4.34), k
i+1;l
< k
i;l
, whih obviously ontradits the previous
result. This shows that our value of j is, in fat, equal to the required k
i+1;l
.
An algorithm to determine j ls(x; y) j using the k
i;l
values is given in Figure 4.15. Vetor
kk is used as the i
th
row of array k. Thus, at the start of eah iteration of i, kk
l
= k
i 1;l
for 0 6 l 6 n, and, on ompletion of the iteration, kk
l
= k
i;l
for 0 6 l 6 n. Note that at
the outset, kk
0
is initialised to 0 and the rest of kk to n + 1, signifying undened values of
k
i;l
. Only a single row is required for the alulations, sine the values down a olumn either
remain unaltered or are dereased and the values along a row remain in a monotonially
inreasing order.
50
| initialise
kk
0
= 0
for l = 1 to n
kk
l
= n+ 1
| alulate j ls(x; y) j
for i = 1 to n
for j = n downto 1
if x
i
= y
j
nd l suh that kk
l 1
< j 6 kk
l
kk
l
= j
print max l suh that kk
l
6= n + 1
Figure 4.15: Hunt-Szymanski alulation of j ls(x; y) j
It is important that the j loop ounts down from n to 1. Consider the ase where x
i
mathes
several symbols in y, y
j
1
; y
j
2
; : : : ; y
j
p
say, suh that k
i 1;l 1
< j
1
< j
2
< : : : < j
p
6
k
i 1;l
. From (4.34), it an be seen that the orret value for k
i;l
is therefore j
1
. By iterating
downwards for j, k
i;l
will be assigned suessively smaller values j
p
; j
p 1
; : : : ; j
1
, ending in
the nal assignment of the required value. If, however, the iteration for j were arried out
forwards, then k
i;l
would rst of all be given the orret value but then k
i;l+1
would be set to
j
2
, k
i;l+2
to j
3
, and so on up to k
i;l+p 1
, whih would not be orret.
At the end of the proedure, the value of j ls(x; y) j is given by the maximum l for whih k
n;l
is
dened. As the values in kk inrease monotonially, the `nd' operation may be implemented
in O(logn) time by performing a binary searh. Sine the if statement is exeuted exatly
n
2
times, it follows that the algorithm runs in O(n
2
logn) time in the worst ase. Note that
for unequal length strings, the bound for the kk initialisation and the outer `i' loop would be
set to m rather than n, where m < n. The results of the proedure are demonstrated below,
whih shows the k
i;l
values for x = preterit and y = zeitgeist. It may be seen from the
nal row that j ls(preterit; zeitgeist) j = 5.
i l 0 1 2 3 4 5 6 7 8
1 p 0 10 10 10 10 10 10 10 10
2 r 0 10 10 10 10 10 10 10 10
3 e 0 2 10 10 10 10 10 10 10
4 t 0 2 4 10 10 10 10 10 10
5 e 0 2 4 6 10 10 10 10 10
6 r 0 2 4 6 10 10 10 10 10
7 i 0 2 3 6 7 10 10 10 10
8 t 0 2 3 4 7 9 10 10 10
The exhaustive testing for mathing symbols in the above algorithm is rather ineÆient and
may be avoided by preproessing the strings so as to obtain a prior reord of the positions at
whih they math. An array of pointers to linked lists, mathlist, may thus be onstruted
suh that mathlist[i℄ provides a linked list of positions, j, in desending order, at whih
51
| reate linked lists
for i = 1 to n
mathlist[i℄ = (j
1
; j
2
; : : : ; j
p
) suh that j
1
> j
2
>    > j
p
and x
i
= y
j
q
for 1 6 q 6 p
| initialise
kk
0
= 0
for l = 1 to n
kk
l
= n+ 1
link
0
= null
| alulate suessive kk values
for i = 1 to n
for j in mathlist[i℄
nd l suh that kk
l 1
< j 6 kk
l
if j < kk
l
kk
l
= j
link
l
= newnode(i; j; link
l 1
)
| extrat ls in reverse order
l = max l suh that kk
l
6= n + 1
pointer = link
l
while pointer 6= null
print (i; j) pair pointed to by pointer
advane pointer
Figure 4.16: Hunt-Szymanski alulation of ls(x; y)
x
i
= y
j
. The same physial list may be used for repeated instanes of a symbol in x. For
example, the atual lists for the ase of x = preterit and y = zeitgeist are as follows.
mathlist[1℄ = ()
mathlist[2℄ = ()
mathlist[3℄ = (6, 2)
mathlist[4℄ = (9, 4)
mathlist[5℄ = mathlist[3℄
mathlist[6℄ = ()
mathlist[7℄ = (7, 3)
mathlist[8℄ = mathlist[4℄
The nal algorithm, whih inorporates this improvement and whih also extrats an ls of
the two strings, is given in Figure 4.16. The ls is reovered by utilising a baktraking devie
employed whilst alulating the suessive kk values. Whenever kk
l
is dened, link
l
is set to
point to the head of a list of (i; j) pairs dening a ommon subsequene of length l. This is
implemented by using the newnode proedure, whih reates a node ontaining the urrent
(i; j) pair together with a pointer to the previous node in the list and returns a pointer to the
node just reated.
The preproessing stage may be implemented by sorting the symbols of eah string, keeping
52
note of their original positions, and then merging the sorted strings to reate the mathlist's.
By using the heapsort algorithm, this phase may be performed in O(n logn) time and O(n)
spae. The initialisation stage takes O(n) time. During the alulation of the kk values,
the inner loop is iterated a total of r times. For eah iteration, a binary searh and a few
other onstant-time operations are performed. This stage therefore takes O(r+ r logn) time
and involves the reation of at most r new nodes. Finally, the atual extration of an ls
takes O(j ls(x; y) j) time, whih is, of ourse, O(n). The overall time and spae omplexities
of the algorithm are therefore O((r + n) logn) and O(r + n) respetively. Again, the more
general ase of unequal length strings is easily atered for by setting the bounds for the kk
initialisation and the `i' loops to m rather than n.
For our example of omputing ls(preterit; zeitgeist), the above proedure produes the
following list of (i; j) pairs for the ls (in reverse order).
(8; 9); (7; 7); (5; 6); (4; 4); (3; 2)
This is illustrated below, where the nodes indiate the points (i; j) suh that x
i
= y
j
. The
ls is formed by the symbols orresponding to the lled-in nodes, namely eteit, and is thus
equivalent to a maximal monotonially inreasing path in the graph.
j 1 2 3 4 5 6 7 8 9
i z e i t g e i s t
1 p
2 r
3 e  Æ
4 t  Æ
5 e Æ 
6 r
7 i Æ 
8 t Æ 
Apostolio and Guerra (1987) have developed a variation of the Hunt-Szymanski algorithm,
whih overomes the latter's degradation in behaviour to worse than quadrati in the worst
ase. Apostolio and Guerra's variant has a temporal omplexity of O(m logn+t log(2mn=t)),
whih is at worst O(mn). Here t 6 r is the number of dominant mathes between the strings.
A k-dominant math is an ordered pair (i; j) suh that x
i
= y
j
, l
i;j
= k, and it is the rst
suh point ourring in the submatrix l
(1;:::;i);(1;:::;j)
(l
i;j
is dened in (4.14)). A math (i; j) is
thus dominant if the ourrenes in the appropriate string prexes of every ls(x(1; i); y(1; j))
have x
i
and y
j
as their nal symbols. Apostolio and Guerra's variant involves an eÆient
means of nding the dominant mathes and employs data strutures known as harateristi
trees to represent the various lists required by the algorithm.
An upper bound, formalised by Jaobson and Vo (1992), for the total number of dominant
mathes, t, between two strings, is obtained as follows. The k-dominant mathes may be
sorted in inreasing order of i to give f(i
1
; j
1
); (i
2
; j
2
); : : : ; (i
t
k
; j
t
k
)g, where t
k
denotes the
number of k-dominant mathes, and i
1
< i
2
<    < i
t
k
, j
1
> j
2
>    > j
t
k
. Sine the
53
i's and j's are monotonially inreasing and dereasing, respetively, we have the following
inequalities, where 1 6 p 6 t
k
.
i
p
  i
1
> p  1
j
p
  j
t
k
> t
k
  p (4.35)
The edit distane between prexes x(1; i) and y(1; j), and the length of their ls are related
as follows.
d
i
p
;j
p
= i
p
+ j
p
  2l
i
p
;j
p
from (4:16)
= i
p
+ j
p
  2k (4.36)
Sine the math (i
1
; j
1
) is k-dominant, we have that i
1
> k, and also sine math (i
t
k
; j
t
k
) is
k-dominant, we have that j
t
k
> k. Combining this with (4.36) gives the following.
d
i
p
;j
p
> i
p
+ j
p
  (i
1
+ j
t
k
)
> (i
p
  i
1
) + (j
p
  j
t
k
)
> (p  1) + (t
k
  p) from (4:35)
> t
k
  1 (4.37)
Let the value of k having the greatest number of k-dominant mathes be denoted by k
0
.
An ls(x; y) built using only dominant mathes must inorporate one of these k
0
-dominant
mathes, (i
p
; j
p
), say. The distane between the omplete strings x and y must then be greater
than or equal to that between the prexes x(1; i) and y(1; j), i.e.
d
m;n
> d
i
p
;j
p
) d
m;n
+ 1 > t
k
0
from (4:37) (4.38)
Sine t
k
0
is a maximal number of dominant mathes, i.e. t
k
0
> t
l
for 1 6 l 6 l
m;n
, we have the
following.
t 6 l
m;n
 t
k
0
(4.39)
Combining this with inequality (4.38) gives the following nal result for the upper bound for
the total number of dominant mathes.
t 6 l
m;n
(d
m;n
+ 1) (4.40)
The number of dominant mathes an be muh less than the total number of mathes, espe-
ially when the distane between the strings is small. The worst ase bound for t is obtained
as follows. Substituting for d
m;n
, from (4.16), gives the following.
t 6 l
m;n
(m+ n  2l
m;n
+ 1) (4.41)
Considering the ase of equal length strings, this bound is maximised when the ls length is
(2n + 1)=4, with a orresponding value of (2n+ 1)
2
=8. Reall that the maximum number of
mathes is n
2
. It may thus be seen that, in the worst ase, the total number of mathes is
asymptotially twie that of the dominant mathes.
54
4.2.4 Masek-Paterson
Masek and Paterson (1980) applied the approah of Arlazarov, Dini, Kronod and Faradzev
(1970) | the `Four Russians' | to Wager and Fisher's string distane proedure to obtain
an O(n
2
= logn) time algorithm. Their resulting tehnique is desribed below.
The distane matrix, d, is broken up into a number of square submatries with overlapping
edge vetors. The (i; j; p) submatrix shall be dened as the (p+1) (p+1) submatrix whose
top left element is d
i;j
. From the denition of the entries in the dmatrix, (4.11), it may be seen
that the values in an (i; j; p) submatrix are derived from its relevant substrings, x(i+1; i+ p)
and y(j + 1; j+ p), and from its initial vetors, i.e. its top row (d
i;j
; d
i;j+1
; : : : ; d
i;j+p
) and its
leftmost olumn (d
i;j
; d
i+1;j
; : : : ; d
i+p;j
).
The rst stage of the algorithm is to alulate the values in the nal vetors, i.e. the bottom
row and the rightmost olumn, of all the possible (i; j; p) submatries of any d matrix for
a given alphabet and ost funtion. To do this involves enumerating all the submatries,
requiring all the ombinations of possible substrings and initial vetors to be listed. In order
to enumerate all the length-p substrings, we must assume that the alphabet is nite. Further-
more, attempting to enumerate every possible initial vetor may be prohibitive. However,
restriting the edit osts to be integral multiples of some real number results in there being
only a nite number of dierenes between onseutive values of d for all distane matries
using the same alphabet and ost funtion. It is thus more pratiable to use these dierential
values, rather than the absolute distanes. Dening a step as the dierene between any two
horizontally or vertially adjaent matrix elements leads to the following orollary of the rule
for alulating d
i;j
, (4.11).
(d
i;j
  d
i 1;j
) = min
8
<
:
w(x
i
; );
d
i;j 1
  d
i 1;j
+ w(; y
j
);
d
i 1;j 1
  d
i 1;j
+ w(x
i
; y
j
)
9
=
;
= min
8
<
:
w(x
i
; );
w(; y
j
) + (d
i;j 1
  d
i 1;j 1
)   (d
i 1;j
  d
i 1;j 1
);
w(x
i
; y
j
)   (d
i 1;j
  d
i 1;j 1
)
9
=
;
(d
i;j
  d
i;j 1
) = min
8
<
:
d
i 1;j
  d
i;j 1
+ w(x
i
; );
w(; y
j
);
d
i 1;j 1
  d
i;j 1
+ w(x
i
; y
j
)
9
=
;
= min
8
<
:
w(x
i
; ) + (d
i 1;j
  d
i 1;j 1
)   (d
i;j 1
  d
i 1;j 1
);
w(; y
j
);
w(x
i
; y
j
)   (d
i;j 1
  d
i 1;j 1
)
9
=
;
(4.42)
Eah submatrix may thus be derived from the appropriate substrings, x(i+1; i+p) and y(j+
1; j+p), a starting value d
i;j
, and its initial step vetors | the top row (d
i;j+1
 d
i;j
; : : : ; d
i;j+p
 
d
i;j+p 1
) and the rst olumn (d
i+1;j
  d
i;j
; : : : ; d
i+p;j
  d
i+p 1;j
). The enumeration of all the
possible submatries may be ahieved by enumerating all pairs of length-p strings over our
nite alphabet, C, and all pairs of length-p step vetors.
55
for eah pair of strings u; v 2 C
p
for eah pair of length-p step vetors T; L
| initialise initial step vetors
for i = 1 to p
V
i;0
= L
i
H
0;i
= T
i
| alulate step matrix entries
for i = 1 to p
for j = 1 to p
V
i;j
= min
8
<
:
w(u
i
; );
w(; v
j
) + V
i;j 1
  H
i 1;j
;
w(u
i
; v
j
)   H
i 1;j
9
=
;
H
i;j
= min
8
<
:
w(u
i
; ) + H
i 1;j
  V
i;j 1
;
w(; v
j
);
w(u
i
; v
j
)   V
i;j 1
9
=
;
| save results
R = (V
1;p
; V
2;p
; : : : ; V
p;p
)
B = (H
p;1
; H
p;2
; : : : ; H
p;p
)
store(R; B; L; T; u; v)
Figure 4.17: Masek-Paterson submatrix preproessing
The algorithm to preompute all the submatries is given in Figure 4.17. The enumeration
of the length-p strings and the length-p step vetors is in lexiographi order, assuming xed
orderings on C and on the nite set of possible step values. A step submatrix is evaluated
for eah pair of strings, u and v, and eah pair of initial step vetors, T and L, aording to
(4.42). In order to failitate later retrieval, a proedure store saves the nal step vetors, B
and R, of the submatrix determined by u, v, T and L. Two matries of steps are omputed
during the proedure | V omprises the vertial step values, and H the horizontal ones.
As the innermost loop takes onstant time and is iterated exatly p
2
times for eah pair of
strings and step vetors, eah submatrix is omputed in O(p
2
) time. The number of pairs of
length-p strings over alphabet C is equal to jC j
2p
. If s is the ardinality of the set of possible
step values, whih we will assume to be nite, then there are s
2p
distint pairs of length-p
step vetors. There is therefore a total of (s  jC j)
2p
dierent submatries, giving an overall
preproessing time of O(p
2
(s  jC j)
2p
).
It was previously stated that, for the restrition on the ost funtion, the set of possible step
values was nite. This will now be examined a little more losely. It may be seen from the
rule for d
i;j
and the boundary onditions, (4.11)-(4.12), that the step values are bounded,
independently of the atual strings involved, as shown below.
 I 6 (d
i;j
  d
i 1;j
) 6 D
 D 6 (d
i;j
  d
i;j 1
) 6 I
where D = maxfw(a; ) : a 2 Cg;
I = maxfw(; b) : b 2 Cg (4.43)
56
| initialise rst olumn of the leftmost step-submatries
for i = 1 to m=p
P
i;0
= (w(x
(i 1)p+1
; ); w(x
(i 1)p+2
; ); : : : ; w(x
ip
; ))
| initialise top row of the uppermost step-submatries
for j = 1 to n=p
Q
0;j
= (w(; y
(j 1)p+1
); w(; y
(j 1)p+2
); : : : ; w(; y
jp
))
| lookup nal step vetors for the submatries
for i = 1 to m=p
for j = 1 to n=p
P
i;j
; Q
i;j
= feth(P
i;j 1
; Q
i 1;j
; x((i  1)p+ 1; ip); y((j   1)p+ 1; jp))
| sum the distane inrements to obtain d
m;n
d = 0
for i = 1 to m=p
d = d+ sum(P
i;0
)
for j = 1 to n=p
d = d+ sum(Q
m=p;j
)
Figure 4.18: Masek-Paterson string distane alulation
The ost funtion is said to be sparse if every member of the set of ost weights, namely
fw(a; ) : a 2 Cg [ fw(; b) : b 2 Cg [ fw(a; b) : a; b 2 Cg, is an integral multiple of some
onstant r. It an be shown that, for nite alphabets, if the ost funtion is sparse, then the
set of step values obtained in the submatries is nite irrespetive of the partiular strings
involved, thereby justifying this earlier assumption.
The algorithm to ompute the string distane from the preproessed submatries is given in
Figure 4.18. Assuming that m mod p = n mod p = 0, i and j step through the submatries
vertially and horizontally respetively. P is a matrix of length-p step-submatrix olumn
vetors, suh that P
i;j
is the rightmost olumn of submatrix (i; j), whih is equal to the
leftmost one of submatrix (i; j+1) sine adjaent (p+1)(p+1) submatries share a ommon
border. Similarly, Q is a matrix of length-p step-submatrix row vetors, suh that Q
i;j
is the
bottom row of submatrix (i; j), equal to the top row of submatrix (i+ 1; j). The 0
th
olumn
and row, respetively, of P and Q are initialised with the steps orresponding to the distane
array boundary onditions given in (4.12). Proedure feth returns the rightmost olumn
and the bottom row of submatrix (i; j) by retrieving the preomputed values addressed by the
leftmost olumn and top row of the submatrix together with the relevant substrings. Finally,
the absolute distane between the two input strings, d
m;n
, is obtained by aumulating the
distane dierenes along a path from d
0;0
to d
m;n
. Funtion sum, employed during this
stage, returns the sum of the omponents of the vetor supplied as its argument.
The initialisation stage of this algorithm runs in linear time, as does the nal summing of the
dierenes to obtain the required distane value. The main alulation stage involves 2mn=p
2
length-p vetor lookups and assignments, and thus takes O(mn=p) time. It was noted earlier
that the submatrix preomputations require O(p
2

2p
) time, where  = s  jC j. If p is taken as
minbflog

m;ng=2, then the preproessing time is asymptotially less than that required for
the main omputation. The overall distane alulation, inluding the preproessing, therefore
57
takes O(mn=p) time. This bound is still valid when m and n are not exat multiples of p.
This ase is handled by appending dummy symbols not in the strings,  say, to x and y in
order to bring their lengths up to integral multiples of p. The edit osts involving  are then
set as follows.
w(; ) = 0
w(; ) = 0
w(a; ) = w(a; ) 8a 2 C
w(; b) = w(; b) 8b 2 C (4.44)
An atual minimal ost edit sequene from x to y may be obtained in a manner similar to the
baktraking tehnique of Wagner and Fisher. Obtaining the required distane values for this
involves either realulating the submatries rossed by the optimal edit path, or storing the
omplete submatries during the preproessing stage. Adopting the latter approah allows the
edit sequene to be determined from the ompleted P and Q matries and the preomputed
submatries in linear time, and requires O(n log
2
n) spae for submatrix storage. Note that
the P and Q matries eah require (1 + m=p)  (1 + n=p)  p storage loations, whih is
O(n
2
= logn). The overall spae omplexity of this method is therefore O(n
2
= logn).
The algorithm may also be applied to the problem of determining ls(x; y), by employing
the ost funtion of the edit distane, given by (4.15). The relationship between d(x; y) and
j ls(x; y) j given in (4.16) then permits the length of the ls to be omputed in O(n
2
= logn)
time. An atual ls may then be reovered using a tehnique similar to that used to obtain
an optimal edit sequene.
The omputation of string distanes using this method has been shown to be asymptotially
faster than the quadrati method of Wagner and Fisher. Pratial savings are, however, only
obtained for very long strings. To illustrate this point, Masek and Paterson (1983) alulated
that, for a binary alphabet and the edit distane ost funtion, superior performane is
ahieved only for strings with lengths in exess of 262418.
4.2.5 Ukkonen
Ukkonen's (1985) method of alulating the distane between two strings and obtaining a
minimal edit sequene is based on the dynami programming approah, but requires O(dm)
time and spae, where d is the distane between strings x and y and m is the smaller of the
two string lengths. The method is thus attrative when the inter-string distane is small,
i.e. when the two strings are similar. The basi idea of the algorithm is to nd the heapest
ost path in a direted graph in the d
i;j
matrix, and during the proess unneessary d
i;j
alulations are avoided.
In the following desription, it is assumed, as before, that m 6 n. Reall that in the dynami
programming method, d
i;j
is derived from a minimisation involving its previously omputed
neighbours using relation 4.11. On ompletion of the alulation of d
m;n
, a minimal ost
edit sequene is then obtained by traing a path omposed of transitions that resulted in the
minimum for the values of d
i;j
through the matrix.
58
The dependenies between the elements of the d matrix may be represented by direted ars
| an ar exists from (i; j) to (i
0
; j
0
) only if d
i
0
;j
0
has been obtained from d
i;j
. The resulting
dependeny graph is a subgraph of the larger graph omposed of all the nodes (i; j) and the
vertial, horizontal, and (top left to bottom right) diagonal ars onneting adjaent nodes.
These ars orrespond to deletions, insertions, and substitutions or mathes, respetively.
The osts of these operations may therefore be assoiated with the relevant ars, and thus
the value of d
i;j
is given by the sum of the osts on any path from (0; 0) to (i; j). The ars
and their assoiated osts are as follows.
d
i 1;j
! d
i;j
w(x
i
; )
d
i;j 1
! d
i;j
w(; y
j
)
d
i 1;j 1
! d
i;j
w(x
i
; y
j
)
The terminal values on a path from (i; j) to (i
0
; j
0
) in the dependeny graph are related as
follows, where d is the sum of the osts of the ars in the path.
d
i
0
;j
0
= d
i;j
+ d (4.45)
The distane between the two strings, d
m;n
, is thus the minimum total ost for a path from
d
0;0
to d
m;n
in the dependeny graph.
Dijkstra's algorithm ould be used to nd the heapest path (Aho, Hoproft and Ullman,
1974) in O(mn logmn) time. However, the topology of the graph lends itself to more eÆient
solutions, suh as the dynami programming method. It is further shown below how the
number of entries of the d array that need be alulated may be redued.
Only entries d
i;j
on some path from (0; 0) to (m;n) are relevant for the value of d
m;n
. Note
that this value is O(n), sine any suh path has at most m+n ars. Consider the problem of
asertaining whether or not d
m;n
exeeds some threshold, h, say. From 4.11, it may be seen
that the values of d
i;j
inrease monotonially along any path in the dependeny graph. Thus,
if d
m;n
6 h, and if some d
i;j
> h, then that d
i;j
annot possibly be part of any path to (m;n).
Let w
min
denote the minimum ost of all indels, that is
w
min
= minfw(a; ); w(; a) : a 2 Cg (4.46)
for alphabet C. For non-zero positive weights, w
min
> 0. Also, let the diagonals of the
distane matrix be numbered with integers, k 2 [ m;n℄, suh that diagonal k onsists of
elements (i; j) for whih j   i = k.
Consider any path from (i; j) to (i
0
; j
0
) in the dependeny graph. Element (i; j) lies on diagonal
k = j   i, and (i
0
; j
0
) on diagonal k
0
= j
0
  i
0
. If k
0
  k 6 0, then the path ontains at least
j k
0
  k j deletions (vertial ars), and if k
0
  k > 0, at least j k
0
  k j insertions (horizontal
ars). From 4.45, we then have the following.
d
i
0
;j
0
> d
i;j
+ j(j
0
  i
0
)   (j   i) j w
min
(4.47)
Thus, d
i;j
> jj i j w
min
for every (i; j) on a path from (0; 0) to (m;n). Also, sine d
i;j
6 d
m;n
for every (i; j) on the path, we have the following.
jj   i j 6 d
i;j
=w
min
6 d
m;n
=w
min
(4.48)
59
It is thus suÆient to onsider elements (i; j) in the diagonal band  d
m;n
=w
min
6 j   i 6
d
m;n
=w
min
in order to alulate d
m;n
. This band may, in fat, be further narrowed as shown
below.
Consider a path from (0; 0) to (m;n) in the dependeny graph. This may be deomposed
into the subsidiary paths from (0; 0) to (i; j) and from (i; j) to (m;n). From 4.47 we have the
following.
d
m;n
> d
0;0
+ jj   i j w
min
+ j(n m)   (j   i) j w
min
) d
m;n
> (jj   i j + j(n m)   (j   i) j)  w
min
(4.49)
There are two ases for the relative sizes of i and j to onsider, namely when j 6 i and when
j > i. For the former ase, 4.49 beomes
d
m;n
> ( (j   i) + n m  (j   i))w
min
) d
m;n
=w
min
  (n m) >  2(j   i) (4.50)
Taken together with the fat that j   i is an integer 6 0, this gives the following
 

d
m;n
w
min
  (n  m)

=2

6 j   i 6 0 (4.51)
For the ase where j > i, there are two further possibilities to onsider, namely when n m >
j   i and when n  m 6 j   i. For the former ase, 4.49 beomes
d
m;n
> (j   i+ n m  (j   i))w
min
) d
m;n
=w
min
> n m (4.52)
and for the latter ase
d
m;n
> (j   i+ j   i  (n m))w
min
) d
m;n
=w
min
  (n  m) > 2(j   i)   2(n m) (4.53)
Taken together with the fat that, in this ase, j  i is an integer > 0, this gives the following

d
m;n
w
min
  (n m)

=2

+ (n m) > j   i > 0 (4.54)
Combining (4.51) and (4.54) gives the following bounds for j  i at points (i; j) lying on some
path from (0; 0) to (m;n) in the dependeny graph.
 p 6 j   i 6 n m+ p (4.55)
where p =
j
d
m;n
w
min
  (n m)

=2
k
A onsequene of (4.55) is that, when testing whether d(x; y) 6 h, the omputation of d
i;j
values may be restrited to the diagonal band between the diagonals  p and n m+p, where
p = b(h=w
min
  (n m)) =2.
An algorithm to perform this threshold test is given in Figure 4.19. The proedure returns a
negative value if the test fails, and returns the value of d
m;n
if this is less than or equal to the
60
distane test(h)
if h=w
min
< n m
| trivial ase
return( 1)
else
| initialise
p = b(h=w
min
  (n m)) =2
d
0;0
= 0
for j = 1 to minfn; n m+ pg
d
0;j
= d
0;j 1
+ w(; y
j
)
| alulate d
m;n
for i = 1 to m
for j = maxf0; i  pg to minfn; i+ n m+ pg
if j = 0
d
i;0
= d
i 1;0
+ w(x
i
; )
else
d
i;j
= minfd
i 1;j
+ w(x
i
; ); d
i;j 1
+ w(; y
j
); d
i 1;j 1
+ w(x
i
; y
j
)g
if d
m;n
6 h
return(d
m;n
)
else
return( 1)
Figure 4.19: Ukkonen string distane threshold test
61
h = (n m+ 1)w
min
while (d = distane test(h)) < 0
h = 2h
Figure 4.20: Ukkonen string distane alulation
threshold value. The test fails in the trivial ase given by (4.52), as the distane must at least
be equal to the dierene in length of the strings times the minimum indel ost. For the non-
trivial ase, d
i;j
values are evaluated in the diagonal strip dened by (4.55). On ompletion
of these omputations, the nal value of d
m;n
is ompared with the threshold value.
For eah of the m+1 rows, the number of matrix elements evaluated is at most n m+2p+1,
whih is O(h), as shown below.
p = b(h=w
min
  (n m)) =2
) 2p 6 h=w
min
  (n m)
) n m+ 2p+ 1 6 1 + h=w
min
= O(h) (4.56)
The threshold test therefore exeutes in O(hm) time, and requires O(hm) spae if only the
d
i;j
values in the diagonal band are stored. Note that if only the value of the string distane is
required, then the spae requirement ould be redued to O(h) loations, as only the previous
row is required in the omputation of the urrent one (f. Hirshberg's linear spae algorithm).
In order to determine the atual distane between the two strings, the threshold test proedure
may be alled with suessively inreasing values of h until the test sueeds, as shown in
Figure 4.20. The threshold is initially set to the minimum possible value of the distane plus
w
min
, and is suessively doubled as long as the threshold test fails. On ompletion, the
distane between the two strings is given by d.
Denoting the initial value of h by h
0
, then the next value, h
1
, equals 2h
0
, and the one
after that, h
2
, equals 2
2
h
0
, and so on. Thus, the r
th
value, h
r
, is equal to 2
r
h
0
. If r alls to
distane test are required to determine d(x; y), then the total time required is O(m
P
r
k=0
h
k
),
i.e.O(m(2h
r
 1)) whih isO(mh
r
). But, sine d > h
r
=2, the overall time omplexity isO(dm).
An atual editing sequene from x to y may be obtained from the omputed d
i;j
values on
ompletion as in the Wagner-Fisher method.
Ukkonen also onsidered the speial ase when all the editing operation osts are equal (i.e. the
alulation of the Levenshtein distane), for whih the storage requirements may be redued
and a more eÆient, O(dm) time, diret method of omputing d(x; y) may be employed.
4.2.6 Heaviest Common Subsequene
Jaobson and Vo's (1992) hs algorithm, derived from the Robinson-Shensted longest in-
reasing subsequene (lis) algorithm (Shensted, 1961), is desribed below. The algorithms
62
presented here are in a modied form so that the onstrution of the direted graphs used in
the proedures is orret.
Firstly, the weight of a subsequene, s, ommon to strings x and y may be dened thus
W (s) =
jsj
X
p=1
f(i
p
; j
p
; s
p
) (4.57)
where (i
p
; j
p
) is a mathed pair for x and y, i.e. x
i
p
= y
j
p
= s
p
, and f is a weight funtion.
The dynami programming method of omputing an ls may be adapted to evaluate an hs.
Let W (hs(x(1; i); y(1; j))) be denoted by W
i;j
. The reurrene relation for W
i;j
is then given
by
W
i;j
= maxfW
i 1;j
;W
i;j 1
;W
i 1;j 1
+ f
i;j
g (4.58)
f
i;j
=

0 if x
i
6= y
j
f(i; j; x
i
) if x
i
= y
j
The above follows from the method of alulating the l matrix. When at position (i; j), i.e.
when onsidering prexes x(1; i) and y(1; j), if x
i
= y
j
, then a new ommon subsequene
may be obtained by appending this symbol to the urrent ommon subsequene of x(1; i  1)
and y(1; j   1), and the weight of the new subsequene is obtained by adding the weight of
this math to that of the previous one. Otherwise position (i; j) does not alter the previous
result. The weight for position (i; j) is then given by the heaviest of the previous neighbouring
positions and, if appliable, the new derived weight.
The Robinson-Shensted algorithm to ompute an lis of string y is given in Figure 4.21. L is
an ordered list of pairs (a; k), where a is a string symbol and k is its position in the string.
The following operations may eah be performed in O(logn) time when L is implemented
using a balaned tree struture.
insert(L; a) { insert objet a into list L
delete(L; a) { delete objet a from list L
next(L; a) { return the least objet in L stritly greater than a
prev(L; a) { return the greatest objet in L stritly less than a
max(L) { return the greatest objet in L
min(L) { return the least objet in L
When not dened, proedures next, prev,max andmin all return a null value. The operations
next(L; null) and prev(L; null) are taken to be equivalent tomin(L) andmax(L) respetively.
For eah symbol in y, y
i
, a is the symbol in L to whih y
i
may be appended whilst preserving
the stritly inreasing nature of the list. The element after a in L is then replaed by y
i
.
A direted graph of string symbols is reated using array node. The elements of node are
pointers to nodes reated by proedure newnode and ontaining a symbol, its position in the
string, and the address of a previous node. On ompletion, an lis of y may be reovered by
traing bak through the graph starting from the maximal omponent of L.
63
| initialise list L
L = null
| alulate lis
for i = 1 to n
(a; k) = prev(L; (y
i
; 0))
(b; l) = next(L; (a; k))
if b 6= null
delete(L; (b; l))
insert(L; (y
i
; i))
node
i
= newnode((y
i
; i); node
k
)
Figure 4.21: Robinson-Shensted lis alulation
| initialise list L
L = null
| alulate his
for i = 1 to n
(a; u; k) = prev(L; (y
i
; 0; 0))
(b; v; l) = next(L; (a; u; k))
while ((b; v; l) 6= null) and (u+ f(i; y
i
) > v)
delete(L; (b; v; l))
(b; v; l) = next(L; (b; v; l))
if ((b; v; l) = null) or (y
i
< b)
insert(L; (y
i
; u+ f(i; y
i
); i))
node
i
= newnode((y
i
; u+ f(i; y
i
); i); node
k
)
NOTE that the and in the while loop is a `onditional and,' whih only evaluates its
right hand operand if its left hand one evaluates to true.
Figure 4.22: Jaobson-Vo his alulation
In order to alulate an his, it is neessary to keep trak of the umulative weights of the his's
of the suessively longer string prexes as the omputation progresses. The omponents of
list L may thus be made triples (a; u; k), where a is a symbol of the input string, u is the total
weight of the his ending with a, and k is the position of a in the string. L remains stritly
monotoni in all three elements of its omponent triples. The ordering of L may therefore be
maintained with referene to the lexiographi symbol ordering of the rst element of eah
pair.
The algorithm to ompute an his, based on the lis algorithm, is given in Figure 4.22. Again,
a gives the largest symbol in L less than y
i
, so that the latter may be appended to any
inreasing subsequene ending with a to form a new inreasing subsequene. If there is no
suh a, then u is taken to be 0. The while loop ensures that the strit monotoniity of the
seond element of the omponent triples of L is maintained. The new triple is then inserted
into L if this is onsistent with the preservation of the strit inrease of the rst element of
the triples. Array node is employed to onstrut a linked struture for the reovery of an
64
atual his on ompletion of the proess.
Consider the state of list L at the end of eah iteration, denoted by L
i
. For eah omponent,
(a; u; k) of L
i
, an inreasing subsequene of prex y(1; i) ending in a may be reovered by
traing bak from a through the direted graph. It will be shown below that for every inreas-
ing subsequene, s(1; k), of prex y(1; i), there exists an element of L
i
, b 6 s
k
, terminating
an inreasing subsequene in the graph at least as heavy as s(1; k). The maximal symbol of
L
i
, together with the graph, thus provides an his of prex y(1; i). Consequently, the maximal
element of L on ompletion of the proess denes an his for the omplete string y.
The above proposition is learly true for i = 1. We may thus assume it to be true for iteration
i   1 and analyse the situation for iteration i. Let s(1; k) be an inreasing subsequene of
prex y(1; i). From the foregoing, if s
k
= y
i
, then there is an element b 6 s
k 1
in L
i 1
dening
an inreasing subsequene at least as heavy as s(1; k  1). Now, element b annot be deleted
from the list on this iteration, as b < y
i
. At the end of the i
th
iteration, y
i
will either have
been inserted into the list or will have already been present. Realling that the umulative
weights in L are stritly inreasing, it may be seen that the inreasing subsequene dened
by y
i
satises the proposition.
Turning our attention to the situation where s
k
6= y
i
, there are two ases to onsider, namely
when s
k
< y
i
and when s
k
> y
i
. The proposition holds for the former ase sine the portion
of the list preeding y
i
is not altered by the i
th
iteration. Thus only the ase where s
k
> y
i
remains to onsider. Sequene s(1; k) must then also be an inreasing subsequene of y(1; i 1).
There will thus exist an inreasing subsequene dened by some b in L
i 1
at least as heavy
as s(1; k). If b is extant in L
i
, then learly the proposition holds. Otherwise, b must have
been eliminated from the L during the i
th
iteration, for whih ase the weight ondition of
the while loop ensures that the sequene dened by y
i
is at least as heavy as that previously
dened by b. The above proposition has thus been shown to be valid under all irumstanes,
and the algorithm therefore orretly alulates an his of the input string.
The main loop of the algorithm is iterated a total of n times, and eah of its omponent
operations takes at most O(logn) time. Note that the total number of iterations of the
inner while loop is at most n, sine eah of the n elements of y may be inserted into, and
deleted from, list L one at most. The temporal omplexity of the his algorithm is therefore
O(n logn).
The operation of the method may be illustrated by way of an example. The (a; u) omponents
of the triples of list L for eah step i are shown below for the ase of y = zeitgeist. Note
that the lis of this string is eigst. However, the weights have been hosen to be equal to 1
for the rst ourrene of a symbol in the string, and equal to 2 for the seond ourrene.
Under these onditions, inreasing subsequene eist using the seond e and i is heavier than
lis eigst. On ompletion, the maximal element of L, (t,7), is used to reover the path e  
i  s  t from the direted graph.
i 1 2 3 4 5 6 7 8 9
y
i
z e i t g e i s t
f(i; y
i
) 1 1 1 1 1 2 2 1 2
L
i
(z,1) (e,1) (e,1) (e,1) (e,1) (e,2) (e,2) (e,2) (e,2)
65
(i,2) (i,2) (g,2) (t,3) (i,4) (i,4) (i,4)
(t,3) (t,3) (s,5) (s,5)
(t,7)
Direted graph
null <--- (z,1)
null <--- (e,1) <--- (i,2) <--- (t,3)
|
----<--- (g,2)
null <--- (e,2) <--- (i,4) <--- (s,5) <--- (t,7)
If the mathes (i; j) between strings x and y are listed in inreasing order of i, and dereasing
order of j for equal i values, then every ommon subsequene of x and y maps to an inreasing
subsequene of the sequene of j values. Conversely, a ommon subsequene may be derived
from an inreasing subsequene of j values. The desending order of j for onstant i prevents
multiple symbols of y mathing a single symbol of x from being inluded in a subsequene.
The example given below is for the strings preterit and zeitgeist. The underlined math
indies show the lis of the j sequene, orresponding to the ls of the two strings.
i 1 2 3 4 5 6 7 8
x
i
p r e t e r i t
- - - - -
j 1 2 3 4 5 6 7 8 9
y
j
z e i t g e i s t
- - - - -
Mathes
i 3 3 4 4 5 5 7 7 8 8
j 6 2 9 4 6 2 7 3 9 4
- - - - -
In the above ordering of the mathes, orresponding weights, f(i; j; x
i
), may be assoiated
with the j values. An hs of the strings may thus be omputed by alulating the his of this
sequene of j values. The method to do this, given in Figure 4.23, is a generalisation of the
Hunt-Szymanski ls algorithm. Note that only the (a; u) omponents of the triples of list L
are shown here for larity.
Array position ontains ordered lists of the positions of the symbols ourring in y. This is
thus similar to the mathlist of the Hunt-Szymanski algorithm. The latter was in desending
order, whereas the lists of position are in asending order. However, the orret ordering
66
| onstrut ordered lists of positions of symbols in y
for i = 1 to n
insert(position
y
i
; i)
| initialise list L
L = null
| alulate hs
for i = 1 to m
j = max(position
x
i
)
while j 6= null
(a; u) = prev(L; (j; 0))
(b; v) = next(L; (a; u))
while ((b; v) 6= null) and (u+ f(i; j; x
i
) > v)
delete(L; (b; v))
(b; v) = next(L; (b; v))
if ((b; v) = null) or (j < b)
insert(L; (j; u+ f(i; j; x
i
)))
j = prev(position
x
i
; j)
NOTE that the and in the while loop is a `onditional and'
Figure 4.23: Jaobson-Vo hs alulation
of the mathes is ahieved by stepping bakwards through the j positions in position
x
i
for
eah i. The his algorithm is applied to the resulting sequene of j values. The orresponding
hs(x; y) may then be reovered on ompletion of the proess from the direted graph, the
onstrution of whih has also been omitted from the algorithm for larity.
The total length of the sequene of j values is equal to the number of mathes, r. The
proessing time also depends on the size of list L, whih is bounded by the length of the
shorter of the two input strings. The overall running time is thus O((r+m) logn).
The following example serves to illustrate the omputation of an hs. As mentioned in
the previous hapter, a minimal distane weight funtion may be applied in order to give
preferene in the seletion of ommon subsequenes to losely aligned mathes. Consider the
pair of strings warfare and forewarn. The longest ommon subsequenes of this pair are
shown below, together with their weights for the minimal distane weight funtion 8  j i  j j,
whih favours losely aligned mathes. The total ls weights are given in parentheses.
i 1 2 3 4 5 6 7
x
i
w a r f a r e
y
j
f o r e w a r n
j 1 2 3 4 5 6 7 8
x
i
w a r w a r w a r
67
i 1 2 3 1 2 6 1 5 6
j 5 6 7 5 6 7 5 6 7
8  j i  j j 4 4 4 (12) 4 4 7 (15) 4 7 7 (18)
x
i
r a r f a r f r e
i 3 5 6 4 5 6 4 6 7
j 3 6 7 1 6 7 1 3 4
8  j i  j j 8 7 7 (22) 5 7 7 (19) 5 5 5 (15)
It may thus be seen that rar is the heaviest ls. The state of list L after eah stage i in
the omputation of hs(warfare; forewarn) for the above weight funtion is shown below,
together with the resulting direted graph. One again, only the (a; u) omponents of the list
triples are given for simpliity.
i 1 2 3 4 5 6 7
x
i
w a r f a r e
L
i
(5,4) (5,4) (3,8) (1,5) (1,5) (1,5) (1,5)
(6,8) (7,12) (3,8) (3,8) (3,10) (3,10)
(7,12) (6,15) (6,15) (4,15)
(7,22) (7,22)
Direted graph
null <--- (5,4) <--- (6,8) <--- (7,12)
null <--- (3,8) <--- (6,15) <--- (7,22)
null <--- (1,5) <--- (3,10) <--- (4,15)
On ompletion, the maximal element in L, namely (7; 22), is used to retrieve path 3 6 7
from the direted graph of j index values, orresponding to the subsequene rar.
A speial ase of the hs problem is when the weights are position-independent, i.e. when f
is a funtion only of the symbols. The hs algorithm may be adapted to take advantage of
this speial ase as disussed below.
When the weight funtion does not depend on position, the weights assoiated with the
positions j in list position
x
i
are all equal. Consider the ase of a given j being inserted into
list L after a. Realling that the j values from the position list are proessed in desending
order, if the next value of j tried is also greater than a, then the old j will be deleted from L
and the new one inserted in its plae, sine they both have the same weight. This ineÆieny
may be avoided by going diretly to the least j in position
x
i
greater than a. So, j may be
assigned the value of next(position
x
i
; a) after the assignments of (a; u) and (b; v).
68
| onstrut ordered lists of positions of symbols in y
for i = 1 to n
insert(position
y
i
; i)
| initialise list L
L = null
| alulate hs
for i = 1 to m
j = max(position
x
i
)
while j 6= null
(a; u) = prev(L; (j; 0))
(b; v) = next(L; (a; u))
j = next(position
x
i
; a)
while ((b; v) 6= null) and (u+ f(x
i
) > v)
delete(L; (b; v))
(b; v) = next(L; (b; v))
insert(position
y
b
; b)
insert(position
y
b
; b)
insert(L; (j; u+ f(x
i
)))
delete(position
x
i
; j)
j = prev(position
x
i
; j)
NOTE that the and in the while loop is a `onditional and'
Figure 4.24: Jaobson-Vo hs alulation for position-independent weights
If j is already in list L, then b is assigned the value of j. If the entry in L immediately
preeding j is the same as that when j was inserted, then the algorithm will simply delete j
from L and then reinsert it at the same point. Thus, when an element j is inserted into L, it
may then be deleted from position
x
i
, in order to irumvent any unneessary dupliation of
eort. It must, however, be reinstated in its position list should it be removed from L or its
predeessor in L be hanged.
Finally, the ondition for inserting j into L is not required when the weights are position-
independent. The value of b annot be less than j, sine a is the greatest value in L less than
j. Thus, b > j. For the ase where b > j, j may be inserted into L between a and b without
upsetting the list's monotoniity. If b = j, then j may safely replae b, sine their weights are
also the same. This then shows that j may be inserted unonditionally into L.
The hs algorithm inorporating the above enhanements for the speial ase of position-
independent weights is given in Figure 4.24, whih, for larity, again omits the onstrution
of the direted graph and only depits the (a; u) pairs of the omponent triples of list L. When
the weights for all the symbols in the alphabet are equal, this redues to the Apostolio-Guerra
ls algorithm.
69
4.3 Approximate String Mathing
4.3.1 Landau-Vishkin k-mismath
In Landau and Vishkin's (1985, 1986a) k-mismath algorithm the text string is analysed using
a 2-dimensional pattern mismath table generated during a pattern preproessing stage. The
alulation of this table, pm[1 : : :m 1; 1 : : :2k+1℄, shall be onsidered in due ourse. Firstly,
however, the text analysis stage shall be examined.
The analysis of the text is performed in a 2-dimensional text mismath array, tm[0 : : :n  
m; 1 : : :k + 1℄. On ompletion of the analysis, row i ontains the positions within x of the
rst k+ 1 mismathes between strings x(1; m) and y(i+ 1; i+m). Thus, if tm[i; v℄ = l, then
y
i+l
6= x
l
and this is the v
th
mismath between x(1; m) and y(i+ 1; i+m) from left to right.
If the number of mismathes between x and substring y(i+1; i+m),  say, is less than k+1,
then the default value m+ 1 appears from loation + 1 onwards, i.e.
tm[i; + 1℄ = tm[i; + 2℄ = : : := tm[i; k+ 1℄ = m+ 1
Note that a value ofm+1 for tm[i; k+1℄ indiates that text substring y(i+1; i+m) diers from
pattern x by at most k mismathes, and therefore provides a solution to the problem. This is
illustrated in the following example. Consider the ase where x = tram, y = thetrippedtrap
and k = 2. It may be seen from the following text mismath table that ourrenes of the
pattern with up to 2 mismathes are to be found starting at y
4
and y
11
, namely substrings
trip and trap.
1 2 3
0 2 3 4
1 1 2 3
2 1 2 3
3 3 4 5
4 1 2 3
5 1 2 3
6 1 2 3
7 1 2 3
8 1 2 3
9 1 2 3
10 4 5 5
The text analysis algorithm is given in Figure 4.25. The pattern is sanned aross the text from
left to right one plae at a time by the for loop. At iteration i, text substring y(i+1; i+m) is
ompared with the pattern. The rightmost position in the text reahed by previous iterations
is given by j, i.e. it is the maximum value of r + tm[r; k+ 1℄, where 0 6 r < i. If i < j, then
proedure merge is alled, whih nds mismathes between x(1; j  i) and y(i+1; j) and sets
b equal to the number found. If this does not exeed k, then proedure extend is invoked.
This sans the text from y
j+1
onwards, until it either nds k + 1 mismathes or it reahes
70
| initialise
tm[0 : : :n m; 1 : : :k + 1℄ = m+ 1
r = 0
j = 0
| san text
for i = 0 to n m
b = 0
if i < j
merge(i; r; j; b)
if b < k + 1
r=i
extend(i; j; b)
Figure 4.25: Landau-Vishkin k-mismath string mathing
extend(i; j; b)
while (b < k + 1) and (j   i < m)
j = j + 1
if y
j
6= x
j i
b = b+ 1
tm[i; b℄ = j   i
Figure 4.26: Proedure extend
y
i+m
with at most k mismathes, in whih ase a pattern ourrene has been found starting
at y
i+1
.
The operation of extend is straightforward, and the proedure is given in Figure 4.26. Pairs
of symbols from the substrings y(j + 1; i+m) and x(j   i+ 1; m) are ompared, and, in the
event of a mismath, b is inremented and the text mismath table is updated.
The operation of proedure merge shall now be examined. As noted earlier, this proedure
nds mismathes between x(1; j   i) and y(i + 1; j) and sets b equal to the number found.
Thus, merge evaluates tm[i; 1 : : : b℄ for b 6 k + 1, and in so doing uses information obtained
previously. Firstly, reall that row r of the text mismath table gives the mismathes obtained
when the start of the pattern is aligned with y
r+1
and that r + tm[r; k + 1℄ is the present
rightmost position examined in the text. Mismathes in row r of tm lying to the right of y
i
in the text are therefore relevant in the omparison of the pattern with the text starting from
y
i+1
. The appropriate values from the text mismath table are thus tm[r; q : : : k+1℄, where q
is the least integer suh that r+ tm[r; q℄ > i. However, aount must be taken of the fat that
these mismathes orrespond to the start of the pattern being aligned with y
r+1
, whereas the
urrent pattern position is aligned with y
i+1
| a dierene of i  r plaes.
The seond soure of information used by merge is thus the preomputed pattern mismath
table, pm, whih gives the positions of the mismathes between the pattern and itself at
various relative shifts. Row i of array pm[1 : : :m 1; 1 : : :2k+1℄ ontains the positions within
71
x of the rst 2k + 1 mismathes between the substrings x(1; m   i) and x(i + 1; m), i.e.
the overlapping portions of the two opies of the pattern for a relative shift of i. Thus, if
pm[i; u℄ = l, then x
i+l
6= x
l
and this is the u
th
mismath between x(1; m  i) and x(i+ 1; m)
from left to right. If the number of mismathes between these substrings,  say, is less than
2k + 1, then the default value m+ 1 appears from loation + 1 onwards, i.e.
pm[i; + 1℄ = pm[i; + 2℄ = : : : = pm[i; 2k+ 1℄ = m+ 1
It is thus row i   r of the pattern mismath table whih is of interest in merge, and the
relevant values are pm[i r; 1 : : :t℄, where t is the rightmost mismath in pm[i r; 1 : : :2k+1℄
suh that pm[i r; t℄< j i+1, sine only the mismathes in substring x(1; j i) are required.
In order to see how the information mentioned above may be utilised in proedure merge,
onsider a text loation p within the relevant range, i.e. i+ 1 6 p 6 j. Position p may satisfy
either, neither, or both of the following onditions.
A: with x
1
aligned with y
r+1
, text position p orresponds to a previously deteted mismath
between pattern and text, i.e. y
p
6= x
p r
, and this is mismath number v, for some v
suh that q 6 v 6 k + 1, i.e. p  r = tm[r; v℄.
B: with two opies of the pattern, having a relative shift of i   r, aligned with the text
suh that their initial symbols lie above y
r+1
and y
i+1
respetively, text position p
orresponds to a mismath between the two patterns, i.e. x
p r
6= x
p i
. This will be the
u
th
mismath for this shift, for some u suh that 1 6 u 6 t, i.e. p  i = pm[i  r; u℄.
Reall that the question of importane at text position p is whether or not the text symbol
there mathes the orresponding pattern symbol when x
1
is aligned with y
i+1
| does y
p
=
x
p i
? The bearing on this question of the possible situations with regards to the above two
onditions shall now be examined.
Case 1: p satises neither Condition A nor Condition B. Thus, y
p
= x
p r
and x
p r
= x
p i
,
so y
p
= x
p i
. No expliit omparison is required between pattern and text as there
must therefore be a math at this position.
Case 2: p satises only one of the two onditions, i.e. either Condition A or Condition B, but
not both. In either ase, y
p
6= x
p i
, whih may be shown as follows. If only Condition
A is satised, then y
p
6= x
p r
and x
p r
= x
p i
, so y
p
6= x
p i
. If, on the other hand,
only Condition B holds, then y
p
= x
p r
and x
p r
6= x
p i
, so again y
p
6= x
p i
. Again,
no expliit omparison is required sine there must be a mismath at text position p.
Case 3: both Condition A and Condition B hold. No onlusion onerning y
p
and x
p i
may
be drawn, requiring the two symbols to be ompared diretly.
In Case 2 and if a mismath is found in Case 3, the mismath ount, b, must be inremented
and tm[i; b℄ updated by proedure merge.
72
merge(i; r; j; b)
u = 1
v = q
while (b < k + 1) and (v < k + 2) and (i+ pm[i  r; u℄ 6 j or tm[r; v℄ 6= m+ 1)
if i+ pm[i  r; u℄> r + tm[r; v℄
| Case 2, Condition A
b = b+ 1
tm[i; b℄ = tm[r; v℄  (i  r)
v = v + 1
else if i+ pm[i  r; u℄ < r + tm[r; v℄
|Case 2, Condition B
b = b+ 1
tm[i; b℄ = pm[i  r; u℄
u = u+ 1
else
| i+ pm[i  r; u℄ = r + tm[r; v℄
| Case 3
if x
pm[i r;u℄
6= y
i+pm[i r;u℄
b = b+ 1
tm[i; b℄ = pm[i  r; u℄
u = u+ 1
v = v + 1
Figure 4.27: Proedure merge
We are now in a position to desribe proedure merge, whih is given in Figure 4.27. As
noted earlier, the relevant table values for merge are pm[i   r; 1 : : :t℄ and tm[r; q : : : k + 1℄.
Variables u and v are set initially to index the rst elements of these two vetors respetively,
and subsequently step through the vetor omponents.
The onditions for terminating the proedure are as follows. Firstly, if b = k + 1, then k + 1
mismathes have been found for the pattern positioned suh that x
1
is aligned with y
i+1
, and
the proedure may therefore be exited. Seondly, realling that the rightmost text position of
interest in merge, namely j, is equal to r+ tm[r; k+1℄, if v = k+2, then tm[r; k+1℄ will have
been used for the previous value of v, namely v = k + 1, and position j must therefore have
been passed. Hene, the proedure may also be exited in this ase. Finally, the proedure
may also be terminated if i + pm[i   r; u℄ > j and tm[r; v℄ = m + 1. If the latter part of
this ondition holds, then r+ tm[r; v℄ will equal j, as will orresponding sums for subsequent
values of v up to k+1. In this ase, the proedure may therefore be terminated if the former
part of the above ondition also holds, as this indiates that text position j has atually been
passed.
It now remains to demonstrate that the number of mismathpositions in the pattern mismath
table is suÆient formerge to nd all or, if there are more than k+1, the rst k+1 mismathes
for y(i+1; j). This may be shown as follows. Condition A holds for at most k+1 text positions
in the range [i+ 1; j℄. Condition B holds for some unknown number of positions in the same
73
interval. Row i   r of the pattern mismath table, pm[i   r; 1 : : :2k + 1℄, ontains at most
2k + 1 mismath positions between two opies of the pattern with a relative shift of i  r. If
pm[i  r; 2k+ 1℄ > j   i, then the table ontains all the pattern-pattern mismath positions
for whih Condition B holds for text positions in the interval [i+ 1; j℄. On the other hand, if
pm[i r; 2k+1℄< j i, then the table an provide 2k+1 text positions in the range [i+1; j 1℄
for whih Condition B holds. Sine j = r + tm[r; k+ 1℄, there are up to k text positions in
the range [i + 1; j   1℄ for whih ondition A holds. In the worst ase, there ould thus be
k positions for whih Case 3 holds, and whih would thus require expliit omparisons. This
would then leave at least k + 1 positions satisfying Condition B but not Condition A (Case
2), whih is suÆient to establish that there are at least k + 1 mismathes between pattern
and text for the partiular pattern alignment.
The temporal omplexity of the text analysis shall now be examined. Exluding alls to
proedures merge and extend, eah of the n   m + 1 iterations of the text analysis loop
takes onstant time, giving a total of O(n) time. The total number of operations performed
by extend during all its alls is O(n), sine it inspets eah text symbol one at most. On
eah all, merge operates on the vetors pm[i   r; 1 : : :2k + 1℄ and tm[r; 1 : : :k + 1℄, whih
have a ombined total of 3k+ 2 elements. The operations of merge may be aounted for by
assoiating onstant time operations with eah of these entries, resulting in a running time
for eah invoation of O(k). It may thus be seen that the overall running time of the text
analysis is O(nk).
All that remains now is to address the omputation of the pattern mismath table during the
pattern preproessing stage. Without loss of generality, it may be assumed that m is some
power of 2. The preproessing algorithm uses a partition of the set of the m  1 rows of pm,
f1; 2; : : : ; m  1g, into the following log
2
m subsets
f1g; f2; 3g; f4; 5; 6; 7g; : : : ; fm=2; : : : ; m  1g
and has log
2
m stages. At stage l, where 1 6 l 6 log
2
m, the rows of pm in set l are omputed,
where set l is f2
l 1
; : : : ; 2
l
  1g.
The method used to ompute the table is based on that used during the text analysis
phase, and the algorithm for stage l is shown in Figure 4.28. At stage l, the inputs to
the pattern analysis algorithm are the pattern substrings x(1; m  2
l 1
) and x(2
l 1
+ 1; m),
whih orrespond to the pattern and text in the text analysis respetively, and the sub-array
pm[1 : : :2
l 1
 1; 1 : : :minf2
log
2
m l
4k+1; m 2
l 1
g℄ whih ontains the outputs from the pre-
vious l   1 stages. (The number of elements in the rows of this sub-array shall be explained
later.) The outputs of stage l are entered into pm[2
l 1
: : :2
l
  1;minf2
log
2
m l
2k+1; m  2
l
g℄.
With the exeption of stage log
2
m, in whih up to 2k + 1 mismathes are found, it is thus
up to minf2
log
2
m l
2k + 1; m  2
l
g mismathes whih are to be found for eah row of pm at
stage l, rather than up to k + 1 as in the text analysis algorithm.
Using arguments similar to those of the previous examination of the orret operation of
proedure merge, it may be shown that in order to nd up to the requisite number of mis-
mathes, minf2
log
2
m l
4k + 1; m  2
l 1
g positions for whih Condition B holds are required
at stage l, and 4k + 1 suh loations for the speial ase of stage log
2
m.
For eah stage, l, of the log
2
m stages of the pattern analysis, the for loop makes 2
l 1
iterations
74
| initialise
pm[2
l 1
: : :2
l
  1; 1 : : :minf2
log
2
m l
2k + 1; m  2
l
g℄ = m+ 1
r = 2
l 1
j = 2
l 1
| alulate rows of pm in set l
for i = 2
l 1
to 2
l
  1
b = 0
if i < j
merge(i; r; j; b)
if b < minf2
log
2
m l
2k+ 1; m  2
l
g
r = i
extend(i; j; b)
Figure 4.28: Landau-Vishkin pattern preproessing
(2
l 1
6 i 6 2
l
  1). Exluding alls to merge and extend, eah iteration takes onstant time.
For all the iterations of stage l, extend requires O(m) time. It was shown earlier that eah all
of merge requires time proportional to the number of mismathes searhed for. Thus, eah
all of merge takes O(minf2
log
2
m l
2k + 1; m  2
l
g), whih is O(2k2
log
2
m l
). The total time
for stage l is therefore O(m+ 2
l 1
(2k2
log
2
m l
)) = O(km). Summing this for all stages gives
a total preproessing time of O(
P
log
2
m
l=1
km) = O(km logm). The overall time omplexity of
the pattern preproessing and the text analysis is thus O(k(n+m logm)).
4.3.2 Landau-Vishkin k-dierene
Landau and Vishkin's (1986b, 1989) k-dierene algorithm is based on an approah similar
to Ukkonen's (1983, 1985a) dynami programming string distane proedure. Before dealing
with their algorithm, the dynami programming method and a Ukkonen-style adaptation
thereof shall rstly be desribed.
Reall that the elements, d
i;j
, dened by (4.9), of the distane matrix give the distane
between prexes of the strings x and y, of lengths i and j respetively. In order to solve
the k-dierene problem, the distane matrix may be adapted suh that d
i;j
represents the
minimum distane between x(1; i) and any substring of y ending at y
j
(Sellers, 1980). This
hange may be aommodated simply by altering the boundary onditions (given by (4.12))
suh that
d
0;j
= 0 for 0 6 j 6 n (4.59)
sine the minimum distane between  and any substring of y is 0.
The rest of the matrix is then omputed as before using the Levenshtein distane edit osts,
(4.10), and the reurrene relation for d
i;j
, (4.11). On ompletion, any value not exeeding k
in the nal row indiates a position in the text where a substring having at most k dierenes
from the pattern ends. This is illustrated in the example given below, whih depits the
distane matrix for the ase of x = ABCDE and y = ACEABPCQDEABCR. For the ase of k = 2, it
may be seen from row 5 of the matrix that ourrenes of the pattern, with up to 2 dierenes,
75
end at text positions 3, 10, 13 and 14. The orresponding text substrings for these positions
are ACE, ABPCQDE, ABC and ABCR.
j 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
i A C E A B P C Q D E A B C R
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 A 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1
2 B 2 1 1 2 1 0 1 2 2 2 2 1 0 1 2
3 C 3 2 1 2 2 1 1 1 2 3 3 2 1 0 1
4 D 4 3 2 2 3 2 2 2 2 2 3 3 2 1 1
5 E 5 4 3 2 3 3 3 3 3 3 2 3 3 2 2
Reall from Ukkonen's string distane algorithm that the matrix diagonals may be numbered
with integers, p 2 [ m;n℄, suh that diagonal p onsists of elements (i; j) for whih j  i = p.
Let r
p;q
represent the largest row i suh that d
i;j
= q and (i; j) is on diagonal p. Thus, q is
the minimal number of dierenes between x(1; r
p;q
) and any text substring ending at y
r
p;q
+p
,
and furthermore x
r
p;q
+1
6= y
r
p;q
+p+1
sine r
p;q
+ 1 would otherwise be a row larger than r
p;q
satisfying the required onditions.
Returning to the previous example, diagonal 9 of the distane matrix is shown below. It may
be seen that the r
p;q
values for this are as follows | r
9;0
= 0, r
9;1
= 4 and r
9;2
= 5.
j 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
i A C E A B P C Q D E A B C R
0 0
1 A 1
2 B 1
3 C 1
4 D 1
5 E 2
An r
p;q
value of m, for q 6 k, indiates that there is an ourrene of the pattern with up to
k dierenes in the text ending at y
m+p
. In order to solve the k-dierene problem, it is thus
suÆient to alulate the r
p;q
values for whih q 6 k.
In order to explain how the r
p;q
values may be omputed, onsider the evaluation of the d
i;j
entries in the dynami programming matrix. If d
i;j
= q, this entry is on diagonal p, and i is
the largest row satisfying these requirements, then by denition r
p;q
= i. The value of d
i;j
will have been derived from one of the adjaent previous values in the matrix aording to
(4.11). It may thus have been obtained from one or more of the following possibilities.
1 d
i 1;j 1
= q and x
i
= y
j
2 d
i 1;j 1
= q   1 and x
i
6= y
j
3 d
i;j 1
= q   1
76
| initialise
r
p; 1
=  1 for 0 6 p 6 n
r
p;jpj 1
= jp j   1, r
p;jpj 2
= jp j   2 for  (k + 1) 6 p 6  1
r
n+1;q
=  1 for  1 6 q 6 k
| alulate r
p;q
values
for q = 0 to k
for p =  q to n
r = maxfr
p;q 1
+ 1; r
p 1;q 1
; r
p+1;q 1
+ 1g
r = minfr;mg
while (r < m) and (r+ p < n) and (x
r+1
= y
r+1+p
)
r = r+ 1
r
p;q
= r
if r
p;q
= m then there is a k-dierene ourrene ending at y
p+m
Figure 4.29: Dynami programming alulation of r
p;q
values
4 d
i 1;j
= q   1
Given that the last ourrene of value q 1 in diagonal p has previously been found, then the
last ourrene on the diagonal of value q may then be found by stepping through subsequent
elements on the diagonal while x
i
= y
j
.
Note that the rst possible k-dierene pattern ourrene is at position j = m   k in the
text. This position in rowm lies on diagonal  k, and thus diagonals p <  k are of no interest
in the omputation. Also note that adjaent elements in the matrix may dier by only 0 or
1. The algorithm given in Figure 4.29 may be used to ompute the r
p;q
values. An initial
value for row r is obtained from previous values r
p;q 1
, r
p 1;q 1
and r
p+1;q 1
, and is then
inremented suessively by one at a time until the orret value of r
p;q
is reahed.
The algorithm may be shown to alulate orretly the r
p;q
values as follows. Consider the
ase where q = 0 and the values of r
p;0
(p > 0) are being alulated. From the boundary
onditions, r is initialised to 0. The while loop nds that x(1; r
p;0
) = y(p+ 1; p+ r
p;0
) and
x
r
p;0
+1
6= y
p+r
p;0
+1
, thus setting r
p;0
to the orret value. For diagonals p < 0, the orret
values for r
p;jpj 1
and r
p;jpj 2
will have been set during the initialisation. Let q = t, and assume
that all r
p;t 1
values are orret. Consider the alulation of r
p;t
(p >  t). Variable r is set
to the largest row on diagonal p suh that d
r;r+p
an beome value t by possibilities 2, 3 or
4 desribed earlier. The while loop then steps through suessive positions on the diagonal
where the pattern and text symbols math to arrive at the orret value for r
p;t
.
The algorithm omputes the r
p;t
values on n+k+1 diagonals. Row variable r may be assigned
at most m dierent values for eah diagonal, leading to an O(mn) omputation time. The
omplexity is thus the same as that for the straightforward dynami programming method
desribed earlier. However, it shall now be shown how the framework of this method has been
employed to develop an algorithm with improved performane.
The improvement in performane is ahieved by altering the way in whih the r
p;q
values are
77
alulated. In the above method, variable r is inremented suessively in unit steps until it
reahes the orret value of r
p;q
. However, by virtue of appropriate preproessing, the value
of r
p;q
may be found in onstant time. The manner in whih this may be ahieved shall be
desribed below, but rst we turn our attention to the requisite preproessing.
The preproessing stage involves the onstrution of a suÆx tree, whih may be dened as
follows. Consider string s = s(1; l), where s
l
= $ is a speial end marker, not part of the
alphabet over whih s(1; l  1) is a string. Eah suÆx of s, s(i; l) for 1 6 i 6 l, denes one of
the l leaves of the suÆx tree of s. All the edges of the tree are direted away from the root,
and the out-degree of eah node is either zero (for a leaf node) or > 2. If s(i; i+ p) is the
longest ommon proper prex of two suÆxes s(i; l) and s(j; l), i.e. s(i; i+ p) = s(j; j+ p) and
s
i+p+1
6= s
j+p+1
, then s(i; i+ p) denes an internal node of the tree. Two nodes representing
a substring of s, b, and a proper prex thereof, a, say, are onneted by an edge only if there
is no node representing a substring of s, , say, suh that  is a proper prex of b and a is a
proper prex of . The funtion of the speial terminator symbol $ is to separate in the tree
suÆxes s(i; l   1) and s(j; l   1) whenever one is a proper prex of the other. Note that a
suÆx tree for a given string is unique up to isomorphism of graphs. Note that the suÆx tree
data struture is onsidered in greater detail in the following setion of this hapter.
As an example, the suÆx tree of the string EWEW$ is depited below, where eah node is
labelled with its dening substring together with a parenthesised node number.
78
root ----------------------- (1) $
| |
| |
| --------- (2) W -------- (4) W$
| |
| |
| --------- (5) WEW$
|
|
------------ (3) EW ------- (6) EW$
|
|
--------- (7) EWEW$
In the onstrution of the tree, the substring dening a partiular node may be represented by
the length of the given substring and its start position in the string. These values for the suÆx
tree given above would then be as follows, where the subsripts represent the appropriate node
number.
start
1
= 5 length
1
= 1
start
2
= 4 length
2
= 1
start
3
= 3 length
3
= 2
start
4
= 4 length
4
= 2
start
5
= 2 length
5
= 4
start
6
= 3 length
6
= 3
start
7
= 1 length
7
= 5
In the preproessing stage, the suÆx tree of the string y#x$, where # and $ are symbols
not in the alphabet over whih x and y are strings, is onstruted using Weiner's (1973)
algorithm (whih is desribed omprehensibly by Chen and Seiferas, 1985). This algorithm
requires linear spae and, for a xed size alphabet, linear time. For unbounded alphabets,
the algorithm may be adapted to run in O(n log ) time, where  is the number of distint
pattern symbols. The preproessing stage thus requires O(n) and O(n logm) time for the
ases of onstant and unbounded alphabets respetively. Alternative suÆx tree preproessing
strategies are onsidered by Galil and Gianarlo (1988).
The algorithm of Figure 4.29 may now be modied in order to take advantage of the above
preproessing. Just before the while loop for diagonal p, r has been assigned a value suh that
x(1; r) has been mathed, up to q dierenes, with some text substring ending at y
r+p
. The
funtion of the while loop is then to nd the largest value, h say, for whih x(r+ 1; r+ h) =
y(r+p+1; r+p+h). This is equivalent to nding the length of the longest ommon prex of
the suÆxes x(r+1; m)$ and y(r+ p+1; n)#x$ of the preproessed onatenated string. The
role of the symbol # is to prevent the situation arising whereby an erroneous prex onsisting
79
of symbols from both y and x might be onsidered. Dening la(r; p) as the lowest ommon
anestor (LCA) in the suÆx tree of the leaves dened by the above suÆxes, then the desired
value of h is given by length
la(r;p)
. The lowest ommon anestor algorithm of Harel and
Tarjan (1984) or of Shieber and Vishkin (1988) may therefore be used in the k-dierene
algorithm as an alternative means of deriving the lengths of the requisite mathing substrings.
The suÆx tree has O(n) nodes. A transformation, in linear time, of the tree is required by
the LCA algorithms in order to support the determination of an LCA in O(1) time. As noted
before, r
p;q
values are alulated on n + k + 1 diagonals. Furthermore, k + 1 suh values
are required for eah diagonal, leading to a total of O(kn) LCA queries. This O(kn) time
omputation is the most signiant element in the overall alulation of the r
p;q
values. The
total running time for this k-dierene algorithm is therefore O(kn) for a xed size alphabet
and O(n logm+ kn) for unbounded alphabets.
Landau and Vishkin (1986b, 1989) have also developed a parallel version of the above al-
gorithm. The suÆx tree may be omputed in O(logn) time using n proessors (Landau,
Shieber and Vishkin, 1987; Apostolio et al., 1988). Shieber and Vishkin's (1988) parallel
algorithm may then be used to preproess the tree in O(logn) time using n= logn proessors,
allowing a single proessor to perform a subsequent LCA query in O(1) time. Employing
n+ k+ 1 proessors thus permits all the required LCA queries to be performed in O(k) time
| a bound whih may still be ahieved by simulating the algorithm on n proessors. The
total time required is thus O(logn + k) for n proessors. Landau and Vishkin also disuss
how geometri deomposition of the problem an lead to an adaptation of the algorithm still
requiring n proessors, but running in O(logm + k) time.
4.4 Longest Repeated Substring
4.4.1 Brute Fore
A brute fore approah to the Longest Repeated Substring problem for the text string y
involves an n  n math matrix,M . The elements of this matrix are dened as follows.
M
i;j
=

1 if y
i
= y
j
0 otherwise
(4.60)
The matrix for the example string PABCQRABCSABTU is given below, showing the symmetry
about the main diagonal. It is thus only elements above, or only those below, the main diag-
onal that need be alulated. Diagonals of ontiguous `1's in the matrix, with the exeption
of the main one, represent maximal reurring substrings within y.
80
j 1 2 3 4 5 6 7 8 9 10 11 12 13 14
i P A B C Q R A B C S A B T U
1 P 1 0 0 0 0 0 0 0 0 0 0 0 0 0
2 A 0 1 0 0 0 0 1 0 0 0 1 0 0 0
3 B 0 0 1 0 0 0 0 1 0 0 0 1 0 0
4 C 0 0 0 1 0 0 0 0 1 0 0 0 0 0
5 Q 0 0 0 0 1 0 0 0 0 0 0 0 0 0
6 R 0 0 0 0 0 1 0 0 0 0 0 0 0 0
7 A 0 1 0 0 0 0 1 0 0 0 1 0 0 0
8 B 0 0 1 0 0 0 0 1 0 0 0 1 0 0
9 C 0 0 0 1 0 0 0 0 1 0 0 0 0 0
10 S 0 0 0 0 0 0 0 0 0 1 0 0 0 0
11 A 0 1 0 0 0 0 1 0 0 0 1 0 0 0
12 B 0 0 1 0 0 0 0 1 0 0 0 1 0 0
13 T 0 0 0 0 0 0 0 0 0 0 0 0 1 0
14 U 0 0 0 0 0 0 0 0 0 0 0 0 0 1
From the above, it may be seen that in this example ABC, with ourrenes y(2; 4) and y(7; 9),
and AB, with ourrenes y(2; 3), y(7; 8) and y(11; 12), are maximal reurring substrings. The
longer of the two, namely ABC, is thus the longest repeated substring.
As mentioned earlier, only elements above the main diagonal, i.e. M
1:::n 1;i+1:::n
, need be
onsidered | a total of n(n   1)=2. The evaluation of the matrix therefore requires O(n
2
)
time.
4.4.2 SuÆx Trees
A more eÆient solution to the problem involves the onstrution of a suÆx tree of the text
string. A desription of this data struture was presented in the previous setion, the salient
points of whih are reiterated below. This is followed by a desription of MCreight's suÆx
tree algorithm, and nally the way in whih the tree may be utilised in this appliation is
examined.
It may be assumed that the given text string is terminated by a speial end marker symbol,
$ say, ourring nowhere else in the string. If this is not the ase, then suh a symbol may
simply be appended to y. The suÆx tree for y has n leaves, eah representing a dierent
suÆx of y, y(i; n) for 1 6 i 6 n. All internal nodes have degree > 2, and represent longest
ommon prexes of suÆxes of y. An edge onnets two nodes representing a substring of y,
b, and a proper prex thereof, a, only if there exists no node representing a substring of y,
 say, suh that  is a proper prex of b and a is a proper prex of . Assoiated with eah
edge is a substring whih, when appended to that represented by its soure node, yields the
substring represented by its destination node. The substrings represented by sibling edges
must therefore start with dierent symbols. The foregoing requirements ensure that the suÆx
tree for a given string is unique up to isomorphism of graphs.
MCreight's (1976) suÆx tree onstrution, to whih he refers as `AlgorithmM,' yields a data
81
struture funtionally equivalent to Weiner's (1973) substring index. It is, however, more
spae eÆient than the original formulation, and a desription of a version of the algorithm
is given below.
Let Y denote the suÆx tree for string y. Further, let Y
i
represent the state of the tree at stage
i of its onstrution. Y
0
is thus an empty tree with only a root node. The tree may be built
by adding paths orresponding to suÆxes of y one at a time starting with the longest and
progressing to the shortest, i.e. suÆxes y(i; n) for 1 6 i 6 n. We therefore have the following
proess, where the insert operation inserts a path orresponding to suÆx y(i; n) into tree
Y
i 1
.
for i = 1 to n
Y
i
= insert(Y
i 1
; y(i; n))
Before proeeding any further, several terms shall rst be dened.
lous of a string { the node representing the string in the suÆx tree
extension of string u { any string having u as a prex
extended lous of string u { the lous of the shortest extension of u that is represented in
the suÆx tree
ontrated lous of string u { the lous of the longest prex of u that is represented in the
suÆx tree
head
i
{ the longest prex of y(i; n) whih is also a prex of y(j; n) for some j < i. That is,
head
i
is the longest prex of y(i; n) whose extended lous exists within Y
i 1
.
tail
i
{ the string suh that y(i; n) = head
i
tail
i
. The requirement for the $ terminator ensures
that tail
i
is non-empty.
Consider the example of y = EWEW$. The various stages in the onstrution of its suÆx tree,
Y, are shown in Figure 4.30. In order to insert suÆx y(i; n) into the tree, the extended lous
of head
i
in Y
i 1
is rst loated. A new internal node representing head
i
is then reated, if
neessary, and inserted into the former's inoming edge. Finally, a new edge is added from
the lous of head
i
to a new leaf representing y(i; n). The onstrution of Y
4
, for example, is
as follows. The new suÆx, y(4; n), is W$, head
4
is W | a prex of the previous suÆx WEW$ |
and tail
4
is thus $. The edge onneting the root to the lous of WEW$ is split and a new node
for W is inserted, and lastly an edge is added from this new node to a new leaf for suÆx W$.
Rather than storing an atual substring for eah node of the tree, it is suÆient to represent
the substring by a pair of integers indexing its start and end positions in the original string.
Alternatively, the start position and the substring length may be employed. Individual nodes
of the tree may thus be implemented as a pair of substring indies together with two node
pointers | one pointing to the node's rst hild, the other to its next sibling. This arrange-
ment for the example tree is illustrated in Figure 4.31, where null pointers are represented by
82
Y0
Y
3
root root ----- EW ----- EWEW$
| |
| ------ EW$
|
--------------- WEW$
Y
1
Y
4
root ----- EWEW$ root ----- EW ----- EWEW$
| |
| ------ EW$
|
------- W ----- WEW$
|
------ W$
Y
2
Y
5
root ----- EWEW$ root ----- EW ----- EWEW$
| | | |
------ WEW$ | | ------ EW$
| |
| ------- W ----- WEW$
| |
| ------ W$
|
------------------ $
Figure 4.30: Stages in the onstrution of the suÆx tree for EWEW$
83
(0,0) * 0
|
|
|
v
(1,2) * *--------------------> (2,2) * *---------> (5,5) 0 0
EW | W | $
| |
| |
v v
(1,5) 0 *-----> (3,5) 0 0 (2,5) 0 *-----> (4,5) 0 0
EWEW$ EW$ WEW$ W$
Figure 4.31: Node implementation for EWEW$ suÆx tree
0. Although not atually stored, the substring for eah node has been inluded in the gure
for onveniene.
One the extended lous of head
i
in Y
i 1
has been found, the neessary modiation of the
tree at step i may be performed in onstant time. So, if the searh for the extended lous
may also be ompleted in onstant time, averaged over all the steps, then the suÆx tree may
be built in time linear in n. Auxiliary suÆx links are employed in the data struture in order
to do just this, and their use relies on the following relationship between head
i 1
and head
i
.
If head
i 1
= az, for some symbol a and some (possibly empty) string z, then z is a prex of
head
i
. This may be proved as follows. If head
i 1
= az, then 9j < i suh that az is a prex
of both y(i   1; n) and y(j   1; n). String z is then a prex ommon to y(i; n) and y(j; n),
and is, by denition, therefore a prex of head
i
.
A suÆx link is added in the tree from eah internal node whih is the lous of az, for some
symbol a and string z, to the lous of z. Note that the latter is never within the subtree
rooted at the former. These auxiliary links allow a short-ut searh to be performed at stage
i for the extended lous of head
i
starting from the lous of head
i 1
, whih will have been
found at the previous stage. The implementation of the exemplar suÆx tree with the suÆx
links for its internal nodes, namely the loi of EW and W, is shown in Figure 4.32.
The operation of the algorithm relies on the fats that (1) in Y
i
, only the lous of head
i
an
fail to have a valid suÆx link, and that (2) in step i, the ontrated lous of head
i
in Y
i 1
(i.e.
the lous of the longest prex of head
i
in Y
i 1
) is visited. At the outset, when i = 1, these
two requirements obviously hold. At stage i of the algorithm, the following three substeps
are arried out.
I three strings u, v and w are found suh that
(a) head
i 1
= uvw
(b) if the ontrated lous of head
i 1
in Y
i 2
is not the root, then it is the lous of uv.
Otherwise, v = 
84
----------------------------------------
| suffix link |
v |
(0,0) * 0 |
| ---------------------------- |
| | suffix link | |
| | | |
v | v |
(1,2) * * *--------------------> (2,2) * * *---------> (5,5) 0 0
EW | W | $
| |
| |
v v
(1,5) 0 *-----> (3,5) 0 0 (2,5) 0 *-----> (4,5) 0 0
EWEW$ EW$ WEW$ W$
Figure 4.32: Node implementation for EWEW$ suÆx tree showing the suÆx links
() ju j6 1, u =  i head
i+1
= 
Sine head
i 1
= uvw, then, as shown earlier, vw is a prex of head
i
(by (), u is at
most a single symbol). Thus, head
i
is given by vwz, for some (possibly empty) string z.
A test on the emptiness of v is now performed, with the following appropriate ations.
 v =  | the root node is designated as  (the lous of v), and the algorithm
proeeds to II.
 v 6=  | from (b), the lous of uv must have already existed in Y
i 2
. From (1),
the suÆx link of this internal node must be dened in Y
i 1
, sine the node must
have been added prior to step i  1. From (2), this node will have been visited in
stage i  1. Its suÆx link is then traversed to an internal node  (the lous of v)
and the algorithm proeeds to II.
II (`resanning') It was shown above that head
i
is given by vwz. By denition, the extended
lous of vw thus exists in Y
i 1
. There is therefore some downward sequene of edges
from  (the lous of v) leading to the lous of some extension of vw. To resan w, the
hild of , e say, suh that e is the lous of an extension, vp say, of v appended with the
rst symbol of w is found. If j p j< jw j, then a reursive resan of q, where w = pq, is
started from node e. If, on the other hand, j p j> jw j, then w is a prex of p, and the
extended lous of vw has been found, ompleting the resan and taking time linear in
the number of nodes visited. A new internal node is reated as the lous of vw if one
does not already exist, whih is the ase only if z = . The lous of vw is designated by
d, and the algorithm proeeds to III.
III (`sanning') If the suÆx link of the lous of uvw (i.e. head
i 1
) is urrently undened, then
it is set to point to node d, ensuring the truth of (1). A downwards searh starting from
d to nd the extended lous of vwz (i.e. of head
i
) is then initiated. While resanning,
the length of w is already known as it has previously been sanned, whereas in sanning,
85
the length of z is unknown. The downward searh is thus ditated by the symbols of
tail
i 1
(of whih z is a prex) one at a time from left to right. The extended lous of
vwz is then found when this downward exursion may proeed no further. The previous
node enountered on this journey is the ontrated lous of head
i
, ensuring the truth
of (2). If neessary, a new internal node is reated to be the lous of vwz. A new
edge is then added from the lous of vwz to a new leaf representing suÆx y(i; n), thus
ompleting stage i.
MCreight gives the following example to illustrate the above proess. Consider step 14 in
the onstrution of the suÆx tree for y = BBBBBABABBBAABBBBB$. The tree Y
13
is shown in
Figure 4.33, where leaf nodes have been labelled with the relevant value of i for the suÆx
y(i; 19) that they represent, and the relevant suÆx links have been inluded. At step 14, suÆx
y(14; 19), namely BBBBB$, is to be inserted into Y
13
. Initially, uv is set to AB as this is the
string represented by the ontrated lous of head
13
in Y
12
, so u = A and v = B. The value
of head
13
(uvw) is equal to ABBB, so w must be BB. In I, v is non-empty, so the suÆx link
from the lous of uv (AB) is traversed to node  (the lous of B). From here, w is resanned in
II, enountering one intermediate node e and ending up at node d, the lous of BBB. In III a
new suÆx link is added from the lous of head
13
to node d, and the downward san from d
is then started using the symbols of tail
13
(BB$), nding that z = BB. A new internal node
is then reated as the lous of vwz (head
14
), whih in this ase is BBBBB, and inserted into
the inoming edge of the extended lous of vwz (the leaf for y(1; 19)). Finally, a new edge is
added from the lous of BBBBB to a new leaf representing suÆx y(14; 19).
For stage i, let res
i
denote the shortest suÆx (i.e. wz  tail
i
) involved in the resanning
operation. For every intermediate node e visited whilst resanning w, p is a non-empty string
ourring in res
i
, but not in res
i+1
. For eah intermediate node, p has a minimum length
of 1, thus the maximum length of res
i+1
is j res
i
j   int
i
, where int
i
is the number of
intermediate nodes enountered during the resan of stage i. The overall total number of
intermediate nodes visited during resanning is thus
P
n
i=1
int
i
, whih is bounded from above
by n. Also, in stage i, the number of symbols whih must be sanned in order to loate head
i
,
i.e. j z j, is equal to j head
i
j   j head
i 1
j + 1. Summing over the omplete algorithm gives
P
n
i=1
jhead
i
j   jhead
i 1
j +1 = head
n
  head
0
+ n = n.
The above shows that the resanning and sanning operations of the overall proess onsume
at most time linear in n. It has also previously been shown that eah of the n stages of the
algorithm takes onstant time, exluding that required for resanning and sanning. The time
taken by the entire algorithm is therefore linear in n. The running time of the algorithm is
also linear in the size of the alphabet of y as eah node may have up to j C j hildren, for
alphabet C.
Although eÆient in spae, the tree implementation depited in Figure 4.32 is slow to searh.
As a ompromise between spae requirement and amenability to be searhed rapidly, M-
Creight suggests an implementation in whih the edges of the tree are enoded as entries in
a hash table (using Lampson's hashing algorithm (see exerise 6.4.13 of Knuth, 1973)).
Having desribed the onstrution of the suÆx tree, we are now in a position to examine how
this data struture may be used in the solution of the longest repeated substring problem.
86
root ----- A ----- 12
| | ________________
| ------ AB ----- 6 ' |
| ' | ' |
| suffix ' | ' |
| link ' ------ ABBB ----- 8 |
| ' (head_13) | |
| | ------ 13 |
| | |
| v |
------- B ----- BA ----- 11 | new
(node ) | | | suffix
| ------ BAB ------ 5 | link
| | |
| -------- 7 |
| |
| (node e) |
------ BB ----- BBA ------ 10 |
| | |
| -------- 4 |
| |
| ___________________|
| '
| '
| |
| v
------ BBB ------ BBBA ----- 9
(node d) | |
| ------- 3
|
|
-------- BBBB ----- 2
|
------- 1
Figure 4.33: Y
13
for string BBBBBABABBBAABBBBB$
87
root ----- AB ------ 11
/ | \ |
| | | ------- ABC ----- 2
| | | |
| | | -------- 7
| | |
| | ------ B ------ 12
| | |
| | ------- BC ------ 3
| | |
| | ------- 8
| |
| ------------------ C ------- 4
| |
| -------- 9
|
--------------------- 1, 5, 6, 10, 13, 14, 15
Figure 4.34: SuÆx tree for string PABCQRABCSABTU$
It is apparent that the longest substring represented by an internal node of Y is the longest
repeated substring of y. It therefore suÆes to keep trak of the maximum length of the
internal node substrings as the tree is being built, whih, as we have seen, may be aomplished
in linear time for a xed alphabet.
The suÆx tree for the previous example string of y = PABCQRABCSABTU$ is shown in Figure
4.34. Again, leaf nodes have been labelled with the relevant value of i for their respetive
suÆxes y(i; 15). Also, for onveniene, all leaves having no parent other than the root node
have been grouped together at the end of a single edge. From the gure it may be seen that
ABC is the longest substring represented by an internal node, and is thus the longest repeated
substring of y.
The longest ommon substring of two strings, x and y, may also be determined in a similar
fashion in O(m+n) time for a xed alphabet. In this ase, the suÆx tree for the string x#y$,
where symbols # and $ are not members of the alphabet over whih x and y are strings, is
onstruted. Again, the required result is given by the maximal length substring represented
by an internal node of the suÆx tree.
Finally, Baker's (1992) approah to the problem of nding all the maximal repeated substrings,
longer than some predetermined threshold value, of an input string involves searhing the
suÆx tree of the string. The substring, u say, represented by the lowest ommon anestor of
two leaves in Y is the longest ommon prex of the two suÆxes represented by those leaves.
It is thus a substring ourring at two distint positions with diering right ontexts in the
input string. To determine whether u is a maximal repeated substring, it remains to establish
whether the left ontexts of its two ourrenes in y also dier. If they do not, then u is
merely a suÆx of some longer reurring substring.
88
The neessary hek on the left ontexts may be performed by reursing over the tree, building
up lists of suÆxes grouped by left ontext and omparing these lists. For eah internal node,
lists of the positions of the suÆxes represented by the leaves in the subtree rooted at the
given internal node and sharing idential left ontexts are reated. Maximal repetitions are
then identied by enumerating, at eah subtree, pairings of positions from pairs of lists with
distint left ontexts. For example, onsider the suÆx tree for the string PABCQRABCSABTU$
(Figure 4.34). Looking for maximal repetitions of length 2 or greater, we nd that AB is
a maximal repeated substring (the left ontext of y(11; 15) is S, and those of y(2; 15) and
y(7; 15) are P and R respetively); whereas BC is not (the left ontexts of y(3; 15) and y(8; 15)
are both A). By restriting this ativity to only those internal nodes representing substrings
longer than the preset threshold, the searh of the tree may be performed in time linear in the
number of pairs of maximal repetitions found, r. Taking into aount the time required to
onstrut the suÆx tree gives an overall temporal omplexity of O(n+r) for a xed alphabet.
The spae requirement is that of the suÆx tree, whih is linear in n. Baker has also extended
this approah in order to nd parameterised mathes | useful, for example, for identifying
similar setions of ode in large omputer programs.
89
90
Chapter 5
Bak to the Original Problem
To be still searhing what we know not by what we know : : :
| John Milton (1608-1674), The Dotrine and Disipline of Divore.
Having reviewed a variety of string searhing algorithms in the foregoing hapters, we shall
now examine in this hapter potential ways in whih the original problem may be solved.
Before doing so, however, it might rst be appropriate to onsider how the study of these
algorithms is relevant to the tentative information proessing system mentioned at the outset.
5.1 The System
The proessing system in question has many laimed appliation areas, inluding information
retrieval and the reognition of approximate patterns. An examination of eÆient methods
of retrieving substrings of a text is therefore relevant to the development of this system. For
example, the Boyer-Moore-Horspool algorithmprovides a fast and easily implementedmethod
for searhing a text for a given substring. The suÆx tree data struture (desribed in x4.4.2)
is also useful here, allowing multiple searhes of a given text to be performed eÆiently. It is
also of interest to note in passing that this data struture permits linear time sequential data
ompression to be performed. (The intended operation of the proessing system is based on
a priniple of data ompression.)
The string distane and approximate string mathing algorithms are learly relevant to the
reognition of approximate patterns, where ourrenes up to some predened distane away
from a given pattern are to be identied. Furthermore, the previous work done with don't are
symbols is partiularly pertinent sine support in the system for suh wildards is intended. Of
singular relevane to the problem are the longest ommon subsequene and longest repeated
substring algorithms, as we shall see in the next setion.
91
A related problem, the solution of whih is inidentally an objetive of the system, is to nd
the patterns ommon to an arbitrary number of input strings. This is eetively a variation
of the N -ls problem. It is therefore interesting to reall that we have seen that the jN -lsj
problem is NP omplete (Maier, 1978).
5.2 The Problem
At the outset, it was stated that it seemed more intuitive to employ a nite alphabet, rather
than allow arbitrary strings of ASCII haraters represent the atomi symbols. As we have
seen, the nite alphabet ase ertainly simplies matters in many string searhing applia-
tions. For example, this permits Boyer-Moore type mathing tehniques to be applied.
It was also initially disussed whether the pattern of the problem should be a substring or a
subsequene. For the former ase, it has been shown that the problem has been solved using
suÆx tree tehniques. To reiterate, the longest repeated substring may be found in linear
time by onstruting the suÆx tree of the text string (MCreight, 1976), and the maximal
repetitions in the string may be found from its tree in O(n + r) time overall, where r is the
number of pairs of repetitions found (Baker, 1992). The latter ase, namely that of repeated
subsequenes, is onsidered below.
It was originally stated that the problem was to nd the maximal disjoint reurring subse-
quenes of an input string. For the time being, it shall also be stipulated that overlapping
subsequenes are to be exluded. A desription of a possible approah to solving this re-
strited problem is given below, and this is then followed by a disussion of a more eÆient
approah whih also aters for the unrestrited problem.
If the seond ourrene of a maximal reurring subsequene in string y starts at position i,
then that subsequene may be found by alulating the longest ommon subsequene of y(1; i 
1) and y(i; n). A possible approah to the problem would therefore be to ompute ls(y(1; i 
1); y(i; n)) for i = 2 to n. Adopting a quadrati time ls algorithm would result in the overall
proess requiring O(n
3
) time. Assuming the use of an O(n
2
) ls algorithm provides a worst
ase bound for a pratial implementation; the average ase may however be more favourable.
As we have previously seen, in addition to the quadrati time ls algorithms (e.g. Hirshberg,
1975), there are also those exhibiting more attrative average ase performane. We have,
for example, Hunt and Szymanski's (1977) O(n logn) time algorithm, whih degrades to
O(n
2
logn) in the worst ase, and Ukkonen's (1983, 1985a) O(nd) time method, whih is
quadrati in the worst ase. Reall also that the algorithm of Masek and Paterson (1980, 1983)
is sub-quadrati in all ases, but at the ost of a prohibitively large onstant of proportionality.
Under the above restrition on the subsequenes, overlapping ourrenes of a maximal re-
urring subsequene would not be found. Relaxing this restrition thus means that we must
also be able to nd interleaved disjoint ourrenes of a maximal reurring subsequene, suh
as ABC in the string AABBCC (there are in fat 4 possible pairs of disjoint ourrenes of ABC
in this string: y
1
y
3
y
5
; y
2
y
4
y
6
; y
1
y
3
y
6
; y
2
y
4
y
5
; y
1
y
4
y
5
; y
2
y
3
y
6
; y
1
y
4
y
6
; y
2
y
3
y
5
). In general,
when interleaved ourrenes are permitted and when eah symbol of a maximal reurring
subsequene ours exatly twie in the string, there may be up to 2
l=2
distint pairs of dis-
92
joint ourrenes in the string of the maximal subsequene { giving an exponential number
of pairs in the worst ase sine l 6 bn=2. When subsequene symbols our more than twie
in the string the potential number of pairs beomes even greater. Also note that there may
be more than one longest reurring subsequene in a string. For example, both ABC and DEF
are longest reurring subsequenes in the string PDEQFABCRSATBCDEUF.
An approah to nding a single pair of ourrenes of a maximal reurring subsequene of
string y is to ompute the heaviest ommon subsequene of y with itself, with the weight
of a math (i; j) equal to 0 if i = j, and equal to 1 otherwise. As we have seen, this may
be performed by using an adaptation of the dynami programming tehnique, or by using
Jaobson and Vo's (1992) hs algorithm. The former requires O(n
2
) time, whereas the time
requirement of the latter is O((r+ n) logn) where r is the number of mathes.
To illustrate the tehnique, the dynami programming hs weight array for the ase of y =
AABBCC is given below. The array is generated using the reurrene relation 4.58 together
with the above math weighting.
j 0 1 2 3 4 5 6
i A A B B C C
0 0 0 0 0 0 0 0
1 A 0 0 1 1 1 1 1
2 A 0 1 1 1 1 1 1
3 B 0 1 1 1 2 2 2
4 B 0 1 1 2 2 2 2
5 C 0 1 1 2 2 2 3
6 C 0 1 1 2 2 3 3
An atual maximal reurring subsequene may then be reovered from the array by traing
bak through its dependenies, as in the dynami programming method for alulating inter-
string edit distanes.
Note that nding a maximal reurring subsequene is thus equivalent to nding a maximal
monotonially inreasing path in the graph omposed of nodes orresponding to mathes (i; j)
where y
i
= y
j
and i 6= j. This is reminisent of the Hunt-Szymanski (1977) ls algorithm,
whih ould therefore be modied to perform the required task in the same asymptoti time
as the Jaobson-Vo hs algorithm. A pair of suh paths orresponding to a distint pair of
ourrenes of the maximal reurring subsequene ABC in the string AABBCC is shown below.
The omponent nodes of one of the pair of disjoint paths are represented by the solid irles,
and those of the other by the hollow ones. The atual pair of paths indiated below represent
the pair of ourrenes y
1
y
4
y
5
and y
2
y
3
y
6
.
93
j 1 2 3 4 5 6
i A A B B C C
1 A 
2 A Æ
3 B Æ
4 B 
5 C 
6 C Æ
To onlude, the use of an hs algorithm thus allows a maximal reurring subsequene of
a string to be found, and this may be ahieved in O((r + n) logn) time by employing the
Jaobson-Vo method. Note, however, that nding all the possible disjoint pairs of ourrenes
of maximal reurring subsequenes is not in general a viable proposition. This is equivalent to
enumerating all the possible pairs of disjoint maximal monotonially inreasing paths in the
graph of the weighted mathes. As intimated earlier, suh a task is of exponential omplexity
in the worst ase.
* * *
Ahilles (to the Tortoise): All that's left is to nd out if the string has Buddha-nature or
not. | Douglas R. Hofstadter, Godel, Esher, Bah.
* * *
94
Aknowledgement
I am indebted to Dan Hirshberg and Phong Vo for suggesting approahes for the solution of the
maximal repeated subsequene problem. The ontributions of many other Internet respondents who
provided numerous pointers to the literature are also gratefully aknowledged.
95
96
Appendix A
String Nomenlature
A glossary of terms in ommon use in string mathing problems is presented below. The
reader is referred to texts suh as Aho, Sethi and Ullman (1986) for further explanation of
these terms.
ALPHABET { any nite set of symbols, also known as a harater lass.
CONCATENATION { the onatenation of two strings x and y, xy, is the string obtained
by appending y to x. The onatenation of two languages A and B, AB, is dened as
AB = fxy : x is in A; y is in Bg
i.e. the set of all strings formed by onatenating any string from A with any one from
B.
EMPTY STRING,  { the string with zero length.
EXPONENTIATION { following on from an analogy between string onatenation and
multipliation, exponentiation is dened as follows
x
0
= ; x
i
= x
i 1
x for integer i > 0
Thus, x
1
= x = x; x
2
= xx; x
3
= xxx : : :
FIBONACCI STRING { a string dened by f
1
= B, f
2
= A, f
n
= f
n 1
f
n 2
n > 2, e.g.
f
5
= ABAAB.
HAMMING DISTANCE { a distane metri between two strings of equal length, equal
to the number of symbol positions at whih the two strings dier, e.g. the Hamming
distane between master and pastes is 2.
KLEENE CLOSURE of a language A, A

{ the language formed by the union of zero and
more onatenations of A, i.e.
A

=
1
[
i=0
A
i
97
LANGUAGE { a set of strings over a given alphabet.
LENGTH of a string x, jx j { the number of instanes of symbols in x.
LEVENSHTEIN DISTANCE { a distane metri between two strings, not neessarily
of the same length, given by the minimum number of symbol insertions, deletions and
substitutions required to transform one string to the other, e.g. the Levenshtein distane
between zeitgeist and preterit is 6.
POSITIVE CLOSURE of a language A, A
+
{ the language formed by the union of one
and more onatenations of A, i.e.
A
+
=
1
[
i=1
A
i
PREFIX of a string x { a string obtained by deleting zero or more symbols from the end of
x, e.g. jorm is a prex of jormungand. A proper prex of x is a non-empty prex of x
not equal to x itself.
SQUARE { a string omprising a repeated non-empty substring, i.e. one of the form x
2
,
e.g. dikdik.
STRING over a given alphabet { a nite sequene of symbols drawn from the alphabet.
The terms sentene and word are sometimes used synonymously with string. Note that
both  and x are prexes, suÆxes, substrings and subsequenes of string x.
SUBSEQUENCE of a string x { a string obtained by deleting zero or more symbols, whih
need not be ontiguous, from x, e.g. nnnaa is a subsequene of ginnungagap. A proper
subsequene of x is a non-empty subsequene not equal to x itself.
SUBSTRING of a string x { a string obtained by deleting zero or more symbols from the
beginning and end of x (i.e. by deleting a prex and a suÆx), e.g. gna is a substring of
ragnarok. A proper substring of x is a non-empty substring not equal to x itself.
SUFFIX of a string x { a string obtained by deleting zero or more symbols from the be-
ginning of x, e.g. asill is a suÆx of yggdrasill. A proper suÆx of x is a non-empty
suÆx not equal to x itself.
SUPERSEQUENCE of a string x { any string y suh that x is a subsequene of y, e.g.
faetious is a supersequene of aeiou.
UNION of languages A and B, A [ B { the language ontaining all the strings of both A
and B, i.e.
A [B = fx : x is in A or x is in Bg
98
Appendix B
Asymptoti Notation
This appendix denes some of the notation ommonly used in the study of asymptotis. A
more thorough treatment of this subjet may be found in Graham, Knuth and Patashnik
(1990).
It is often useful to lassify ertain funtions of n in terms of the relative speed with whih
they approah innity for inreasing values of n. The relational operator , introdued by
Paul du Bois-Reymond (1871) may be used for this purpose and is dened below.
f(n)  g(n) , lim
n!1
f(n)
g(n)
= 0 (B.1)
The use of this operator is demonstrated in the following asymptoti hierarhy.
1  logn  n
a
 n
b
 b
n
 n
n
where a and b are onstants suh that 0 < a < 1 < b.
Another useful notation is Big Oh, introdued by Paul Bahman (1894), whih provides a
onvenient means of expressing upper bounds. It is dened as follows.
f(n) = O(g(n)) , jf(n) j 6  jg(n) j for some onstant  > 0 (B.2)
Note that it has beome ustomary to use the `=' sign in the above type of expression. It
does not, however, stand for equality and should be read as `is' (unidiretional) rather than
`equals' (symmetrial). It is sometimes impliitly assumed that the Big Oh relation is for
suÆiently large n, i.e. as n!1. In this ase the strit ondition is that
jf(n) j 6  jg(n) j for n > n
0
(B.3)
for some onstant n
0
.
The orresponding notation for lower bounds is Big Omega. This is dened as follows.
f(n) = 
(g(n)) , jf(n) j >  jg(n) j for some onstant  > 0 (B.4)
99
If g(n) is a lower bound for f(n), then f(n) must be an upper bound for g(n), i.e.
f(n) = 
(g(n)) , g(n) = O(f(n)) (B.5)
Finally, Big Theta is used to indiate an exat order of growth, ie.
f(n) = (g(n)) , f(n) = O(g(n)) and f(n) = 
(g(n)) (B.6)
* * *
I give you the end of a golden string;
Only wind it into a ball,
It will lead you in at Heaven's gate,
Built into Jerusalem's wall.
| William Blake (1757-1827)
100
Referenes
Aho A.V., Hoproft J.E., Ullman J.D. (1974) \The design and analysis of omputer al-
gorithms," Addison-Wesley, Reading, MA.
Aho A.V., Corasik M.J. (1975) \EÆient stringmathing: an aid to bibliographi searh,"
Communiations of the ACM, Vol. 18, No. 6, p. 333-40, June 1975.
Aho A.V., Hirshberg D.S., Ullman J.D. (1976) \Bounds on the omplexity of the long-
est ommon subsequene problem," Journal of the ACM, Vol. 23, No. 1, p. 1-12, January
1976.
Aho A.V. (1980) \Pattern mathing in strings," in Book R.V. (ed.) Formal Language The-
ory, p. 325-47, Aademi Press, New York.
Aho A.V., Sethi R., Ullman J.D. (1986) Compilers { Priniples, Tehniques, and Tools,
Addison-Wesley, Reading, MA.
Aho A.V. (1990) \Algorithms for nding patterns in strings," Chapter 5 (p. 255-300) of
Leeuwen J. van (ed.) Handbook of Theoretial Computer Siene, Elsevier Siene Pub-
lishers, Amsterdam.
Altshul S.F., Gish W., Miller W., Myers E.W., Lipman D.J. (1990) \Basi loal al-
ignment searh tool," Journal of Moleular Biology, Vol. 215, p. 403-10.
Apostolio A. (1985) \The myriad virtues of subword trees," in Apostolio A., Galil Z.
(eds.) Combinatorial Algorithms on Words, NATO ASI Series, Vol. F12, p. 85-96,
Springer-Verlag, Berlin.
Apostolio A., Gianarlo R. (1986) \The Boyer-Moore-Galil string searhing strategies
revisited," SIAM Journal on Computing, Vol. 15, No. 1, p. 98-105, February 1986.
Apostolio A., Guerra C. (1987) \The longest ommon subsequene problem revisited,"
Algorithmia, Vol. 2, p. 315-36.
Apostolio A., Iliopoulos C., Landau G.M., Shieber B., Vishkin U. (1988) \Paral-
lel onstrution of a suÆx tree with appliations," Algorithmia, Vol. 3, p. 347-65.
Arlazarov V.L., Dini E.A., Kronod M.A., Faradzev I.A. (1970) \On eonomi on-
strution of the transitive losure of a direted graph," (Russian) Doklady Akademii
101
Nauk SSR, Vol. 194, p. 487-8, also translated in Soviet Math. Doklady, Vol. 11, p.
1209-10, 1975.
Arratia R., Waterman M.S. (1985a) \Critial phenomena in sequene mathing," The
Annals of Probability, Vol. 13, No. 4, p. 1236-49.
Arratia R., Waterman M.S. (1985b) \An Erd}os-Renyi law with shifts,"Advanes in Math-
ematis, Vol. 55, p. 13-23.
Arratia R., Gordon L., Waterman M. (1986) \An extreme value theory for sequene
ma-
thing," The Annals of Statistis, Vol. 14, No. 3, p. 971-93.
Bahman P. (1894) Die analytishe Zahlentheorie, Teubner, Leipzig.
Baeza-Yates R.A. (1989a) \Improved string mathing," Software | Pratie and Experi-
ene, Vol. 19, No. 3, p. 257-71, Marh 1989.
Baeza-Yates R.A. (1989b) \String searhing algorithms revisited," Leture Notes in Com-
puter Siene, Vol. 382, p. 75-96.
Baeza-Yates R.A. (1991) \Searhing subsequenes," Theoretial Computer Siene, Vol.
78, No. 2, p. 363-76.
Baker B.S. (1992) \A program for identifying dupliated ode," Proeedings of the 24th
Symposium on the Interfae: Computer Siene and Statistis, College Station, Texas,
18-21 Marh 1992.
Bellman R., Dreyfus S. (1962) \Applied Dynami Programming," Prineton University
Press, Prineton NJ.
Blumer A., Blumer J., Ehrenfeuht A., Haussler D., MConnel R. (1984a) \Build-
ing a omplete inverted le for a set of text les in linear time," Proeedings of the 16th
ACM Symposium on the Theory of Computing, p. 349-58.
Blumer A., Blumer J., Ehrenfeuht A., Haussler D., MConnel R. (1984b) \Build-
ing the minimal DFA for the set of all subwords of a word on-line in linear time," in
Paredaens J. (ed.), Proeedings of the 11th International Colloquium on Automata,
Languages and Programming, Leture Notes in Computer Siene, Vol. 172, p. 109-18,
Springer-Verlag, Berlin.
Blumer A., Blumer J., Ehrenfeuht A., Haussler D., Chen M.T., Seiferas J. (1985)
\The smallest automaton reognizing the subwords of a word," Theoretial Computer
Siene, Vol. 40, No. 1, p. 31-56.
Blumer A., Blumer J., Haussler D., MConnel R., Ehrenfeuht A. (1987) \Comp-
lete inverted les for eÆient text retrieval and analysis," Journal of the ACM, Vol. 34,
No. 3, p. 578-95.
Bois-Reymond P. du (1871) \Sur la grandeur relative des innis des fontions," Annali di
Matematia pura e appliata, series 2, Vol. 4, p. 338-53.
102
Boyer R.S., Moore J.S. (1977) \A fast string searhing algorithm," Communiations of
the ACM, Vol. 20, No. 10, p. 762-72, Otober 1977.
Burkowski F.J. (1982) \A hardware hashing sheme in the design of a multiterm string
omparator," IEEE Transations on Computers, Vol. C-31, No. 9, p. 825-34, September
1982.
Chen M.T., Seiferas J. (1985) \EÆient and elegant subword-tree onstrution," in Apos-
tolio A., Galil Z. (eds.) Combinatorial Algorithms on Words, NATO ASI Series, Vol.
F12, p. 97-107, Springer-Verlag, Berlin.
Cook S.A. (1972) \Linear time simulation of deterministi two-way pushdown automata,"
Information Proessing, Vol. 71, p. 75-80, North-Holland, Amsterdam.
Crohemore M. (1986) \Transduers and repetitions," Theoretial Computer Siene, Vol.
45, p. 63-86.
Curry T., Mukhopadhyay A. (1983) \Realization of eÆient non-numeri operations th-
rough VLSI," Proeedings of VLSI `83.
Davies G., Bowsher S. (1986) \Algorithms for pattern mathing," Software | Pratie
and Experiene, Vol. 16, No. 6, p. 575-601, June 1986.
Fisher M.J., Paterson M.S. (1974) \String-mathing and other produts," in Karp R.M.
(ed.), Complexity of Computation, SIAM-AMS Proeedings, Vol. 7, p. 113-25.
Foster M.J., Kung H.T. (1980) \The design of speial-purpose VLSI hips," IEEE Com-
puter, Vol. 13, p. 26-40, January 1980.
Galil Z. (1979) \On improving the worst ase running time of the Boyer-Moore string searh-
ing algorithm," Communiations of the ACM, Vol. 22, No. 9, p. 505-8.
Galil Z., Gianarlo R. (1986) \Improved stringmathing with kmismathes," Sigat News,
Vol. 17, p. 52-4.
Galil Z., Gianarlo R. (1988) \Data strutures and algorithms for approximate string
mathing," Journal of Complexity, Vol. 4, p. 33-72.
Gonnet G.H., Baeza-Yates R. (1991) \Text algorithms," Chapter 7 (p. 251-88) of Hand-
book of Algorithms and Data Strutures in Pasal and C, 2
nd
edition, Addison-Wesley,
Wokingham UK.
Graham R.L., Knuth D.E., Patashnik O. (1990) Conrete Mathematis, Addison-Wes-
ley, Reading, MA (5
th
printing with orretions, July 1990).
Guibas L.J., Odlyzko A.M. (1977) \A new proof of the linearity of the Boyer-Moore
string searhing algorithm," Proeedings of the 18
th
Annual IEEE Symposium on Foun-
dations of Computer Siene, p. 189-95 (also SIAM Journal on Computing, Vol. 9, No.
4, p. 672-82, 1980).
Halaas A. (1983) \A systoli VLSI matrix for a family of fundamental searh problem,"
Integration VLSI Journal, Vol. 1, No. 4, p. 269-82, Deember 1983.
103
Hall P.A.V., Dowling G.R. (1980) \Approximate mathing," Computing Surveys, Vol.
12, No. 4, p. 381-402, Deember 1980.
Hamming R. (1982) \Coding and Information Theory," Prentie Hall, Englewood Clis
NJ.
Harel D., Tarjan R.E. (1984) \Fast algorithms for nding nearest ommon anestors,"
SIAM Journal on Computing, Vol. 13, No. 2, p. 338-55.
Harrison M.C. (1971) \Implementation of the substring test by hashing," Communiations
of the ACM, Vol. 14, No. 12, p. 777-9, Deember 1971.
Haskin R.L. (1981) \Speial purpose proessors for text retrieval," Database Engineering,
Vol. 4, No. 1, p. 16-29, September 1981.
Haskin R.L., Hollaar L.A (1983) \Operational harateristis of a hardware-based pat-
tern mather," ACM Transations on Database Systems, Vol. 8, No. 1, p. 15-40, Marh
1983.
Haton J.P. (1973) \Contribution a l'analyse, parametrisation et la reonnaissane automa-
tique de la parole," These de dotorat d'etat, Universite de Nany, Nany Frane.
Hirata M., Yamada H., Nagai H., Takahashi K. (1988) \A versatile data string-searh
VLSI," IEEE Journal of Solid-State Ciruits, Vol. 23, No. 2, p. 329-35, April 1988.
Hirshberg D.S. (1975) \A linear spae algorithm for omputing maximal ommon subse-
quenes," Communiations of the ACM, Vol. 18, No. 6, p. 341-3, June 1975.
Hirshberg D.S. (1978) \An information theoreti lower bound for the longest ommon
subsequene problem," Information Proessing Letters, Vol. 7, p. 40-1.
Hirshberg D.S. (1983) \Reent results on the omplexity of ommon subsequene prob-
lems," in Sanko D., Kruskall J.B. (eds.) Time warps, string edits, and maromoleules:
the theory and pratie of sequene omparison, Chapter 12, p. 325-30, Addison-Wesley,
Reading MA.
Hollaar L.A. (1979) \Text retrieval omputers," IEEE Computer, Vol. 12, p. 40-50.
Horspool R.N. (1980) \Pratial fast searhing in strings," Software | Pratie and Ex-
periene, Vol. 10, No. 6, p. 501-6.
Hsu W., Du M. (1984) \Computing a longest ommon subsequene for a set of strings,"
BIT, Vol. 24, p. 45-59.
Hume A., Sunday D. (1991) \Fast string searhing," Software | Pratie and Experiene,
Vol. 21, No. 11, p. 1221-48, November 1991.
Hunt J.W., MIlroy M.D. (1976) \An algorithm for dierential le omparison," Com-
puting Siene Tehnial Report 41, AT&T Bell Laboratories, Murray Hill NJ.
Hunt J.W., Szymanski T.G. (1977) \A fast algorithm for omputing longest ommon
subsequenes," Communiations of the ACM, Vol. 20, No. 5, p. 350-3, May 1977.
104
Ivanov A.G. (1984) \Distinguishing an approximate word's inlusion on Turing mahine in
real time," Izv. Akademii Nauk SSSR Ser. Mat., Vol. 48, p. 520-68 (Russian).
Jaobson G., Vo K-P. (1992) \Heaviest inreasing/ommon subsequene problems," Pro-
eedings of the Combinatorial Mathing Conferene, Tuson, Arizona, April 1992.
Karp R.M. (1972) \Reduibility among ombinatorial problems," in Miller R.E., Thather
J.W. (eds.) Complexity of Computer Computations, p. 85-103, Plenum Press.
Karp R.M., Rabin M.O. (1987) \EÆient randomized pattern-mathing algorithms," IBM
Journal of Researh and Development, Vol. 31, No. 2, p. 249-60, Marh 1987.
Knuth D.E. (1973) \The art of omputer programming, Volume 3: Sorting and Searhing,"
Chapter 6.3, p. 490-3, Addison-Wesley, Reading MA.
Knuth D.E., Morris J.H., Pratt V.R. (1977) \Fast pattern mathing in strings," SIAM
Journal on Computing, Vol. 6, No. 2, p. 323-50, June 1977.
Landau G.M., Vishkin U., Nussinov R. (1985) \An eÆient string mathing algorithm
with k dierenes for nuleotide and amino aid sequenes," Tehnial Report TR-37/85,
Department of Computer Siene, Tel Aviv University.
Landau G.M., Vishkin U. (1985) \EÆient string mathing in the presene of errors,"
Proeedings of the 26th IEEE Symposium on the Foundations of Computer Siene, p.
126-36.
Landau G.M., Vishkin U. (1986a) \EÆient string mathing with k mismathes," Theo-
retial Computer Siene, Vol. 43, p. 239-49.
Landau G.M., Vishkin U. (1986b) \Introduing eÆient parallelism into approximate str-
ing mathing and a new serial algorithm," Proeedings of the 18th ACM Symposium on
the Theory of Computing, p. 220-30.
Landau G.M., Shieber B., Vishkin U. (1987) \Parallel onstrution of a suÆx tree,"
Proeedings of the 14th ICALP, Leture Notes in Computer Siene, Vol. 267, p. 314-25,
Springer-Verlag, New York/Berlin.
Landau G.M., Vishkin U. (1988) \Fast string mathing with k dierenes," Journal of
Computer and System Sienes, Vol. 37, No. 1, p. 63-78.
Landau G.M., Vishkin U. (1989) \Fast parallel and serial approximate string mathing,"
Journal of Algorithms, Vol. 10, p. 157-69.
Lee D., Lohovsky F. (1985) \Text retrieval mahine," OÆe Automation | Conepts
and Tools, setion 14, Springer-Verlag, New York.
Lee K.C., Mak V.W. (1989) \Design and analysis of a parallel VLSI string searh algo-
rithm," Leture Notes in Computer Siene, Vol. 368, p. 215-29.
Levenshtein V.I. (1965) \Binary odes apable of orreting deletions, insertions, and re-
versals," (Russian) Doklady Akademii Nauk SSR, Vol. 163, No. 4, p. 845-8, also Cyber-
netis and Control Theory, Vol. 10, No. 8, p. 707-10, 1966.
105
Lipman D.J., Pearson W.R. (1985) \Rapid and sensitive protein similarity searhes,"
Siene, Vol. 227, No. 4693, p. 1435-41, 22 Marh 1985.
Lowrane R., Wagner R.A. (1975) \An extension of the string-to-string orretion prob-
lem," Journal of the ACM, Vol. 22, No. 2, p. 177-83.
Maier D. (1978) \The omplexity of some problems on subsequenes and supersequenes,"
Journal of the ACM, Vol. 25, No. 2, p. 322-36, April 1978.
Majster M.E., Reiser A. (1980) \EÆient on-line onstrution and orretion of position
trees," SIAM Journal on Computing, Vol. 9, No. 4, p. 785-807, November 1980.
Masek W.J., Paterson M.S. (1980) \A faster algorithm for omputing string-edit dist-
anes," Journal of Computer and Systems Sienes, Vol. 20, No. 1, p. 18-31.
Masek W.J., Paterson M.S. (1983) \How to ompute string-edit distanes quikly," in
Sanko D., Kruskall J.B. (eds.) Time warps, string edits, and maromoleules: the
theory and pratie of sequene omparison, Chapter 14, p. 337-49, Addison-Wesley,
Reading MA.
MCreight E.M. (1976) \A spae-eonomial suÆx tree onstrution algorithm," Journal
of the ACM, Vol. 23, No. 2, p. 262-72, April 1976.
Menio C. (1989) \Faster string searhes," Dr. Dobb's Journal, p. 74-5, July 1989.
Morrison D.R. (1968) \PATRICIA | pratial algorithm to retrieve information oded in
alphanumeri," Journal of the ACM, Vol. 15, No. 4, p. 514-34.
Mukhopadhyay A. (1980) \Hardware algorithms for string proessing," Proeedings of
ICCC, p. 508-11.
Myers E.W. (1986) \An O(ND) dierene algorithm and its variations,"Algorithmia, Vol.
1, p. 251-66.
Needleman S.B., Wunsh C.D. (1970) \A general method appliable to the searh for
similarities in the amino-aid sequene of two proteins," Journal of Moleular Biology,
Vol. 48, p. 443-53.
Pinter R.Y. (1985) \EÆient string mathing with don't are patterns," in Apostolio A.,
Galil Z. (eds.), Combinatorial Algorithms on Words, NATO ASI Series, Vol. F12, p.
11-29, Springer-Verlag, Berlin.
Pirklbauer K. (1992) \A study of pattern-mathing algorithms," Strutured Programming,
Vol. 13, p. 89-98, Springer Verlag New York.
Regnier M. (1988) \Knuth-Morris-Pratt algorithm: an analysis," INRIA, Roquenourt,
Frane (unpublished).
Reihert T.A., Cohen D.N., Wong A.K.C. (1973) \An appliation of information the-
ory to geneti mutations and the mathing of polypeptide sequenes," Journal of The-
oretial Biology, Vol. 42, p. 245-61.
106
Rivest R.L. (1977) \On the worst-ase behaviour of string searhing algorithms," SIAM
Journal on Computing, Vol. 6, No. 4, p. 669-74.
Robert D.C. (1982) \A speialized omputer arhiteture for text retrieval," Proeedings
of the 4th Workshop on Computer Arhiteture, p. 51-9.
Rodeh M., Pratt V.R., Even S. (1981) \Linear algorithm for data ompression via string
mathing," Journal of the ACM, Vol. 28, No. 1, p. 16-24, January 1981.
Rytter W. (1980) \A orret preproessing algorithm for Boyer-Moore string-searhing,"
SIAM Journal on Computing, Vol. 9, p. 509-12.
Sakoe H., Chiba S. (1970) \A similarity evaluation of speeh patterns by dynami pro-
gramming," (Japanese) Institute of Eletroni Communiations Engineering of Japan,
p. 136, July 1970.
Sakoe H., Chiba S. (1971) \A dynami programmingapproah to ontinuous speeh reog-
nition," 1971 Proeedings of the International Congress of Aoustis, Budapest, Hun-
gary, Paper 20 C 13.
Salton G. (1980) \Automati information retrieval," IEEE Computer, Vol. 13, p. 41-55.
September 1980.
Sanko D. (1972) \Mathing sequenes under deletion-insertion onstraints, " Proeedings
of the National Aademy of Sienes of the USA, Vol. 69, p. 4-6.
Sanko D., Kruskall J.B. (eds.) (1983) \Time warps, string edits, and maromoleules:
the theory and pratie of sequene omparison," Addison-Wesley, Reading MA.
Shabak R. (1988) \On the expeted sublinearity of the Boyer-Moore algorithm," SIAM
Journal on Computing, Vol. 17, No. 4, p. 648-58.
Shensted C. (1961) \Largest inreasing and dereasing subsequenes," Canadian Journal
of Mathematis, Vol. 13, p. 179-91.
Shieber B., Vishkin U. (1988) \On nding lowest ommon anestors: simpliation and
parallelization," SIAM Journal on Computing, Vol. 17, No. 6, p. 1253-62.
Shonhage A., Strassen V. (1971) \Shnelle Multiplikation grosser Zahlen," Computing
(Arh. Elektron. Rehnen), Vol. 7, p. 281-92.
Sedgewik R. (1983) \String searhing," Chapter 19 (p. 241-55) of Algorithms, Addison-
Wesley, Reading MA.
Sellers P.H. (1980) \The theory and omputation of evolutionary distanes: pattern reog-
nition," Journal of Algorithms, Vol. 1, p. 359-73.
Sibbald P.R., White M.J. (1987) \How probable are antibody ross-reations?," Journal
of Theoretial Biology, Vol. 127, p. 163-9.
Smit G. de V. (1982) \A omparison of three string mathing algorithms," Software |
Pratie and Experiene, Vol. 12, p. 57-66.
107
Smith P.D. (1991) \Experiments with a very fast substring searh algorithm," Software |
Pratie and Experiene, Vol. 21, No. 10, p. 1065-74, Otober 1991.
Smith T.F., Waterman M.S. (1981) \Identiation of ommon moleular subsequenes,"
Journal of Moleular Biology, Vol. 147, p. 195-7.
Sunday D.M. (1990) \A very fast substring searh algorithm," Communiations of the
ACM, Vol. 33, No. 8, p. 132-42, August 1990.
Ukkonen E. (1983) \On approximate string mathing," Proeedings of the International
Conferene on Foundations of Computer Siene, Leture Notes in Computer Siene,
Vol. 158, p. 487-95, Springer-Verlag, Berlin.
Ukkonen E. (1985a) \Algorithms for approximate string mathing," Information and Con-
trol, Vol. 64, p. 100-18.
Ukkonen E. (1985b) \Finding approximate patterns in strings," Journal of Algorithms, Vol.
6, No. 6, p. 132-7.
Velihko V.M., Zagoruyko N.G. (1970) \Automati reognition of 200 words, " Interna-
tional Journal of Man-Mahine Studies, Vol. 2, p. 223-34.
Vintsyuk T.K. (1968) \Speeh disrimination by dynami programming,"Cybernetis, Vol.
4, No. 1, p. 52-7, also (Russian) Kibernetika, Vol. 4, No. 1, p. 81-8.
Vo K-P. (1986) \More <urses>: the <sreen> library, Tehnial Report, AT&T Bell Lab-
oratories.
Wagner R.A., Fisher M.J. (1974) \The string-to-string orretion problem," Journal of
the ACM, Vol. 21, No. 1, p. 168-73, January 1974.
Wagner R.A. (1975) \On the omplexity of the extended string-to-string orretion prob-
lem," Proeedings of the Seventh Annual ACM Symposium on Theory of Computing, p.
218-23, also in Sanko D., Kruskall J.B. (eds.) Time warps, string edits, and maro-
moleules: the theory and pratie of sequene omparison, Addison-Wesley, Reading
MA, 1983.
Weiner P. (1973) \Linear pattern mathing algorithm," Proeedings of the 14th IEEE Sym-
posium on Swithing and Automata Theory, p. 1-11.
Wong C.K., Chandra A.K. (1976) \Bounds for the string editing problem," Journal of
the ACM, Vol. 23, No. 1, p. 13-6, January 1976.
Woude J. van der (1989) \Playing with patterns, searhing for strings," Siene of Com-
puter Programming, Vol. 12, No. 3, p. 177-90, Elsevier Siene Publishers.
Yamada H., Hirata M., Nagai H., Takahashi K. (1987) \A high-speed string-searh en-
gine," IEEE Journal of Solid-State Ciruits, Vol. SC-22, No. 5, p. 829-34, Otober 1987.
Yianilos P.N. (1983) \A dediated omparator mathes symbol strings fast and intelli-
gently," Eletronis, Vol. 56, No. 5, p. 113-7, Deember 1983.
Ziv J., Lempel A. (1977) \A universal algorithm for sequential data ompression," IEEE
Transations on Information Theory, Vol. IT-23, p. 337-43, May 1977.
108

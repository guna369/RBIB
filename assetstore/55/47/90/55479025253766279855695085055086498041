Arnold Beckmann
Costas Dimitracopoulos
Benedikt Lo¨we (Editors)
Logic and Theory of Algorithms
Fourth Conference on Computability in Europe, CiE 2008
Local Proceedings
June 15–20, 2008 University of Athens

Preface
CiE 2008: Logic and Theory of Algorithms
Athens, Greece, June 15–20, 2008
Computability in Europe (CiE) is an informal network of European scientists
working on computability theory, including its foundations, technical develop-
ment, and applications. Among the aims of the network is to advance our the-
oretical understanding of what can and cannot be computed, by any means
of computation. Its scientific vision is broad: computations may be performed
with discrete or continuous data by all kinds of algorithms, programs, and ma-
chines. Computations may be made by experimenting with any sort of physical
system obeying the laws of a physical theory such as Newtonian mechanics,
quantum theory, or relativity. Computations may be very general, depending
on the foundations of set theory; or very specific, using the combinatorics of
finite structures. CiE also works on subjects intimately related to computation,
especially theories of data and information, and methods for formal reasoning
about computations. The sources of new ideas and methods include practical
developments in areas such as neural networks, quantum computation, natural
computation, molecular computation, computational learning. Applications are
everywhere, especially, in algebra, analysis and geometry, or data types and pro-
gramming. Within CiE there is general recognition of the underlying relevance
of computability to physics and a broad range of other sciences, providing as it
does a basic analysis of the causal structure of dynamical systems.
The conference was based on invited tutorials and lectures, a set of special
sessions on a range of subjects as well as contributed papers and informal presen-
tations. The formal proceedings volume appeared in the Springer LNCS series,
Vol. 5028 and contains 25 of the invited lectures and 36 of the submitted con-
tributed papers. The present volume represents the informal abstract booklet
which contains another 86 of the contributed talks and two invited plenary talks
as well as 23 abstracts of informal presentations. The informal abstract booklet
is not a scholarly publication, and papers appearing in it should not be consid-
ered published or as selected in a peer review process. There will be a number of
post-proceedings publications, including special issues of Theory of Computing
Systems, Archive for Mathematical Logic, and Journal of Algorithms.
VI
The first three meetings of CiE were at the University of Amsterdam in
2005, at the University of Wales Swansea in 2006, and at the University of
Siena in 2007. The large number of mathematicians and computer scientists at-
tending these conference had their view of computability theory enlarged and
transformed: they discovered that its foundations were deeper and more myste-
rious, its technical development more vigorous, its applications wider and more
challenging than they had known. The Athens meeting promised to extend and
enrich that process.
The annual CiE conference, based on the Computability in Europe network,
has become a major event, and is the largest international meeting focused on
computability theoretic issues. The series is coordinated by the CiE Conference
Series Steering Committee:
Arnold Beckmann (Swansea)
Paola Bonizzoni (Milan)
S. Barry Cooper (Leeds)
Benedikt Lo¨we (Amsterdam, Chair)
Elvira Mayordomo (Zaragoza)
Dag Normann (Oslo)
Peter van Emde Boas (Amsterdam).
We will reconvene 2009 in Heidelberg and 2010 in Ponta Delgada (Ac¸ores).
Organization and Acknowledgements
The conference CiE 2008 was organized by: Dionysis Anapolitanos (Athens),
Arnold Beckmann (Swansea), Costas Dimitracopoulos (Athens, Chair), Michael
Mytilinaios (Athens) †, Thanases Pheidas (Heraklion), Stathis Zachos (Athens
and New York NY).
The Programme Committee was chaired by Arnold Beckmann and Costas
Dimitracopoulos:
Luigia Aiello (Rome)
Thorsten Altenkirch (Nottingham)
Klaus Ambos-Spies (Heidelberg)
Giorgio Ausiello (Rome)
Arnold Beckmann (Swansea)
Lev Beklemishev (Moscow)
Paola Bonizzoni (Milan)
Stephen A. Cook (Toronto ON)
Barry Cooper (Leeds)
Costas Dimitracopoulos (Athens)
Rod Downey (Wellington)
Elias Koutsoupias (Athens)
Orna Kupferman (Jerusalem)
Sophie Laplante (Orsay)
Hannes Leitgeb (Bristol)
Benedikt Lo¨we (Amsterdam)
Elvira Mayordomo (Zaragoza)
Franco Montagna (Siena)
Michael Mytilinaios (Athens) †
Mogens Nielsen (Aarhus)
Isabel Oitavem (Lisbon)
Catuscia Palamidessi (Palaiseau)
Thanases Pheidas (Heraklion)
Ramanujam (Chennai)
Andrea Schalk (Manchester)
Uwe Scho¨ning (Ulm)
Helmut Schwichtenberg (Munich)
Alan Selman (Buffalo NY)
Andrea Sorbi (Siena)
Ivan Soskov (Sofia)
Christopher Timpson (Oxford)
Stathis Zachos (Athens and New York
NY)
Preface VII
We are delighted to acknowledge and thank the following for their essential
financial support: City of Athens, Bank of Greece, Graduate Program in Logic,
Algorithms and Computation (MPLA), Hellenic Ministry of Education, John S.
Latsis Foundation, Kleos S.A., National and Kapodistrian University of Athens,
Public Power Corporation S.A., Rizareio Foundation, The Elsevier Foundation.
The high scientic quality of the conference was possible through the consci-
entious work of the Programme Committee, and the special session organizers.
While papers in this abstract booklet are not to be considered publications, some
of the papers in this volume greatly benefitted from comments of the referees
during our reviewing process, and we would like to thank the referees for their
work. The list of referees is given on pp. IX - X of the mentioned LNCS volume
for this conference.
We thank Andrej Voronkov for his EasyChair system which facilitated the
work of the Programme Committee and the editors considerably. We also thank
Nikolaos S. Papaspyrou for his great support in producing this abstract booklet.
Swansea, Athens and Amsterdam, June 2008 Arnold Beckmann
Costas Dimitracopoulos
Benedikt Lo¨we
Table of Contents
Invited Talks.
The Computing Species . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
Keith Devlin
Logic, Automata, Games, and Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Moshe Y. Vardi
Contributed Talks.
On the Performance of Automata Minimization Algorithms . . . . . . . . . . . . 3
Marco Almeida, Nelma Moreira, Roge´rio Reis
Decidability Results for Mobile Membranes derived from Mobile Ambients 15
Bogdan Aman, Gabriel Ciobanu
Sophisticated Infinite Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Luis Antunes, Andre´ Souto
On Detecting Deadlock in the Pi-Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Tiago Azevedo, Mario Benevides, Fa´bio Protti, Marcelo Sihman
Algorithms for Analytic Functions and Applications to Toeplitz Operators 45
Edwin Beggs, Annelies Gerber
Triviality and Minimality in the Degrees of Monotone Complexity . . . . . . . 55
William Calhoun
Extraction of Efficient Programs from Correct Proofs: The Case of
Structural Induction over Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Luca Chiarabini
Phase Shifts of LFSM as Pseudorandom Number Generators for BIST
for VLSI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Sung-Jin Cho, Un-Sook Choi, Han-Doo Kim, Yoon-Hee Hwang, Jin-
Gyoung Kim
Introducing Service Schemes and Systems Organization in the Theory
of Interactive Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Antoˆnio Carlos Costa, Grac¸aliz Dimuro
Online-division with Periodic Rational Numbers . . . . . . . . . . . . . . . . . . . . . . 97
Gregorio de Miguel Casado, Juan Manuel Garc´ıa Chamizo, Higinio
Mora Mora
Table of Contents IX
Abstract Geometrical Computation with Accumulations: Beyond the
Blum, Shub and Smale model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Je´roˆme Durand-Lose
Notions of Bisimulation for Heyting-Valued Modal Languages . . . . . . . . . . 117
Pantelis Eleftheriou, Costas Koutras, Christos Nomikos
Algorithmic Properties of Structures for Languages with Two Unary
Functional Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Ekaterina Fokina
Verification of Newman’s and Yokouchi’s Lemmas in PVS . . . . . . . . . . . . . . 137
Andre´ Luiz Galdino, Mauricio Ayala-Rinco´n
Computation over Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
Christine Gaßner
Singularities of Holomorphic Functions in Subsystems of Second Order
Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
Yoshihiro Horihata, Keita Yokoyama
Modelling Linear Cellular Automata with the Minimum Stage
Corresponding to CCSG based on LFSR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, Han-Doo Kim
Multitape Ordinal Machines and Primitive Recursion . . . . . . . . . . . . . . . . . . 175
Bernhard Irrgang, Benjamin Seyfferth
Prescribed Learning of Indexed Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
Sanjay Jain, Frank Stephan, Nan Ye
Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 195
Maurice Jansen
The Use of Information Affinity in Possibilistic Decision Tree Learning
and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
Ilyes Jenhani, Salem Benferhat, Zied Elouedi
Ordering Finite Labeled Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
Herman Ruge Jervell
Towards Reverse Proofs-as-Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
Reinhard Kahle
Plotkin Definability Theorem for Atomic-Coherent Information Systems . 234
Basil Kara´dais
Safety Properties Verification for Pfaffian Dynamics . . . . . . . . . . . . . . . . . . . 246
Margarita Korovina, Nicolai Vorobjov
XOn Extending Wand’s Type Reconstruction Algorithm to Handle
Polymorphic Let . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
Sunil Kothari, James Caldwell
Simulations Between Tilings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
Gre´gory Lafitte, Michael Weiss
Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 274
Ste´phane Le Roux
Combining Concept Maps and Petri Nets to Generate Intelligent
Tutoring Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
Maikel Leo´n, Isis Bonet, Zenaida Garc´ıa
Query-Optimal Oracle Turing Machines for Type-2 Computations . . . . . . . 294
Chung-Chih Li
From Hilbert’s Program to a Logic Toolbox . . . . . . . . . . . . . . . . . . . . . . . . . . 304
Johann Makowsky
From Program Verification to Certified Binaries . . . . . . . . . . . . . . . . . . . . . . . 324
Angelos Manousaridis, Michalis Papakyriakou, Nikolaos Papaspyrou
Limiting Recursion, FM–Repressentability, and Hypercomputations . . . . . 332
Marcin Mostowski
Using Tables to Construct Non-Redundant Proofs . . . . . . . . . . . . . . . . . . . . . 344
Vivek Nigam
Classifying the Phase Transition Threshold for Unordered Regressive
Ramsey Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
Florian Pelupessy, Andreas Weiermann
Almost Partial m-Reducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
Katya Petrova, Boris Solon
Two-Dimensional Cellular Automata Transforms for a Novel Edge
Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
Yongri Piao, Seok-Tae Kim, Sung-Jin Cho
Computable Counter-examples to the Brouwer Fixed-point Theorem . . . . 377
Petrus Potgieter
Polynomial Iterations over Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
Mihai Prunescu
Simulations of Quantum Turing Machines by Quantum Multi-Counter
Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
Daowen Qiu
Table of Contents XI
Optimal Proof Systems and Complete Languages . . . . . . . . . . . . . . . . . . . . . 407
Zenon Sadowski
On the Complexity of Computing Winning Strategies for Finite Poset
Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
Michael Soltys, Craig Wilson
A Statistical Mechanical Interpretation of Algorithmic Information Theory 425
Kohtaro Tadaki
Solving Tripartite Matching by Interval-valued Computation in
Polynomial Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
A´kos Tajti, Benedek Nagy
Probabilistic Machines vs. Relativized Computation . . . . . . . . . . . . . . . . . . . 445
Hayato Takahashi, Kazuyuki Aihara
Quantum Query Algorithms for AND and OR Boolean Functions . . . . . . . 453
Alina Vasilieva
Space Complexity in Ordinal Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . 463
Joost Winter
Reverse Mathematics for Fourier Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . 473
Keita Yokoyama
Induced Matchings in Graphs of Maximum Degree Three . . . . . . . . . . . . . . 483
Graz˙yna Zwoz´niak
Abstracts of Informal Presentations.
Subsystems of Iterated Inductive Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . 493
Bahareh Afshari, Michael Rathjen
Query Algorithms for Detecting Hamming and Reed-Solomon Codes . . . . 494
Ruben Agadzˇanjan
Expressive Power Of Graph Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
Timos Antonopoulos, Anuj Dawar
The Lost Melody Theorem for Infinite Time Register Machines . . . . . . . . . 496
Merlin Carl, Peter Koepke
Comparing Notions of Fractal Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
Chris Conidis
Clockable Ordinals for Infinite Time Register Machines . . . . . . . . . . . . . . . . 498
Tim Fischbach, Peter Koepke, Miriam Nasfi, Gregor Weckbecker
Embedding the Enumeration Degrees in the ω-Enumeration Degrees . . . . . 499
Hristo Ganchev
XII
Computable Models Spectras of Ehrenfeucht Theories . . . . . . . . . . . . . . . . . 500
Alexander Gavryushkin
Deterministic Subsequential Transducers with Additional FIFO-memory . 501
Stefan Gerdjikov
Proof Fragments and Cut-Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
Stefan Hetzl
q-Overlaps in the Random Exact Cover Problem . . . . . . . . . . . . . . . . . . . . . . 503
Gabriel Istrate, Romeo Negrea
Inductive Definitions over Domain Representable Spaces . . . . . . . . . . . . . . . 504
Petter Kristian Køber
The Computable Dimension of Free Projective Planes . . . . . . . . . . . . . . . . . 505
Nurlan Kogabaev
A Classification of Theories of Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
Graham Leigh, Michael Rathjen
Monotonicity Conditions over Characterisations of PSPACE . . . . . . . . . . . . 507
Bruno Loff, Isabel Oitavem
Some Results on Local LR-degree Structures . . . . . . . . . . . . . . . . . . . . . . . . . 508
Anthony Morphett
The Axiomatic Derivation of Absolute Lower Bounds . . . . . . . . . . . . . . . . . . 509
Yiannis Moschovakis
Exploring the Computational Contribution of a Non-constructive
Combinatorial Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
Diana Ratiu, Trifon Trifonov
Autostability of Automatic Linear Orders . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
Alexandra Revenko
Quantifiers on Automatic Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
Sasha Rubin
The Almost Zero ω-Enumeration Degrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
Ivan N. Soskov
A Sequent Calculus for Intersection and Union Logic . . . . . . . . . . . . . . . . . . 514
Anastasia Veneti, Yiorgos Stavrinos
Anhomomorphic Logic: The Logic of Quantum Realism . . . . . . . . . . . . . . . . 515
Petros Wallden
The Computing Species
Keith Devlin
Center for the Study of Language and Information (CSLI)
Standford University
Cordura Hall, 210 Panama Street
Stanford, CA 94305-4115 USA
devlin@csli.stanford.edu
We are characteristically the “symbolic species,” Terrence Deacon claimed
in his celebrated 1997 book of that name. Deacon’s focus was on language, a
capacity unique to humans, the development of which changed in fundamental
ways the way humans think and live. Computation is a particular form of sym-
bolic processing. At four distinct stages in the development of modern society,
acquisition of the ability to carry out certain new kinds of computation likewise
changed - also in fundamental ways - how we humans understand the world and
live our lives.
The fourth such change is taking place during our lifetime, brought about by
the invention of machines that can be instructed to compute for us. The others
occurred in 8,000 B.C., the 13th century, and the 17th century. I’ll look at how
human life and cognition changed at each of those three stages.
Logic, Automata, Games, and Algorithms
Moshe Y. Vardi
Department of Computer Science
Rice University
6100 S. Main Street
Houston, TX 77005-1892, USA
vardi@cs.rice.edu
The automata-theoretic approach to decision procedures, introduced by
Buechi, Elgot, and Trakhtenbrot in the late 1950s, is one of the most funda-
mental approaches to decision procedures. Recently, this approach has found
industrial applications in formal verification of hardware and software systems.
The path from logic to practical algorithms goes not only through automata,
but also through games, whose algorithmic aspects were studies by Chandra,
Kozen, and Stockmeyer in the late 1970s. In this tutorial we describe the path
from logic to algorithms via games and automata.
On the Performance of Automata Minimization
Algorithms
Marco Almeida, Nelma Moreira, and Roge´rio Reis
{mfa,nam,rvr}@ncc.up.pt
DCC-FC & LIACC, Universidade do Porto
R. do Campo Alegre 1021/1055, 4169-007 Porto, Portugal
Abstract. Apart from the theoretical worst-case running time analysis
not much is known about the average-case analysis or practical perfor-
mance of finite automata minimization algorithms. On this paper we
compare the running time of four minimization algorithms based on
experimental results, and apply them to both deterministic and non-
deterministic random automata. Although there is no clear winner, some
conclusions can be taken for specific cases. Hopcroft’s algorithm performs
better for DFAs with small alphabets, and, regardless of the alphabet
size, Brzozowski’s algorithm is the clearly the fastest dealing with NFAs.
Key words: deterministic finite automata, non-deterministic finite au-
tomata, minimal automata, minimization algorithms, random generation
1 Introduction
The problem of writing efficient algorithms to find the minimal DFA equivalent
to a given automaton can be traced back to the 1950’s with the works of Huffman
[Huf55] and Moore [Moo58]. Over the years several alternative algorithms were
proposed. Authors typically present the running time worst-case analysis of their
algorithms, but the practical experience is sometimes different. The comparison
of algorithms performance is always a difficult problem, and little is known about
the practical running time performance of automata mininimization algorithms.
In particular, there are no studies of average-case analysis of these algorithms, an
exception being the work of Nicaud [Nic00], where it is proved that the average-
case complexity of Brzozowski’s algorithm is exponential for group automata.
Lhota´k [Lho00] presents a general data structure for DFA minimization algo-
rithms to run in O(kn log n), where n is the number of states of the DFA and
k is the size of the alphabet. Bruce Watson [Wat95] presents some experimen-
tal results but his data sets were small and biased. Tabakov and Vardi [TV05]
compared Hopcroft’s and Brzozowski’s algorithms. Baclet et al. [BP06] analysed
different implementations of the Hopcroft’s algorithm. More recently, Bassino et
al. [BDN07] compared Moore’s and Hopcroft’s algorithms.
Using the Python programming language, we implemented the algorithms
due to Hopcroft (H) [Hop71], Brzozowski (B) [Brz63], Watson (W) [Wat95],
and also using full memoization (WD) [WD03]. The choice of the algorithms
4 Marco Almeida et al.
was due to the disparate worst-case complexities and doubts about the prac-
tical behaviour of each algorithm. The input data was obtained with random
automata generators. For the (initially-connected) deterministic finite automata
we used a uniform random generator and thus our results are statistically accu-
rate. Lacking an equivalent uniform random generator for NFAs, we implemented
a non-uniform one. Although not statistically significant, the results in this case
are still fairly interesting.
The text is organised as follows. In Section 2 we present some definitions and
the notation used throughout the paper. In Section 3 we describe each of the
algorithms, briefly explain them, and present the respective pseudo-code. In Sec-
tion 4 we describe the generation methods of the random automata. In Section
5 we present the experimental results and in Section 6 we analyze these results.
Finally, on Section 7 we summarize and compare our results with previous work.
2 Preliminaries
A deterministic finite automaton (DFA) D is a tuple (Q,Σ, δ, q0, F ) where Q is
a finite set of states, Σ is the input alphabet (any non-empty set of symbols),
δ : Q×Σ → Q is the transition function, q0 is the initial state and F ⊆ Q is the
set of final states. When the transition function is total, the automaton D is said
to be complete. Any finite sequence of alphabet symbols a ∈ Σ is a word. Let Σ?
denote the set of all words over the alphabet Σ and  denote the empty word.
We define the extended transition function δˆ : Q×Σ? → Q in the following way:
δˆ(q, ) = q; δˆ(q, xa) = δ(δˆ(q, x), a). A state q ∈ Q of a DFA D = (Q,Σ, δ, q0, F ) is
called accessible if δˆ(q0, w) = q for some w ∈ Σ?. If all states in Q are accessible, a
complete DFA D is called (complete) initially-connected (ICDFA). The language
accepted by D, L(D), is the set of all words w ∈ Σ? such that δˆ(q0, w) ∈ F .
Two DFAs D and D′ are equivalent if and only if L(D) = L(D′). A DFA is
called minimal if there is no other equivalent DFA with fewer states. Given
a DFA D = (Q,Σ, δ, q0, F ), two states q1, q2 ∈ Q are said to be equivalent,
denoted q1 ≈ q2, if for every w ∈ Σ?, δˆ(q1, w) ∈ F ⇔ δˆ(q2, w) ∈ F . Two
states that are not equivalent are called distinguishable. The equivalent minimal
automaton D/≈ is called the quotient automaton, and its states correspond to
the equivalence classes of ≈. It is proved to be unique up to isomorphism.
A non-deterministic finite automaton (NFA) is also a tuple (Q,Σ,∆, I, F ),
where I is a set of initial states and the transition function is defined as ∆ : Q×
Σ → 2Q. Just like with DFAs, we can define the extended transition function ∆ˆ :
2Q ×Σ? → 2Q in the following way: ∆ˆ(S, ) = S; ∆ˆ(S, xa) = ⋃q∈∆ˆ(S,x) δ(q, a).
The language accepted by N is the set of all words w ∈ Σ? such that ∆ˆ(I, w)∩
F 6= ∅. Every language accepted by some NFA can also be described by a DFA.
The subset construction method takes a NFA A as input and computes a DFA
D such that L(A) = L(D). This process is also referred to as determinization
and has a worst-case running time complexity of O
(
2|Q|
)
.
Following Leslie [Les95], we define the transition density of an automaton
A = (Q,Σ,∆, I, F ) as the ratio t|Q|2|Σ| , where t is the number of transitions in
On the Performance of Automata Minimization Algorithms 5
A. This density function is normalised, giving always a value between 0 and 1.
We also define deterministic density as the ratio of the number of transitions t
to the number of transitions of a complete DFA with the same number of states
and symbols, i.e., t|Q||Σ| .
The reversal of a word w = a0a1 · · · an, written wR, is an · · · a1a0. The re-
versal of a language L ⊆ Σ? is LR = {wR |w ∈  L}. Further details on regular
languages can be found in the usual references (Hopcroft [HMU00] or Kozen
[Koz97], for example).
3 Minimization Algorithms
Given an automaton, to obtain the associated minimal DFA we must compute
the equivalence relation ≈ as defined in Section 2. The computation of this
relation is the key difference of the several minimization algorithms. Moore’s
algorithm and its variants, for example, aim to find pairs of distinguishable
states. H, on the other hand, computes the minimal automaton by refining a
partition of the states’ set.
Of the three minimization algorithms we compared, H has the best worst-
case running time analysis. B is simple and elegant, and, despite its exponential
worst-case complexity, it is supposed to frequently outperform other algorithms
(including H). B also has the particularity of being able to take both DFAs and
NFAs as input. W can be halted at any time yielding a partially minimized
automaton. The improved version, WD, includes the use of full memoization.
Because one of our motivations was to check the minimality of a given automa-
ton, not to obtain the equivalent minimal one, this algorithm was of particular
interest.
3.1 Hopcroft’s Algorithm (H)
H [Hop71], published in 1971, achieves the best known running time worst-case
complexity for minimization algorithms. It runs on O(kn log n) time for a DFA
with n states and an alphabet of size k. Let D = (Q,Σ, δ, q0, F ) be a DFA. H,
proceeds by refining the coarsest partition until no more refinements are possible.
The initial partition is P = {F,Q−F} and, at each step of the algorithm, a block
B ∈ P and a symbol a ∈ Σ are selected to refine the partition. This refinement
process splits each block B′ of the partition according to whether the states of
B′, when consuming the symbol a, go to a state which is in B or not. Formally,
we call this procedure split and define it by
split(B′, B, a) = (B′ ∩ δˇ−1(B, a), B′ ∩ δˇ−1(B, a))
where δˇ(S, a) =
⋃
q∈S δ(q, a).
The algorithm terminates when there are no more blocks to refine. In the
end, each block of the partition is a set of equivalent states. Because, for any
two blocks B,B′ ∈ P , every state q ∈ B is distinguishable from any state
6 Marco Almeida et al.
q′ ∈ B′, the elements of P represent the states of a new minimal DFA. The
complete pseudo-code is presented in Algorithm 1.1.
def hopcro f t ( ) :
L = {}
i f |F | < |Q−F | :
P = {Q−F, F} ; L = {F}
else :
P = {F, Q−F} ; L = {Q−F}
while L 6= ∅ :
S = ex t r a c t (L)
for a in Σ :
for B in P:
(B1 , B2 ) = s p l i t (B, S , a )
P = P − {B} ; P = P ∪ {B1 } ; P = P ∪ {B2}
i f |B1 | < |B2 | :
L = L ∪ {B1}
else :
L = L ∪ {B2}
return P
Algorithm 1.1. Hopcroft’s algorithm (H).
The set L contains blocks of P not yet treated. The extract procedure
removes one element of L to be used in the splitting process. The choice of the
element does not influence the correctness of the algorithm.
3.2 Brzozowski’s Algorithm (B)
B [Brz63] is based on two successive reverse and determinization operations and
the full pseudo-code is presented (in one single line!) on Algorithm 1.2.
def brzozowski ( f a ) :
return det ( rev ( det ( rev ( f a ) ) ) )
Algorithm 1.2. Brzozowski’s algorithm (B).
Having to perform two determinizations, the worst-case running time com-
plexity of B is exponential. Watson’s thesis, however, presents some surprising
results about B practical performance, usually outperforming H. As for the pe-
culiar way that this algorithm computes a minimal DFA, Watson assumed it
to be unique and, in his taxonomy, placed it apart all other algorithms. Later,
Champarnaud et al. [CKP02] analysed the way the sequential determinizations
perform the minimization and showed that it does compute state equivalences.
3.3 An Incremental Algorithm (W)
Watson presented an incremental DFA minimization algorithm (W) [Wat95].
This algorithm can be halted at any time yielding a partially minimized DFA that
recognises the same language as the input DFA. Later, Watson and Daciuk pre-
sented an improved version of the same algorithm (WD) [WD03] which makes
On the Performance of Automata Minimization Algorithms 7
use of full memoization. While the first algorithm has a worst-case exponential
running time, the memoized version yields a O(n2α((n2)) algorithm, where α is
the inverse of the Ackermann function. Since α(x) ≤ 5 for any x ≤ 2216 , α(x)
can be considerend a constant for all “practical” values of x. It was not clear,
however, that this algorithm would outperform H as the experimental results
in [WD03] seemed to point to. Since the use of memoization introduces some
considerable overhead in the algorithm, we wanted to discover at what point this
extra work begins to payoff.
W uses an auxiliary function, equiv, to test the equivalence of two states.
The third argument, an integer k, is used to control the recursion depth only for
matters of efficiency. Also for matters of efficiency, a variable S that contains a
set of presumably equivalent pairs of states is made global. The pseudo-code for
a non-memoized, specialized for ICDFAs, implementation of equiv is presented
in Algorithm 1.3. The memoized algorithm (WD) is quite extensive and can be
found in Watson and Daciuk’s paper [WD03].
def equiv (p , q , k ) :
i f k = 0 :
return (p in F and q in F) or (not p in F and not q in F)
e l i f (p , q ) in S :
return True
else :
eq = (p in F and q in F) or (not p in F and not q in F)
S = S ∪ {(p , q )}
for a in Σ :
i f not eq :
return False
eq = eq and equiv (δ(p, a) , δ(q, a) , k−1)
S = S − {(p , q )}
return eq
Algorithm 1.3. Pairwise state equivalence algorithm.
Having a method to verify pairwise state equivalence, it is possible to imple-
ment a test that calls equiv for every pair of states and returns False if some
pair is found to be equivalent.
4 Random Automata Generation
Even if we consider only (non-isomorphic) ICDFAs, the number of automata
with n states over an alphabet of k symbols grows so fast [RMA05] that trying
to minimize every one is not feasible, even for small values of n and k. The same
applies to NFAs. In order to compare the practical performance of the mini-
mization algorithms, we must have an arbitrary quantity of “good” randomly
generated automata available, i.e. the samples can not be unbiased. This can
achieved with a uniform random generator. We used the DFA string representa-
tion and random generation method proposed by Almeida et al. [AMR07]. This
approach, unlike the one by Bassino et al. [BN07], does not require a rejection
8 Marco Almeida et al.
step. The generator produces a string that is a canonical representation of an
ICDFA with n states and k symbols without final states information, as de-
scribed by Reis et al. [RMA05,AMR07]. Given an order over Σ, it is possible to
define a canonical order over the set of states by traversing the automaton in a
breadth-first way choosing at each node the outgoing edges using the order of Σ.
In the string representation, each of the i blocks, for 1 ≤ i ≤ n, corresponds to
the transitions from the state i−1. The random generator produces the random
strings from left to right, taking into account the number of ICDFAs that, at a
given point, would still be possible to produce with a given prefix. The set of
final states is computed by generating an equiprobable bitstream of size n and
considering final all the states that correspond to a non-zero position.
Lacking a uniform random generator for NFAs, we implemented one which
combines the van Zijl bit-stream method, as presented by Champarnaud et al.
[CHPZ04], with one of Leslie’s approaches [Les95], which allows us both to gener-
ate initially connected NFAs (with one initial state) and to control the transition
density. Leslie presents a “generate-and-test” method which may never stop, so
we added some minor changes that correct this situation. A brief explanation of
the random NFA generator follows. Suppose we want to generate a random NFA
with n states over an alphabet of k symbols and a transition density d. Let the
states (respectively the symbols) be named by the integers 0, . . . , n− 1 (respec-
tively 0, . . . , k − 1). A sequence of n2k bits describes the transition function in
the following way: the occurrence of a non-zero bit at the position ink + jk + a
denotes the existence of a transition from state i to state j labelled by the
symbol a.
Starting with a sequence of zero bits, the first step of the algorithm is to
create a connected structure and thus ensure that all the states of the final NFA
will be accessible. In order to do so, we define the first state as 0, mark it as
visited, generate a transition from 0 to any not-visited state i, and mark i as
visited. Next, until all states are marked as visited, randomly choose an already
visited state q1, randomly choose a not-visited state q2, add a transition from q1
to q2 (with a random), and mark q2 as visited. At this point we have an initially
connected NFA and proceed by adding random transitions. Until the desired
density is achieved, we simply select one of the bitstream’s zero bits and set it to
one. By maintaining a list of visited states on the first step and keeping record of
the zero bits on the second step, we avoid generating either a disconnected NFA
or a repeated transition and guarantee that the algorithm always halts. The set
of final states can be easily obtained by generating an equiprobable bitstream of
size n and considering final all the states that correspond to a non-zero position
in the bitstream.
5 Experimental Results
To compare algorithms is always a difficult problem. The choice of the program-
ming language, implementation details, and the hardware used may harm the
rigour of any benchmark. In order to produce realistic results, the input data
On the Performance of Automata Minimization Algorithms 9
should be random so that it represents a typical usage of the algorithm and the
test environment should be identical for all benchmarks. We implemented all the
algorithms in the Python 2.4 programming language, using similar data struc-
tures whenever possible. All the tests were executed in the same computer, an
Intel R© Xeon R© 5140 at 2.33GHz with 2GB of RAM. We used samples of 10.000
automata, with 5 ≤ n ≤ 100 states and alphabets with k ∈ {2, 5, 10, 20} sym-
bols. For the data sets obtained with the uniform random generator, the size of
each sample is sufficient to ensure results with a 95% confidence level within a
1% error margin. It is calculated with the formula n = ( z2 )
2, where z is obtained
from the normal distribution table such that P (−z < Z < z)) = γ,  is the error
margin, and γ is the desired confidence level.
5.1 Random ICDFA Minimization
On his thesis, Watson used a fairly biased sample. It consisted of 4833 DFAs
of which only 7 had 23 states. As Watson himself states, being constructed
from regular expressions, the automata “... are usually not very large, they have
relatively sparse transition graphs, and the alphabet frequently consists of the
entire ASCII character set.”. On their paper on the incremental minimization
algorithm [WD03], Watson and Daciuk also present some performance compar-
isons of automata minimization algorithms. They used four different data sets,
one from experiments on finite-state approximation of context-free grammars
and three that were automatically generated. These are not, however, uniform
random samples, and thus, do not represent a typical usage of the algorithms.
The graphics on Figure 1 show the running times of the tested algorithms
while minimizing samples of 10.000 random ICDFAs. This running time is the
total time required for minimizing an entire sample and was limited to 24 hours.
The batches which did not finish under this limit appear with the value −1, in
order to be distinguished from those that actually took almost 0 seconds.
For small alphabets (k = 2), H is always the fastest. When the alphabet
size grows (k ≥ 5), however, H is clearly outperformed by the WD, which
was over twice as fast as H when minimizing ICDFAs with an alphabet of size
k ≥ 10. W showed itself quite slow in all tests. It is important to point out
that for k ≥ 5 all the automata were already minimal and so the speed of the
incremental algorithm can not be justified by the possibility of halting whenever
two equivalent states are found. The fact that almost all ICDFAs are minimal
was observed by several authors, namely Almeida et al. [AMR07] and Bassino
et al.[BDN07]. As Watson himself stated, the incremental algorithm may show
exponential performance for some DFAs. This was the case in one of our tests.
For the sample of 5 symbols and 15 states WD took an unusual amount of time.
B is never the fastest algorithm. In fact, even for small alphabets it was not
possible to use it on ICDFAs with more than 15 states.
10 Marco Almeida et al.
2 symbols
0
1
2
3
4
5
10 20 50 100
# states
time - log(s)
5 symbols
0
1
2
3
4
5
10 20 50 100
# states
time - log(s)
10 symbols
0
1
2
3
4
5
10 20 50 100
# states
time - log(s)
20 symbols
0
1
2
3
4
5
10 20 50 100
# states
time - log(s)
H W WD B
Fig. 1. Running time results for the minimization of 10.000 ICDFAs with k ∈
{2, 5, 10, 20}.
5.2 Random NFAs Minimization
The next set of graphics shows the execution times of the three algorithms when
applied to a set of 10.000 random NFAs. The running time limit for all algo-
rithms was set to 15 hours and, again, the batches that did not finish under
this time limit are shown with the value −1. It is important to note that the
NFA generator we used is not a uniform one, and so we can not prove that each
sample is actually a good representative of the universe. Because we are dealing
with NFAs, the transition density is an important factor and so each sample
was generated with three different transition densities (d): 0.2, 0.5, and 0.8. For
both the WD and H, which are only able to minimize DFAs, we also accounted
for the time spent in the subset construction method. For alphabets with two
symbols there are no significant differences in any of the algorithms’ general
performance, although B is usually the fastest. H outperforms B for less than
4% only when d = 0.5 and n ∈ {50, 100}. For alphabets with k = 5, B is always
the fastest and, except for occasional cases, H is slightly faster than the incre-
mental algorithm. When the alphabet size increases, B’s performance becomes
quite remarkable. For an alphabet with size k ∈ {10, 20}, B is definitively the
fastest, being the only algorithm to actually finish the minimization process of
almost all random samples within the 15 hour limit. As for H and WD, except
for two cases, there are no significant performance differences. For d = 0.2 and
On the Performance of Automata Minimization Algorithms 11
Transition density d = 0.2
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.2
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.5
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.5
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.8
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.8
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
H WD B
Fig. 2. Running time results for the minimization of 10.000 NFAs with k = 2 (left)
and k = 5 (right).
n = 10 all the algorithms showed a particularly bad performance. For k = 5 only
B finished the minimization process (taking an unusual high amount of time)
and for k ∈ {10, 20} none of the algorithms completed the minimization process
within the 15 hour time limit. This result corroborates Leslie’s conjecture [Les95]
about the number of states obtained with the subset construction method for a
given deterministic density dd. Leslie’s conjecture states that randomly gener-
ated automata exhibit the maximum execution time and the maximum number
of states at an approximate deterministic density of 2.0. While generating the
random NFAs, we considered the transition density d = tn2k , which is related to
the deterministic density dd = tnk by dd = nd. It is easy to see that in our case
dd = nd = 10×0.2 = 2.0, which will make the subset construction computation-
ally expensive. In order to achieve the same exponential behaviour in the subset
method for d ∈ {0.5, 0.8} the number of states would have to be n ∈ {4, 2.5}, but
for such a small number of states the exponential blowup is not meaningful. This
12 Marco Almeida et al.
Transition density d = 0.2
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.2
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.5
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.5
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.8
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
Transition density d = 0.8
0
1
2
3
4
5 10 20 50 100
# states
time - log(s)
H WD B
Fig. 3. Running time results for the minimization of 10.000 NFAs with k = 10 (left)
and k = 20 (right).
explains why there are no similar results for the test batches with d ∈ {0.5, 0.8}.
Considering we used a variation of one of Leslie’s random NFA generators, this
result does not come with any surprise.
6 Analysis of the Results
In this work, we experimentally compare four automata minimization algorithms
(H, W, WD and B). As data sets we use two different types of randomly
generated automata (ICDFAs and NFAs) with a range of different number of
states and symbols. The ICDFAs’ data set was obtained using a uniform random
generator and is large enough to ensure a 95% confidence level within a 1% error
margin. For ICDFAs with only two symbols, H is the fastest algorithm. As the
On the Performance of Automata Minimization Algorithms 13
alphabet size grows, WD begins to perform better than H. With alphabets
larger than 10, WD becomes clearly faster. As for W and B algorithms, we can
safely conclude that neither performs well, regardless of the number of states and
symbols. As for the NFAs, it is important to note that the random generator used
was not a uniform one, and thus does not have the same statistical accuracy as
the first one. B is definitively the fastest algorithm. BothH andWD consistently
show equally slower results.
Since all these algorithms make use of the subset construction at least once,
the reason for B’s good performance is even less evident. It would be interesting
to make an average-case running time complexity analysis for the DFA reversal,
and thus possibly explain B’s behaviour with ICDFAs minimization.
7 Comparison with Related Work
In this final section we summarise and compare our experiments with some of
the results of the works cited before.
Bruce Watson implemented five minimization algorithms: two versions of
Moore’s algorithm as well as H, W and B. As we have mentioned before, the
data set then used was fairly biased and his results lead him to conclude that
the two Moore based algorithms perform very poorly and that B normally out-
performs the other two. Baclet et al. [BP06] implemented H with two different
queue strategies: LIFO and FIFO. The implementations were tested with sev-
eral random (non-uniform) automata having thousands of states but very small
alphabets (k ≤ 4), concluding that the LIFO strategy is better, at least for
alphabets of one or two symbols. Bassino et al. [BDN07] used an ICDFAs’ uni-
form random generator based on a Boltzmann sampler to create a data set and
compared the performance of the H (with the two strategies refereed above)
and Moore’s algorithms. The automata generated have up to some thousands of
states for alphabets of size 2. Their results are statistically accurate, and indicate
that Moore’s algorithm is, for the average case, more efficient than H. Moreover,
no clear difference were found between the two queue strategies for H. These
results are interesting because they neither corroborate the results of the works
mentioned above nor the general idea that H outperforms Moore’s algorithm
in practice. It remains unstudied a comparison between Moore’s algorithm with
WD using a uniformly generated data set. Finally, Tabakov and Vardi [TV05]
studied H and B performance with a data set of random NFAs. The random
model they used is similar to the one we describe in Section 4 but considering a
deterministic density, dd. They choose 0 ≤ dd ≤ 2.5 and the samples were rela-
tively small: 200 automata with n < 50 and k = 2. The conclusion was that H
is faster for low-density automata (dd < 1.5), while B is better for high-density
automata. For the example studied of n = 30 the deterministic density dd < 1.5,
corresponds to a normalised transition density that we used, d < 0.05. This
phase transition may be due to the fact that for such a low normalised tran-
sition density the probability of a connected NFA being deterministic is very
high.
14 Marco Almeida et al.
References
[AMR07] M. Almeida, N. Moreira, and R. Reis. Enumeration and generation with a
string automata representation. Theoretical Computer Science, 387(2):93–
102, 2007.
[BDN07] F. Bassino, J. David, and C. Nicaud. A library to randomly and exhaustively
generate automata. In Implementation and Application of Automata, volume
4783 of LNCS, pages 303–305. Springer-Verlag, 2007.
[BN07] F. Bassino and C. Nicaud. Enumeration and random generation of accessible
automata. Theoretical Computer Science, 381(1-3):86–104, 2007.
[BP06] M. Baclet and C. Pagetti. Around hopcroft’s algorithm. pages 114–125,
Taipei, Taiwan, 2006. Springer.
[Brz63] J. A. Brzozowski. Canonical regular expressions and minimal state graphs
for definite events. In J. Fox, editor, Proc. of the Sym. on Math. Theory of
Automata, volume 12 of MRI Symposia Series, pages 529–561, NY, 1963.
[CHPZ04] J.-M. Champarnaud, G. Hansel, T. Paranthoe¨n, and D. Ziadi. Random
generation models for nfas. J. of Automata, Lang. and Comb., 9(2), 2004.
[CKP02] J.-M. Champarnaud, A. Khorsi, and T. Paranthoe¨n. Split and join for mini-
mizing: Brzozowski’s algorithm. In M. Bal´ık and M. Sima´nek, editors, Proc.
of PSC’02, Report DC-2002-03, pages 96–104. Czech Technical University
of Prague, 2002.
[HMU00] John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. Introduction to
Automata Theory, Languages and Computation. Addison Wesley, 2000.
[Hop71] J. Hopcroft. An n logn algorithm for minimizing states in a finite automaton.
In Proc. Inter. Symp. on the Theory of Machines and Computations, pages
189–196, Haifa, Israel, 1971. AP.
[Huf55] D. A. Huffman. The synthesis of sequential switching circuits. The Journal
of Symbolic Logic, 20(1):69–70, 1955.
[Koz97] D. C. Kozen. Automata and Computability. Undergrad. Texts in Computer
Science. Springer-Verlag, 1997.
[Les95] T. Leslie. Efficient approaches to subset construction. Master’s thesis, Uni-
versity of Waterloo, Ontario, Canada, 1995.
[Lho00] O. Lhota´k. A general data structure for efficient minimization of determin-
istic finite automata. Technical report, University of Waterloo, 2000.
[Moo58] E. F. Moore. Gedanken-experiments on sequential machines. The Journal
of Symbolic Logic, 23(1):60, 1958.
[Nic00] C. Nicaud. E´tude du comportement en moyenne des automates finis et des
langages rationnels. PhD thesis, Universite´ de Paris 7, 2000.
[RMA05] R. Reis, N. Moreira, and M. Almeida. On the representation of finite au-
tomata. In C. Mereghetti, B. Palano, G. Pighizzini, and D.Wotschke, editors,
Proc. of DCFS’05, pages 269–276, Como, Italy, 2005.
[TV05] D. Tabakov and M. Vardi. Evaluating classical automata-theoretic algo-
rithms. In LPAR’05, 2005.
[Wat95] B. W. Watson. Taxonomies and toolkit of regular languages algortihms. PhD
thesis, Eindhoven Univ. of Tec., 1995.
[WD03] B. W. Watson and J. Daciuk. An efficient DFA minimization algorithm.
Natural Language Engineering, pages 49–64, 2003.
Decidability Results for Mobile Membranes
Derived from Mobile Ambients
Bogdan Aman1 and Gabriel Ciobanu1,2
1 Romanian, Academy, Institute of Computer Science
Blvd. Carol I no.8, 700505 Ias¸i, Romania
2 “A.I.Cuza” University, Faculty of Computer Science
Blvd. Carol I no.11, 700506 Ias¸i, Romania
baman@iit.tuiasi.ro, gabriel@info.uaic.ro
Abstract. Mobile ambients and membrane systems are two new com-
putation models. We investigate some decidability properties in mobile
membranes, using also some known decidability results of the mobile
ambients.
1 Introduction
Ambients calculus [6] and membrane systems [15] represent two new computa-
tion models which are quickly evolving. During the last decade, both of them
were emergent research fields in computer science. Both mobile ambients and
mobile membranes (a variant of membrane systems introduced in [11]) have
similar structures and common concepts. Both formalisms work with bound-
aries representing locations, have a hierarchical structure representing the nest-
ing relationship among locations, and describe mobility in complex systems. We
consider these new computing models, studying their computability aspects and
their relationship.
Mobile ambients are designed to model both mobile computing (provided
by mobile devices as laptops or PDA’s), and mobile computation (mobile code
like applets or agents moving between computers and devices). An ambient is a
location (bounded place) where computation happens. Ambients can be nested,
having a hierarchical structure. An ambient has a name, and can move with
all the subambients and processes it contains. Ambient calculus is designed to
describe distributed and mobile computation. In contrast with other formalisms
for mobility such as the pi-calculus [13] whose computational model is based on
the notion of communication, the ambient calculus is based on the notion of
movement. An ambient is the unit of movement. Ambients mobility is controlled
by the capabilities in, out, and open. Moreover, in an ambient we have processes
which may exchange messages.
Membrane systems are introduced in [14] as a class of parallel computing
devices inspired by the biological systems which are complex hierarchical struc-
tures, with a flow of materials and information which underlies their functioning.
Essentially, the membrane systems (also called P systems) are composed of var-
ious compartments with different tasks, all of them working simultaneously to
16 Bogdan Aman and Gabriel Ciobanu
accomplish a more general task. There are several variants of membrane systems.
The mobile membranes were introduced in [2] where the mobility is expressed
through the operations of gemmation and fusion of mobile membranes. In this
paper we use mobile membrane systems (also called P systems with mobile
membranes) which express mobility using two biologically inspired operations:
exocytosis and endocytosis.
The structure of the paper is as follows. In Section 2 we present the ambient
calculus. In the last sections of the paper we present the mobile membrane
systems and investigate some decidability problems (reachability, boundedness,
periodicity) in mobile membranes. Conclusions and references end the paper.
2 Mobile Ambients
In this section we provide a short description of the mobile ambients; more details
can be found in [6]. The following table describes the syntax of mobile ambients.
Table 1: Mobile Ambients Syntax
c channel name P,Q ::= processes
n,m ambient names 0 inactivity
x variable M.P movement
M ::= capabilities n[P ] ambient
in n can enter n P |Q composition
out n can exit n (νn)P restriction
open n can open n c!〈m〉.P output action
c?(x).P input action
∗P replication
Process 0 is an inactive process (it does nothing). A movement M.P is pro-
vided by a capability M , followed by the execution of process P . An ambient
n[P ] represents a bounded place labelled by n in which a process P is executed.
P |Q is a parallel composition of processes P and Q. (νn)P creates a new unique
name n within the scope of process P . An output action c!〈m〉.P releases a name
m on channel c, and then behaves as process P . An input action c?(x). P cap-
tures a name from channel c, and binds it to a variable x within the scope of
process P . ∗P denotes an unbounded replication of a process P , producing as
many parallel copies of process P as needed.
Semantics of the ambient calculus is provided by two relations: structural
congruence and reduction. The structural congruence P ≡amb Q relates differ-
ent syntactic representations of the same process; it is also used to define the
reduction relation. The reduction relation P ⇒amb Q describes the evolution of
ambients and processes. We denote by ⇒∗amb the reflexive and transitive closure
of ⇒amb, and by ⇒+amb its transitive closure.
The structural congruence is defined as the least relation over processes sat-
isfying the axioms from the table below:
Decidability Results for Mobile Membranes Derived from Mobile Ambients 17
Table 2: Structural Congruence
P |Q ≡amb Q |P P ≡amb Q implies Q ≡amb P
(P |Q) |R ≡amb P | (Q |R) P ≡amb Q, Q ≡amb R implies P ≡amb R
∗P ≡amb P | ∗ P P ≡amb Q implies (νn)P ≡amb (νn)Q
(νn)(νm)P ≡amb (νm)(νn)P if n 6= m P ≡amb Q implies P |R ≡amb Q |R
(νn)(P |Q) ≡amb P | (νn)Q if n /∈ fn(P ) P ≡amb Q implies ∗P ≡amb ∗Q
(νn)m[P ] ≡amb m[(νn)P ] if n 6= m P ≡amb Q implies n[P ] ≡amb n[Q]
P |0 ≡amb P P ≡amb Q implies M.P ≡amb M.Q
(νn)0 ≡amb 0 ∗0 ≡amb 0 P ≡amb Q implies c?(x).P ≡amb c?(x).Q
P ≡amb P P ≡amb Q implies c!〈m〉.P ≡amb c!〈m〉.Q
The rules from the left side of the table describe the commutativity and asso-
ciativity of the parallel components, unfolding recursion, stretching of a restric-
tion scope, renaming of bounded names. The rules from the right side describe
how structural congruence is propagated across processes. The set fn(P ) of free
names of a process P is defined as:
fn(P ) =
8>>><>>>:
∅ if P = 0
fn(R) ∪ {n} if P = in n.R or P = out n.R or P = open n.R
or P = n[R] or P = c!〈n〉.R
fn(R) ∪ fn(Q) if P = R | Q
fn(R)− {n} if P = (νn)R
fn(R) if P = c?(x).R or P = ∗R
The reduction relation is defined as the least relation over processes satisfying
the following set of axioms:
Table 3: Reduction Rules
(In) n[in m. P |Q] |m[R]⇒amb m[n[P |Q] |R]
(Out) m[n[out m. P |Q] |R]⇒amb n[P |Q] |m[R]
(Open) open n. P |n[Q]⇒amb P |Q
(Com) c!〈m〉. P | c?(x). P ′ ⇒amb P |P ′{m/x}
(Res) P ⇒amb Q implies (νn)P ⇒amb (νn)Q
(Amb) P ⇒amb Q implies n[P ]⇒amb n[Q]
(Par) P ⇒amb Q implies P |R→amb Q |R
(Struct) P ′ ≡ P, P ⇒amb Q, Q ≡ Q′ implies P ′ ⇒amb Q′
The first four rules are the one-step reductions for in, out, open and communi-
cation. In the rule (Com) we write P ′{m/x} for the substitution of name m for
each free occurrence of variable x in process P ′. The next three rules propagate
reductions across scopes, ambient nesting and parallel composition. The final
rule allows the use of structural congruence during the evolution given by the
reduction relation.
3 Decidability Results for Mobile Membranes
A membrane system consists of a hierarchy of nested membranes, placed inside
a distinguishable membrane called skin. The space outside the skin membrane is
18 Bogdan Aman and Gabriel Ciobanu
called environment. A membrane contains multisets of objects, evolution rules,
and possibly other membranes. The multisets of objects from a membrane cor-
respond to the chemicals swimming in the solution in the cell compartment,
while the rules correspond to the chemical reactions possible in the same com-
partment. The rules contain target indications specifying the membranes where
the new obtained objects are sent. The new objects either remain in the same
membrane whenever they have attached a here target, or they pass through
membranes in two directions: they can be sent out of the membrane, or can be
sent in one of the nested membranes which is precisely identified by its label. In
one step, the objects can pass only through one membrane. A membrane without
any other membranes inside is called elementary, while a membrane containing
other membranes is a composite one.
The membrane systems are synchronous: at each time unit of a global clock,
a transformation of the system takes place by applying the rules in a nondeter-
ministic and maximally parallel manner. This means that the objects and the
rules involved in such a transformation are chosen in a nondeterministic way,
and the application of rules is performed in a maximally parallel way. After a
choice was made, no rule can be applied anymore in the same evolution step:
there are not enough objects and membranes available for any rule to be applied.
Several membranes can evolve in parallel. Many variants of this basic model are
discussed in the literature [7, 15]. In this paper we use mobile membrane systems.
This is a variant of membrane systems with active membranes, but having none
of the features like polarizations, label change and division of non-elementary
membranes.
Reachability is the problem of deciding whether a system may reach a given
configuration during its execution. This is one of the most critical properties
in the verification of systems; most of the safety properties of computing sys-
tems can be reduced to the problem of checking whether a system may reach a
“unintended state”.
Boundedness is a property of systems whose production and consumption of
resources may be bounded. From a biological point of view, boundedness can
be interpreted as a property of sustainable development, in the sense that a cell
can accumulate and work with only a finite amount of material.
When working with Petri nets, reachability and boundedness are two general
properties of interest. Given a net with initial marking µ0, we say that the
marking µ is reachable if there exists a sequence of firings µ0 → µ1 → . . . µn = µ
of the net. We say that a net is bounded if the set of reachable markings is finite.
A k-bounded net implies that there exists some integer k bounding the number
of tokens which may be present at each place.
The boundedness and reachability problems are decidable in Petri nets, even
if they tend to have a very large complexity in practice. A good survey of the
known decidability issues for Petri nets is given in [9].
We use various results to investigate reachability and boundedness for mobile
membranes.
Decidability Results for Mobile Membranes Derived from Mobile Ambients 19
3.1 Mobile Membranes with Replication
We prove in [1] that reachability in the class of mobile membranes defined by
Definition 1 can be decided by reducing it to the reachability problem of a version
of pure and public ambient calculus from which the open capability has been
removed. It is proven in [4] that the reachability for this fragment of ambient
calculus is decidable by reducing it to marking reachability for Petri nets, which
is proven to be decidable in [12]. Problems like reachability and boundedness
are investigated in [8] for other classes of P systems, namely for extensions of
PB systems with volatile membranes. In what follows we present a sketch of the
work done in [1], and we extend it with new results.
First we define a new class of mobile membranes with replication rules.
Definition 1. A mobile membrane system with replication rules is a structure∏
= (V ∪ V ,H ∪H,µ,w1, . . . , wn, R), where:
1. n ≥ 1 represents the initial degree of the system;
2. V ∪ V is an alphabet (its elements are called objects), where V ∩ V = ∅;
3. H ∪H is a finite set of labels for membranes, where H ∩H = ∅;
4. µ is a membrane structure, consisting of n membranes, labelled (not neces-
sarily in a one-to-one manner) with elements of H;
5. w1, w2, . . . , wn are multisets of objects from V ∪V placed in the n membranes
of the system;
6. R is a finite set of developmental rules, of the following forms:
(a) [a↓→ a↓ a↓]h, for h ∈ H, a↓∈ V , a↓∈ V ; replication rule
The objects a↓ are used to create new objects a↓ without being consumed.
(b) [a↓ a↓→ a↓]h, for h ∈ H, a↓∈ V , a↓∈ V ; consumption rule
The objects a↓ are consumed.
(c) [a↑→ a↑ a↑]h, for h ∈ H, a↑∈ V , a↑∈ V ; replication rule
The objects a↑ are used to create new objects a↑ without being consumed.
(d) [a↑ a↑→ a↑]h, for h ∈ H, a↑∈ V , a↑∈ V ; consumption rule
The objects a↑ are consumed.
(e) [ a↓ ]h [ ]a → [ [ ]h ]a, for a, h ∈ H, a↓∈ V ; endocytosis
An elementary membrane labelled h enters the adjacent membrane la-
belled a, under the control of object a↓. The labels h and a remain un-
changed during this process; however the object a↓ is consumed during
the operation. Membrane a is not necessarily elementary.
(f) [ [ a↑ ]h ]a → [ ]h [ ]a, for a, h ∈ H, a↑∈ V ; exocytosis
An elementary membrane labelled h is sent out of a membrane labelled
a, under the control of object a↑. The labels of the two membranes re-
main unchanged; the object a↑ of membrane h is consumed during the
operation. Membrane a is not necessarily elementary.
(g) [ ]h → [ ]h[ ]h for h ∈ H, h ∈ H division rules
An elementary membrane labelled h is divided into two membranes la-
belled by h and h having the same objects.
20 Bogdan Aman and Gabriel Ciobanu
In 2., V ∩ V = ∅ states that the objects from V can participate only in rules
of type (a) and (b). Similarly in 3., H ∩H = ∅ states that the membranes having
labels from the set H can participate only in rules of type (g).
The rules are applied using the following principles:
1. In biological systems molecules are divided into classes of different types.
We make the same decision here and split the objects into four classes:
a ↓ - objects which control the endocytosis, a ↑ - objects which control the
exocytosis, and a ↓, a ↑ - objects which produce new objects from the first
two classes without being consumed.
2. All the rules are applied in parallel, non-deterministically choosing the rules,
the membranes, and the objects in such a way that the parallelism is maxi-
mal; this means that in each step we apply a set of rules such that no further
rule, no further membranes and objects can evolve at the same time.
3. A membrane a from each rule of type (e) and (f) is said to be passive, while
membrane h is said to be active. In any step of a computation, any object and
any active membrane can be involved in at most one rule, but the passive
membranes are not considered involved in the use of rules (hence they can
be used by several rules at the same time).
4. When a membrane is moved across another membrane, by endocytosis or
exocytosis, its whole content (its objects) is moved.
5. If a membrane is divided, then its content is replicated in the two new copies.
6. The skin membrane can never be divided.
According to these rules, we get transitions among the configurations of the
system. For two mobile membrane systems M and N , we say that M reduces to
N if there is a sequence of rules applicable in the membrane system M in order
to obtain the membrane system N .
Theorem 1 ([1]). For two arbitrary mobile membrane systems M1 and M2, it
is decidable whether M1 reduces to M2.
The main steps of the proof are as follows:
1. we reduce mobile membranes systems to pure and public mobile ambients
without the capability open;
2. we show that the reachability problem for two arbitrary mobile membranes
can be expressed as the reachability problem for the corresponding mobile
ambients;
3. we use the result that the reachability problem is decidable for a fragment
of pure and public mobile ambients without the capability open, using the
following decidability theorem from Petri nets:
Theorem 2 ([12]). For all Petri nets P , for all markings m,m′ for P , one can
decide whether m′ is reachable from m.
We define the following translation steps:
Decidability Results for Mobile Membranes Derived from Mobile Ambients 21
1. any object a↓ is translated into a capability in a;
2. any object a↑ is translated into a capability out a;
3. any object a↓ is translated into a replication !in a;
4. any object a↑ is translated into a replication !out a;
5. a membrane h is translated into an ambient h;
6. an elementary membrane h is translated into a replication !h[ ] where all the
objects inside membrane h are translated into capabilities of ambient h by
using the above steps.
A correspondence exists between the rules from mobile membranes and the rules
from mobile ambients:
rule (e) corresponds to rule (In);
rule (f) corresponds to rule (Out);
rules (a), (b), c and (d) correspond to instances of the structural congruence
rule ∗P ≡amb P | ∗ P , namely (a) and (c) for creation of new instances, while
(b) and (d) for consuming existing instances.
If we consider a mobile membrane systemM , we denote by T (M) the mobile am-
bient obtained using the above translation steps. For example, starting from the
membrane system M = [m↓ m↑]n[ ]m, we obtain T (M) = n[in m | out m] | m[ ].
Proposition 1 ([1]). Given two mobile membrane systems M and N , M re-
duces to N by applying one rule if and only if T (M) reduces to T (N) by applying
only one rule.
Theorem 3 ([1]). For two arbitrary ambients A and B of the restricted frag-
ment, it is decidable whether A reduces to B.
By restricted fragment we refer to the class of mobile ambients obtained through
translation from the class of mobile membrane systems from Definition 1. As
a corollary, we prove that periodicity is a decidable property. Periodicity is a
central notion in biological processes which have a periodic or quasi-periodic
evolution, such as the ATP cycle, for example.
Theorem 4 (Periodicity). Given a mobile membrane system and an initial
configuration µ0, if the system evolves it is decidable whether the system is peri-
odic.
Proof. From Theorem 1, considering µ0 both as an initial and as a target con-
figuration.
We say that a mobile membrane system, with an initial configuration µ0,
is bounded if there are only finitely many configurations which are reachable
starting from µ0. Since we have obtained the translation of the class of mobile
membrane systems with replication into Petri nets, the following result can be
viewed as a corollary.
Theorem 5 (Boundedness). Given a mobile membrane system and an initial
configuration µ0, it is decidable whether the system is bounded.
22 Bogdan Aman and Gabriel Ciobanu
The decidability of the deadlock problem is also a consequence of the encoding
into the Petri nets.
Theorem 6 (Non-termination). Given a mobile membrane system and an
initial configuration µ0, it is decidable whether all computations starting with µ0
are infinite.
For the decidability results from Petri nets we refer to [9].
3.2 Mobile Membrane Systems with Dissolution
A mobile membrane system with dissolution provides, in addition to the rules
from Definition 1, a new rule of the form:
a
δ
[ ]a → ε
where ε is the empty multiset. The intuitive meaning of this new rule is that
after applying it, the membrane a is dissolved and its content, including the
inner membranes (if any), are moved to the parent of membrane a.
In order to simulate this rule in mobile ambients we use the following trans-
lation step: the object a
δ
is translated into a capability open a. The class of
pure mobile ambients containing the capability open is shown to be undecidable
in [5]. Using this result and the fact that the mobile membrane systems from
Definition 1 together with the dissolution rule can be encoded in pure mobile
ambients with the capability open, we obtain the following result:
Theorem 7. Reachability is undecidable in mobile membrane systems with dis-
solution rules.
4 Termination in Mobile Membrane Systems
A computation terminates if all its reduction sequences are of finite length.
Termination is an interesting property in concurrency. For some configurations
and sets of rules, we may want to know that an answer is eventually produced.
Termination alone does not guarantee that an answer is eventually produced
since other properties, e.g. deadlock-free, are also involved. A first attempt to
formalize some halting conditions has been done quite recently in [10], by defining
certain sets of configurations which satisfy some conditions.
In order to assure that a computation terminates, the set of rules applied
to a membrane system must not contain rules which produce periodicity in the
system. In the rest of this section we study some of rules that can produce
periodicity.
If
∏
is a membrane system, then
∏
i denoted the membrane systems obtained
after i steps. Inspired from [3] we define in what follows the periodicity of a
membrane system:
Definition 2. A membrane systems
∏
is periodic if
∏
i=
∏
for some i > 0.
Decidability Results for Mobile Membranes Derived from Mobile Ambients 23
Definition 3. A membrane systems
∏
is eventually periodic if
∏
i+k=
∏
k for
some i, k > 0. In this case k is called the transient and i the period.
Generally this can be described as:
u
u1,1 u1,2 ...................... u1,k1
u2,1 u2,2 ...................... u2,k2
............................................
uj,k1 uj,knuj,k2......................
...................... ......................
u
In the figure above all the objects from line 1 are obtained using only objects
from the initial multiset u, then the multisets from line 2 are obtained using only
objects obtained in the first line, and so on. A special case is when at each step
we apply a single rule, namely we have
u1 → u2, u3 → u4 . . . u2n−1 → u2n, with u1, . . . , u2n ∈ V ∗,
u2i+1  u2i, for i ∈ {1, . . . , n− 1} and u1 = u2n
The simplest case has the following form
u→ u, where u ∈ V ∗
for which if the precondition is satisfied, then it can be applied forever.
5 Conclusion
Ambient calculus is a process algebra using interleaving semantics, composition-
ality and bisimulation. Membrane computing is a branch of natural computing
defining computation in the Turing sense, i.e., making use of automata, formal
languages and complexity tools. This paper presents some existing computabil-
ity results of the mobile ambient calculus and mobile membrane systems, and
emphasizes on a formal relationship between mobile ambients and mobile mem-
branes.
We investigate the problem of reaching a certain configuration of a system of
mobile membranes starting from another configuration. In order to do this we
use a result of [4] where the reachability problem for the pure and public ambient
calculus without open capability is proven to be decidable. The same problem
is tackled in [5], where the authors do not take into account the replication
of ambients which in our case is used to simulate the division rules in mobile
24 Bogdan Aman and Gabriel Ciobanu
membranes. We proved that the reachability can be decided by reducing this
problem to the reachability problem for a fragment of ambient calculus. Starting
from this approach, some other decidability results with respect to boundedness,
periodicity and termination are proven.
Acknowledgments
We thank Oana Agrigoroaiei for her useful remarks and help in improving early
drafts of the paper.
References
1. B. Aman, G. Ciobanu. On the Reachability Problem in P Systems with Mobile
Membranes. Prooceedings of the Eight Workshop on Membrane Computing, 111-
121, 2007.
2. D. Besozzi, C. Zandron, G. Mauri, N. Sabadini. P Systems with Gemmation of
Mobile Membranes. Italian Conference on Theoretical Computer Science, Lecture
Notes in Computer Science vol.2202, Springer, 136-153, 2001.
3. L. Bianco, F. Fontana, G. Franco, V. Manca. P Systems for Biological Dynamics.
Applications of Membrane Computing, Springer, 83-128, 2005.
4. I. Boneva, J.-M. Talbot. When Ambients Cannot be Opened. Foundations of
Software Science and Computation Structures, Lecture Notes in Computer Science
vol.2620, Springer, 169-184, 2003.
5. N. Busi, G. Zavattaro. Deciding Reachability in Mobile Ambients. Lecture Notes
in Computer Science, Springer vol.3444, 248-262, 2005.
6. L. Cardelli, A. Gordon. Mobile Ambients. Theoretical Aspects of Computer Soft-
ware, Lecture Notes in Computer Science vol.1378, Springer, 140-155, 1998.
7. G. Ciobanu, Gh. Pa˘un, M.J. Pe´rez-Jime´nez. Application of Membrane Computing,
Springer, 2006.
8. G. Delzanno, L. Van Begin. On the Dynamics of PB Systems with Volatile Mem-
branes. Proceedings of the Eight Workshop on Membrane Computing, 279-300,
2007.
9. J. Esparza, M. Nielson. Decidability issues for Petri nets - a survey. Journal of
Informatik Processing and Cybernetics, vol.30, 143-160, 1994.
10. R. Freund, S. Verlan. A Formal Framework for P Systems. Prooceedings of the
Eight Workshop on Membrane Computing, 317-330, 2007.
11. S.N. Krishna, Gh. Pa˘un. P Systems with Mobile Membranes. Natural Computing,
vol.4(3), Springer, 255-274, 2005.
12. E.W. Mayr. An Algorithm for the General Petri Net Reachability Problem. SIAM
Journal of Computing, vol.13(3), 441-460, 1984.
13. R. Milner. Communicating and mobile systems: the pi-calculus, Cambridge Univer-
sity Press, 1999.
14. Gh. Pa˘un. Computing with membranes. Journal of Computer and System Sciences,
vol.61(1), 108-143, 2000.
15. Gh. Pa˘un. Membrane Computing. An Introduction, Springer, 2002.
Sophisticated Infinite Sequences
Luis Antunes and Andre´ Souto?
University of Porto
lfa@ncc.up.pt, andresouto@ncc.up.pt
Abstract. In this paper we revisit the notion of sophistication for infinite
sequences. Koppel defined sophistication of an object as the length of the
shortest (finite) total program (p) that with some (finite or infinite) data (d)
produce it and |p|+ |d| is smaller than the shortest description of the object
plus a constant. However the notion of “description of infinite sequences”
is not appropriately defined. In this work, we propose a new definition of
sophistication for infinite sequences as the limit of the ratio of sophistication
of the initial segments and its length. As the main result we prove that
highly sophisticated sequences are dense when the sophistication is defined
with lim sup and are sparse when we consider the lim inf. We also prove
that, similarly to what happens for finite strings, sophistication and depth,
for infinite sequences are distinct complexity measures.
Key words: Kolmogorov complexity, Sophistication, Constructive dimen-
sions
1 Introduction
Solomonoff [Sol64], Kolmogorov [Kol65] and Chaitin [Cha66] independently defined
the complexity of an object x as the length of the shortest program that produces x.
The Kolmogorov structure function divides the smallest program producing the ob-
ject into two parts: the part accounting for the useful regularity present in the object
and the part accounting for the remaining accidental information. Kolmogorov sug-
gested that the useful information is represented by a finite set in which the object
is a typical member, so that the two-part description of the finite set together with
the index of the object is as shortest as the one-part description. This approach has
been generalized to computable probability mass functions. The combined theory
has been developed in detail in [GTV01] and called “Algorithmic Statistics”. The
most general way to proceed is perhaps to express the useful information as a recur-
sive function. The resulting measure has been called the sophistication of the object
in [Kop87,Kop88,AK91]. Later Antunes and Fortnow [AF01] revisited the notion of
sophistication for finite strings and formalize a connection between sophistication
and a variation of computational depth (intuitively the useful or nonrandom infor-
mation in a string), proving the existence of strings with maximum sophistication
and showing that they are the deepest of all strings.
In 1982, at a seminar in the Moscow State University (see [V’y99]), Kolmogorov
raised the question if “absolutely non-random” (or non-stochastic) objects exist.
Ga´cs et al. [GTV01] and Antunes and Fortnow [AF01], independently, proved that
these (finite) objects do exist.
In this work we take a fresh look at the sophistication of infinite sequences. We
start by redefining the notion of sophistication for infinite sequences, introducing the
? Departamento de Cieˆncia de Computadores, Rua Campo Alegre, 1021/1055,
4169 - 007 Porto - Portugal; The authors are partially supported by KCrypt
(POSC/EIA/60819/2004) and funds granted to LIACC through the Programa de Fi-
nanciamento Plurianual, Fundac¸a˜o para a Cieˆncia e Tecnologia and Programa POSI
26 Luis Antunes and Andre´ Souto
lower and upper sophistication as the lim inf and lim sup, respectively, of the ratio
between the sophistication of the initial segment and the length of that segment.
Then we prove that the set of sequences with upper sophistication different from 0 is
dense and the set of sequences with lower sophistication different from 0 is sparse. So,
the answer for Kolmogorov’s question for infinite sequences is affirmative if we use
the upper sophistication and probably negative if we use the lower sophistication.
Koppel claimed the equivalence between sophistication and logical depth. How-
ever, he used a different definition of logical depth imposing totality in the functions
and defining the length of a time bound by the smallest program describing it. Later
Antunes and Fortnow [AF01] gave an example, for the finite case where the equiva-
lence is not valid considering the classical definition of Kolmogorov complexity and
computational depth. In this work we go further and give an example of an infinite
sequence, the diagonal of the Halting Problem, for which the sophistication is small
and the dimensional depth is high.
The rest of this paper is organized as follows: in the next section, we give some
background on the basic concepts necessary to the rest of the paper. In section 3 we
formally present the new definitions of sophistications and establish a relationship
with Hausdorff dimension and packing dimension. In section 4 we prove that highly
sophisticated sequences are dense if sophistication is defined with lim sup and are
rare if in the definition we consider lim inf. In section 5 we relate sophistication with
dimensional depth by proving that these two complexity measures are of different
type.
2 Preliminaries
To avoid confusions any element of Σ∞ will be called a sequence and will be denoted
by Greek letters and any element of Σ∗ will be called a string. We denote the initial
segment of length n of a sequence α by α[1..n] and the ith bit by αi. The length of a
binary string x is denoted by |x|. The function log will always mean the logarithmic
function of base 2. If x and y are strings x ≤ y means that x is prefix of y.
2.1 Kolmogorov Complexity
We refer the reader to the book of Li and Vita´nyi [LV97] for a complete study
on Kolmogorov complexity. Here we give essential definitions and basic results in
Kolmogorov complexity necessary to the rest of this paper.
Definition 1. Let U be a fixed universal Turing machine. The plain Kolmogorov
complexity of x ∈ Σ∗ is, C(x) = min
p
{|p| : U(p) = x}. If t is a constructible function,
the t-time-bounded Kolmogorov complexity of x ∈ Σ∗ is, Ct(x) = minp{|p| : U(p) =
x in at most t(|x|) steps}.
Notice that the choice of Universal Turing machine affects the running time of a
program at most by a logarithmic factor and the program length at most a constant
number of extra bits. So the Kolmogorov complexity theory is machine independent.
Definition 2. A string x is c-incompressible if C(x) ≥ |x| − c.
Proposition 1. There are at least 2n × (1− 2− c) + 1 binary strings x ∈ Σn that
are c-incompressible.
We need prefix-free Kolmogorov complexity defined using prefix free Turing ma-
chines: Turing machines with a one-way input tape (the input head can only read
from left to right and crashes if it reads past the end of the input), a one-way output
tape and a two-way work tape.
Sophisticated Infinite Sequences 27
Definition 3. Let U be a fixed prefix free universal Turing machine. Then for any
string x ∈ Σ∗, the prefix free Kolmogorov complexity of x is, K(x) = minp{|p| :
U(p) = x}.
For any time constructible t, the t-time-bounded prefix-free Kolmogorov complex-
ity of x ∈ Σ∗ is, Kt(x) = minp{|p| : U(p) = x in at most t(|x|) steps}.
We extend the definition of Kolmogorov complexity to finite sets in the follow-
ing way: the Kolmogorov complexity of a set S (denoted C(S)) is the Kolmogorov
complexity of its characteristic sequence. As noted by Cover [Cov85], Kolmogorov
proposed in 1973 at the Information Theory Symposium, Tallin, Estonia the fol-
lowing function:
Definition 4. The Kolmogorov structure function Hk(x|n) for x ∈ Σn is defined
by
Hk(x|n) = min{log |S| : x ∈ S and C(S|n) ≤ k}.
Of special interest is the value
C∗(x|n) = min{k : Hk(x|n) + k = C(x|n)}.
A program for x can be written in two stages:
1. Use p to print the indicator function for S.
2. The desired string is the ith string in a lexicographic ordering
of the elements of this set.
This program has length |p| + log |S| + O(1), and C∗(x|n) is the length of the
shortest program p for which this two-stage description is as concise as the shortest
one stage description. Note that x must be maximally random (a typical element)
with respect to S otherwise the two stage description could be improved, contradict-
ing the minimality of C(x|n). Ga´cs et al. [GTV01] generalize the model class from
finite sets to probability distributions, where the models are computable probability
density functions.
In 1982, at a seminar in the Moscow State University (see [V’y99]), Kolmogorov
raised the question if “absolutely non-random” (or non-stochastic) objects exist.
Definition 5. Let α and β be natural numbers. A string x ∈ Σn is called (α, β)-
stochastic if there exists a finite set S such that x ∈ S, C(S) ≤ α and C(x|S) ≥
log |S| − β.
Ga´cs et al. [GTV01] and Antunes and Fortnow [AF01], independently, proved
that (α, β)-stochastic objects do exist.
2.2 Sophistication
Koppel [Kop87] used total functions to represent the useful information, and the
resulting measure has been called sophistication. The definition of sophistication is
based on process (monotonic) complexity defined by Schnorr. A function f : Σ∗ →
Σ∗ is monotonic if x ≤ y (x is a prefix of y) implies that f(x) ≤ f(y) for all x and
y. SΣ is a sample space consisting of all finite and infinite sequences over Σ.
Definition 6. Let U be the reference monotone machine. The monotone complexity
of x with respect to y, is defined as,
Km(x|y) = min
p
{|p| : U(p, y) = xω, ω ∈ SΣ}.
28 Luis Antunes and Andre´ Souto
Koppel [Kop87] defined a description of a finite or infinite binary string α as a
pair (p, d) such that p is a total, i.e., U(p, d) is defined for all d, p is a self-delimiting
program and U(p, d) ≤ α, i.e., U(p, d) is an initial segment of α. He also defined the
complexity of α by
H(α) = min{|p|+ |d| : (p, d) is a description of α}.
Definition 7 ([Kop87]). The c-sophistication of α ∈ Σ∞, is
sophc(α) = min
p
{|p| : exists d s.t. (p, d) is a description of α and |p|+|d| ≤ H(α)+c}
Note that Koppel’s notion of description of infinite sequences is not appropri-
ately defined. Also, as Koppel remark in [Kop88], it is not defined for every sequence
α. In order to avoid this problem Koppel defined a weak version of sophistication
based on “weak” compression programs for α. Antunes and Fortnow [AF01] later,
revisited the notion of sophistication and, using the plain Kolmogorov complexity,
adapted Koppel’s definition for finite sequences as:
Definition 8 ([AF01]). Let c be a constant, x ∈ Σn and U the universal reference
Turing machine. The c-sophistication of x is
sophc(x) = min
p
{
|p| : p is total and exists d s.t. U(p, d) = x and |p|+|d| ≤ C(x)+c
}
.
Remark 1. An important observation about sophistication is the fact that it is not
known rather if it is or not a robust measure. Indeed it is not known if slightly
variations on the parameter c influences largely the length of sophistication program.
The definitions of sophistication for infinite sequences introduced here use this no-
tion of sophistication for finite strings. We will need the following result, on the
existence of highly sophisticated finite strings.
Theorem 1 ([AF01]). Let c be a constant. There exists x ∈ Σn such that
sophc(x) > n− 2 log n− 2c.
2.3 Dimension
Hausdorff [Haus79] augmented Lebesgue measure theory with a theory of dimension.
It assigns to every subsetX of a given metric space a real number dim(X), called the
Hausdorff dimension of X. Lutz [Lut00] proved a gale characterization of Hausdorff
dimension. This characterization gives an exact relationship between the Hausdorff
dimension of a set X consisting of all infinite binary sequences, and the growth rates
achievable by martingales betting on the sequences in X. This gale characterization
of Hausdorff dimension was a breakthrough since it enabled the definition of effective
versions of Hausdorff dimension by imposing various computability and complexity
constraints on the gales.
Later Mayordomo [May02] showed that constructive Hausdorff dimension can
be fully characterized in terms of Kolmogorov complexity.
Theorem 2 ([May02]). For every sequence α,
dim(α) = lim inf
n→∞
K(α[1..n])
n
where dim(α) is the constructive Hausdorff dimension.
Sophisticated Infinite Sequences 29
Packing dimension was introduced independently by Tricot [Tic82] and Sullivan
[Sul84]. Later, Athreya et al. [AHLM07] showed how to characterize packing dimen-
sion in terms of gales, a dual of the gale characterization of the Hausdorff dimension.
By imposing computational and complexity constrains on the gales Athreya et al.
obtained a variety of effective strong dimensions that are exactly duals of the effec-
tive Hausdorff dimensions. In particular, it was proved the following characterization
in terms of algorithmic information theory that is the dual of the previous one.
Theorem 3 ([AHLM07]). For every sequence α,
Dim(α) = lim sup
n→∞
K(α[1..n])
n
where Dim(α) is the constructive packing dimension.
3 The Sophistication for Infinite Sequence Redefined
Based on the strong relationship between constructive Hausdorff dimension (respec-
tively packing dimension) and the lim sup (respectively lim inf) of the ratio between
the Kolmogorov complexity of the initial segment and its length, in this section we
introduce a new and clean approach to the sophistication of infinite sequences.
Definition 9. We define lower sophistication of a sequence α ∈ Σ∞ by
soph
c
(α) = lim inf
n
sophc(α[1..n])
n
and the upper sophistication by
sophc(α) = lim sup
n
sophc(α[1..n])
n
The first observation is that the lower and the upper sophistication of a sequence
are well defined. Notice that the lim inf and lim sup of well defined real numbers are
themselves well defined. Now we give some properties of the new measure and estab-
lish a connection to some known concepts, namely constructive Hausdorff dimension
and constructive Packing dimension.
Proposition 2. For all sequence α and constant c, soph
c
(α) ≤ dim(α).
Proof.
soph
c
(α) = lim inf
n
sophc(α[1..n])
n
≤ lim inf
n
C(α[1..n]) + c
n
≤ lim inf
n
C(α[1..n])
n
+ lim inf
n
c
n
≤ lim inf
n
K(α[1..n])
n
+ lim inf
n
c
n
= lim inf
n
K(α[1..n])
n
= dim(α)
Proposition 3. For all sequence α and constant c, sophc(α) ≤ Dim(α).
Proof.
sophc(α) = lim sup
n
sophc(α[1..n])
n
≤ lim sup
n
C(α[1..n]) + c
n
≤ lim sup
n
C(α[1..n])
n
+ lim sup
n
c
n
≤ lim sup
n
K(α[1..n])
n
+ lim sup
n
c
n
= lim sup
n
K(α[1..n])
n
= Dim(α)
30 Luis Antunes and Andre´ Souto
In the next proposition we show that the lower and upper sophistication of infinite
sequences is a non increasing function with c.
Proposition 4. If c > c′ then soph
c
(α) ≤ soph
c′
(α) and sophc(α) ≤ sophc′(α).
The proof of this result follows immediately from the fact that for any finite
string x, if c > c′ then sophc(x) ≤ sophc′(x).
We can, in fact, prove a sharper result for the upper sophistication. We show
that there are sequences for which the upper sophistication is strictly smaller than
the packing dimension.
Proposition 5. There exist sequences α such that sophc(α) < Dim(α) for some
constant c.
Proof. The idea is to use a sequence with high Kolmogorov complexity. Chaitin, in
[Cha66] and Martin-Lo¨f [ML71] observed that there exists α such that from some
n0 onwards K(α[1..n]) ≥ n− log n− log log n.1 Thus
Dim(α) = lim sup
n
K(α[1..n])
n
≥ lim sup
n
n− log n− log log n
n
= 1
On the other hand, it is known that infinitely many initial segments of α satisfy
C(α[1..n]) = n−c′ for some constant c′. Hence for some appropriate constant c, that
only depends on c′ sophc(α[1..n]) = O(1), for infinitely many n, since the program p
that prints α[1..n] when α[1..n] is given as data satisfies |p|+ |α[1..n]| ≤ C(α[1..n])+ c.
So,
sophc(α) = lim sup
n
sophc(α[1..n])
n
≤ lim sup
n
O(1)
n
= 0.
4 On the Existence of Highly Sophisticated Sequences
In this section we investigate the existence of highly sophisticated sequences. In
particular, we show that the set of sequences with upper sophistication different
from 0 is dense and the set of sequences with lower sophistication different from 0
is sparse. To formally present these results, we consider the standard metric in the
Cantor space Σ∞ and use a known result for complete metric spaces.
Definition 10. In the Cantor set Σ∞, given α and β in Σ∞, the standard metric
is defined by:
d(α, β) = max
i
{2−i : αi 6= βi}
It is well known that (Σ∞, d) is a complete metric space, i.e., is a metric space in
which every Cauchy sequence converges.
Remark 2. The less the distance between α and β, the bigger the initial segment
common to α and β.
Consider the following set:
Vi,e = {α ∈ Σ∞ : (∀n ≥ i)sophc(α[1:n]) ≤ ne}
where c and e are fixed constants and 0 < e < 1 is a rational number. Vi,e is the set
of sequences that from its ith bit their initial segments are not highly sophisticated.
Then:
1 In fact, almost all sequences α with respect to the binary measure have this property,
since
P
n∈N 2
− logn−log logn converges.
Sophisticated Infinite Sequences 31
1. Vi,e ⊂ Vi+1,e.
If α ∈ Vi,e then for all n ≥ i, sophc(α[1:n]) ≤ ne. In particular, for all n ≥ i+ 1,
we have that sophc(α[1:n]) ≤ ne. So α ∈ Vi+1,e.
2. Vi,e is non empty.
For example the sequence such that all bits are equal to 0 has low sophistication
since it has low Kolmogorov complexity.
3. For all e and sufficiently large i, Vi,e 6= Σ∞.
Set n > i+1 such that n−2 log n−2c > ne. Notice that lim
n
n− 2 log n− 2c
ne
=∞
and thus such n exists. Consider x ∈ Σn that satisfies sophc(x) ≥ n− 2 log n−
2c. This string exists by Theorem 1. Then the sequence α = x000... satisfies
sophc(α[1:n]) ≥ n− 2 log n− 2c > ne and thus α 6∈ Vi,e.
4. All sets Vi,e are closed subsets of Σ∞.
To prove this fact we show that Σ∞ − Vi,e are open subsets of (Σ∞, d). This
can be done by showing that given α ∈ Σ∞ − Vi,e there exists ε > 0 such that
B(α, ε) = {β ∈ Σ∞ : d(α, β) < ε} ⊂ Σ∞ − Vi,e.
If α ∈ Σ∞−Vi,e then there exists n such that sophc(α[1:n]) ≥ ne. Set ε = 2n−1.
Then, if d(α, β) < ε it implies that for all i ≤ n, αi = βi. So sophc(β[1:n]) =
sophc(α[1:n]) ≥ ne, which proves that β ∈ Σ∞ − Vi,e.
We now present a known result for complete metric spaces that gives a condition
to prove that a certain sequence of subsets of a metric space have dense intersection.
Theorem 4 (Baire’s theorem). Let (X,m) be a complete metric space and let
(An)n∈N be a sequence of open dense subsets of X. Then
⋂
n∈N
An is dense.
So if we prove that Σ∞ − Vi,e are dense then we prove that the set of all highly
sophisticated sequences is dense inΣ∞ since
⋂
i∈N
Σ∞ − Vi,1−1/i = Σ∞ −
⋃
i∈N
Vi,1−1/i.
Notice that α ∈
⋃
i∈N
Vi,1−1/i if an only if α satisfies sophc(α) = 0.
To prove that each Σ∞ − Vi,e is dense it is sufficient to show that given ε > 0
and α ∈ Vi,e there exists a sequence β ∈ Σ∞ − Vi,e such that d(α, β) < ε.
Intuitively this fact is true since we can consider the first bits of α (to insure that
d(α, β) < ε) and construct a sophisticated string with that prefix of a reasonable
size.
Proposition 6. Each set Σ∞ − Vi,e is dense.
Proof. Let α be an element in Vi,e and ε > 0 a real number. We construct β as
follows:
Let i0 be the index such that 2−i0 ≤ ε/2. Set βi = αi for all i ≤ i0. With this
condition we guarantee that d(α, β) < ε.
Let n be sufficiently large satisfying n − 2 log n − 6i0 − 2kc > (n + i0)e and
n > i. Consider x ∈ Σn such that soph3i0+kc(x) > n − 2 log n − 6i0 − 2kc where k
is also a positive constant. Notice that since i0 is a constant value depending only
on ε, this string x always exists by theorem 1. We stress that y = α[1:i0]x has high
sophistication.
Let p be a program corresponding to sophc(y). Then there exists data f such
that |p|+ |f | ≤ C(y) + c. Consider the following program q:
Algorithm 5 Given some data 〈1i0if ′〉:
1. If the data does not have this structure, stop outputting  the empty string.
Otherwise
2. Run U with p and f ′ to produce y′.
32 Luis Antunes and Andre´ Souto
3. Print all the string except the first i bits.
Since p is total it follows that q is also total. Notice that if f ′ = f and i = i0,
U(q, 〈1i0if ′〉) = x. Notice also that:
|q|+ |〈1i00i0f〉| ≤ |p|+ |f |+ 2i0 +O(1) = sophc(y) + 2i0 +O(1)
≤ C(y) + 2i0 +O(1)
≤ C(α[1:i0]) + C(x) + 2i0 +O(1) ≤ C(x) + 3i0 + kc
So |q| is an upper bound of soph3i0+kc(x) and
sophc(y) ≥ soph3i0+kc(x) > n− 2 log n− 6i0 − 2kc > (n+ i0)e = |y|e
So let β be the sequence β = α[1:i0]x0000 · · ·. Since n > i the above discussion
means that β 6∈ Vi,e.
Thus, using Theorem 4 and Proposition 6 we have:
Theorem 6. The set of sequences α such that for some c sophc(α) 6= 0 is dense.
In what follows, we prove that the set of sequences with lower sophistication
different from 0 is sparse. Consider now the following set:
Ai,e = {α ∈ Σ∞ : ∃n > i such that sophc(α[1..n]) < ne}
where c and e are constants such that 0 < e < 1 and is a rational number.
Ai,e is the set of sequences for which there exists a value n > i such that the
sophistication of α[1..n] is less than ne.
Proposition 7. Ai,e is open.
Proof. The idea is to use the same argument that proves that Σ∞ − Vi,e is open.
Let α be a sequence in Ai,e. Let n be the index such that sophc(α[1..n]) < ne. Then
taking ε = 2−n it follows that if d(α, β) < ε then βi = αi for all i ≤ n. So β ∈ Ai,e.
Proposition 8. Ai,e is dense.
Proof. Let α ∈ Σ∞ − Ai,e and ε > 0. Consider β defined by: βi = αi for all
i ≤ − log ε and βi = 0 otherwise. Then d(α, β) < ε and it is clear that β ∈ Ai,e.
So using again Baire’s theorem we conclude the following:
Theorem 7. The set of sequences α such that for some constant c, soph
c
(α) 6= 0
is sparse.
Proof. By Baire’s theorem
⋂
i∈N
Ai,1−1/i is dense. So if α ∈
⋂
i∈N
Ai,1−1/i then there ex-
ists a sequence of indexes (in)n∈N such that sophc(α[1..in]) ≤ (in)e. So sophc(α) = 0.
Notice that the sequence (in)n∈N can be constructed inductively.
5 Sophistication vs Depth of Infinite Sequences
Bennett [Ben88] formally defined the s-significant logical depth of an object x as the
time required by a standard universal Turing machine to generate x by a program
that is no more than s bits longer than the shortest descriptions of x. A deep string
x should take a lot of effort to construct from its short description. Incompress-
ible strings are trivially constructible from their shortest description, and therefore
computationally shallow.
Sophisticated Infinite Sequences 33
Koppel [Kop87], claimed that sophistication and logical depth are equivalent,
for all infinite sequences. However the proof uses a different definition of logical
depth imposing totality in the functions defining it.
The claimed equivalence between sophistication and logical depth would be, in
fact, an unexpected result, as sophistication measures program length that is upper
bounded by the length of the string and logical depth measures running time that
can grow unbounded.
In [AF01] the authors proved that computational depth and sophistication are
distinct measures of complexity for finite strings contradicting Koppel’s intuition. In
this section, we reenforce the distinctness of these two measures for infinite sequence
by proving the existence of sequences that are deep but not very sophisticated.
Definition 11. The dimensional depth of a sequence α is defined as
depthtdim(α) = lim inf
n→∞
δ(α[1..n]/2−K
t(α[1..n]))
n
.
where δ is the random deficiency defined by δ(x | µ) =
⌊
log
2−K(x)
µ(x)
⌋
.
The next example shows that the diagonal of the Halting Problem is deep but
not very sophisticated.
Example 1. Let H be the diagonal of Halting problem i.e., H = {i : Mi(i) halts}.
From Barzdini’s Lemma (see [LV97]) we know that for all n
C(χH [1..n]) ≤ C(n) + C(χH [1..n]|n) ≤ 2 log n.
So
soph
c
(χH) ≤ sophc(χH) = lim sup
n
sophc(χH [1..n])
n
≤ lim sup
n
2 log n
n
= 0
On the other hand we know that:
depthtdim(χ) = lim inf
n→∞
δ(χH [1..n]/2
−Kt(χH [1..n]))
n
= lim inf
n→∞
log(2−K(χH [1..n])/2−K
t(χ[1..n]))
n
= lim inf
n→∞
Kt(χH [1..n])−K(χH [1..n])
n
= lim inf
n→∞
deptht(χH [1..n])
n
= 1
The last inequality is due to the fact that χH [1..n] is very deep, i.e., depth
t(χH [1..n])
≈ n.
Conclusions
In this paper we proposed two definitions for sophistication for infinite sequences.
The major improvement of this work to the existent one is the fact that sophistica-
tion is a concept well defined for all sequences. The most important result proved
here concerns the existence of highly sophisticated sequences. In fact, we proved
that if sophistication is defined with lim sup then the set of sequences that have
sophistication different from 0 is dense and if it is defined with lim inf this set is
rare. The last result stating that sophistication and depth are different complexity
measures strengthens a similar result already proved in [AF01] for finite strings.
34 Luis Antunes and Andre´ Souto
Acknowledgments
We thank to the CiE 2008 anonymous reviewers for pointing out some problems in
the proofs of some results.
References
[AF01] L. Antunes and L. Fortnow, “Sophistication Revisited”, Proceedings of the 30th
International Colloquium on Automata, Languages and Programming, Lecture Notes in
Computer Science, 2719:267-277, Springer, 2003.
[AFMV06] L. Antunes, L. Fortnow, D. van Melkebeek and N. V. Vinodchandran, “Com-
putational depth: concept and applications”, Theor. Comput. Sci., 354(3): 391-404, 2006.
[AMVS07] L. Antunes, A. Matos, A. Souto and P. Vita´nyi, “Depth as Randomness Defi-
ciency”, submitted to Theory of Computing Systems.
[AHLM07] K. Athreya, J. Hitchcock, J. Lutz and E. Mayordomo, “Effective Strong Di-
mension in Algorithmic Information and Computational Complexity”, SIAM Journal on
Computing, 37(3):671-705, 2007
[AK91] H. Atlan and M. Koppel, An almost machine-independent theory of program-length
complexity, sophistication, and induction, in Inf. Sci. 56(1-3):23-33, Elsevier Science
Inc., 1991
[Ben88] C. H. Bennett. Logical depth and physical complexity. In R. Herken, editor, The
Universal Turing Machine: A Half-Century Survey, pages 227–257. Oxford University
Press, 1988.
[Cha66] G. Chaitin. On the length of programs for computing finite binary sequences.
Journal of the ACM, 13:4(1966), 145-149.
[Cov85] T. M. Cover, Kolmogorov Complexity, Data Compression and Inference, Chapter
in The Impact of Processing Techniques on Communications. Series E: Applied Sciences
(91), Martinus Nijhoff Publishers, 1985
[Gacs88] P. Gacs, “Lecture notes on descriptional complexity and randomness”, Tech.
report, Boston University, Computer Science Dept., Boston, MA 02215, 1988.
[GTV01] P. Ga´cs, John Tromp and P. Vita´nyi. Algorithmic Statistics. In IEEE Trans.
Inform. Theory, 47:6, 2443-2463, 2001
[Haus79] F. Hausdorff, “Dimension und a¨ußeres Maß”, in Math. Ann., 79:157–179, 1919
[Kol65] A. N. Kolmogorov. Three approaches to the quantitative definition of information.
Problemy Inform. Transmission, 1(1): 1-7,1965
[Kop87] M. Koppel, “Complexity, Depth, and Sophistication”, in Complex Systems 1,
pages = 1087-1091, 1987
[Kop88] M. Koppel, “Structure”, in “The universal Turing machine: a half-century
survey”, pages = 403–419, Springer-Verlag New York, Inc., 1988
[LV97] M. Li and P. Vita´nyi, “An introduction to Kolmogorov complexity and its applica-
tions”, Springer Verlag, 2nd edition, 1997.
[Lut00] J. H. Lutz, “Dimension in complexity classes”, Proceedings of the 15th IEEE
Conference of Computational Complexity, IEEE Computer Society Press, 2000.
[Lut02] J. H. Lutz, “The dimensions of individual strings and sequences”, Technical
Report cs. CC/0203017, ACM Computing Research Repository, 2002.
[ML71] P. Martin-Lo¨f, “Complexity oscillations in infinite binary sequences”, Zeitschrift
Wahrscheinlichkeitstheory und Verwandte Gabiete, 19, pages=225-230, 1971.
[May02] E. Mayordromo, “A Kolmogorov complexity characterization of constructive
Hausdorff dimension”. Information Processing Letters, 84:1-3, 2002.
[Sol64] R. Solomonoff. “A Formal Theory of Inductive Inference, Part I”, Information
and Control, 7(4):1-22, 1964
[Sul84] D. Sullivan, “Entropy, Hausdorff measures old and new, and limit sets of geomet-
rically finite Kleinian groups”, in Acta Math., 153:259–277, 1984
[Tic82] C. Tricot, “Two definitions of fractional dimension” in Math. Proc. Cambridge
Philos. Soc., 91:57–74, 1982.
[V’y99] V.V. V’yugin. “Algorithmic complexity and stochastic properties of finite binary
sequences.” The Computer Journal, 42(4):294-317, 1999.
On Detecting Deadlock in the pi-Calculus
Tiago Azevedo1, Mario R. F. Benevides2, Fa´bio Protti3, and Marcelo Sihman1
1 COPPE/PESC - Universidade Federal do Rio de Janeiro
Caixa Postal 68511 - 21945-970 - Rio de Janeiro - RJ - Brazil
2 IM and COPPE/PESC - Universidade Federal do Rio de Janeiro
Caixa Postal 68511 - 21945-970 - Rio de Janeiro - RJ - Brazil
3 IM and NCE - Universidade Federal do Rio de Janeiro
Caixa Postal 2324 - 20001-970 - Rio de Janeiro - RJ - Brazil
tiagoazevedo@cos.ufrj.br, mario@cos.ufrj.br
fabiop@nce.ufrj.br, marcelosihman@ufrj.br
Abstract. In this work we deal with the notion of deadlock in the asyn-
chronous polyadic pi-calculus. Based on the proposed definition, we show
that detecting deadlock in the asynchronous polyadic pi-calculus is usu-
ally an undecidable problem. However, when restricting the language
by forbidding replicating input prefixed processes, the detection turns
out to be decidable, and a polynomial deadlock detection algorithm is
presented.
Key words: Concurrency; Program Correctness; Programming Cal-
culi; Program Specification.
1 Introduction
When specifying distributed systems and concurrent programs, one crucial ques-
tion is to ensure absence of deadlock and run-time errors. However, standard
mechanisms for detecting automatically such situations can hardly be found in
general, since those notions are usually difficult to be dealt with.
In a previous work by Vasconcelos and Ravara [9], the notion of run-time
(communication) errors in the asynchronous polyadic pi-calculus was shown to
be undecidable. Their result supports the use of type systems, a static method of
analysis in preventing run-time errors. In many cases, when a language enjoys the
subject reduction property, it is possible to prove type-safety, and this implies the
absence of run-time errors. Therefore, the method is correct, but not complete
as not all well-behaved programs are well-typed. However, the undecidability of
the run-time error notion does not allow us to do better.
Although this undecidability result is not surprising, its proof uses and ex-
tends non-trivial results from the encoding of the λ-calculus into the asyn-
chronous pi-calculus described in [6, 8](see Section 2.1), and adapts a technique
to transfer undecidability results: it reduces the problem to a known undecidable
property in the source language, and prove that if it is decidable in the target
language, it would also be in the source language, attaining a contradiction.
The motivation of this work is to extend the above result to the notion of
deadlock. We believe that it is worth obtaining formal proofs of undecidability
36 Tiago Azevedo, Mario R. F. Benevides, Fa´bio Protti, and Marcelo Sihman
for such situations in the pi-calculus, since they have remained up to now as
“folklore” results. The use of the asynchronous pi-calculus as the setting of this
work was shown to be an easier way to give continuity to the ideas inspiring the
cited paper [9]. This choice is justified by the fact that if these results hold in the
asynchronous pi-calculus, then they must also hold in superlanguages (e.g. the
synchronous pi-calculus) – while the inverse implication may not be necessarily
true. It turns out more useful and simpler to prove undecidability results using
this smaller language, which is the basis of several implementations.
The definition of deadlock proposed here (Section 3)intends to capture the
potentiality a process possesses of reaching this situation after performing an
empty experiment (a sequence of silent actions, where external agents observe
nothing). A similar definition of deadlock can be found in [10–14], where the
deadlock state is defined through the concept of annotations.
The undecidability proof presented here (Section 4) follows the strategy
employed in [9]: the problem of deciding whether a lambda term has a normal
form [3] is reduced to the problem of deciding whether a process is potentially
capable of reaching a deadlock state. This is done by defining a computable
function f from λ-terms into processes of the pi-calculus such that f(M) is a
deadlocking process if and only if the λ-term M is convergent. The definition
of f embodies the above-mentioned encoding of the lazy λ-calculus into the
asynchronous pi-calculus in [6, 8].
When restricting the asynchronous polyadic pi-calculus by forbidding repli-
cating input prefixed process, deadlock detection turns out to be decidable. In
section 5, a polynomial-time deadlock detection algorithm is presented for this
case.
2 Background
The concepts and definitions of the asynchronous polyadic pi-calculus are used
here as usual [5, 4]. Below, we briefly present some definitions. Small letters
a, b, p, q, x, y, . . . are names, v˜ is a sequence of names, and x˜ is a sequence of
pairwise distinct names.
The set Π of processes of the asynchronous polyadic pi-calculus is given by
the following grammar: P ::= a¯[v˜] | a(x˜).P | P |Q | νxP | !a(x˜).P | 0
Here, ‘|’ is the operator for parallel composition of processes; ‘ν’ restricts
the scope of a name to a process; and ‘!a(x˜).P ’ is a replicating input prefixed
process. Prefixed operations (α. and νa) bind more tightly than composition.
The set of free names in a process P is denoted by fn(P ). Alpha-conversion
is denoted by ≡α. If v˜ and x˜ are sequences with the same length, the process
P [v˜/x˜] is obtained by substituting the names in v˜ for every free occurrence of
the corresponding name in x˜.
The structural congruence, written ≡, is determined by the following equa-
tions: P ≡ Q if P ≡α Q, P |0 ≡ P, P |Q ≡ Q|P, (P |Q)|R ≡ P |(Q|R),
νx0 ≡ 0, νxyP ≡ νyxP, νxP |Q ≡ νx(P |Q) if x /∈ fn(Q), !P ≡ !P | P .
On Detecting Deadlock in the pi-Calculus 37
The set of action labels is given by the following grammar, where {x˜} ⊆
{v˜}\{a}:
α ::= τ | a(v˜) | νx˜a¯[v˜]
The symbol τ denotes a silent action, an internal communication within a
process. The remaining actions are observable actions (external communications
between processes): the input action a(v˜) denotes the reception on a of the
sequence of names v˜, and the output action νx˜a¯[v˜] denotes the emission to a of the
sequence of names v˜, where some of them are bound. The sets fn(α) and bn(α)
consist of the free names and the bound names in an action α, respectively. Given
an observable action α = a(v˜) or α = νx˜a¯[v˜], the name a is called the subject
of the action and is denoted by subj(α). The asynchronous transition relation
contains exactly those transitions which can be inferred from the following rules:
(IN) a(x˜).P
a(v˜)−→ P [v˜/x˜]
(OUT ) a¯[v˜]
a¯[v˜]−→ 0
(COM) a(x˜).P | a¯[v˜] τ−→ P [v˜/x˜]
(PAR) P
α−→ Q
P | R α−→ Q | R (bn(α) ∩ fn(R) = Ø)
(RES) P
α−→ Q
νxP
α−→ νxQ (x /∈ bn(α) ∪ fn(α))
(OPEN) P
νx˜a¯[v˜]
−−−→ Q
νxP
νxx˜a¯[v˜]
−−−→ Q
(a /∈ {x, x˜})
(STRUCT ) P
α−→ P ′
Q
α−→ Q′ if P ≡ Q and P
′ ≡ Q′
Note that there is no specific rule for replication. Its required behavior is
given by the last rule of structural congruence.
The process a(x˜).P and a¯[v˜] in rule (COM) are called component processes.
The symbol =⇒ denotes the reflexive and transitive closure of τ−→, and α=⇒
denotes =⇒ α−→=⇒. Let us simply write P α−→ (resp. P α=⇒) to mean that there
exists Q such that P α−→ Q (resp. P α=⇒ Q).
Let s be a sequence s = α1, α2, . . . , αn of action labels. An experiment
s=⇒
is defined in the following way:
s=⇒ def= =⇒ α1−→=⇒ α2−→=⇒ . . . =⇒ αn−→=⇒ .
Again, write P s=⇒ to mean that there exists Q such that P s=⇒ Q.
The empty experiment is defined for n = 0; in this case, s=⇒ is the same as
=⇒, indicating that silent actions may occur even if we observe nothing.
38 Tiago Azevedo, Mario R. F. Benevides, Fa´bio Protti, and Marcelo Sihman
When no silent actions can occur within a process P , we shall call P stable
(cfr. [7], p.35). Stability implies that P needs an observable action to proceed
any further. Processes that have already finished all their possible computations
and are reduced to 0 are also stable.
A relation R ⊆ Π × Π is called an expansion(we refer the reader to [8]) if
PRQ implies:
1. if P α−→ P ′ then there exists Q′ such that Q α=⇒ Q′ and P ′RQ′;
2. if Q α−→ Q′ and α is an observable action then there exists P ′ such that
P
α−→ P ′ and P ′RQ′;
3. if Q τ−→ Q′ then either PRQ′ or there exists P ′ such that P τ−→ P ′ and
P ′RQ′.
We write P <∼ Q if PRQ for some expansion R. Relation <∼ is a preorder
and a congruence.
2.1 Encoding the lazy λ-calculus into the asynchronous polyadic
pi-calculus.
The transfer of results from the λ-calculus to the asynchronous polyadic pi-
calculus is achieved by using the encoding of the lazy λ-calculus described below.
Definition 1. [2, 3, 8] The set Λ of λ-terms is defined by the following grammar,
where x ranges over the set of λ-calculus variables:
M ::= x | λx.M | MN
Free variables, closed terms, substitution, alpha-conversion etc. are defined as
usual. The reduction relation is −→, and the reflexive and transitive closure of
−→ is =⇒. We write M ↓ if M is convergent, and M ↑ otherwise. In the lazy
λ-calculus [1], the redex is always at the left extreme of a term.
The proofs in Section 5 need the following tools:
Theorem 1. [3] The predicate ‘M ↓’ is undecidable.
Definition 2. [6, 8] The encoding of the lazy λ-calculus into the asynchronous
polyadic pi-calculus is given by the following rules:
[[λx.M ]]p
def
= p(x, q).[[M ]]q
[[x]]p
def
= x¯[p]
[[MN ]]p
def
= νuv([[M ]]u | u¯[v, p] | !v(q).[[N ]]q)
Lemma 1. [8, 9] The following properties of the above encoding are valid:
(a) If M =⇒ λx.N then [[M ]]p p(x,q)=⇒ >∼ [[N ]]q
(b) If M =⇒ x then [[M ]]p x¯[p]=⇒ >∼ 0
(c) M ↑ if and only if for no observable α [[M ]]p α=⇒
On Detecting Deadlock in the pi-Calculus 39
3 Notion of deadlock
In this section we define the set DEAD of processes which are understood to be
potentially deadlocked.
Informally, P is capable of deadlock if it may reach a situation in which the
computation cannot evolve internally. The first definition of this section says
that such P is a deadlocking process if it reduces to a restricted composition of
mutually non-communicating stable processes P1, P2, . . . , Pn.
We use the following notation: for a process P , let
obs(P ) = { subj(α) | α is observable and P α−→}.
In addition, say that two observable actions α, µ are complementary if subj(α) =
subj(µ) and either α is an input action and µ is an output action, or α is an
output action and µ is an input action.
Definition 3. The set DEAD of processes which are capable of deadlock is
defined in the following way:
DEAD = {P | P =⇒ νx˜(P1 | P2 | . . . | Pn),
where: (i) Pj is stable, for all j = 1, . . . , n;
(ii) for any a and for all j = 1, . . . , n, if a ∈ obs(Pj) then a occurs in x˜;
(iii) for all j, k (j 6= k), there are no complementary actions α, µ such
that Pj
α−→ and Pk µ−→;
(iv) obs(Pi) 6= Ø for some i ∈ {1, . . . , n}. }
Let us discuss the elements of the above definition.
– Condition (i) says that every Pj needs an observable action (with respect
to itself) to proceed. (Note that if some silent actions can still occur within
some Pj , we can not properly speak of a ’deadlocking state’.)
– Condition (ii) means that the process νx˜(P1 | P2 | . . . | Pn) cannot establish
communication with the external world. By excluding this condition, it would
be possible to continue the computation via non-restricted names.
– Condition (iii) ensures that P1, . . . , Pn are mutually non-communicating.
– Condition (iv) excludes from the definition of DEAD those processes that
terminate for all possible computations, e.g. Q1
def
= 0. The case n = 1 covers
examples such asQ2
def
= νa(a().0), which is considered a deadlocking process.
4 Undecidability proof
Now we are ready to present the undecidability result.
Theorem 2. The predicate ‘P ∈ DEAD’ is undecidable.
40 Tiago Azevedo, Mario R. F. Benevides, Fa´bio Protti, and Marcelo Sihman
Proof. Let us show that if the predicate ‘P ∈ DEAD’ is decidable, then the
predicate ‘M ↓’ is decidable, leading to a contradiction.
First, let f : Λ −→ Π be such that
f(M)
def
= νpx1x2 . . . xn([[M ]]p)
where p is a new name and x1, x2, . . . , xn are the free variables occurring in M .
The function f is clearly computable by the encoding of the lazy λ-calculus into
the asynchronous polyadic pi-calculus.
Now, given M ∈ Λ, let us show that f(M) ∈ DEAD if and only if M ↓.
Assume first that f(M) ∈ DEAD. Under these assumption, observe that
process [[M ]]p, after zero or more silent actions, must reach a point in which
it depends on an observable action to proceed further, since condition (iv) in
the definition of DEAD must be satisfied. This means that [[M ]]p
α=⇒, for some
observable α. Therefore, by Lemma 1(c), M ↓.
On the other hand, assume that f(M) 6∈ DEAD. Since f(M) trivially sat-
isfies conditions (ii) and (iii) in the definition of DEAD, it is mandatory that
process [[M ]]p keeps continuously performing internal computations, in order to
invalidate either condition (i) or condition (iv). This means that [[M ]]p does not
perform any observable action after any sequence of silent actions. Therefore, by
Lemma 1(c) again, M ↑.
To conclude the proof, assume that ‘P ∈ DEAD’ is decidable, that is, given
P ∈ Π, there is a finite time algorithm that decides whether P belongs to DEAD
or not. Then the following algorithm decides whether a λ-term M is convergent
or not: compute f(M) and verify whether f(M) ∈ DEAD; if f(M) ∈ DEAD
then the answer is ‘yes’, otherwise the answer is ‘no’. This contradicts the result
of Theorem 1.
5 An algorithm for deadlock detection in the restricted
pi-calculus
Now we define the new setΠ of processes of the restricted asynchronous polyadic
pi-calculus, given by the following grammar:
P ::= νxQ
Q ::= a¯[v˜] | a(x˜).Q | Q|Q′ | 0
In other words, there is a single scope restriction acting on a parallel composition
of processes (also called component processes). This simplification is reasonable
since it is easy to see that every process containing no replication can be rewritten
to the above form, via alpha-conversions and structural congruences.
The algorithm for deadlock detection in the restricted pi-calculus receives
as input a process and then simulates it, step by step, testing all the execution
paths. In case there is some possibility of deadlock, it is detected and the location
On Detecting Deadlock in the pi-Calculus 41
of the deadlock is determined. The algorithm uses the fact that the deadlock state
of a process can be identified by a knot in the waiting graph (see these definitions
below). In addition, the algorithm shows how to locate it.
A description of the algorithm is given below.
Algorithm 1 Deadlock Detection
1: /* External call, P is the input process*/
2: V (G)← {P} /* Vertex set of Global State Graph*/
3: E(G)← {} /* Edge set of Global State Graph*/
4: NewGlobalState(P)
5: procedure NewGlobalState(P )
6: for all A occurring in P do
7: Build GA,P
8: /*GA,P is the waiting graph for A in P*/
9: if GA,P does not contain knot then
10: for all possible handshake h of A in P do
11: Let Ph be the new global state
12: if Ph 6∈ N then
13: N ← N S{Ph}
14: E ← ES{P, Ph}
15: NewGlobalState(Ph)
16: end if
17: if (P, Ph) 6∈ E then
18: E ← ES{P, Ph}
19: end if
20: end for
21: else
22: if current action of A is restricted AND
23: not passed as a parameter by another process then
24: Deadlock!
25: end if
26: end if
27: end for
28: end procedure
We use the term handshake to mean a communication over some action x
between two component processes A and B, according to
rule (COM).
A global state is the state reached by the component processes after a se-
quence of handshakes starting at the input process P (which is the initial global
state). Each global state represents a single situation, i.e., there cannot exist two
identical global states.
The global state graph G = (N,E) is the (directed) graph where each node in
N represents a global state, two nodes P, P ′ ∈ N being connected by a directed
42 Tiago Azevedo, Mario R. F. Benevides, Fa´bio Protti, and Marcelo Sihman
edge (P, P ′) ∈ E labelled h whenever global state P ′ is reached from P after
executing the handshake h. In this case, we denote P ′ by Ph.
The waiting graph of a component process A in P is the (directed) graph
GA,P = (NP , EA,P ), where NP is the set of component processes occurring in P,
and EA,P is a set of labelled edges. If an edge (A,B) ∈ EA,P is labelled x, then
it means that process A is waiting for a handshake over action x which can be
provided by process B.
A knot in a directed graph G is a strong connected component C of G such
that either (i) C consists of a single node and is equal to G or (ii) C contains
at least two nodes and no directed edge leaves it.
A scope extrusion occurs when n process A contains a private action x shared
with a process B, and intends to send this action (as a parameter) to another
process C. When this action is exported to C, the scope of the restriction is
extended to C, characterizing the scope extrusion.
Now we will describe the execution of the algorithm. First, for every compo-
nent process A in global state P , we build GA,P . Next, we check whether P is
in deadlock or not. Each component process is tested separately, each one with
its corresponding waiting graph.
To build GA,P (line 7 of the algorithm), we look for the complement of the
current action x of A in other component process (unification algorithm), and
for each complementary action found in a component process B we add to EA,P
a directed edge (A,B) (if x is an input action) or (B,A) (if x is an output
action), labelled with x. If the complement of x is the current action in B, then
no other edge is added from/to this node, because this action will be the next
to execute. Otherwise, we recursively repeat the search for complements of the
current action in B, adding labelled edges to GA,P .The construction of GA,P
finishes when no more edges can be added in any of the nodes, meaning that all
the paths (sequences of edges that are incrementally added) have ended. Note
that some paths may have turned out to be cycles.
After constructing GA,P , our goal is to detect whether A will succeed to
communicate its current action with another process in the sequel of the sim-
ulation, i.e., if A is in deadlock or not. This is equivalent to checking whether
GA,P contains a knot (line 8 of the algorithm). For this purpose we use a knot
detection algorithm, which computes strongly connected components of GA,P .
In case that no knot is found in GA,P , the simulation continues from the
current global state P . For each possible handshake h between A and another
component process in P , we add a new global state Ph to N (if Ph does not
exist yet) and an edge (P, Ph) to E (if it has not been added yet). See lines
9 − 19 of the algorithm. If Ph is a pre-existing global state, the simulation of
this path in G finishes, because Ph has already generated its associated simula-
tion branches; otherwise, the simulation continues until we get to the first case
(coming back to a pre-existing global state) or until a deadlock is detected (lines
21− 23). The information on which handshakes can be executed in a step, from
the current global state to the next one, is obtained from the waiting graphs of
the component processes.
On Detecting Deadlock in the pi-Calculus 43
In case that GA,P contains a knot C, we have to check whether C cannot be
repaired by a scope extrusion (lines 21− 22). If so, a deadlock is detected. (line
23)
5.1 Complexity of the algorithm
In this section we calculate the complexity of the deadlock detection algorithm.
The number of process is denoted by n, and the number of actions in the process
by m.
The time needed to build the waiting graph for some process is in the worst
case O(n) for creating a vertex to each process, and O(n2) to create all edges,
because each of these edges always leaves a vertex representing a process and
reaches another vertex. Thus the overall complexity is O(n2).
To search for complementary ports, we use the unification algorithm. This
algorithm has a linear time complexity on the size of the input process, thus the
complexity of this task takes O(n+m) time.
The complexity to determine whether the waiting graph contains a knot is
O(n2) (linear on the size of the waiting graph), via a depth-first search.
Finally as we build a waiting graph for each process in each created global
state, in the worst case we spend O(n) time (processes) times O(n2) (waiting
graphs), which gives an O(n3) time. Moreover, we spend O(n3) time to check
whether these waiting graphs contain knots, and in the affirmative case, we
search for complementary ports in O(n+m) time. As we have O(m) handshakes
(because there are m actions) and therefore O(m) global states (because there is
a recursive call to each global state), the complexity for the deadlock detection
algorithm wis O(m(n3 + n3 +m+ n)) = O(m(2n3 + n+m)) = O(n3m+m2)).
5.2 The CCS Case
In the CCS case, processes cannot exchange actions with other process. The
algorithm for deadlock detection is thus simpler, because we can determine a
priori which processes can execute a handshake, preventing the search for com-
plementary ports; moreover we only build waiting graphs for restricted process.
Thus the complexity for the CCS case is O(m(n3+n3)) = O(2mn3) = O(mn3).
References
1. S. Abramsky. The lazy lambda calculus. Research Topics in Functional Program-
ming, pp. 65–116. Addison-Wesley, 1989.
2. H. Barendregt. The Lambda Calculus: Its Syntax and Semantics. Studies in Logic,
vol. 103. North Holland, 1984. Revised edition.
3. J. R. Hindley and J. P. Seldin. Introduction to Combinators and λ-Calculus. Cam-
bridge University Press, 1986, p. 63.
4. K. Honda and M. Tokoro. An object calculus for asynchronous communication. 5th
European Conference on Object-Oriented Programming, LNCS 512 (1991) 141–162.
Springer-Verlag.
44 Tiago Azevedo, Mario R. F. Benevides, Fa´bio Protti, and Marcelo Sihman
5. R. Milner. The polyadic pi-calculus: a tutorial. In F. L. Bauer, W. Brauer, and
H. Schwichtenberg, eds. Logic and Algebra of Specification, Springer-Verlag, 1993.
6. R. Milner. Functions as processes. Journal of Mathematical Structures in Computer
Science 2(2) (1992) 119–141.
7. R. Milner. Communicating and Mobile Systems: the pi-Calculus. Cambridge Uni-
versity Press, 1999.
8. D. Sangiorgi and D. Walker. The Pi-Calculus: A Theory of Mobile Processes.
Cambridge University Press, 2001.
9. V. T. Vasconcelos and A. Ravara. Communication errors in the pi-calculus are
undecidable. Information Processing Letters 71(5-6) (1999), 229–233.
10. Naoki Kobayashi A New Type System for Deadlock-Free Processes CONCUR
233-247, Springer, Lecture Notes in Computer Science, 2006
11. Naoki Kobayashi Type-based information flow analysis for the pi-calculus Acta
Informatica, Vol. 42, No 4-5 pp. 291-347, 2005
12. Naoki Kobayashi and Shin Saito and Eijiro Sumii An Implicitly-Typed Deadlock-
Free Process Calculus Lecture Notes in Computer Science, 1877, pp. 489–??,2000
13. Naoki Kobayashi A Partially Deadlock-Free Typed ProcessCalculus ACM Trans-
actions on Programming Languages and Systems Volume 20, pp. 436–482, 1998
14. Eijoro Kobayashi and Naoki Kobayashi A Generalized Deadlock-Free Process Cal-
culus HLCL, 1998
Algorithms for Analytic Functions and
Applications to Toeplitz Operators
E. J. Beggs and A. Gerber
Mathematics Department, Swansea University, Singleton Park,
Swansea SA2 8PP, Wales
E.J.Beggs@Swansea.ac.uk, A.Gerber@Swansea.ac.uk
Abstract. In this paper we present two algorithms for analytic func-
tions. They are based on a quadtree datatype and we use methods from
object oriented programming to describe them. The first algorithm de-
cides at each level n whether a given square of length 2−n lies further
than a minimal distance away from the graph of a function f(z) and then
computes the winding number for the centre of this square with respect
to f(z). The second algorithm computes zeros of an analytic function
f(z). Many algorithms for computing zeros of analytic functions exist
already. Our algorithm explicitly deals with the problem of when zeros
occur on the boundaries of squares and when the domain of the func-
tion is potentially complicated. We perturbing the relevant edges of such
squares to guarantee the method working. Then, we briefly explain how
we can apply these algorithms to particular linear operators on Hilbert
spaces called Toeplitz operators. The first algorithm allows us to draw
the spectrum of such a Toeplitz operator T (a). The second algorithm is
used to determine the spectrum of a perturbed Toeplitz operator.
Key words: computable analysis, real computation, zeros of analytic
functions, Toeplitz operators
1 Introduction
Computing the number of zeros of an analytic function in a given region by
sub-dividing that region into smaller and smaller squares or rectangles is a well-
known procedure. Several algorithms for locating the zeros of complex analytic
functions f : C→ C exist. In [9] a derivative-free method for computing zeros of
analytic functions specifically taylored to certain problems from physics is given.
Dellnitz et al. [8] present an argument based algorithm that sub-divides a given
region into rectangles, where any rectangle, which does not contain a zero, is
discarded. They point out that zeros could be on the boundary of a rectangle
and assign the value ∞ to these rectangles but do not explicitly deal with them.
In [11], [12] Kravanja and Van Barel present a derivative-free algorithm based
on winding numbers for computing zeros of analytic functions. The package
ZEAL [13] written in Fortran 90 computes the actual zeros. If zeros occur on
the initial outer boundary this boundary is perturbed by ZEAL. If an inner
edge is too close to a zero, this edge is shifted. The guiding principle is that
46 E. J. Beggs and A. Gerber
subdivisions of rectangles are chosen randomly and in an asymmetric way so
that the probability of a zero lying on an inner edge is effectively zero. The total
number of zeros in a (perturbed) square is obtained in the same way as in the
package ZEBEC [14] (see also [16]).
Here, we propose an algorithm to detect the zeros of an analytic function
within a given, possibly complicated domain. We use a quadtree data structure
that allows us to keep track of all rectangles examined and we also move edges
to within a maximal threshold distance should there be a possibility of a zero
occurring on the that edge. The difference between our algorithm and the one
presented in [15] is similar in nature to the difference between a computable
subset of N and a computably enumerable subset. For a computable subset of N
the algorithm computing that set can find all elements in an initial segment of N
in finite time. The algorithm we present can determine, up to a given error ε > 0,
all zeros of the function more than a given distance r > 0 from the boundary of
a bounded domain, and terminate in finite time.
We consider the Hilbert space H = l2(N), i.e. the space of square summable
sequences ||x|| = √∑∞n=1 |xn|2 < ∞ over C (for further details consult for
instance [7] or [18]). Banded Toeplitz operators on H are particular bounded
operators. Work by Bo¨ttcher et al. regarding Toeplitz operators such as in [2–4]
proved particularly useful for our purposes.
2 Computability
Turing [20] introduced what is now called a Turing machine and computability
theory is based on Turing machines. One approach to computability in analysis
taken by Weihrauch and others is based on TTE (type-2 theory of effectivity)
[21]. For real numbers all definitions of computability are equivalent. We now
give the following version:
Definition 1. A real number x is computable if there exists a computable se-
quence {rk} of rationals which converges effectively to x. Such a sequence {rk}
converges effectively to a real number x if there exists a recursive function
e : N→ N such that for all n ∈ N: k ≥ e(n) implies |rk − x| ≤ 2−n.
We will work with effective or computably Cauchy sequences {xi}. This means
that there is a computable function e : N→ N so that for all n ∈ N we have
∀i, j ≥ e(n) |xi − xj | < 2−n .
In the applications we present later in the paper we will make use of a computabil-
ity structure on a separable Hilbert space H and in particular of computable
linear operators on H. The finite dimensional case was discussed in detail in two
articles by Ziegler and Brattka [23, 24]. In a further article [6] the computability
of the spectrum of self-adjoint operators is examined. Pour-El and Richards [17]
present three important results concerned with computability on Banach spaces
and introduce notions such as that of an effectively separable Hilbert space.
Algorithms for Analytic Functions and Applications to Toeplitz Operators 47
It may also be of interest to know when the inverse of a computable linear
operator is computable. In [5] Brattka proves a number of results regarding
the computability of inverses of linear bijective operators on Banach spaces. He
uses the compact-open topology whereas our results are based on the operator
topology.
3 Algorithm for Winding Numbers Using Quadtrees
In this section we are going to present a method that first checks whether a given
square is on the graph of a given computable function f : S1 → C (here S1 is the
unit circle in the complex numbers). Then (if the square is not on the curve) the
winding number of the curve given by f around the square is calculated. We use
an algorithm for the winding number of a curve, and the reader can find other
results on this in [8, 11, 12]. However, we will give some details of the algorithm
we use now, as the algorithm for zeros will refer back to this.
We denote the winding number of a point λ ∈ C with respect to the function
f(z), meromorphic inside γ, by wind(f, λ). For λ = 0 the winding number of
f(z) along a closed curve γ is given by∮
γ
f ′(t)
f(t)
dt = 2pii(N − P ) = 2pii · wind(f, 0) , (1)
where N is the number of zeros and P the number of poles inside γ.
We are going to use an object-oriented approach and introduce the two classes
Graph and Square. We also assume that the curve is contained in the square
[0, 1]2 for convenience - it is not hard to remove this condition by scaling.
Graph - Describes a continuous function f from the unit circle to the complex
numbers.
Methods:
findValue(x, ) - Returns a value of f(e2piix) (for dyadic x ∈ [0, 1]) within an
error of  > 0.
findDelta() - Given  > 0 returns a value of δ > 0 for which, for all z, w ∈ S1 if
|z − w| < δ then |f(z)− f(w)| < .
Square - Describes a square in the quadtree data structure on [0, 1]2.
Variables:
level - gives the depth of the object in the quadtree data structure.
center - gives the coordinates of the center of the square.
north, south, east, west - gives the squares in those directions.
onGraph - value ‘no’ (in which case distanceFromGraph is set) or ‘don’t know’.
distanceFromGraph - a lower bound on the distance from the square to the
graph.
windingNumber - integer - the winding number of the graph around the square.
Methods:
findDistance(graph, ) - returns a lower bound on the distance from the square to
the graph or ‘don’t know’. Is guaranteed to return a bound if the actual distance
48 E. J. Beggs and A. Gerber
is greater than  > 0.
findWindingNumber(graph) - uses distanceFromGraph (and is thus only called
if onGraph=‘no’). Returns the windingNumber for the graph.
Now start at level zero (the square [0, 1]2), where the square at level n has
side 2−n.
Main routine: enmumerate over level n squares
call findDistance(graph, 2−n) and set onGraph †
If onGraph=‘no’ set distanceFromGraph
If onGraph=‘no’ call findWindingNumber(graph)
terminate (according to a given resolution) or move to next level
The important fact is that in the line † the method findDistance(graph, 2−n)
can be written to return a value in finite time because of the limit on the res-
olution 2−n and the data on uniform continuity contained in the object graph.
The price paid is that certain squares are recorded as ‘don’t know’ even if they
are actually a distance > 0 from the graph. But this will be remedied at a
lower level with the subsquares of these squares. The procedure could be made
faster if we let subsquares inherit variables from their parent, if the parent had
onGraph=‘no’.
We will now give a description of the method findWindingNumber. For sim-
plicity we assume that the center of the square is at the origin. The winding
number is 12pi times the change in the argument (or angle) of the complex num-
ber f(e2piix) as x moves from 0 to 1. The critical point here is that for complex
numbers z, w with |z|, |w| ≥ r, we have | z|z| − w|w| | ≤ 2|z − w|/r. If a square of
side 2−n is known not to intersect the graph, then we know that every point on
the graph is distance ≥ 2−n−1 = 2r from the center. Now set  = 2−n−6, and
we are guaranteed that |findValue(x, )| ≥ 2−n−2 = r. The previous inequality
gives ∣∣∣ findValue(x, )|findValue(x, )| − f(e2piix)|f(e2piix)| ∣∣∣ ≤ 2 r = 18 .
From this data we can find an approximation to the argument of f(e2piix). How-
ever this angle is only known up to adding a multiple of 2pi, and in moving round
the circle in a finite number of steps we need to be careful to avoid the angle
changing by more than half this amount in one step. Using the inequality again,
for all x, y ∈ [0, 1],∣∣∣ findValue(x, )|findValue(x, )| − findValue(y, )|findValue(y, )| ∣∣∣ ≤ 4 r + 2 |f(e2piix)− f(e2piiy)|r .
We call findDelta(), set δ to be the value returned, and if |x− y| < δ we have∣∣∣ findValue(x, )|findValue(x, )| − findValue(y, )|findValue(y, )| ∣∣∣ ≤ 6 r = 38 . (2)
Now step from x = 0 to x = 1 in a finite number of steps of length less than δ,
and calculate the argument at each point. By (2) we can avoid skipping multiples
Algorithms for Analytic Functions and Applications to Toeplitz Operators 49
of 2pi and get the winding number up to an error. But the winding number must
be an integer, so we take the closest integer. Figures 3.1 and 3.2 illustrate this
procedure.
fi-
Fig 3.10
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 0 0 0 0 0
0
-1 -1 -1 -1 -1
+1
+1
+1
+1
+1
+1
%
6
$
"!
# 
 r
t
f(x3)
tf(x2)
t
f(x4)
tf(x5) tf(x6)
t
f(x7)
tf(x8)
tf(x1)
Fig 3.2
4 Computing the Zeros of Holomorphic Functions
The total number of zeros (counted with multiplicity) of a complex analytic
function f inside a given contour γ can be calculated by evaluating the winding
number of f(z) as z moves counter-clockwise once round γ. We are going to
make use of this and also of the important principle of isolated zeros for any
analytic function [19]. This means that in any compact subset of the domain of
the function, there can only be finitely many zeros. In the example we shall give
later, the domain of the function is itself quite complicated, it is the set of points
not on the graph where the winding number of the graph is zero. To describe
such complicated domains, we will use a quadtree datatype. Then, given that a
particular square is in the domain, we just calculate the winding number of f
round its edges to find the number of zeros in the square. Unfortunately there
is a complication - the edge may contain a zero of the function.
Consider a complex analytic function f defined on an open set (its domain)
in the square [0, 1]2 in C. We assume that f has no zeros on the edges of [0, 1]2.
The domain of f is defined by a quadtree data structure beginning with the level
50 E. J. Beggs and A. Gerber
0 square [0, 1]2. A square at level n has a variable inDomain. If this takes the
value ‘true’ then a slightly bigger square (for example, say the same center, but
double the sides) lies in the domain of f . The other value of inDomain is ‘don’t
know’. The union of all the squares for which inDomain=‘true’ is the domain of
the function. Each level n square in the quadtree has nominal edges of side 2−n,
but for our purposes these are not the actual edges that we choose. We choose
edges of length 5/2n+2 at a position shifted away from the center by an amount
between 0 and 2−n−3 from the nominal side. Fig. 4.1 shows a nominal square
(with thick lines) and examples of the shifted edges. We only allow horizontal
edges to move East and vertical edges to move South, which is sufficient.
Fig. 4.1
Since the possible set occupied by any one shifted edge is a compact subset of
the domain, by the principle of isolated zeros there can only be finitely many such
shifted edges containing a zero of the function. We keep testing edges until we
have one without a zero. Repeat for all four edges, then calculate the winding
number. Of course, it is not quite that simple. First we can’t have adjacent
squares disagreeing about what the common edge is. We take a class Edge
and each edge is shifted once, so each object of class Edge contains a variable
recording this shift. Squares query the objects of class Edge, referenced by the
labels North, South, East and West.
Secondly, we can’t actually test if the function vanishes on a given shifted
edge, as such a program may never terminate. What we can do is to run a test for
an error  > 0, which is guaranteed to return a strictly positive lower bound for
the function along the shifted edge if |f(z)| >  on the shifted edge, but may also
return a ‘don’t know’ result. We test a list of trial shifted edges at successively
reducing errors (say  = 1m on the m
th trial). The list includes infinitely many
different edges, but also mentions each of these different edges infinitely many
times. Then we must have a result in finite time giving a shifted edge and a
strictly positive lower bound for |f(z)| on that edge.
The last complication is that the shifted edges are unlikely to form a simple
grid pattern, but will have more complicated intersections at the corners. There
are four possibilities, illustrated by the following diagrams in Fig. 4.2.
Fig 4.2
In each of the four possible cases we add any zeros occurring in the small
rectangle to the perturbed, adjacent square to the North-West of it. It is impor-
Algorithms for Analytic Functions and Applications to Toeplitz Operators 51
tant to evaluate the integrals along the contours in each possible case detailed
by Fig. 4.2 in the appropriate direction so as not to doubly count or omit zeros.
4.1 Classes for Computing Zeros
Function - Describes an analytic function f defined on the domain.
Methods:
findValue(z, ) - returns a value of f(z) for z in the domain within an error of
 > 0.
findDelta(, edge) - given  > 0 returns a value of δ > 0 for which, for all z, w in
the Edge edge if |z − w| < δ then |f(z)− f(w)| < .
Square - a quadtree datastructure square
Variables:
level, center, northSquare, southSquare, eastSquare, westSquare, northEdge,
southEdge, eastEdge, westEdge, inDomain, numberOfZeros
Methods:
findInDomain - determines if the enlarged square is in the domain
findNumberOfZeros(function) - determines the winding number by calling
methods from the surrounding edges.
HorizontalEdge - a quadtree datastructure edge
Variables:
edgePosition, level, northSquare, southSquare, inDomain, lowerBound
Methods:
findEdgePosition(function) - finds a shifted edgePosition where f(z) is guaran-
teed not to be zero, and a strictly positive lowerBound on |f(z)| there.
changeInArgument(function, x, y) - finds the change in argument of f(z) along
the shifted edge from real part x to real part y to an accuracy of 116 of a revo-
lution.
VerticalEdge - a quadtree datastructure edge - similar to HorizontalEdge
Main routine: enmumerate over level n squares
call findInDomain and set inDomain for square and surrounding
edges
enmumerate over level n edges with inDomain=true
call findEdgePosition and set edgePosition and lowerBound
enmumerate over level n squares
use edgePosition for surrounding edges to determine which of the
corner cases applies
call changeInArgument(function, x, y) for surrounding edges and set
numberOfZeros
We do not go into details of the calculation of the winding numbers, since
this is similar to the previous discussion. The only difference is that we use
52 E. J. Beggs and A. Gerber
squares not circles, and do the calculation separately for each edge, only taking
the closest integer on combining the data for the edges of the square.
There are a couple of points we should point out to the reader. The first is that
the -δ uniform continuity estimates for the function are carried out separately
for the compact set of possible shifted edges in each Edge object. The reason
is quite simple - the function is not analytic in the whole level 0 square, but on
some open subset, the domain. Typically the function may become very badly
behaved towards the boundary of the domain, so probably there is no global
choice of δ because the function is not globally uniformly continuous.
Secondly the output does not count the zeros in the original nominal squares,
but in some slightly enlarged squares. However we have taken care not to double
count any zeros, so a zero near an original nominal edge may end up counted in
one or other of the neighbouring squares, but not both.
Thirdly we count zeros multiply, so f(z) = (z − a)2 counts as having two
zeros at z = a. However the data given cannot distinguish between an actual
double zero and two single zeros too close to separate at the given resolution.
5 Applications to Banded Toeplitz Operators
In a chosen basis (ei) on a separable Hilbert space H, following the notational
conventions in [2], a Toeplitz operator T (a) is a linear operator given by
a0 a−1 a−2 . . .
a1 a0 a−1 . . .
a2 a1 a0 . . .
. . . . . . . . . . . .
 ,
where ai ∈ C for i ∈ Z. We can associate a function a(t) : S1 → C with each
Toeplitz operator consisting of the Laurent series with coefficients ai. Here, we
are interested in banded Toeplitz operators, which have a finite number of entries
{a−m, a−m+1, . . . , a0, . . . , am} only. In this case the associated function a(t) is
simply a Laurent polynomial
a(t) =
m∑
k=−m
akt
k . (3)
Criteria for the invertibility of Toeplitz operators are given in [2]. It is known
[10] that the spectrum sp(T ) of a Toeplitz operator T (a) is given by
sp(T ) = a(S1) ∪ {λ ∈ C\a(S1) : wind(a, λ) 6= 0} . (4)
If T (a) is simply a shift operator then sp(T ) consists of the closed unit disk.
Bo¨ttcher uses MATLAB [22] to compute sp(T ) for some examples in [4].
We now state how sp(T ) can be computed by using the algorithm from
Section 3. For that we assume that all entries of a Toeplitz operator are dyadic
fractions, i.e. of the form a
2b
, where a and b are non-negative integers. In order to
Algorithms for Analytic Functions and Applications to Toeplitz Operators 53
compute sp(T ) we simply need to use the methods for Graph for the Laurent
polynomial (3) and then the main routine in Section 3. This will compute the
complement of sp(T ) up to a given accuracy depending on the level n of the
squares in the quadtree.
From a private communication with Bo¨ttcher we learned that the spectrum of
a Toeplitz operator T with a finite rank perturbation K, denoted by A = T +K
and where K is contained in the first n rows and columns, is given by
sp(A) = sp(T ) ∪ {λ | f(λ) := det(I+ Pl(T − λI)−1KPl) = 0} . (5)
Here Pl is the projection operator onto the first l basis vectors. Since f(λ) is
analytic and not identically zero, this means that only countably many λ at most
will be added to sp(T ) to make up sp(A).
Now, we briefly explain how sp(A) can be computed. The continuous part
of sp(A) is simply sp(T ) and can be computed as just stated. For the sec-
ond, discrete part of sp(A) we retrieve all squares with centre λ for which
windNumber = 0. Then, we check whether these λ are in the domain of f(λ)
and run the main routine from Section 4 to determine the zeros of f(λ). If a
square with centre λ does not contain any zeros then it belongs to sp(A)C . We
subdivide all squares containing zeros and so on. This allows us to approximate
sp(A)C up to an accuracy depending on the level n of the quadtree structure.
Further results on computability of perturbed Toeplitz operators and a de-
tailed account of the functional analysis behind the theory of Toeplitz operators
are being written up in [1].
Acknowledgements
The authors are indepted to Professor A Bo¨ttcher for numerous valuable discus-
sions on Toeplitz operators and for providing very helpful advice on the litera-
ture. We also would like to thank Professor V Brattka for discussions relating
to computable functional analysis. Both authors would like to thank EPSRC for
supporting this work by the grant EP/C525361/1 .
References
1. Beggs, E.J., Gerber, A.: Computability of banded Toeplitz Operators with Finite
Rank Perturbations. In preparation
2. Bo¨ttcher, A., Grudsky, S.M.: Spectral properties of Toeplitz operators. SIAM (2005)
3. Bo¨ttcher, A., Embree, M., Sokolov, V.I.: Infinite Toeplitz and Laurent matrices with
localised impurities. Lin. Alg. Appl. 343-344 (2002) 101-118
4. Bo¨ttcher, A., Embree, M., Sokolov, V.I.: On large Toeplitz band matrices with an
uncertain block. Lin. Alg. Appl. 366 (2003) 87-97
5. Brattka, V.: The inversion problem for computable linear operators. STACS 2003.
LNCS 2607 Springer, Berlin (2003), 391-402
6. Brattka, V., Dillhage, R.: Computability of the spectrum of self-adjoint operators.
J. Univ. Comput. Sci. 11 (2005) 1884-1900
54 E. J. Beggs and A. Gerber
7. Debnath, L., Mikusinski, P.: Introduction to Hilbert spaces with applications. 2nd
edition, Academic Press, (1999)
8. Dellnitz, M., Schu¨tze, O., Zheng, Q.: Locating all the zeros of an analytic function
in one complex variable. J. Comput. Appl. Math. 138 (2002) 325-333
9. Gillan, C.J., Schuchinsky, A., Spence, I.: Computing zeros of analytic functions
in the complex plane without using derivatives. Comp. Phys. Comm. 175 (2006)
304-313
10. Gohberg, I.C.: On an application of the theory of normed rings to singular integral
equations. Uspekhi Matem. Nauk SSSR 112 (1952) 149-156
11. Kravanja, P., Van Barel, M.: A derivative-free algorithm for computing zeros of
analytic functions. Computing 63 (1999) 69-91
12. Kravanja, P., Van Barel, M.: Computing the zeros of analytic functions. Lecture
Notes in Mathematics 1727 Springer, Berlin (2000)
13. Kravanja, P., Van Barel, M., Ragos, O., Vrahatis, M.N., Zafiropoulos, F.A.: ZEAL:
A mathematical software package for computing zeros of analytic functions. Comp.
Phys. Comm. 124 (2000) 212-232
14. Kravanja, P., Ragos, O., Vrahatis, M.N., Zafiropoulos, F.A.: ZEBEC: A mathe-
matical software package for computing zeros of Bessel functions of real order and
complex argument. Comp. Phys. Comm. 113 (1998) 220-238
15. Matheson, A., McNicholl, T.H.: Computable analysis and Blaschke products. Proc.
AMS 136 (2008) 321-332
16. Piessens, R., de Doncker-Kapenga, E., U¨berhuber, C.W., Kahaner, D.K.: QUAD-
PACK: A Subroutine package for automatic integration. Springer Series in Compu-
tational Mathematics Vol. 1 Springer, Berlin (1983)
17. Pour-El, M.B., Richards, J.I.: Computability in analysis and physics. Springer,
(1989)
18. Rudin, W.: Functional analysis. 2nd edition, McGraw-Hill, (1991)
19. Rudin, W.: Principles of mathematical analysis. 3rd edition, McGraw-Hill, (1976)
20. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proc. Lond. Math. Soc. Series 2 42 (1936/37) 230-265
21. Weihrauch, K.: Computable analysis. Springer, Berlin (2000)
22. Wright, T.G.: MATLAB Pseudospectra GUI, 2000-2001. Available online at
http://www.comlab.ox.ac.uk/pseudospectra/psagui
23. Ziegler, M., Brattka, V.: Computability in linear algebra. Theor. Comput. Sci. 326
(2004) 187-211
24. Ziegler, M., Brattka, V.: A computable spectral theorem. CCA 2000, Swansea,
LNCS 2064 (2001) 378-388
Triviality and Minimality in the Degrees of
Monotone Complexity
William C. Calhoun
Department of Mathematics, Computer Science and Statistics
Bloomsburg University, Bloomsburg, PA 17815, USA
wcalhoun@bloomu.edu
Abstract. This work extends the author’s article Degrees of Mono-
tone Complexity (2006). Monotone complexity, Km, is a variant of Kol-
mogorov complexity that was introduced independently by Levin and
Schnorr. Monotone complexity was used in the previous article to define
the relative complexity (or randomness) of reals. Equivalence classes of
reals under monotone complexity are the Km-degrees, similar to the K-
degrees defined via prefix-free complexity. In this paper, the Km-trivial
reals are defined and shown to be equivalent to the (Km,K)-trivial reals.
A nondecreasing, function f : ω → ω is defined to be computably infinites-
imal if it is dominated by every computable, nondecreasing, unbounded
function. It is shown that if a real is Km-trivial then its monotone com-
plexity is computably infinitesimal. If a Km-minimal real exists, it is
Km-trivial. The operation ⊗, defined previously, horizontally stretches
the complexity graph of a real α by a strictly increasing computable
function f . Here, α is defined to be invariant under computable stretch-
ing if α⊗ f≡Kmα for any such f . It is shown that any Km-minimal real
must be invariant under computable stretching.
Key words: Kolmogorov complexity; monotone complexity; random-
ness; trivial reals
1 Introduction
Kolmogorov complexity was introduced by Solomonoff [17], Kolmogorov [7] and
Chaitin [2] [3] in the 1960’s and has been studied by many mathematicians
and computer scientists since then. The present work is a continuation of the
mathematical investigation of Kolmogorov complexity, in this case applied to the
relative complexity (or relative randomness) of reals (binary sequences). We will
use monotone complexity, a variant of Kolmogorov complexity defined by Levin
[8]. Schnorr [13] independently defined a similar notion called process complexity
that he later made equivalent to monotone complexity by slightly adjusting the
definition [14]. In a previous article [1], the author studied the relative complexity
of reals derived from monotone complexity for strings. Monotone complexity is
arguably more appropriate for defining the complexity of a real than the more
commonly used prefix-free complexity, K(σ). Monotone programs can describe
both finite strings and computable reals, and the bottom monotone degree is
56 William C. Calhoun
the set of computable reals. In contrast, a prefix-free program can only describe
a finite string and the bottom prefix-free degree is the set of K-trivial reals.
See the book by Li and Vita´nyi [9] for additional background on monotone and
prefix-free complexity.
Here we will focus on reals with very little complexity: Km-trivial and Km-
minimal reals. We will define them formally in the following sections. Before
delving into the mathematical details, it might be appropriate, in the spirit
of the Computability in Europe conferences, to comment briefly on a potential
application of Kolmogorov complexity. The close connection between Shannon’s
notion of entropy in information theory [15] and Kolmogorov complexity is well
known. One example of a practical application of entropy is in computer data
recovery and computer forensics [16]. Computing the entropy of recovered data
can help determine the source of the data when directory information about the
source file has been lost. High entropy indicates (seemingly) random data that
is likely to come from a compressed or encrypted file. Low entropy indicates the
data is redundant, possibly coming from a text or bitmap graphics file. Since
the entropy is based on the frequencies of bytes in a file, it does not take into
account the order of the bytes. Some researchers in computer forensics have
used randomness measures based on Kolmogorov complexity to take the byte
order into account. [19] In the setting of computer forensics, information may
be sparsely scattered in data via steganography, a method of hiding messages in
other data. One might think of the Km-trivial reals informally as data in which
information is sparsely scattered. The complexity of the initial segments grows
so slowly that the real might be said to be “almost” computable. Nevertheless,
Km-trivial reals are noncomputable reals containing infinitely much information.
Of course our primary reason for studying Km-trivial and Km-minimal re-
als is to understand them as mathematical objects, rather than for any direct
applications to computer forensics. The Km-trivial reals are defined similarly to
the intensively studied K-trivial reals. The connections and contrasts between
Km-trivial and K-trivial reals will be explored in the next section. The main
question about Km-minimal reals is whether they exist. The existence of mini-
mal elements in a degree structure is one way the structure can fail to be dense.
Density is considered an important property in the study of degree structures.
We close the introduction with some basic definitions that will be used in the
analysis of trivial and minimal reals. A monotone machine M is a computably
enumerable set of pairs 〈p, σ〉 where p, σ ∈ 2<ω and for every 〈p, σ〉, 〈q, τ〉 ∈ M ,
p ⊆ q implies σ ⊆ τ or τ ⊆ σ. (It is important to note that, p and q may be
comparable or even equal.) We define the monotone complexity of a string σ
with respect to M to be KmM (σ) = min{|p| : 〈p, τ〉 ∈ M for some τ ⊇ σ}.
One can show there is an optimal monotone machine U : for any other monotone
machine M , there is a constant c such that KmU (σ) ≤ KmM (σ) + c for all
σ. For any string σ define Km(σ) = KmU (σ). For any α ∈ 2<ω we define an
optimal monotone description of α to be a program p of minimal length such
that 〈p, β〉 ∈ U for some β ⊇ α. In some constructions we approximate Km by
the computable functions Kms(σ) = min{|p| : 〈p, τ〉 ∈ Us and σ ⊆ τ}.
Monotone Triviality and Minimality 57
A useful method for constructing prefix-free machines is given by the Kraft-
Chaitin Theorem.
Theorem 1. (Kraft-Chaitin) From a computable sequence of pairs (〈ni, σi〉)i∈ω
(known as axioms) such that Σi∈ω2−ni ≤ 1, we can effectively obtain a prefix-
free machine M such that for each i there is a τi of length ni with M(τi) ↓= σi,
and M(µ) ↑ unless µ = τi, for some i.
When applying the Kraft-Chaitin theorem we shall refer to Σσi=µ2
−ni as the
measure assigned to µ. We will refer to Σi∈ω2−ni as the total measure assigned.
Note that the total measure assigned is the sum over all strings µ of the mea-
sure assigned to µ. Using this terminology, the hypothesis of the Kraft-Chaitin
theorem may be restated as follows: the total measure assigned is less than or
equal to one.
As is done with prefix-free complexity, monotone complexity for binary strings
may be used to define the complexity of reals via the complexity functions on
initial segments. We will consider two complexity functions to be equivalent if
their difference is bounded. We use the notation f  g or g  f to mean there
is a constant c ∈ ω such that f(n) ≤ g(n) + c for all n ∈ ω. We use the no-
tation f  g to mean that f  g and g  f . We write α≤Kmβ or β≥Kmα if
Km(α  n)  Km(β  n). We write α≡Kmβ if α≤Kmβ and β≤Kmα. The equiv-
alence classes of ≡Km are the degrees of monotone complexity. See the book by
Downey and Hirschfeldt [4] for additional background on relative randomness.
2 Km-Trivial Reals
Solovay [18] was the first to construct a noncomputable K-trivial real. (See also
[5], [10] and [11].) The following definition was used in [1] to provide a notion of
triviality for the monotone degrees.
Definition 2. A real α is (Km,K)-trivial if Km(α  n)  K(n).
Here we will more simply define triviality for the monotone degrees using
only monotone complexity. First, we define the complexity of an integer n to
be the complexity of a particular string of length n. For prefix-free complexity,
we may define K(n) = K(1n). For monotone complexity, we must use a family
of uniformly “self-delimiting” strings so that the length of the string can be
computed from the pattern of bits in the string. A simple choice for this family
is λn = 0n−1a1 for n > 0. (Let λ0 be the empty string.)
Definition 3. The monotone complexity of a positive integer n is defined by
Km(n) = Km(λn)
We can now define a real to be Km-trivial if the monotone complexity of its
initial segments is dominated by the monotone complexity of the lengths of the
initial segments.
Definition 4. A real α is Km-trivial if Km(α  n)  Km(n).
58 William C. Calhoun
The definitions (Km,K)-trivial and Km-trivial are actually equivalent as we
will show. We begin with a theorem that shows that the Km and K complexities
are equivalent when applied to any computable set of incomparable strings.
Theorem 5. If {αn}n∈ω is a computable set of pairwise incomparable binary
strings, then Km(αn)  K(αn).
Proof. Since K dominates Km, it is obvious that Km(αn)  K(αn). For the
other direction, we will use the Kraft-Chaitin Theorem to define a prefix-free
machine M such that KM (αn)  Km(αn). Since K(αn)  KM (αn), it follows
that K(αn)  Km(αn). Let Sn = {s : Kms(αn) < Kms−1(αn). (For this defini-
tion we setKms(αn) =∞ whenKms(αn) ↑.) Note that for each n there is a stage
tn at which Kmtn(αn) = Km(αn). Therefore, Sn is finite for each n. To define
M , we enumerate an axiom 〈Kms(αn) + 1, αn〉 for each n and s where s ∈ Sn.
Note that in the axioms (〈ni, σi〉)i∈ω assigned for each n, the ni are distinct, and
the least such ni is Km(αn) + 1. Then, for each n, the measure assigned to αn
is Σs∈Sn2
−(Kms(αn)+1) < 2(2−(Km(αn)+1)) = 2−Km(αn). So the total measure
assigned is less than Σn∈ω2−Km(αn). Since the αn are pairwise incomparable,
it follows that the total measure assigned is less than one. By the Kraft-Chaitin
theorem, there is a prefix-free machine M such that KM (αn) = Km(αn) + 1 for
all n. Noting that KM (αn)  Km(αn), the proof is complete. uunionsq
As a corollary, we may deduce thatK(n) andKm(n) are equivalent functions.
Corollary 6. K(n)  Km(n).
Proof. First, it is easy to see that K(1n)  K(λn). Then we use the previous
theorem to conclude that K(λn)  Km(λn). uunionsq
We can conclude that (Km,K)-triviality is equivalent to Km-triviality.
Corollary 7. A real is (Km,K)-trivial iff it is Km-trivial.
As noted in [1] (for (Km,K)-triviality), it is obvious that every K-trivial
real is Km-trivial, and hence that there are noncomputable Km-trivial reals. D.
Hirschfeldt and A. Nies have shown that every K-trivial real is Turing incom-
plete. [5] [10] On the other hand, F. Stephan has shown that the real α defined
by α(n) = 1 ⇐⇒ ∀m > n(K(m) > K(n)) is Km-trivial and Turing complete.
[12] (The argument uses the fact that a universal machine with special properties
can be constructed. [6]) Thus α is a Km-trivial real that is not K-trivial.
Since Km(n) and K(n) are equivalent functions, we may deduce the asymp-
totic behavior of Km(n) from known results for K(n).
Corollary 8.
1. limn→∞Km(n) =∞.
2. There is no upper bound on Km(n1)−Km(n2) for n1 < n2.
Proof. The corresponding statements are true of K(n). uunionsq
Monotone Triviality and Minimality 59
For any real α it is clear thatK(|α|)  K(α). Hence, if α isK-trivial, we have
K(α  n)  K(n). However, the corresponding fact does not hold for monotone
complexity as indicated by the following theorem.
Theorem 9. For any real α, Km(α  n) 6 Km(n).
Proof. Suppose for a contradiction that Km(α  n)  Km(n) for some real α.
Then for some constant b, Km(α  n) − b ≤ Km(n) ≤ Km(α  n) + b. But
by Corollary 8 we may choose n1 < n2 such that Km(n1) − Km(n2) > 2b .
Combining the inequalities, we obtain Km(α  n1) + b ≥ Km(n1) > Km(n2) +
2b ≥ Km(α  n2) + b. But then Km(α  n1) > Km(α  n2), which is impossible
since Km(α  n) is nondecreasing. uunionsq
It follows immediately that Km-trivial reals must have monotone complexity
strictly below the complexity of the length.
Corollary 10. If α is a Km-trivial real, then Km(α  n) ≺ Km(n).
However, it should be understood that the previous result only means that
Km(n)−Km(α  n) is bounded below and unbounded above. The difference can
oscillate so that Km(n)−Km(α  n) is small (less than a fixed bound) infinitely
many times. This raises the question of whether a noncomputable real can have
monotone complexity unboundedly less than the complexity of the length. We
now state this question more precisely.
Definition 11. We write f(n) ≺≺ g(n) if limn→∞ g(n)− f(n) =∞.
Question 12. Is there a noncomputable real α such that Km(α  n) ≺≺ Km(n)?
We now show that Km-trivial reals must have extremely slow-growing com-
plexities. To state this precisely, we give a definition for nondecreasing functions
that are below any computable, nondecreasing, unbounded function.
Definition 13. A nondecreasing, function f : ω → ω is computably infinitesi-
mal if for every computable, nondecreasing, unbounded function g, f(n)  g(n).
Note that a computably infinitesimal function must grow more slowly than
such slow-growing computable functions as logk for any k or the inverse Acker-
mann function. We now show the monotone complexity of any Km-trivial real
is computably infinitesimal, although it is unbounded.
Theorem 14. If α is a Km-trivial real, then Km(α  n) is computably infinites-
imal.
Proof. Let g be a computable, nondecreasing, unbounded function. We wish to
show thatKm(α  n)  g(n). We may assume that g(0) = 0 and g(n+1)−g(n) ≤
1 for all n, since otherwise we can replace g with a function h(n)  g(n) that
satisfies those properties. We define a computable sequence {ni}i∈ω by ni = the
greatest n such that g(n) = i. The definition provides a description of ni of size
60 William C. Calhoun
depending only on the size of i. Hence Km(ni)  i = g(ni). Since α is Km-
trivial, Km(α  ni)  Km(ni)  g(ni). Since Km(α  n) and g(n) are monotone
functions and g(ni+1)− g(ni) = 1, it follows that Km(α  n)  g(n).
uunionsq
As mentioned above, there is a Km-trivial real that is not K-trivial. However,
as a corollary to the previous result, we can show that every Km-trivial real
is “almost” K-trivial, in the sense that the difference between the prefix-free
complexity of a Km-trivial real and K(n) is computably infinitesimal.
Corollary 15. If α is Km-trivial then K(α  n)−K(n) is computably infinites-
imal.
Proof. Let α be a Km-trivial real. Then K(α  n)  K(n) + 2Km(α  n). Hence
K(α  n) −K(n)  2Km(α  n). Since Km(α  n) is computably infinitesimal,
so is 2Km(α  n). uunionsq
3 Km-Minimal Reals
We know turn our attention to Km-minimal reals.
Definition 16. A real α is Km-minimal if α>Km0 and for every real β≤Kmα
either β≡Km0 or β≡Kmα.
It is not known whether aKm-minimal real exists. We will prove several prop-
erties that such a real would have to satisfy. The following theorem connecting
Km-triviality and Km-minimality is from [1].
Theorem 17. If a real is Km-minimal then it is Km-trivial.
As an immediate corollary, a Km-minimal real must satisfy all the properties
of Km-trivial reals from the previous section.
Corollary 18. If a real α is Km-minimal then
1. Km(α  n) ≺ Km(n),
2. Km(α  n) is computably infinitesimal
3. and K(α  n)−K(n) is computably infinitesimal.
We will close with a discussion of an additional property of Km-minimal
reals. An operation ⊗ was introduced in [1] to “horizontally stretch” the graph
of a real α by a strictly increasing function f .
Definition 19. Given any real α and a strictly increasing function f : ω → ω,
let α⊗ f be the real defined by
(α⊗ f)(n) =
{
α(f−1(n)) if n ∈ range(f)
0 otherwise
Monotone Triviality and Minimality 61
Theorem 20. If α is any real and f : ω → ω is a strictly increasing computable
function, then Km(α⊗f  n)  Km(α  f−1[n]) where f−1[n] = max{k : f(k) ≤
n}.
It is conceivable that Km(α  n) could grow so slowly that α⊗ f has mono-
tone complexity equivalent to that of α, for any strictly increasing computable
function f .
Definition 21. A real α is invariant under computable stretching if for all
strictly increasing computable functions f , α⊗ f≡Kmα.
The monotone complexity of a real that is invariant under computable stretch-
ing must grow extremely slowly, as indicated by the next theorem.
Theorem 22. If a real α is invariant under computable stretching then for any
strictly increasing computable function f , Km(α  f(n))−Km(α  n) is bounded.
Proof. Let α be invariant under computable stretching and f be a strictly in-
creasing computable function. Since α⊕ f≡Kmα, we have Km(α⊕ f  f(n)) 
Km(α  f(n)). By Theorem 20 Km(α  f(n))  Km(α  f−1[f(n)]) = Km(α 
n). Therefore Km(α  f(n))  Km(α  n) and the result follows. uunionsq
If a real α is invariant under computable stretching, then by Theorem 20
its monotone complexity g(n) = Km(α  n) satisfies g(f−1[n])  g(n) for any
strictly increasing computable function f . It is not hard to construct a function
g that satisfies that property.
Theorem 23. There is a function g : ω → ω such that g(f−1[n])  g(n) for
any strictly increasing computable function f .
Proof. Let {fi}i∈ω be a list of a strictly increasing computable functions. Define
a sequence {an}n∈ω by a0 = 0 and, for n > 0, an = max{fi(an−1) : i < n}+ 1.
Let g(x) = max{n : an ≤ x}. Now we verify that g satisfies the desired property.
For any strictly increasing computable function f , f = fi for some i. For any
integer x ≥ i there is a unique integer n ≥ i such that an ≤ x < an+1. By
definition an > f(an−1). So an−1 ≤ f−1[an] ≤ f−1[x] ≤ x. Since g is increasing,
g(an−1) ≤ g(f−1[x]) ≤ g(x). By definition of g, we have g(an−1) = n − 1 and
g(x) = n. Therefore, g(x)− 1 ≤ g(f−1[x]) ≤ g(x). uunionsq
The function g constructed in the previous theorem is not necessarily the
monotone complexity of a real, leaving open the question of whether a real
exists that is invariant under computable stretching. However, if a Km-minimal
real exists, it must be invariant under computable stretching.
Theorem 24. Every Km-minimal real is invariant under computable stretch-
ing.
Proof. Let α be a Km-minimal real and let f be a strictly increasing com-
putable function. We have Km(α ⊗ f  n)  Km(α  f−1[n]) by Theorem 20.
From α>Km0 we may conclude that limn→∞Km(α  f−1[n]) = ∞. Therefore
0<Kmα⊗ f≤Kmα. Since α is Km-minimal, α⊗ f≡Kmα. uunionsq
62 William C. Calhoun
Perhaps the results of this section will be useful in settling the main open
question about Km-minimal reals.
Question 25. Is there a Km-minimal real?
Acknowledgments: The author would like to thank Frank Stephan for
correspondence regarding the existence ofKm-trivial reals that are notK-trivial.
The author would also like to thank the anonymous referees for detailed helpful
comments.
References
1. Calhoun, W.C.: Degrees of Monotone Complexity, J. Symbolic Logic 71,
1327–1341 (2006).
2. Chaitin, G.J.: On the Length of Programs for Computing Finite Binary
Sequences, J. ACM 13, 547–569 (1966).
3. Chaitin, G.J.: On the Length of Programs for Computing Finite Binary
Sequences: Statistical Considerations, J. ACM 16, 145–159 (1969).
4. Downey, R.G., Hirschfeldt, D.R.: Algorithmic Randomness and Complexity,
Springer-Verlag (to appear).
5. Downey, R.G., Hirschfeldt, D.R., Nies, A., Stephan, F.: Trivial Reals. In:
Downey, R.G., Hui, Q.Y., Ping, T.S., Decheng, D., Yasugi, Y. (eds.) Pro-
ceedings of the 7th and 8th Asian Logic Conferences, pp. 103-131. World
Scientific (2003).
6. Figueira, S., Stephan, F., Wu, G.: Randomness and Universal Machines, J.
Complexity 22, 738–751 (2006).
7. Kolmogorov, A.N.: Three Approaches to the Quantitative Definition of In-
formation, Problems Inform. Transmission 1, 1–7 (1965).
8. Levin, L.A.: On the Notion of a Random Sequence, Soviet Math. Dokl. 14
1413–1416 (1973).
9. Li, M., Vita´nyi, P.: An Introduction to Kolmogorov Complexity and Its
Applications (2nd ed.). Springer-Verlag, New York (1997).
10. Nies, A., Lowness Properties and Randomness, Advances in Math. 197, 274–
305 (2005).
11. Nies, A., Stephan, F., Terwijn, S.: Randomness, Relativization and Turing
Degrees, J. Symbolic Logic 70, 515–535 (2005).
12. Stephan, F., personal correspondence, (2006, 2008).
13. Schnorr, C.P.: Process Complexity and Effective Random Tests, J. Comput.
System Sci. 7, 376–388 (1973).
14. Schnorr, C.P.: A Survey of the Theory of Random Sequences. In: Butts,
R.E., Hintikka, J. (eds.) Basic Problems in Methodology and Linguistics,
pp. 193–210. D. Reidel (1977).
15. Shannon, C.E.: The Mathematical Theory of Communication, Bell System
Tech. J. 27, 379–423, 623–656 (1948).
16. Shannon, M.M.: Forensic Relative Strength Scoring: ASCII and Entropy
Scoring, Int. J. Digital Evidence 2 , 1–19 (2004).
17. Solomonoff, R.J.: A Formal Theory of Inductive Inference (Part 1 and Part
2), Inform. Contr. 7, 1–22, 224–254 (1964).
Monotone Triviality and Minimality 63
18. Solovay, R.M.: Draft of a Paper (or Series of Papers) on Chaitin’s Work
(Unpublished notes), IBM Thomas J. Watson Research Center, Yorktown
Heights, NY (1975).
19. Veenman, C.J.: Statistical Disk Cluster Classification for File Carving. In:
IEEE Third International Symposium on Information Assurance and Secu-
rity, 393–398 (2007).
Extraction of Efficient Programs from Proofs:
The Case of Structural Induction over Natural
Numbers
Luca Chiarabini
LMU Mathematisches Institut, Theresienstrasse 39, D-80333 Mu¨nchen, Germany
chiarabi@mathematik.uni-muenchen.de
Abstract. Transforming a recursive procedure into a tail recursive one
brings many computational benefits; in particular in each recursive call
there is no context information to store. In this paper we consider the
particularly simple induction schema over natural numbers, and we pro-
pose two methods to automatically turn it into another proof with tail
recursive content: one continuation and one accumulator based.
Key words: Functional Programming, Program extraction from con-
structive proofs, Program development by proof transformation, CPS-
Transformation, Defunctionalization.
1 Introduction
LetM be a proof by induction over n (natural number) of the property ∀n.ϕ(n),
and let, by the Proofs-as-Program paradigm, [[M ]] be the (recursive) content of
M . In this paper we will try to answer the following question: If [[M ]] is not tail
recursive, is it possible (and if yes, how) to turn automatically M into another
proof, say N , with tail recursive content? During an informal chat in the first
MATHLOGAPS meeting in Fischbachau, Germany, Andrej Bauer suggested to
me an interesting idea[1] to mimic the behavior of a let-expression by a pure
λ-calculus function. Here we present and develop his idea in a formal setting,
which will provide a solution for the problem proposed above. Just to give an
intuition of what we will see, let
define FACT = fun n -> (if n=0 then 1
else (n* (FACT (n-1))))
be the factorial function, written in an Ocaml-like syntax. FACT is not tail
recursive because in each step of the computation the compiler has to store the
context (n∗[]), evaluate FACT (n-1) 7→ v, and returns (n∗v). We turn FACT into
a simpler function where it is not necessary to store any context information:
define FACT’ = fun n -> (define FACT’’= fun n,m,y ->
(if n=0 then y else FACT’’(n-1)(m+1)((m+1)*y))) n 0 1
Extraction of Efficient Programs from Proofs 65
Now suppose FACT to be the computational content of the proof by induction
M , with end formula ∀n.ϕ(n), that states that for each natural n there exists its
factorial. Given a natural n, (FACT’’n) is a function that takes the natural m,
the witness y for ϕ(m) and returns a witness for ϕ(n+m). One of the two proofs
transformations proposed in the present paper (the accumulator based one) will
regards how to turn M into a new proof M ′ with end formula (∀n,m.ϕ(m) →
ϕ(n+m)) and computational content equal to Fact’’. It is clear that given n,
(FACT’’n 0 1) is the witness for ϕ(n) as desired.
The paper is organized as follows: section two is a short introduction to min-
imal logic and program extraction. In section three we address two transforma-
tions over proofs in order extract continuation and accumulator based programs.
The last section regards future works. All the proofs presented in the paper were
developed with the MINLOG proof assistant[2].
2 Modified Realizability for First Order Minimal Logic
2.1 Go¨del’s T
Types are built from base types N (Naturals) , L(ρ) (lists with elements of type
ρ) and B (booleans) by function (→) and pair (×) formation. The Terms of
Go¨del’s T[3] are simply typed λ-calculus terms with pairs, projections (pii) and
constants (constructors and recursive operators for the basic types)
Types ρ, σ ::= N |B |L(ρ) | ρ→ σ | ρ× σ
Const c ::= 0N |SuccN→N | ttB |ffB | []L(ρ) | ::ρ→L(ρ)→L(ρ) |Rσ
N
|Rσ
L(ρ)|R
σ
B
Terms r, s, t ::= c |xρ|(λxρrσ)ρ→σ|(rρ→σsρ)σ|(pi0t
ρ×σ)ρ| (pi1t
ρ×σ)σ | (rρ, sσ)ρ×σ
The types and conversion rules for the recursive operators, applications and
projections are:
Rσ
N
: σ → (N→ σ → σ)→ N→ σ RσL(ρ) : σ → (L(ρ)→ σ → σ)→ L(ρ)→ σ
(Rσ
N
b f) 0 7−→ b (Rσ
L(ρ) b f) [] 7−→ b
(Rσ
N
b f) (n+ 1) 7−→ f n ((Rσ
N
b f)n) (Rσ
L(ρ) b f) (a :: l) 7−→ f l ((R
σ
L(ρ) b f) l)
Rσ
B
: σ → σ → B→ σ pi0(r, s) −→β r
(Rσ
B
r s)tt 7−→ r pi1(r, s) −→β s
(Rσ
B
r s)ff 7−→ s (λx.r)s −→β r[x := s]
The term (Rσ
B
r s)t is normally printed as (if t r s). By −→Rηβ we indicate
the union of −→β , 7−→, and −→η the η-reduction defined as λx.rs −→η r if
x 6∈ FV(r). Finally we define the extensional equality relation =Rηβ , as the
least equivalence relation that contain −→Rηβ . The extensional equality relation
capture the idea that two functions should be considered equal if they yield equal
results whenever applied to equal arguments.
66 Luca Chiarabini
2.2 Heyting Arithmetic
We define Heyting Arithmetic HAω for our language based on Go¨del’s T, which
is finitely typed. For falsity we can take the atomic formula F := atom(ff) –
called arithmetical falsity – built from the boolean constant ff and the predicate
operator on booleans atom. Below we will also need the (logical) falsity ⊥, which
we can view as just a particular propositional symbol. We define negation ¬ϕ
by ϕ→ F or ϕ→ ⊥ (depending on the context).
Formulas: Atomic formulas (Ptρ) (P a predicate symbol, t, ρ lists of terms
and types), ϕ→ ψ, ∀xρϕ, ∃xρϕ, ϕ ∧ ψ.
Derivations: In spite of the Curry-Howard correspondence it is convenient to
write derivations as terms: we define λ-terms Mϕ for natural deduction proofs
of formulas ϕ together with the set OA(M) of free assumptions in M :
(ass) uϕ, OA(u)={u}
(∧+) (〈Mϕ, Nψ〉ϕ∧ψ), OA(〈M,N〉)=OA(M) ∪ OA(N)
(∧−0 ) (M
ϕ∧ψ0)ϕ , OA(M0)=OA(M)
(∧−1 ) (N
ϕ∧ψ1)ψ , OA(N1)=OA(N)
(→+) (λuϕ.Mψ)ϕ→ψ , OA(λu.M)=OA(M)\{u}
(→−) (Mϕ→ψNϕ)ψ, OA(MN)=OA(M) ∪ OA(N)
(∀+) (λxρ.Mϕ)∀x
ρ.ϕ, OA(λx.M)=OA(M)
provided xρ 6∈ FV(ϕ), for any uϕ ∈OA(M)
(∀−) (M∀x
ρϕtρ)ϕ, OA(Mt)=OA(M)
Usually we will omit type and formula indices in derivations if they are
uniquely determined by the context or if they are not relevant and sometimes
we will write M : ϕ instead of Mϕ. In the above definition of derivations as
terms we have left out the standard connectives ∃ and ∨ because for simplicity
we want our derivation terms to be pure lambda terms formed just by lambda
abstraction, application, pairing and projections. In spite of this omission we can
use ∃ and ∨ in our logic, if we allow appropriate axioms as constant derivation
terms, e.g. for ∃:
∃+xρ,ϕ : ∀x
ρ(ϕ→ ∃xρϕ)
∃−xρ,ϕ,ψ : ∃x
ρϕ→ (∀xρϕ→ ψ)→ ψ
with the usual proviso x 6∈ FV (ψ). For ∨ we could introduce similar axioms, but
we do not do here, since we can define ∨ from ∃ via:
ϕ ∨ σ , ∃pB.(p→ ϕ) ∧ ((p→ ⊥)→ ψ)
Finally the inductions axioms associated to the types N,B and L(ρ) are:
Indn,ϕ(n) : ϕ(0)→ (∀n.ϕ(n)→ ϕ(n+ 1))→ ∀n
N.ϕ(n)
Indt,ϕ(t) : ϕ(tt)→ ϕ(ff)→ ∀t
B.ϕ(t)
Indl,ϕ(l) : ϕ([])→ (∀a, l.ϕ(l)→ ϕ(a :: l))→ ∀l
L(ρ).ϕ(l)
Extraction of Efficient Programs from Proofs 67
2.3 Modified Realizability
Clearly proper existence proofs have computational content. A well-known and
natural way to define this concept is the notion of realizability, which can be seen
as an incarnation of the Brouwer-Heyting-Kolmogorov interpretation of proofs.
Type of a Formula We define τ(ϕ) as the type of the term (or “program”) to
be extracted from a proof of ϕ. More precisely, we assign to every formula ϕ an
object τ(ϕ) (a type or the “nulltype” symbol ε). In case τ(ϕ) = ε proofs of ϕ
have no computational content; such formulas ϕ are called Harrop formulas. The
definition can be conveniently written if we extend the use of ρ→ σ and ρ× σ,
to the nulltype symbol: (ρ → ε) := ε, (ε → σ) := σ, (ε → ε) := ε, (ρ × ε) := ρ,
(ε× σ) := σ, (ε× ε) := ε:
τ(P tρ) := ε
τ(ϕ→ ψ) := (τ(ϕ)→ τ(ψ))
τ(ϕ ∧ ψ) := (τ(ϕ)× τ(ψ))
τ(∃xρ.ϕ(xρ)) := ρ× τ(ϕ)
τ(∀xρ.ϕ) := (ρ→ τ(ϕ))
Realize a formula For a convenient definition we extend the use of term
application and projection to the nullterm symbol: εt := ε, tε := t, εε := ε,
pi0ε := ε, pi1ε := ε. We define the formula t mr ϕ, to be read t realizes ϕ:
t mr P t := P t
t mr ∃x.ϕ(x) := pi0t mrϕ(pi1t)
t mr (ϕ→ ψ) := ∀x.(x mr ϕ→ txmrψ)
t mr (ϕ ∧ ψ) := (pi0tmrϕ ∧ pi1tmrψ)
t mr (∀x.ϕ) := ∀x.txmrϕ
Formulas which do not contain ∃ play a special role in this context; we call
them negative. Their crucial property is (ε mr ϕ) = ϕ. Clearly every formula of
the form (t mr ϕ) is negative.
Program Extraction We now define the extracted term [[M ]] of a derivation
M [4]. For derivations Mϕ where τ(ϕ) = ε (i.e., ϕ is a Harrop formula) let
[[M ]] := ε (the nullterm symbol). We extend the use of terms in the form (t1, t2)
to nullterm symbol: (ε, t2) := t2, (t1, ε) := t1, (ε, ε) := ε. Moreover in case
τ(ϕ) = ε, x
τ(ϕ)
u := ε and (λε.[[M ]]) means just [[M ]]. Now assume that M derives
a formula ϕ with τ(ϕ) 6= ε. Then
68 Luca Chiarabini
[[uϕ]] := x
τ(ϕ)
u (x
τ(ϕ)
u uniquely associated with uϕ)
[[(λuϕ.Nψ)ϕ→ψ]] := λx
τ(ϕ)
u .[[N ]]
[[(Mϕ→ψNϕ)ψ]] := [[M ]][[N ]]
[[〈Mϕ, Nψ〉ϕ∧ψ]] := ([[M ]], [[N ]])
[[(Mϕ∧ψi)]] := pii[[M ]]
[[(λxρ.Mϕ)∀xϕ]] := λxρ.[[M ]]
[[(M∀x
ρϕtρ)ϕx[t]]] := [[M ]]t
We also need to define extracted terms for our axioms:
[[∃−xρ,ϕ,ψ]] := λp
ρ×τ(ϕ), fρ→τ(ϕ)→τ(ψ).(f (pi0p) (pi1p)), assuming τ(ϕ) 6= ε
[[∃+xρ,ϕ]] := λx
ρ, yτ(ϕ).(x, y) , assuming τ(ϕ) 6= ε
[[IFϕ]] := λb
B, lτ(ϕ), rτ(ϕ).(if b l r) , assuming τ(ϕ) 6= ε
[[Indn,ϕ(n)]] := R
σ
N
[[Indl,ϕ(l)]] := R
σ
L(ρ)
[[Indt,ϕ(t)]] := R
σ
B
Theorem 2.1 (Soundness) Let M be a derivation of a formula ϕ from as-
sumptions ui : ϕi. Then we can find a derivation of the formula ([[M ]] mr ϕ)
from assumptions u¯i:xui mr ϕi.
Proof. Induction on M .
3 Proof Manipulation
Definition 3.1 (Tail Expressions [5]) The tail expressions of t ∈ Terms, are
defined inductively as follows:
1. If t ≡ (λx.e) then e is a tail expression.
2. If t ≡ (if t r s) is a tail expression, then both r and s are tail expressions.
3. If t ≡ (Rι r s) is a tail expression, then r and s are tail expressions.
4. Nothing else is a tail expression.
With ι ∈ {N,L(ρ)}.
Definition 3.2 A tail call is a tail expression that is a procedure call.
Definition 3.3 (Tail Recursion [6]) A recursive procedure is said tail recur-
sive when it tail calls itself or calls itself indirectly through a series of tail calls.
Now, Let consider F be the following induction proof over N:
Indn,ϕ(n)
|M
ϕ(0)
|N
∀n.ϕ(n)→ ϕ(n+ 1)
∀n.ϕ(n)
The content of F is (Rσ
N
b f) with b and f base and step case of the recursion
operator, content of the proofs M and N .
Extraction of Efficient Programs from Proofs 69
3.1 Continuation based Tail Recursion
Given the procedure (Rσ
N
b f) defined in the previous section, let Λ be the term:
R
(σ→σ′)→σ′
N
(λk.kb)(λn, p, k. p λu.k(f nu))
In Λ we name continuation the input parameter of type (σ → σ′). Λ is a function
with just one tail recursive call and a functional accumulator parameter k with
the follow property: for each n, at the i-th (0 < i ≤ n) step of the computation
of (Λn (λx.x)) the continuation has the form λu.(f (n− 1) (. . . (f (n− i)u) . . .).
At the n-th step the continuation λu.(f (n− 1) (. . . (f 0u) . . .) is applyed to the
term b and returned. We see that such returned valued correspond to (Rσ
N
b f)n.
This fact is stated formally in the following,
Theorem 3.1 For each natural n:
Λn =Rηβ λk
σ→σ′ . k((Rσ
N
b f)n)
Proof. Appendix A
By Theorem 3.1, (λn.Λn (λx.x)) and (Rσ
N
b f) are extensionally equals and
by definition 3.3 Λ is tail recursive. The procedure call in (λn.Λn (λx.x)) is tail
being the body of a lambda abstraction. The question that motivated the writing
of this paper is: being
[[F ]] = (Rσ
N
b f)
then it is possible to turn F into F ′ such that
[[F ′]] = (λn.Λn (λx.x))?
The answer is “yes”. The key point is understand the logical role of the contin-
uation parameter in Λ: given a natural n, at each step i : n, . . . , 0 in computing
(Λn (λx.x)), the continuation is a function that takes the witness for ϕ(i) and
returns the witness for ϕ(i+m), for m such that i+m = n. So we expect Λ to
be the computational content of a proof with end formula:
∀n∀ncm.(ϕ(n)→ ϕ(n+m))→ ϕ(n+m) (1)
We observe that the counter m is introduced to count the “decrease” of n during
the computation. It plays a “logical” role, but at programming level it is irrele-
vant. As we expect to extract programs from proofs, we explicitly underline the
“hidden” role of m quantifying over it by the special non-computational quan-
tifier ∀nc [7, page 47]. Supposing to know the derivation terms M : ϕ(0) and
N : ∀n.ϕ(n)→ ϕ(n+ 1), let’s prove (1):
Proposition 3.1 ϕ(0) → (∀n.(ϕ(n) → ϕ(n + 1))) → ∀n∀ncm.(ϕ(n) → ϕ(n +
m))→ ϕ(n+m)
Proof. Assume b : ϕ(0) and f : ∀n.(ϕ(n)→ ϕ(n+ 1)). By induction on n.
70 Luca Chiarabini
n = 0 We have to prove
∀ncm.(ϕ(0)→ ϕ(m))→ ϕ(m)
So assume m and k : (ϕ(0)→ ϕ(m)). Apply k to b : ϕ(0).
n+ 1 Assume n, the recursive call p : ∀ncm.(ϕ(n) → ϕ(n +m)) → ϕ(n +m),
m, and the continuation k : ϕ(n+ 1)→ ϕ(n+m+ 1). We have to prove:
ϕ(n+m+ 1)
Apply p to (m + 1) obtaining (p (m + 1)) : (ϕ(n) → ϕ(n + m + 1)) →
ϕ(n+m+ 1). So, if we are able to prove the formula ϕ(n)→ ϕ(n+m+ 1),
by some proof t, we can just apply (p (m+ 1)) to t and we are done.
So let’s prove
ϕ(n)→ ϕ(n+m+ 1)
Assume v : ϕ(n). We apply k to (f n v). ⊓⊔
For a formal proof in Natural Deduction style of Proposition 3.1 the reader is
invited to consult the Appendix C. Now we are able to provide a new proof of
the induction principle on natural numbers:
Proposition 3.2 ϕ(0)→ (∀n.(ϕ(n)→ ϕ(n+ 1)))→ ∀n.ϕ(n).
Proof. Assume b : ϕ(0), f : ∀n.(ϕ(n)→ ϕ(n+1)) and n. To prove ϕ(n), instan-
tiate the formula proved in Proposition 3.1 on b, f , n, 0 and ϕ(n)→ ϕ(n). ⊓⊔
The term extracted from the previous proof is the following:
[b,f,n].(Rec nat => (sigma => sigma’) => sigma’)
[k](k b)
[n,p,k]p([u] k(f n u))
n ([x]x)
Here ([x1,...,xN]t) stands for (λx1, . . . , xN .t) and ((Rec nat=>sigma) r s)
for (Rσ
N
r s).
Remark: Although the functional parameter in Λ is named as continuation, Λ
is not the CPS-transformed scheme of the recursion over naturals. In fact f and
b are not altered in our transformation and they could contain bad expressions,
like not tail calls. The formula (1) could be substituted by the more general
∀n.(ϕ(n) → ⊥) → ⊥, but the author think that the approach proposed in this
paper supply a clearer idea of the logical property the continuation parameter
is supposed to satisfy. Moreover, such approach represent a not trivial usage of
the not computational quantifiers ∀nc.
3.2 Accumulator based tail recursion
Here we present the essence of the Bauer’s[1] original idea. Given the procedure
(Rσ
N
b f) defined in the last section, let Π be:
RN→σ→σ
N
(λm, y.y) (λn, p,m, y. p (m+ 1) (f my))
Extraction of Efficient Programs from Proofs 71
In Π there are two accumulators parameters: a natural and parameter of type
σ where are stored partial results. For each natural n, at the i-th (0 < i ≤ n)
step of the computation of (Π n 0 b) the accumulator of the partial results will
be equal to the expression (f (i− 1) (. . . (f 0 b) . . .)). At the n-th step (base case
of Π) the accumulator of the partial results is returned and it correspond to
(Rσ
N
b f)n. This fact is stated in the Theorem below.
Definition 3.4 For all n,m, let f
N→N→σ→σ
be a function such that:
fm n = f (n+m)
Proposition 3.3 For all naturals n and m:
(Rσ
N
(fm 0 b) fm+1)n =Rηβ (R
σ
N
b fm) (n+ 1)
Proof. By induction on n. ⊓⊔
Theorem 3.2 For all natural n,
Π n =Rηβ λm, y.(R
σ
N
y fm)n
Proof. Appendix B.
By Theorem 3.2, λn.(Π n 0 b) and (Rσ
N
b f) are extensionally equals, moreover by
definition 3.3, Π is tail recursive and the procedure application in λn.(Π n 0 b)
is a tail call being the body of a lambda abstraction . As done in the last section
we address the following question: being
[[F ]] = (Rσ
N
b f)
then how to turn F into F ′ such that:
[[F ′]] = λn.(Π n 0 b)?
We note that given a natural n:
(λn.(Π n 0 b))n =Rηβ (Π n 0 b)
...
=Rηβ (f (n− 1) (. . . (f 0 b) . . .))
So given two natural indexes i , j, with i+ j = n, (Π i j) is a function that takes
the witness for ϕ(j) and returns the witness for ϕ(i+ j). So we expect Π to be
the computational content of a proof with end formula:
∀n,m.ϕ(m)→ ϕ(n+m)
that use the proofs termsMϕ(0) and N∀n.ϕ(n)→ϕ(n+1) as assumptions. We prove
this claim in the following,
72 Luca Chiarabini
Proposition 3.4 ϕ(0)→ (∀n.ϕ(n)→ ϕ(n+ 1))→ ∀n,m.ϕ(m)→ ϕ(n+m)
Proof. Assume b : ϕ(0) and f : ∀n.ϕ(n)→ ϕ(n+ 1). By induction on n:
n = 0 We have to prove
∀m.ϕ(m)→ ϕ(m)
this is trivially proved by (λm, u.u).
n+ 1 Let’s assume n, the recursive call p : ∀m.ϕ(m) → ϕ(n +m), m and the
accumulator y : ϕ(m). We have to prove
ϕ(n+m+ 1)
Apply f to m and y obtaining (f my) : ϕ(m + 1). Now apply p to (m + 1)
and (f my). ⊓⊔
See Appendix D for the formal proof in Natural Deduction style. The accumulator-
based program transformation provides us with a new proof of the induction
principle over natural numbers:
Proposition 3.5 ϕ(0)→ (∀n.ϕ(n)→ ϕ(n+ 1))→ ∀n.ϕ(n).
Proof. Assume b : ϕ(0), f : (∀n.ϕ(n)→ ϕ(n+1)) and n. To prove ϕ(n): instan-
tiate the formula proved in Proposition 3.4 on n, 0 and b : ϕ(0) ⊓⊔
The term extracted from the previous proof is the following:
[b,f,n].(Rec nat => nat => sigma => sigma)
[m,y]y
[n,p,m,y]p (m+1)(f m u))
n 0 b
4 Conclusions and future work
The expression Π (section 3.2) represent a way to mimic a let expression just
by a pure lambda calculus expression (original goal of Bauer in designing it).
Moreover, while Π is tail recursive the recursive schema over natural written
by a let would not. The terms Λ and Π compute, applied to opportune input
parameters, the same function (Rσ
N
b f). But we note that Λ is in some way
more general than Π. The modification of Λ in order to make it working on
lists (let’s name it ΛL(ρ)) instead of naturals is straightforward; but more im-
portant, the proof from which ΛL(ρ) can be extracted is obtained by a slightly
modification of the proof from which Λ is extracted. In the case of lists the end
formula to prove should be: ∀lL(ρ).(P (l) → ⊥) → ⊥. Unfortunately we can not
extend in the same way Π and it’s proof: Π looks intrinsically dependent from
the algebra of natural numbers. Our current work regard the study of such ex-
tensions. Which is the formal connection between Λ and Π? We claim is possible
transform Λ into Π by the so called defunctionalization technique (Reynolds [8],
Danvy and others [9]) a whole program transformation to turn higher-order into
Extraction of Efficient Programs from Proofs 73
first-order functional programs. But also this aspect need a deeper investigation.
Possible applications of Λ and Π go beyond the tail recursion. We noted that
there exists proofs from which are extracted programs that run in exponential
time that can be turned (by the proofs transformations proposed here) in new
proofs from which is possible to extract polynomial time algorithms. This can
appear pretty amazing and we are currently working in order to state such re-
sult more precisely. Another application of the proofs transformations proposed
here is an extension of the CPS-transformation over formal proofs (Schwicht-
enberg[10] and Griffin[11]) but this time concerning the induction axiom. The
proposal is perform CPS over proofs in two stages: a preprocessing step where
are transformed all the proofs by induction, and a second stage where the canon-
ical CPS-transformation is applied. Currently we are studying also this aspect
but it need a deeper investigation.
Acknowledgments
I really thank Helmut Schwichtenberg, Philippe Audebaud, Amr Sabry Stefan
Schimanski, Freiric Barral, Volker Heun, Andrej Bauer for the helpful discussions
and feedback. Thank’s to the anonymous referee for their useful comments.
References
1. http://math.andrej.com/2005/09/16/proof-hacking/.
2. http://www.minlog-system.de/.
3. M.H. Sørensen and P.Urzyczyn. Lectures on the Curry-Howard Isomorphism, vol-
ume 149 of Studies in Logic and the Foundations of Mathematics. Elsevier, 2006.
4. G. Kreisel. Interpretation of Analysis by means of Functionals of Finite Type. In
A. Heyting, editor, Constructivity in Mathematics, 1959.
5. R. Kelsey, W. Clinger, and J. Rees (eds.). Revised5 report on the algorithmic
language scheme. Higher-Order and Symbolic Computation, 11(1), August, 1998.
6. K. Kent Dybvig. The Scheme Programming Language. Mit Press, 1996.
7. Helmut Schwichtenberg. Minimal Logic for Computable Functionals.
http://www.mathematik.uni-muenchen.de/~chiarabi/mlcf.pdf, December
2008.
8. John C. Reynolds. Definitional interpreters for higher-order programming lan-
guages. Higher-Order and Symbolic Computation, 11(4):363–397, 1998. Reprinted
from the proceedings of the 25th ACM National Conference (1972).
9. Olivier Danvy and Lasse R.Nielsen. Defunctionalization at work. In editor Har-
ald Søndergaard, editor, Proceedings of the Third International Conference of Prin-
ciples and Practice of Declarative Programming, pages 162–174, Firenze, Italy,
September 2001. ACM Press. Extended version available as the technical report
BRICS RS-01-23.
10. Helmut Schwichtenberg. Proofs, lambda terms and control operators. In Logic of
computation. Proceedings of the NATO ASI.Marktoberdorf, Germany, 1995.
11. Timothy G. Griffin. A formulae-as-types notion of control. In 17th Annual ACM
Symp. on Principles of Programming Languages, POPL’90, San Francisco, CA,
USA, 1990.
74 Luca Chiarabini
A
Theorem A.1 For each natural n:
Λn =Rηβ λk
σ→σ′ . k((Rσ
N
b f)n)
Proof. By induction over n:
n = 0
Λ 0 =Rηβ λk.kb
=Rηβ λk. k((R
σ
N
b f)0)
n+ 1
Λ (n+ 1) =Rηβ (λn, p, k. p λu.k(f nu))n (Λn)
=Rηβ λk.(Λn)λu.k(f nu)
=Rηβ λk.(λk.k((R
σ
N
b f)n))λu.k(f nu) (By IH)
=Rηβ λk.(λu.k(f nu))((R
σ
N
b f)n)
=Rηβ λk.k(f n ((R
σ
N
b f)n))
=Rηβ λk.k((R
σ
N
b f)(n+ 1))
⊓⊔
B
Theorem B.1 For all natural n,
Π n =Rηβ λm, y.(R
σ
N
y fm)n
Proof. By induction on n:
n = 0
Π 0 =Rηβ λm, y.y
=Rηβ λm, y.(R
σ
N
y fm) 0
n+ 1
Π (n+ 1) =Rηβ (λn, p,m, y.p(m+ 1)(f my))n (Πn)
=Rηβ λm, y.(Πn) (m+ 1) (f my)
=Rηβ λm, y.(λm, y.(R
σ
N
y fm )n) (m+ 1) (f my) By IH
=Rηβ λm, y.(R
σ
N
(f my) fm+1)n
=Rηβ λm, y.(R
σ
N
(fm 0 y) fm+1)n By Def.3.4
=Rηβ λm, y.(R
σ
N
y fm )(n+ 1) By Prop. 3.3
⊓⊔
Extraction of Efficient Programs from Proofs 75
C
Proposition C.1 ϕ(0) → ∀n.ϕ(n) → ϕ(n + 1)) → ∀n∀ncm.(ϕ(n) → ϕ(n +
m))→ ϕ(n+m)
Proof. Assume b : ϕ(0) and f : ∀n.ϕ(n)→ ϕ(n+ 1)). By induction on n.
n = 0
[k : ϕ(0)→ ϕ(m)] [b : ϕ(0)]
(k b) : ϕ(m)
(λ k.k b) : (ϕ(0)→ ϕ(m))→ ϕ(m)
(λm, k.k b) : ∀ncm.(ϕ(0)→ ϕ(m))→ ϕ(m)
n > 0
[k : ϕ(n+ 1)→ ϕ(n+m+ 1)]
[f : ∀n.ϕ(n)→ ϕ(n+ 1)] n
(f n) : ϕ(n)→ ϕ(n+ 1) [v : ϕ(n)]
(f n v) : ϕ(n+ 1)
(k (f n v)) : ϕ(n+m+ 1)
(λ v.k (f n v)) : ϕ(n)→ ϕ(n+m+ 1)
[p : ∀ncm.(ϕ(n)→ ϕ(n+m))→ ϕ(n+m)] (m+ 1)
(p(m+ 1)) : (ϕ(n)→ ϕ(n+m+ 1))→ ϕ(n+m+ 1)
(p (m+ 1) (λ v.k (f n v))) : ϕ(n+m+ 1)
(λ k.p (m+ 1) (λ v.k (f n v))) : (ϕ(n+ 1)→ ϕ(n+m+ 1))→ ϕ(n+m+ 1)
(λm, k.p (m+ 1) (λ v.k (f n v))) : ∀ncm.(ϕ(n+ 1)→ ϕ(n+m+ 1))→ A(n+m+ 1)
(λ p,m, k. p (m+ 1) (λ v.k (f n v))) : (∀ncm.(ϕ(n)→ ϕ(n+m))→ ϕ(n+m))→
∀
ncm.(ϕ(n+ 1)→ ϕ(n+m+ 1))→ ϕ(n+m+ 1)
(λn, p,m, k. p (m+ 1) (λ v.k (f n v))) : ∀n.(∀ncm.(ϕ(n)→ ϕ(n+m))→ ϕ(n+m))→
∀
ncm.(ϕ(n+ 1)→ ϕ(n+m+ 1))→ ϕ(n+m+ 1)
D
Proposition D.1 ϕ(0)→ (∀n.ϕ(n)→ ϕ(n+ 1))→ ∀n,m.ϕ(m)→ ϕ(n+m)
Proof. Assume b : ϕ(0), f : (∀n.ϕ(n)→ ϕ(n+ 1)). By induction on n:
n = 0
[u : ϕ(m)]
(λu.u) : ϕ(m)→ ϕ(m)
(λm, u.u) : ∀m.ϕ(m)→ ϕ(m)
76 Luca Chiarabini
n > 0
[f : ∀n.ϕ(n)→ ϕ(n+ 1)] m
(f m) : ϕ(m)→ ϕ(m+ 1) [y : ϕ(m)]
(f my) : ϕ(m+ 1)
[p : ∀m.ϕ(m)→ ϕ(n+m)] (m+ 1)
(p (m+ 1)) : ϕ(m+ 1)→ ϕ(n+m+ 1)
(p (m+ 1) (f my)) : ϕ(n+m+ 1)
(λ y. p (m+ 1) (f my)) : ϕ(m)→ ϕ(n+m+ 1)
(λm, y. p (m+ 1) (f my)) : ∀m.ϕ(m)→ ϕ(n+m+ 1)
(λ p,m, y. p (m+ 1) (f my)) : (∀m.ϕ(m)→ ϕ(n+m))→
(λ p,m, y. p (m+ 1) (f my)) : aaaaaaaaaaaa(∀m.ϕ(m)→ ϕ(n+m+ 1))
(λn, p,m, y. p (m+ 1) (f my)) : ∀n.(∀m.ϕ(m)→ ϕ(n+m))→
(λn, p,m, y. p (m+ 1) (f my)) : ∀n.aaaaaaaa(∀m.ϕ(m)→ ϕ(n+m+ 1))
Phase Shifts of LFSM as Pseudorandom
Number Generators for BIST for VLSI ?
Sung-Jin Cho1, Un-Sook Choi2, Han-Doo Kim3, Yoon-Hee Hwang4, and
Jin-Gyoung Kim5
1 Division of Mathematical Sciences, Pukyong National University
Busan 608-737, Korea, sjcho@pknu.ac.kr
2 Department of Multimedia Engineering, Tongmyong University
Busan 626-847, Korea, choies@tu.ac.kr
3 Institute of Mathematical Sciences and School of Computer Aided Science
Inje University, Gimhae 621-749, Korea, mathkhd@inje.ac.kr
4 Department of Information Security, Graduate School
Pukyong National University
Busan 608-737, Korea, yhhwang@pknu.ac.kr
5 Department of Applied Mathematics, Graduate School
Pukyong National University
Busan 608-737, Korea, 5892587@hanmail.net
Abstract. Large phase shifts are generally desirable as they result in
less correlation and, therefore, higher fault coverage in the testing of
VLSI. In this paper, we investigate the phase shifts of the sequences gen-
erated by companion matrices of primitive polynomials. Also we propose
an algorithm for finding phase shifts of the sequences.
1 Introduction
The bit sequences generated by successive cells of built-in test pattern genera-
tors suffer in general from correlations and/or linear dependencies. This is prob-
lematic for pseudorandom and/or pseudoexhaustive generation of test patterns,
structures commonly employed in a variety of test applications ([1] ∼ [3]). A
linear feedback shift register (LFSR) is usually used as an on-chip test pattern
generator, which drives flip-flop chains in parallel. In the context of digital cir-
cuit testing, LFSRs have been used as very popular signature analyzers. As the
number of gates or digital circuits that can be integrated into one chip of silicon
becomes hundreds of million, the testing of VLSI has become a very important
engineering problem. Since the number of states that a VLSI can assume is as-
tronomically large, it is impossible to check all the states of each chip of VLSI
before shipping. So we check it by some mechanism of random sampling. It was
customary up to several years ago to use an LFSR as a test pattern generator for
this purpose. A linear finite state machine (LFSM) is an external XOR LFSRs.
? This work was supported by grant No. (R01-2006-000-10260-0) from the Basic Re-
search Program of the Korea Science and Engineering Foundation.
78 Sung-Jin Cho et al.
Linear dependencies among bit positions in the patterns generated by an LFSM
are to be avoided because they limit the number of different bit combinations
seen by the circuit under test when that LFSM is used for pseudorandom test-
ing of the circuit. The issue of linear dependencies has been studied by various
researchers ([3] ∼ [6]). In general, the bit sequences produced by an LFSM are
shifted versions of each other. The difference in the number of shift positions
that each bit sequence exhibits with respect to a reference sequence is referred
to as the phase shift of that sequence. Large phase shifts are generally desir-
able as they result in less correlation and, therefore, higher fault coverage in the
testing of VLSI. For many years, many researchers have been working on how
to find phase shifts of maximum-length sequences. The phase shift analysis of
90/150 CA whose characteristic polynomials are primitive, has been investigated
by Bardell [7], Das et al. [8], Sarkar ([9], [10]) and Cho et al. ([11], [12]). Also
the phase shift analysis of LFSMs whose characteristic polynomials are primitive
has been investigated by Bardell et al. [2] and Kagaris et al. ([4], [13] ∼ [17]).
In this paper, we investigate the phase shifts of the sequences generated by
companion matrices of primitive polynomials. Also we propose an algorithm for
finding phase shifts of the sequences.
2 Preliminaries
Let GF (2) be the Galois field with cardinality 2. In this section, we investi-
gate some properties of sequences generated by companion matrices of primitive
polynomials.
Definition 2.1. Let f(x) = xn+cn−1xn−1+cn−2xn−2+ · · ·+c1x+1, where
ci ∈ GF (2). Then the following n × n matrix T is said to be the companion
matrix of f(x).
T =

cn−1 1 0 · · · 0
cn−2 0 1 · · · 0
...
...
...
. . .
...
c1 0 0 · · · 1
1 0 0 · · · 0
 =
C(n−1)×1 In−1
1 O1×(n−1)

, where In−1 is an (n−1)×(n−1) identity matrix and C = (cn−1, cn−2, · · · , c1)t.
Since |T | = |In−1| = 1, T is nonsingular. The inverse T−1 of T is
T−1 =

0 0 · · · 0 0 1
1 0 · · · 0 0 cn−1
...
...
. . .
...
...
...
0 0 · · · 1 0 c2
0 0 · · · 0 1 c1
 =

O1×(n−1) 1
In−1 C(n−1)×1

Definition 2.2. ([18], [19]) Let f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · · +
c1x+1 be an n-degree primitive polynomial, where c1, · · · , cn−1 ∈ GF (2). Then
Phase Shifts of LFSM as Pseudorandom Number Generators 79
f(x) generates a periodic sequence whose period is 2n−1. This sequence is called
a pseudo-noise(PN) sequence.
PN sequences are of fundamental importance in computer science, cryptology
and engineering. The following are some basic properties of PN sequences of
degree n [18].
(P1) The period of a PN sequence of degree n is 2n − 1.
(P2) In a PN sequence of period 2n−1, the 0-run of length n−1 occurs exactly
once.
3 Analysis of sequences generated by companion matrices
of primitive polynomials
In this section, the method for finding phase shifts of the sequences generated
by companion matrices of primitive polynomials is presented. The following the-
orems give various methods to compute phase shifts.
Let S be the (2n−1)×nmatrix that has as rows x0, Tx0, · · · , T 2n−2x0, where
x0 = (
n−1︷ ︸︸ ︷
0, · · · , 0, 1)t. Then S is the (2n−1)×n matrix consisting of n independent
PN sequences as its columns. Denote T ix0 by xi for each i(1 ≤ i ≤ 2n − 2).
S = (sij)(2n−1)×n =

x0t
x1t
...
xn−1t
xnt
...
x2n−2t

=

0 0 0 · · · 0 0 1
0 0 0 · · · 0 1 0
...
...
...
. . .
...
...
...
1 0 0 · · · 0 0 0
cn−1 cn−2 cn−3 · · · c2 c1 1
...
...
...
. . .
...
...
...
1 cn−1 cn−2 · · · c3 c2 c1

Let ps(i) be the phase shift of the ith column with respect to the 1st column
of S. From the definition of the phase shift we obtain the following theorems.
Theorem 3.1. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. Then ps(1) = 0
and ps(n) = 1.
Theorem 3.2. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1 and let cn−1 =
cn−2 = · · · = cn−m = 0 and cn−m−1 = 1. Then ps(i) = 2n − i (2 ≤ i ≤ m+ 1).
Example 3.3. We can find the phase shifts of the primitive polynomial
x6 + x + 1 by Theorem 3.1 and Theorem 3.2. ps(1) = 0, ps(6) = 1 by Theorem
80 Sung-Jin Cho et al.
3.1. ps(2) = 26 − 2 = 62, ps(3) = 26 − 3 = 61, ps(4) = 26 − 4 = 60 and
ps(5) = 26 − 5 = 59 by Theorem 3.2.
Lemma 3.4. Let T be the companion matrix of the n-degree primitive poly-
nomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let T k+i +
T k+(i+m) = T i(k ∈ N, 0 ≤ i ≤ n− 2 and 1 ≤ m ≤ n− 1), where N is the set of
all positive integers. Then xk+i + xk+(i+m) = xi.
Lemma 3.5. Let f(x) be the n-degree primitive polynomial f(x) = xn +
cn−1xn−1 + cn−2xn−2 + · · ·+ c1x+ 1. And let the (j + 1)th row vector of S be
(x1, x2, · · · , xn). Then the (j+2)th row vector xj+1t of S is (x2, x3, · · · , xn, 0)+
x1(cn−1, · · · , c1, 1).
Theorem 3.6. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · ·+ c1x+1. And let cn−1 = 1
and T k + T k+1 = I for some k ∈ N. Then ps(2) = k.
Proof. Since T k + T k+1 = I, T k+i + T k+(i+1) = T i(0 ≤ i ≤ n − 1). By Lemma
3.4, xk+i + xk+i+1 = xi(0 ≤ i ≤ n− 1). That is,
sk+1,n + sk+2,n = 1,
sk+2,n−1 + sk+3,n−1 = 1,
...
sk+n,1 + sk+n+1,1 = 1.
Hence the 1st (resp. 2nd) elements of the (n − 1) vectors xkt, · · · ,xk+n−2t
are the same. Suppose that the 1st and the 2nd elements of the (n − 1) vec-
tors xkt, · · · ,xk+n−2t are of the form ′′0 ∗′′ . In the 1st column of S there
are two vectors (s11, · · · , sn−1,1)t = (0, · · · , 0)t and (sk+1,1, · · · , sk+n−1,1)t =
(0, · · · , 0)t. Since the 0-run of length n − 1 occurs exactly once in a PN se-
quence of period 2n − 1 by (P2), this is a contradiction. This means that the
1st and the 2nd elements of the (n − 1) vectors xkt, · · · ,xk+n−2t must be of
the form ′′1 ∗′′ . Suppose that the 1st and the 2nd elements of the (n − 1) vec-
tors xkt, · · · ,xk+n−2t are of the form ′′11′′. Since xk = (1, 1, ∗, · · · , ∗)t, xk+1 =
(1, ∗, · · · , ∗, 0)t + (cn−1, cn−2, · · · , c1, 1)t = (0, ∗, · · · , ∗, 1)t by Lemma 3.5. But
xk + xk+1 = (0, · · · , 0, 1)t. This is a contradiction. Thus the 1st and the 2nd
elements of the (n− 1) vectors xkt, · · · ,xk+n−2t must be all ′′10′′. Since (n− 1)
consecutive 0′s occur from the (k+1)th element to the (k+n− 1)th element in
the 2nd column of S, ps(2) = k.
Example 3.7.We can find the phase shift ps(2) of the primitive polynomial
x7 + x6 + x5 + x4 + 1 by Theorem 3.6. Since T 86 + T 87 = I, ps(2) = 86.
Theorem 3.8. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · ·+ c1x+ 1. And let ci+1 = 1
and ci = ci−1 = · · · = c1 = 0 for some i(1 ≤ i ≤ n − 2). Then ps(n − j) =
j + 1(1 ≤ j ≤ i).
Phase Shifts of LFSM as Pseudorandom Number Generators 81
Proof. By Lemma 3.5, the (n + 2)th row vector xn+1t of S is of the following
form:
xn+1t = cn−1(cn−1, · · · , 1,
n−i
0 , · · · , 0, 0, 1) + (cn−2, · · · ,
n−i−1
0 , 0, · · · , 0, 1, 0)
= (cn−1 + cn−2, · · · , cn−1,
n−i
0 , · · · , 0, 1, cn−1).
By the similar method, S is of the following form:

x0t
x1t
...
xit
xi+1t
...
xn−1t
xnt
xn+1t
xn+2t
...
xn+i−1t
xn+it
...

=

0 0 · · · 0 n−i0 0 · · · 0 0 1
0 0 · · · 0 0 0 · · · 0 1 0
...
...
. . .
...
...
...
. . .
...
...
...
0 0 · · · 0 1 0 · · · 0 0 0
0 0 · · · 1 0 0 · · · 0 0 0
...
...
. . .
...
...
...
. . .
...
...
...
1 0 · · · 0 0 0 · · · 0 0 0
cn−1 cn−2 · · · 1
∗ ∗ · · · cn−1
∗ ∗ · · · ∗
...
...
. . .
... B
∗ ∗ · · · ∗
∗ ∗ · · · ∗
...
...
. . .
...
...
...
. . .
...
...
...

, where B is the following (i+ 1)× (i+ 1) submatrix of S:
B =
 sn+1,n−i sn+1,n−i+1 · · · sn+1,n... ... . . . ...
sn+i+1,n−i sn+i+1,n−i+1 · · · sn+i+1,n

=

0 0 · · · 0 0 1
0 0 · · · 0 1 ∗
0 0 · · · 1 ∗ ∗
...
...
. . .
...
...
...
0 1 · · · ∗ ∗ ∗
1 ∗ · · · ∗ ∗ ∗

For 1 ≤ j ≤ i, since sj+1,n−j = 1, sj+2,n−j = sj+3,n−j = · · · = sj+n,n−j = 0
and sj+n+1,n−j = 1, the number of consecutive 0′s in the (n− j)th column of S
is n− 1. Thus ps(n− j) = j + 1.
Example 3.9. We can find the phase shifts ps(3) = 3 and ps(4) = 2 of the
primitive polynomial x5 + x3 + 1 by Theorem 3.8.
Corollary 3.10. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · ·+ c1x+ 1. And let ci+1 = 1
and ci = ci−1 = · · · = c1 = 0. Then ps(k) = n− k + 1(n− i ≤ k ≤ n− 1).
82 Sung-Jin Cho et al.
Theorem 3.11. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn+ cn−1xn−1+ cn−2xn−2+ · · ·+ c1x+1. And let ci+1 = 0.
Then ps(n− i− 1) = ps(n− i) + 1(0 ≤ i ≤ n− 2).
Proof. Let ps(n − i) = p. Then sp+1,n−i = sp+2,n−i = · · · = sp+(n−1),n−i = 0
and sp+n,n−i = 1 by (P2). Since sk+1,n−i−1 = sk,n−i + ci+1sk,1(1 ≤ k ≤ 2n − 2)
by Lemma 3.5 and ci+1 = 0,
sp+2,n−i−1 = sp+1,n−i = 0,
sp+3,n−i−1 = sp+2,n−i = 0,
...
sp+n,n−i−1 = sp+n−1,n−i = 0,
sp+n+1,n−i−1 = sp+n,n−i = 1.
Thus ps(n− i− 1) = ps(n− i) + 1.
Corollary 3.12. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let ci+k =
ci+(k−1) = · · · = ci+1 = 0 and ps(n− i) = p. Then ps(n− i− j) = ps(n− i)+ j =
p+ j(1 ≤ j ≤ k).
Theorem 3.13. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. And let cn−1 =
· · · = cn−m+1 = 0 and cn−m = 1. If T k + T k+m = I for some k ∈ N, then
ps(m+ 1) = k.
Proof. Since T k + T k+m = I, T k+i + T k+m+i = T i(0 ≤ i ≤ n − 1). By Lemma
3.4, xk+i + xk+m+i = xi(0 ≤ i ≤ n − 1). Since xk + xk+m = x0, sk+1,m+1 =
· · · = sk+m+2,m+1 = 0. From xk+i + xk+m+i = xi(1 ≤ i ≤ n − 1), we obtain
sk+m+3,m+1 = · · · = sk+(n−1),m+1 = 0. Thus sk+1,m+1 = sk+2,m+1 = · · · =
sk+(n−1),m+1 = 0 and sk+n,m+1 = 1. Hence by (P2) ps(m+ 1) = k.
Example 3.14. We can find the phase shift ps(3) = 14 of the primitive
polynomial x6 + x4 + x3 + x+ 1 by Theorem 3.13.
Theorem 3.15. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn+cn−1xn−1+cn−2xn−2+ · · ·+c1x+1. And let ci = 1 and
ci−1 = · · · = c1 = 0. If T k + T k−i = I for some k ∈ N, then ps(n− i) = k + 1.
Proof. The matrix obtained by (i+ 1) row vectors xk−it,xk−i+1t, · · · ,xkt of S
is of the following form: 
xk−it
xk−i+1t
xk−i+2t
...
xk−1t
xk
t
 =
Phase Shifts of LFSM as Pseudorandom Number Generators 83
sk−i+1,1 · · · n−isk−i+1,n−i n−i+1sk−i+1,n−i+1 · · · sk−i+1,n
sk−i+2,1 · · · sk−i+1,n−i+1 + ci · sk−i+1,1 ∗ · · · sk−i+1,1
sk−i+3,1 · · · sk−i+1,n−i+2 + ci · sk−i+2,1 ∗ · · · sk−i+2,1
...
sk,1 · · · sk−i+1,n−1 + ci · sk−1,1 sk−i+1,n sk−i+1,1 · · ·
sk−i+1,1 · · · sk−i+1,n−1 sk−i+1,1 · · · sk−i+1,n

Since ci−1 = ci−2 = · · · = c1 = 0, sk+1,n−i+1 = sk−i+1,1. Since T k−i + T k =
I, by Lemma 3.4, xk−i + xk = x0 and thus sk−i+1,n−i+1 + sk+1,n−i+1 = 0.
Since sk+1,n−i+1 = sk−i+1,1, sk−i+1,n−i+1 = sk−i+1,1. Therefore sk−i+1,n−i+1 +
ci · sk−i+1,1 = 0 because ci = 1. By the similar method we can show that
sk−i+3,n−i = sk−i+4,n−i = · · · = sk,n−i = 0. Also sk−i+1,n−i = sk−i+1,n +
ci · sk,1 = sk−i+1,n + sk−i+1,n = 1. Thus sk+1,n−i = 1 and sk+2,n−i = · · · =
sk+n,n−i = 0. Hence ps(n− i) = k + 1.
Theorem 3.16. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn + cn−1xn−1 + cn−2xn−2 + · · · + c1x + 1. Let c1 = c2 =
· · · = cm = 1 and cm+1 = 0. And let T ki + T ki+1 + · · ·+ T ki+i = I(1 ≤ i ≤ m).
Then ps(n− i) = ki + i+ 1.
Proof. For ki such that T ki + T ki+1 + · · · + T ki+i = I, the matrix obtained by
(i+ 1) vectors xki ,xki+1, · · · ,xki+i of S is of the following form:
xki
t
xki+1
t
...
xki+i−2
t
xki+i−1
t
xki+i
t
 =

n− i
∗ ∗ · · · 0 ∗ ∗ · · · ∗
∗ ∗ · · · 0 ∗ ∗ · · · ∗
...
...
...
...
...
...
...
...
0
1
ski+(i+1),1 ski+(i+1),2 · · · 1 ∗ · · · · · · ski+(i+1),n

Since c1 = c2 = · · · = ci = 1, ski+1,n−i = · · · = ski+(i−1),n−i = 0 and ski+i,n−i =
ski+(i+1),n−i = 1, so that ski+(i+2),n−i = · · · = ski+(i+n),n−i = 0. Thus ps(n −
i) = ki + i+ 1.
Example 3.17. For the primitive polynomial x8+x6+x5+x4+1, ps(1) = 0
and ps(8) = 1 by Theorem 3.1. By Theorem 3.2, ps(2) = 254. By Theorem 3.11,
ps(5) = 4, ps(6) = 3 and ps(7) = 2. Since T 48+T 50 = I, ps(3) = 48 by Theorem
3.16. Since T 100 + T 96 = I, ps(4) = 101 by Theorem 3.15.
The following theorem is the main theorem in this section.
Theorem 3.18. Let T be the companion matrix of the n-degree primitive
polynomial f(x) = xn+cn−1xn−1+cn−2xn−2+ · · ·+c1x+1. Let cj = 1 for some
j (1 < j < n). And let T k + cj+1T k+1 + · · ·+ cn−1T k+(n−j−1) + T k+(n−j) = I.
Then ps(n− j + 1) = k.
Example 3.19. Phase shifts for the primitive polynomial f(x) = x12+x10+
x9 + x8 + x6 + x2 + 1 are in Table 1.
84 Sung-Jin Cho et al.
Table 1. Phase shifts for f(x)
Phase shifts Theorem name
ps(1) = 212 − 1 Theorem 3.1
ps(2) = 4094 Theorem 3.2
ps(3) = 1156 Theorem 3.18
ps(4) = 2511 Theorem 3.18
ps(5) = 935 Theorem 3.18
ps(6) = 934 Theorem 3.11
ps(7) = 1162 Theorem 3.11
ps(8) = 1161 Theorem 3.11
ps(9) = 1160 Theorem 3.11
ps(10) = 1159 Theorem 3.15
ps(11) = 2 Theorem 3.10
ps(12) = 1 Theorem 3.1
4 Algorithms to compute Phase shifts
Using the theorems outlined above, we give an algorithm to find the phase shifts
of the sequences generated by companion matrices of primitive polynomials.
Algorithm FindPhaseShiftsOfLFSM
Input: n, The 1st column (cn−1, cn−2, · · · , c1, 1) of the companion matrix T of
an n-degree primitive polynomial f(x) = xn+cn−1xn−1+cn−2xn−2+· · ·+c1x+1.
Output: The phase shifts ps(i)(1 ≤ i ≤ n) of the ith column of S.
BEGIN
ps(1)← 2n − 1
ps(n)← 1
count ← 0
Step 1. Find m = min{j|cj = 1, 1 ≤ j < n};
Step 2. for h← 1 to m− 1
do {
ps(n− h)← h+ 1
count ← count + 1
}
if count > n− 3 then Stop.
Step 3. ps(n−m)← k subject to T k + T k−m = I
count ← count + 1
if count > n− 3 then Stop.
Step 4. Find M = max{j|cj = 1,m ≤ j ≤ n− 1 };
Step 5. for l← n− 1 downto M + 1
do {
ps(n− l + 1)← ps(n− l)− 1
Phase Shifts of LFSM as Pseudorandom Number Generators 85
Table 2. Phase shifts for f(x)
(In this table, 86540 stands for the polynomial x8 + x6 + x5 + x4 + 1.)
n f(x) phase shifts
8 86540 0,254,48,101,4,3,2,1
8 87530 0,242,241,69,68,3,2,1
8 8765420 0,121,179,108,246,245,2,1
9 94310 0,510,509,508,507,24,317,316,1
9 9643210 0,510,509,275,274,356,331,460,1
9 986543210 0,107,106,92,459,325,248,109,1
10 10,4,3,1,0 0,1022,1021,1020,1019,1018,966,533,532,1
10 10,6,5,3,2,1,0 0,1022,1021,1020,769,326,325,856,450,1
10 10,7,6,5,4,3,2,1,0 0,1022,1021,479,527,315,292,45,439,1
11 11,10,8,1,0 0,308,309,316,315,314,313,312,311,310,1
12 12,10,2,1,0 0,4094,639,640,641,642,643,644,645,646,2369,1
17 17,3,0 0, 217 − 2, 217 − 3, · · · , 217 − 14,3,2,1
count ← count + 1
}
if count > n− 3 then Stop.
Step 6. while count ≤ n− 3,
do {
if CM = 1, then
{ps(n−M + 1) = k
subject to T k+cM+1T k+1+· · ·+cn−1T k+(n−M−1)+T k+(n−M) = I
count ← count + 1
M ←M − 1
}
else
{
ps(n−M + 1)← ps(n−M)− 1
M ←M − 1
}}
END
Phase shifts with respect to the 1st column of the given primitive polynomial
are in Table 2.
5 Conclusion
In this paper, we investigated the phase shifts of the sequences generated by
companion matrices of primitive polynomials. And we proposed an algorithm
for finding phase shifts of the sequences.
86 Sung-Jin Cho et al.
References
1. P.H. Bardell, Calculating the effects of linear dependencies on m-sequences used as
test stimuli, IEEE Trans. CAD of Integrated Circuits and Systems, 11 (1992) 83-86
2. P.H. Bardell, W.H. McAnney and J. Savir, Built-In test for VLSI: pseudorandom
techniques, John Wiley & Sons, 1987
3. C.L. Chen, Linear dependencies in linear feedback shift registers, IEEE Trans. Com-
put, 32 (1986) 1086-1088
4. J. Kakade and D. Kagaris, Phase shifts and linear dependencies, in Proc. IEEE Int.
Symp. Circuits Syst., (2006) 1595-1598
5. A. Lempel, Analysis and synthesis of polynomials and sequences over GF (2), IEEE
Trans. Inform. Theory, IT-17 (1971) 297-303
6. J. Rajski and J. Tyszer, On linear dependencies in subspaces of LFSR-generated
sequences, IEEE Trans. Computers, 45 (1996) 1212-1221
7. P.H. Bardell, Analysis of cellular automata used as pseudorandom pattern genera-
tors, Proc. IEEE int. Test. Conf. (1990) 762-767
8. A.K. Das and P.P. Chaudhuri, Vector space theoretic analysis of additive cellular
automata and its application for pseudo-exhaustive test pattern generation, IEEE
Trans. Comput. 42 (1993) 340-352
9. P. Sarkar, Computing Shifts in 90/150 cellular automata sequences, Finite Fields
Their Appl. 42 (2003) 340-352
10. P. Sarkar, The filter-combiner model for memoryless synchronous stream ciphers,
in Proceedings of Crypto 2002, LNCS, 2442 (2002) 533-548
11. S.J. Cho, U.S. Choi, Y.H. Hwang, Y.S. Pyo, H.D. Kim and S.H. Heo, Computing
phase shifts of maximum-length 90/150 cellular automata sequences, LNCS, 3305
(2004) 31-39
12. S.J. Cho, U.S. Choi, H.D. Kim, Y.H. Hwang, J.G. Kim and S.H. Heo, New synthe-
sis of one-dimensional 90/150 linear hybrid group cellular automata, IEEE Trans.
Comput.-Aided Design Integr. Circuits Syst., 26 (2007) 1720-1724
13. D. Kagaris, F. Makedon and S. Tragoudas, A method for pseudoexhaustive test
pattern generation, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., 13
(1994) 1170-1178
14. D. Kagaris and S. Tragoudas, Avoiding linear dependencies in LFSR test pattern
generators, J. Electron. Testing: Theory Applicat. 6 (1995) 229-241
15. D. Kagaris, Linear dependencies in extended LFSMs, IEEE Trans. Comput.-Aided
Design Integr. Circuits Syst., 21 (2002) 852-859
16. D. Kagaris, A unified method for phase shifter computation, ACM Trans. Des.
Autom. Electron. Syst., 10 (2005) 157-167
17. J. Kakade and D. Kagaris, Minimization of linear dependencies through the use of
phase shifters, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., 26 (2007)
1877-1882
18. S.W. Golomb, Shift register sequences, Holden Day, 1967
19. R. Lidl and H. Niederreiter, Finite fields, Cambridge University Press, 1997
Introducing Service Schemes and Systems
Organization in the Theory of Interactive
Computation
Antoˆnio Carlos da Rocha Costa and Grac¸aliz Pereira Dimuro
Programa de Po´s-Graduac¸a˜o em Informa´tica, ESIN, Universidade Cato´lica de
Pelotas, Rua Felix da Cunha 412, 96010-000 Pelotas, Brazil.
e-mail: {rocha,liz}@ucpel.tche.br
Abstract. This paper introduces the notion of service scheme to allow
for a formal approach to the study of the realizability of services by
interactive systems. It shows how the organization-theoretic conceptual
framework required by the notion of system service can be formally intro-
duced, and shows examples of its application that allow the identification
of some basic service schemes, that seem to be common to every inter-
active system. Finally, it briefly considers the conceptual framework of
the Theory of Interactive Computation proposing that its central prob-
lem is the formal characterization of the realizability of service schemes,
that is, the identification of the class of all service schemes realizable by
interactive computational means.
Key words: Interactive computation, service schemes, systems orga-
nization.
1 Introduction
This paper aims to contribute to the foundations of the Theory of Interactive
Computation (TIC) [1, 2] through the formalization of the notion of service.
Services are understood in their usual intuitive sense, but are introduced as an
organizational notion distinct from other notions such as the notion of organi-
zational role that a component may play in an interactive system (IS), or of the
notion of a behavior that a component may perform in such system.
The paper adopts the common terminology of the area of Organization of
Multiagent Systems (MAS) [3–7] to express the concepts necessary for the for-
malization of the notion of service, but it is not essentially dependent on the
conceptual framework of that area: an agent, as considered here, is simply a
system component, i.e., a computing element integrated in a computing system
where organizational features (roles, services, behaviors, relationships, interac-
tions, etc.) are present. No other implication on the concept of agent is assumed.
We have refrained from adopting any of the current formalisms for describ-
ing interactive systems, like the various versions of process algebras [8, 9], or
equivalent formalisms. Instead, we have adopted an intuitive trace-based frame-
work for the presentation of our concepts, a choice that has led us to conceive
88 Antoˆnio Carlos da Rocha Costa and Grac¸aliz Pereira Dimuro
system processes (i.e., organized sets of actions performed by sets of agents) as
sets of interleaved action streams (IAS). Wherever possible, we have remained
as much informal as we could, in order not to overwhelm the presentation of
the conceptual framework with technical details that are not mandatory at the
current stage of our work. Of course, this informality determines that the results
presented in this paper can not be seen as completely accomplished.
The two main results of the paper are: (1) a definition of the notion of
service scheme, a formal construct that contributes to methodology by allowing
for suitable formal organizational specifications of IS; (2) the suggestion (of a
foundational character) that the true object of the TIC is the mathematical
determination of the class of computable service schems, i.e., the class of services
that can be effectively realized by interactive computational means.
2 Services and the Theory of Interactive Computation
Interactive computation (IC) is computation where data is continually exchanged
between the computing system and its environment. In an IC, the stream of input
information does not need to be completely specified before the computation
starts, and since the specification of the content of the input information may
happen along with the performance of the computation, such specification may
depend on the very stream of output information that it helps to generate.
Circular constructions like that, making the specification of the system’s
input depend on the system’s own output, traditionally called “feedback loop”
in the context of Cybernetics, is neatly out of the scope of the classical models
of algorithmic computation, such as (any non-extended variant of) the Turing
machine model (see [10, 2], for further elaboration of this idea).
Peter Wegner and Dina Golding ([1] and other papers by them, cited there)
have long claimed that interaction takes computing power beyond the level at-
tained by algorithms (i.e., computing based on the classical Turing machine
model). They have stressed the importance of interaction for the right concep-
tion of software structures, and emphasized that interactive computations are
service-oriented, not question answering-oriented.
We have previously argued [2, 10, 11] that the conceptual framework needed
for the explication of service-oriented computations should take as its foundation
the general conceptual framework of the cybernetic-oriented theories of systems
dynamics and organization. This requires taking into account both the con-
cept of behavior, which Theoretical Computer Science has already formalized in
its various theories of concurrent processes (e.g. [8, 9, 12]), and the concept of
role played by a component within a system, which is currently being formal-
ized following various alternative methodological approaches in the area of MAS
(e.g., [4–7]). Besides that, however, that foundational proposal also requires the
formalization of the concept of service (or, systemic function) performed by a
system component to other system components and/or to the system as a whole,
such formalization demanding that the concept of service be considered distinct
from the other two concepts of role and behavior [10, 11].
Service Schemes and Systems Organization in Interactive Computation 89
In this paper we formalize the concept of service by introducing the formal
construct of a service scheme, aiming to further the TIC in this respect.
On the other hand, and as a consequence of this preliminary formalization
of the concept of service, the paper also addresses the foundational question:
“What object is computed when an IC is carried on?” Such question, which
got the definite answer of “a mathematical function”, in the Classical Theory of
Computation, seems to have not yet been adequately settled in the case of the
TIC. We offer it the answer: “a service scheme”.
3 Interleaved Action Streams
In the following, let T = (t0, t1, . . .) be a discrete linear time structure and
∆ = [ti, tj ] ⊆ T , with ti ≤ tj , be a time interval. We take a trace-based approach
to computational behavior, and formally model system processes as (sets of)
interleaved action streams (IAS):
Definition 1. Let Acts be a set of actions that can be performed during ∆. An
action stream performed during ∆ is a partial function as∆ : ∆→ Acts. For any
t ∈ ∆, if as∆(t) is defined, we write as∆(t)↓, and as∆(t)↑ if as∆(t) is undefined.
Let Ag be a set of agents, and ag ∈ Ag. Let Actsag be the set of actions that
agent ag may possibly perform. We denote by as∆ag any action stream that agent
ag may perform during the time interval ∆, employing actions of Actsag.
Definition 2. Let a1, a2 ∈ Ag be two agents. Let as∆a1 and as∆a2 be the two
action streams performed by the agents a1 and a2 during the time interval ∆,
so that for any t ∈ ∆, as∆a1(t)↓ implies that as∆a2(t)↑, and as∆a2(t)↓ implies that
as∆a1(t)↑. The interleaved action stream of agents a1 and a2 during ∆ is defined
as the function ias∆a1,a2 : ∆→ T , given by
ias∆a1,a2(t) =

as∆a1(t) if as
∆
a1(t)↓,
as∆a2(t) if as
∆
a2(t)↓,
undefined otherwise.
In the following, we will often let implicit the reference to the interval ∆.
An alternating action stream established by two agents a1 and a2 is a kind
of interleaved action stream iasa1,a2 where actions of a1 and a2 alternate:
Definition 3. An alternating action stream established by two agents a1 and a2
is an interleaved action stream ias∆a1,a2(t) where, for any sub-interval [ti, tk] ⊆
∆ such that iasa1,a2(ti) ↓, iasa1,a2(tk) ↓ and, for any tj with ti < tj < tk,
iasa1,a2(tj)↑:
− if iasa1,a2(ti) = asa1(ti) then iasa1,a2(tk) = asa2(tk);
− if iasa1,a2(ti) = asa2(ti) then iasa1,a2(tk) = asa1(tk).
90 Antoˆnio Carlos da Rocha Costa and Grac¸aliz Pereira Dimuro
The concept of interleaved action stream can be extended in the obvious
way to sets of agents {a1, a2, . . . , an}, where n ≥ 2. The concept of alternating
action stream can then be correspondingly extended in a natural way to the
concept of cyclicly interleaved action stream. The various sequences of actions
performed during the intervals [ti, tk] which structure the alternation of actions
are informally called the cycles that characterize the action stream.
In an interleaved action stream iasa1,...,an performed by a set of agents
{a1, . . . , an}, the action α performed at time t by agent ai is denoted by 〈ai : α〉t ,
or by 〈ai : α〉, when we are not interested in the time at which the action oc-
curred. A segment (with length k) of an interleaved action stream iasa1,...,an can
be denoted by [〈ai1 : t1 〉α1 , . . . , 〈aik : tk 〉αk ], or simply by [〈ai1 :〉α1, . . . , 〈aik :〉αk].
4 Interactions Modelled with Interleaved Action Streams
Let’s model the behavior of a simple client-server system with the help of IAS.
We assume that the system operates through cycles of requests and service
performances, each cycle servicing a single request from a single client, the server
serving possibly many different clients in a first-come first-served basis. Also,
every performance of the service is initiated by a request from a client.
To structure the notion of request-service cycle, a relation is needed between
request actions and the corresponding response actions. We capture this with
the relation m ⊆ ⋃iActscli × Actss, called an activation relation, relating each
possible response of the server to the request by a client that may initiated it.
Example 1. Consider a server s and a set of clients Cl = {cl1, cl2, . . . , cln}. Let
Actss be the possible actions of the server, and Actscli the possible actions of
the client cli. Let iass,Cl be any interleaved action stream involving the server s
and the clients in the set Cl during the time interval ∆. Then, the client-service
cycle occurs repeatedly at some sub-intervals [ti, tk] ⊆ ∆ as the server performs
services for its clients. It goes on according to the following rules:
− for any time interval [ti, tk] such that iass,Cl(ti)↓, iass,Cl(tk)↓ and, for any
ti < tj < tk, iass,Cl(tj)↑, then: (1) if iass,Cl(ti) = ascl(ti), for some cl ∈ Cl , then
iass,Cl(tk) = ass(tk); (2) if iass,Cl(ti) = ass(ti), then iass,Cl(tk) = ascl(tk), for
some cl ∈ Cl ;
− for any time interval [ti, tk] in which it holds that iass,cl1,...,cln(ti) = ascl(ti)
and iass,cl1,...,cln(tk) = ass(tk), with no defined intervening action between ti and
tk, it happens that ascl(ti)m ass(tk).
Example 2. Let db be a database server and supplier and producer be two of
its clients, and the set of all possible service performances be specified as follows
(Actv is the activation relation):
− Actsdb = {confirm, reply}; Actssup = {update}; Actsprod = {query};
− Actvdb,sup,prod = {update m confirm, query m reply};
− if at time t it holds that iasdb,sup,prod(t)↓ and iasdb,sup,prod(t) = assup(t) =
update then at the next time t′ where iasdb,sup,prod(t′)↓ it should also hold that
iasdb,sup,prod(t′) = asdb(t′) = confirm;
Service Schemes and Systems Organization in Interactive Computation 91
− if at time t it holds that iasdb,sup,prod(t)↓ and iasdb,sup,prod(t) = asprod(t) =
query then at the next time t′ where iasdb,sup,prod(t′)↓ it should also hold that
iasdb,sup,prod(t′) = asdb(t′) = reply .
Then, for instance, the following are possible interactions between the agents:
− [〈sup : update〉 , 〈db : confirm〉 , 〈prod : query〉 , 〈db : reply〉], and
− [〈prod : query〉 , 〈db : reply〉 , 〈prod : query〉 , 〈db : reply〉 , 〈sup : update〉 ,
〈db : confirm〉 , 〈prod : query〉 , 〈db : reply〉].
The activation relation between the actions of the server and the clients
captures the interaction constraints imposed by the nature of the service being
performed by the server. In fact, such activation relation can be taken as the
basis of the formal notion of service scheme, as is done in the next section.
5 Interaction Constraints as Services Schemes
The concept of a service scheme as a constraint imposed on the interactions that
may happen between a server and its clients is established here, in a tentative
way, under the simplifying assumption that the server is able to serve just one
client at a time. This allows us to abstract the set of clients of the service into one
single generalized client, able to represent any client in such set, thus establishing
the concept of generalized interleaved action stream.
Definition 4. Let a1 and a2 be two agents, the server and the (generalized)
client. Let Actsa1 and Actsa2 be their respective set of possible actions. The
service scheme performed by the agent a1 to the agent a2 during the time interval
∆ is given by the structure Srv∆a1,a2 = (Ias
∆
a1,a2 ,m) where m ⊆ Actsa2 × Actsa1
is an activation relation between the actions of a1 and a2, and Ias∆a1,a2 is the set
of all possible alternating action streams iasa1,a2 : ∆→ Actsa1 ∪Actsa2 , so that
the following statements hold, for any iasa1,a2 ∈ Ias∆a1,a2 :− if t is the least time at which iasa1,a2(t)↓ then iasa1,a2(t) = asa2(t) (that
is, any alternating action stream initiates with a client request);
− in any alternating action stream iasa1,a2 ∈ Iasa1,a2 , if at t and t′ there
are iasa1,a2(t) = asa2(t) and iasa1,a2(t
′) = asa1(t
′), with no intervening action
between them, then the activation relation is respected, asa2(t) m asa1(t′), i.e.,
the server’s response is coherent with the (generalized) client’s request.
Ias∆a1,a2 is said to be the generalized alternating action stream underlying the
service scheme Srv∆a1,a2 . We say that the agent a1 performs the service scheme
Srv∆a1,a2 for a2. Also, we say that the interactive system composed of the agents
a1 and (all agents represented by) a2 realizes the service scheme Srv∆a1,a2 .
Definition 5. Given the service scheme Srv∆a1,a2 = (Ias
∆
a1,a2 ,m) performed by
agent a1 to agent a2, the interaction constraint on Ias∆a1,a2 induced by the activa-
tion relation m is the relation B ⊆ Actsa2 ×Actsa1 , such that, for any ias∆a1,a2 ∈
Ias∆a1,a2 and t, t
′ ∈ ∆, if t is such that ias∆a1,a2(t) = as∆a2(t) = α2 and t′ is the
92 Antoˆnio Carlos da Rocha Costa and Grac¸aliz Pereira Dimuro
next time after t at which ias∆a1,a2(t
′)↓ such that ias∆a1,a2(t′) = as∆a1(t′) = α1 and
α2mα1, then 〈a2 : α2 〉t B 〈a1 : α1 〉t
′
. If times t and t′ can be understood from the
context, the interaction constraint on Iass,cl induced by the action α2 of a client
a2 and the action α1 of the server a1 can be denoted by 〈a2 : α2 〉 B 〈a1 : α1 〉.
Srva1,a2 = (Iasa1,a2 ,m) can then be denoted by Srva1,a2 = (Iasa1,a2 ,B).
Definition 6. Two interaction constraints B1 and B2 are said to be equivalent,
denoted by B1 ≡ B2, if and only if the following condition holds: whenever a2
and a4 are clients and a1 and a3 are servers, it holds that 〈a2 : α2 〉 B1 〈a1 : α1 〉
and 〈a4 : α2 〉 B2 〈a3 : α3 〉 if and only if α1 = α3.
Definition 7. Service schemes Srva1,a2 = (Iasa1,a2 ,Ba1,a2) and Srva3,a4 =
(Iasa3,a4 ,Ba3,a4) are said to be equivalent, denoted by Srva1,a2 ≡ Srva3,a4 , if
and only if: (1) the repertoires of actions of their corresponding agents are equal:
rep(a1) = rep(a3) and rep(a2) = rep(a4); (2) their corresponding interaction
constraints are equivalent: Ba1,a2 ≡ Ba3,a4 .
6 Composition of Service Schemes
Here, we introduce a notation to represent compositions of service schemes.
Definition 8. The sequential composition of two services Srv1 and Srv2 is
the service scheme denoted by Srv1 V Srv2. An interleaved action stream ias
satisfies Srv1 V Srv2 during an interval [ti, tk] if and only if there is a time tj,
with ti < tj < tk, such that ias satisfies Srv1 in [ti, tj ] and Srv2 in [tj , tk].
Definition 9. The n-fold repetition of a service Srv (for n ≥ 1) is the service
scheme denoted by Srvn. An interleaved action stream ias satisfies Srvn during
an interval [ti, tk] if and only if there is an n-fold partition of [ti, tk], in the
form [tj0 , tj1 ], [tj1 , tj2 ] . . . , [tjn−1 , tjn ], where tj0 = ti and tjn = tk, such that ias
satisfies S in each interval [tji , tji+1 ].
Of course, Srvn = Srv V . . . V Srv (n times). Also, we denote by Srv+ an
undefined number of repetitions of Srv, but including at least one repetition.
Definition 10. The non-deterministic choice between two service schemes Srv1
and Srv2 is denoted by Srv1 unionsq Srv2. An interleaved action scheme ias satisfies
the service scheme Srv1unionsqSrv2 during an interval [ti, tk] if and only if it satisfies
service scheme Srv1 or it satisfies service scheme Srv2.
7 Basic Service Schemes
Example 3. Let prod be the producer agent and con represent any of its con-
sumer agents. Consider that Actsprod = {produce, accept delivery request , deliver ,
deny delivery request , delivery denied} and Actscons = {delivery request , receive,
consume}. Then, the service is characterized by the interaction constraint:
[〈cons : delivery request〉 B [〈prod : deliver〉 unionsq 〈prod : delivery denied〉]]+.
Service Schemes and Systems Organization in Interactive Computation 93
We note that characterizing a service is not the same as characterizing the be-
haviors of the server and its clients. It’s just characterizing how the server and the
client interact. Thus, in Ex. 3, the service performed by the producer is the deliv-
ery of the product to the client, not the full working necessary for the production
of the product (which includes actions like produce and accept delivery request).
The latter is the behavior that an agent has to perform in order to fulfill the role
of producer. A similar conclusion can be drawn with respect to the consumer.
Example 4. Now, we model both the service scheme performed by an intermedi-
ary agent int that delivers products to a generalized consumer agent cl (the de-
livery of the products that it will consume) and the service performed by the pro-
ducer agent prod to the intermediary agent int (the delivery of the products that
the latter will deliver to the consumers). Let Actscons = {delivery request , receive,
consume}, Actsprod = {produce, accept delivery request , deliver , deny delivery request ,
delivery denied}, Actsint = {delivery request , receive, accept delivery request , deliver ,
deny delivery request , delivery denied}. The service schemes may be character-
ized by [〈int : delivery request〉 B [〈prod : deliver〉 unionsq 〈prod : delivery denied〉]]+, and
[〈cons : delivery request〉 B [〈int : deliver〉 unionsq 〈int : delivery denied〉]]+.
The interaction constraint that characterizes the service performed by the
producer to the consumer in Ex. 3 has the same structure as the interaction
constraint which characterizes the service performed by the producer to the
intermediary, and that of the intermediary to the consumer, in Ex. 4. Thus:
Definition 11. Two agents a1 (the deliverer) and a2 (the generalized receiver),
realize the delivery service scheme if their generalized alternating action stream
Iasa1,a2 satisfies the interaction constraint:
[〈a2 : delivery request〉 B 〈a1 : deliver〉 unionsq 〈a1 : delivery denied〉]+.
Let’s now examine the case of agents that receive objects delivered to them
by other agents. Let rec be an agent that is able to receive objects sent to it by
agent sen. Let Actsrec = {accept reception request , receive, deny reception request ,
reception denied} and Actssen = {reception request , deliver}. The service scheme
realized by rec and sen can then be characterized by:
[〈sen : reception request〉 B [〈rec : receive〉 unionsq 〈rec : reception denied〉]]+.
This motivates the general definition:
Definition 12. Two agents, a1 (the receiver) and a2 (the generalized sender),
realize the reception service scheme if their generalized alternating action stream
Iasa1,a2 satisfies the interaction constraint:
[〈a2 : reception request〉 B [〈a1 : receive〉 unionsq 〈a2 : reception denied〉]]+.
We note that in the reception service scheme the initiative of realization of
the service scheme is taken by the agent that delivers the object, while in the
delivery service scheme the initiative is with the agent that receives the object.
This is so because in any service scheme the initiative towards the realization of
the scheme is always taken by a client of the service, not the server.
94 Antoˆnio Carlos da Rocha Costa and Grac¸aliz Pereira Dimuro
Example 5. Consider the case of agents that store objects for other agents, so
that objects can be sent for storage at some time and got back later, when re-
quested. Let sto be an agent that is able to store objects for a generalized client
cl. Let Actssto = {accept reception request , receive, accept delivery request , deliver ,
deny reception request , reception denied} and Actscl = {reception request , deliver ,
delivery request , receive}. Then, the service performed by sto to cl is characterized
by (using exponents as repetition operators, with m ≥ n):
[[〈cl : reception request〉 B [〈sto : receive〉 unionsq 〈sto : reception denied〉]]m V
[〈cl : delivery request〉 B 〈sto : deliver〉n]+
This motivates the general definition:
Definition 13. Two agents, a1 (the store) and a2 (the accessor), realize the
storage service scheme if their generalized alternating action stream satisfies the
service scheme, for m ≥ n:
[[〈a2 : reception request〉B〈a1 : receive〉]mV [〈a2 : delivery request〉B〈a1 : deliver〉]n]+
The following proposition illustrates the equivalence of service schemes:
Proposition 1. Let
1)Storagea1,a2 = (Iasa1,a2 ,B
Sto
a1,a2), where B
Sto
a1,a2 is given by (for m ≥ n):
[[〈a2 : reception request〉B〈a1 : receive〉]mV[〈a2 : delivery request〉B〈a1 : deliver〉]n]+
2)Receptiona1,a2 = (Iasa1,a2 ,B
Rec
a1,a2), where B
Rec
a1,a2 is given by:
〈a2 : reception request〉 B 〈a1 : receive〉
3)Deliverya1,a2 = (Iasa1,a2 ,B
Del
a1,a2), where B
Del
a1,a2 is given by:
〈a2 : delivery request〉 B 〈a1 : deliver〉
Then, one notice that BStoa1,a2 ≡ [[BReca1,a2 ]m V [BDela1,a2 ]n]+, and so:
Storagea1,a2 ≡ [[Receptiona1,a2 ]m V [Deliverya1,a2 ]n]+.
Example 6. Lendinga1,a2 = [Deliverya1,a2 V Receptiona1,a2 ]+, the lending ser-
vice scheme, is a compound service scheme where agent a1 (the lender) lends an
object to the agent a2 (the borrower), which later returns the lent object.
With the help of the above examples, we claim that the reception and delivery
service schemes are two of the basic service schemes that can be found in any
interactive computing system.
8 Organizing Interactive Systems with Service Schemes
As stated in the Introduction, and stressed in Sect. 7, we think that interactive
computations cannot be fully explained by simply adding the concept of inter-
action to the conceptual framework usually employed in the understanding of
classical computations. As established in [10] and [11], and further elaborated
in [2], it seems to us that the explication of interactive computing requires the
framing of interactive systems in a more complex conceptual framework, where
the concept of organization [2, 10, 13] should have a preeminent role.
Organization is a concept highly worked out in areas like Biology and Soci-
ology. It involves many component concepts, such as:
Service Schemes and Systems Organization in Interactive Computation 95
1. organizational role that, as explained in Sect. 7, concerns the full behavior
that an agent may have to perform when participating in an adequate way
in the organization;
2. organizational function [11], which can be equated to the notion of service
that we have been using here;
3. organizational process [3], which can be equated to the notion of generalized
IAS that was formally introduced here;
4. organizational link, which concerns the way organizational roles interact [3],
and which has a strong relationship to the notion of service scheme, also
formally introduced here.
Disentangling the concept of service from the concepts of role and behav-
ior should help the methodology of interactive systems design, as the different
concerns of those organizational concepts should allow for their different method-
ological imports. It also allows the advancing of issues related to the structural
dynamics of systems, and the consequent possibility of introducing into the dy-
namics of the system a notion of system development [2, 3, 10, 13, 14].
9 Related Work
A connection can be established between the work presented here and WSCL
(Web Service Conversation Language), a language defined by the W3C (World
Wide Web Consortium) for the abstract specification of service interfaces, whose
definition is available at http://www.w3.org/TR/wscl10/.
In WSCL, a service interface is a conversation (i.e., an exchange of XML
documents) that the server and its clients should realize to get the service per-
formed. A conversation is essentially defined as a set of two types of elements:
interactions (i.e., operations of exchange of documents) and transactions (i.e.,
pairwise temporal orderings of interactions). A conversation specifies all the pos-
sible sequences of document exchanges that allow for the successful performance
of the service (as well as the sequences that lead to failure in such performance).
The way to connect WSCL and service schemes seems to be immediate, by
making service schemes serve as statements of formal behavioral requirements
for WSCL conversations: an interaction constraint of a service scheme states a
behavioral requirement for a WSCL transaction, and a service composition of a
service scheme states a behavioral requirement for a WSCL interaction.
10 Conclusion: On the Object of the TIC
With the formalization of the concept of service scheme, this paper also aims
to contribute to a foundational issue: it proposes that the object computed by
an interactive system is, simply, a service scheme. Computability, in the con-
text of interactive computations, is not a property of mathematical functions
or relations, as it was in the context of classical computations, but is a prop-
erty of schemes of system services: computability in the context of interactive
computations is synonymous of realizability of service schemes.
96 Antoˆnio Carlos da Rocha Costa and Grac¸aliz Pereira Dimuro
The fundamental question of the TIC seems to be, thus: What is the formal
characterization of the class of all service schemes that are effectively realizable
by interactive systems?
Acknowledgements. This work was supported by FAPERGS and CNPq. We are
grateful to the referees for the comments that helped us to improve the paper.
References
1. Goldin, D., Smolka, S., Wegner, P., eds.: Interactive Computation: The New
Paradigm. Springer-Verlag, New York (2006)
2. Costa, A.C.R., Dimuro, G.P.: Interactive computation: Stepping stone in the path-
way from classical to developmental computation. ENTCS 141(5) (2005) 5–31
3. Demazeau, Y., Costa, A.C.R.: Populations and organizations in open multi-agent
systems. In: Proc. 1st. Symp. Parallel and Distributed AI, Hyderabad, India (1996)
4. Zambonelli, F., Jennings, N.R., Wooldridge, M.: Developing multiagent systems:
the Gaia methodology. ACM Transactions on Software Engineering and Method-
ology 12(3) (2003) 317–370
5. Hu¨bner, J.F., Sichman, J.S., Boissier, O.: A model for the structural, functional,
and deontic specification of organizations in multiagent systems. In Bittencourt,
G., Ramalho, G.L., eds.: Advances in Artificial Intelligence: 16th Braz. Symp. on
Artificial Intelligence. Volume 2507 of LNAI., Berlin, Springer (2002) 118–128
6. Dignum, V., Va´zquez-Salceda, J., Dignum, F.: Omni: Introducing social structure,
norms and ontologies into agent organizations. In Bordini, R.H., Dastani, M., Dix,
J., Seghrouchni, A.E.F., eds.: Programming Multi-Agent Systems: 2nd. Intl. Work.
ProMAS 2004. Volume 3346 of LNCS., New York, Springer (2004) 181–198
7. Ferber, J., Gutknecht, O., Michel, F.: From agents to organizations: an organiza-
tional view of multi-agent systems. In Giorgini, P., Mu¨ller, J.P., Odell, J., eds.:
Agent-Oriented Software Engineering IV. Volume 2935 of LNCS., Berlin, Springer
(2004) 214–230
8. Milner, R.: Communication and Concurrency. Prentice-Hall, New Jersey (1989)
9. Hoare, C.A.R.: Communicating Sequential Processes. Prentice-Hall, NJ (1985)
10. Costa, A.C.R.: Machine Intelligence: sketch of a constructive approach. PhD thesis,
CPGCC/UFRGS, Porto Alegre (1993) (in Portuguese).
11. Costa, A.C.R., Castilho, J.M.V., Claudio, D.M.: Toward a constructive notion of
functionality. Cybernetics and Systems 26(4) (1995) 443–480
12. Reisig, W.: Petri Nets, An Introduction. Springer-Verlag, Berlin (1985)
13. Costa, A.C.R., Dimuro, G.P.: Semantical concepts for a formal structural dynamics
of situated multiagent systems. In Sichman, J., Noriega, P., Padget, J., Ossowski,
S., eds.: Coordination, Organizations, Institutions, and Norms in Agent Systems
III. Number 4870 in LNAI, Berlin (2008) 139–154
14. Dimuro, G.P., Costa, A.C.R.: Toward a domain-theoretic model of developmen-
tal machines. In Cooper, S.B., Kent, T.F., Lo¨we, B., Sorbi, A., eds.: CiE 2007:
Computation and Logic in the Real World. Quaderni del Dipartimento di Scienze
Matematiche e Informatiche Roberto Magari of the Univ. of Siena (2007) 114–122
Online-division with Periodic Rational Numbers
G. de Miguel Casado1?, J.M. Garc´ıa Chamizo2, and H. Mora Mora2
1 GISED, University of Zargoza, Spain
{gmiguel}@unizar.es
2 I2RC-SPALab, University of Alicante, Spain
{juanma,hmora}@dtic.ua.es
Abstract. This paper discusses different approaches for exact rational
arithmetic and proposes a novel representation for periodic rational num-
bers with double mantissa (fixed and periodic) based on signed-digit
arithmetic. The representation is proposed under the scope of Type-2
Theory of Effectivity (TTE) and the extension of an online-arithmetic
algorithm for division is analyzed.
Key words: computer arithmetic algorithms; Type-2 Theory of Effec-
tivity
1 Introduction
Data codification formats in digital computers define the features of the numeric
sets and the arithmetic operations which support higher abstraction level opera-
tions required by computer applications [19]. In this context, positional fractional
codifications provide a direct way to express rational numbers by means of both
integer and fractional parts. One of the most representative formats is the float-
ing point specification IEEE754, which has been adopted as standard in most of
general purpose computer systems and is currently under revision. After some
years, the lack of reliability of this standard for scientific computing applications
[16, 20] has motivated the development of alternative methods for numerical data
codification and operation [2, 10]. Among the whole research in the topic, some
researchers in computer arithmetics have focused their efforts in symbolic com-
puting with algebraic expressions [17]. This approach has led to proposals in
which rational numbers are represented by means of fractions [13, 14]. However,
its main drawback is the fact that there are no low complexity algorithms to
obtain irreducible fractions (Euclid’s Greatest Common Divisor Algorithm) and
therefore the codification redundancy cannot be easily restrained [4]. In addition,
some operators such as comparators, outline a high complexity when compared
with those based on fractional positional notations.
The continuous fractions approach also offers an exact representation method
for rational numbers. In this case, the codification of numbers is performed by
successive fractions [5]. However, early hardware designs [17] outlined a high
? The authors thank the Spanish Education Ministry for financial support (Project
TIN2005-08832-C03-02)
98 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora
complexity of the arithmetic operations involved [21]. As it happened in sym-
bolic computing approaches, if a numeric result in fractional positional notation
is required, some additional operations have to be performed and then, impre-
cisions in expression translations may appear. Another interesting proposal for
error-free codification of rational numbers is based on the explicit representa-
tion of the periodic development of fractional numbers [12]. Nevertheless, as this
research was limited to the theoretical formulation and no suitable procedures
and architectures where developed at that time, no interest was shown by the
scientific community.
The online arithmetic approach, based on signed digit arithmetic, deals with
the hardware implementation of digit-serial left-to-right (online or Most Signifi-
cant Bit First) arithmetic operators for signed digit numbers [9]. As the operation
dynamics resembles that of the Turing Machine model, a conceptual conver-
gence with Type-2 Theory of Effectivity (TTE) can be realized [22, 6]. Under
this scope, partial implementation of Turing Machines can be done (according
to the limited memory resources) and therefore, hardware design support for
scientific computing tasks can be developed.
Let us consider the set-builder notation of the set of rational numbers Q :={
a
b | a, b ∈ Z, b 6= 0
}
3. A rational number can be expressed as a fraction ab where
a and b are integers and b 6= 0. The set Q is countable, continuous and dense
in R. In Type-2 Theory of Effectivity, Q stands for the basis for higher ab-
straction level representations: the Cauchy representation of real numbers, some
computable topological spaces, continuous functions in C [0; 1], Lp Spaces and
Sobolev Spaces, to name a few [22, 15, 23]. There are, at least, two standard no-
tations for the rational numbers: νQ [22, Def. 3.1.2] for a rational representation
derived from the set-builder notation and νsd [22, Def. 7.2.4] for a fractional posi-
tional notation for non periodic binary rational numbers. A normalized notation
for these numbers is also proposed in [7].
This paper is focused on the proposal of a novel codification of rational num-
bers which aims for a feasible hardware implementation of operators based on
online-arithmetic. We propose the extension of the fractional positional notation
νsd for non periodic binary rational numbers so that to include the periodic ones.
In addition, normalization criteria is proposed for the representation so that to
match the digital computer design criteria standards, which aim for simplifying
hardware designs as well as for improving operand memory packaging and stor-
age. An extension of online algorithms for the basic rational number operations
is proposed.
The paper is arranged in the following sections: after the introduction and
the preliminary section, Section 3 develops a fractional positional notation for
the rational numbers with double mantissa νdmQ , Section 4 deals with the mod-
ified online-division algorithm for these periodic rational numbers in fractional
positional notation and, finally, Section 5 summarizes the conclusions.
3 The symbol Z derives from the German word Zahl (number) [8] and Q derives from
the German word Quotient (ratio) [3, p. 671].
Online-division with Periodic Rational Numbers 99
2 Preliminaries
A fractional positional representation provides a direct value number, which is a
desirable feature for a numeric codification format. The codification of rational
numbers can be effectively performed with an exact positional representation,
as it can be expressed by means of a finite concatenation of digits such as:
∀w ∈ Q⇔ ∃ i, j, k|w = w0 . . . wi  wi+1 . . . wjwj+1 . . . wkwj+1 . . . wk . . . , (1)
where i, j, k ∈ N, such that ∀wl ∈ w,wl ∈ Γn, Γn = {0, . . . n− 1} and n ≥ 2
is the number base.
This codification is based on the concatenation of a sequence of digits around
the fractional point (fixed mantissa w0 . . . wi  wi+1 . . . wj) and a periodic series
of digits (periodic mantissa wj+1 . . . wk). The shortest periodic sequence which
is repeated is called the period. Conventionally, this periodic part is represented
only once, embraced by an upper circumference arch, which permits the exact
codification of a rational number with a finite number of symbols:
w = w0 . . . wi  wi+1 . . . wj ̂wj+1 . . . wk. (2)
According to the features of the period, the following cases can be considered:
– All the periodic digits are zero (wj+1, . . . , wk = 0). Then, the fixed mantissa
codifies the exact rational number: w = w0 . . . wi  wi+1 . . . wj .
– All the periodic digits are equal to the highest digit of the number base n.
Then, the number is codified by adding one unit in the last position (ulp)
to the less significative digit of the fixed mantissa and removing the periodic
digit w = w0 . . . wi  wi+1 . . . wj + ulp, with ulp = v0 . . . vi  vi+1 . . . vj−1vj ,
v0, . . . , vi, vi+1, . . . vj−1 = 0 and vj = n− 1.
The length of the periodic mantissa p = k − (j + 1) is tightly related to
the denominator of the irreducible fraction with respect to the set-builder based
representation of a rational number. Particularly is less than the denominator
value:
∀w ∈ Q, w = ab , a, b ∈ Z , b 6= 0, gcd (a, b) = 1⇒
w = mfm̂p ∧ p = Ψ (b) < b, (3)
where mf = w0 . . . wi  wi+1 . . . wj , mp = ̂wj+1 . . . wk, gcd is the greatest
common divisor of the numerator a and the denominator b, and Ψ (b) is a function
based on the congruence relationship (≡) [1, 11]:
Ψ (b) ≡
{
1mod(b) if gcd(n, b) = 1,
1mod(b′) if gcd(n, b) > 1, (4)
where mod is the modular operator (integer division). Notice that when
gcd(n, b) > 1 follows that ∃b′,m ∈ N | b = m · b′ ∧ gcd(n, b′) = 1 ∧ m =∏p
i=1z
j
i , i > 0 ∈ N, z1 . . . zp ∈ N is the set of prime factors of the number base
n and j ∈ N such as ab = 1m ab′ ∧ gcd(n, b′) = 1 ∧ Ψ (b) ≡ 1mod(b′).
100 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora
According to the modular arithmetic rules, as 1m is composed by prime factors
z1 . . . zp ∈ N of the base n it makes p = 0. There exist infinitely many values
which fulfill the previous congruence relationship, as every multiple of p satisfies
the equation:
Ψ (b) ≡ 1mod (b) =⇒ ∀j ∈ N, nj·p ≡ 1mod (b) (5)
This fact justifies that every digit group which includes the period several
times and whose length is a multiple of p is itself a period. In addition, all the
rational numbers are periodic, despite the fact that the period is not codified
when all the digits are 0. The amount of periodic numbers with p > 0 is related
with the base of the rational number to be codified.
There exist rational numbers in decimal with p = 0 which in binary base
produce a number with p > 0, whereas 2 is a prime factor of 10 and then all the
binary number with p = 0 produces, at the same time, a decimal number with
p = 0. This has a great impact in numerical codification, because in the domain
of the binary numbers there are fractional numbers with period 0 which can
generate errors in the binary codification because the impossibility of codifying
infinite fractional digits.
Definition 1. (Periodic number) A periodic number is a rational number
whose periodic mantissa has length greater than 0 (p > 0) such that
w ∈ Q, ∃f, p ∈ N | w = mfm̂p, (6)
where mf = w0 . . . wi  wi+1 . . . wj , m̂p = wj+1 . . . wkwj+1 . . . wk . . ., i, j, k ∈ N,
such that ∀wl ∈ w,wl ∈ Γn, Γn = {0, . . . n− 1}, n ≥ 2, and n is the number
base.
Definition 2. (Non periodic number) A non periodic number is a rational
number whose periodic mantissa has length 0 (p = 0) such that
w ∈ Q, ∃f, p ∈ N | w = mf , (7)
where mf = w0 . . . wi  wi+1 . . . wj , m̂p = λ (empty string), i, j ∈ N, such that
∀wl ∈ w,wl ∈ Γn, Γn = {0, . . . n− 1}, n ≥ 2, and n is the number base.
Definition 3. (Pure periodic number) A pure periodic number is a rational
number whose fixed mantissa is 0 (f = 1) such that
w ∈ Q, ∃f, p ∈ N | w = mfmp, (8)
where mf = 0, m̂p = wj+1 . . . wkwj+1 . . . wk . . ., j, k ∈ N, such that ∀wl ∈
w,wl ∈ Γn, Γn = {0, . . . n− 1}, n ≥ 2, and n is the number base.
Now, consider the following notations and representations in TTE.
Let Σ = {1, 0, 1} and let Σ∗ and Σω denote the sets of finite and infinite
sequences over Σ, respectively. We always assume that there exists an element
# /∈ Σ and denote Σ# := Σ ∪ {#}. Let ιw : Σ∗ → Σ∗# be a wrapping function
Online-division with Periodic Rational Numbers 101
that assigns to a word u = u0 . . . uk ∈ Σ∗ with u0, . . . , uk ∈ Σ and k ∈ N the
word ιw(u) := ##u0#u1#...#uk## and let ιu : Σ∗# → Σ∗ be the correspond-
ing unwrapping function that obtains from a word v ∈ Σ∗# the word ιu(v) := w
with w = w0 . . . wk ∈ Σ∗, and k ∈ N.
Definition 4. (Some standard notations and representations)[22, Def.
3.1.2 (1-4)].
1. The identity idΣ∗ : Σ∗ −→ Σ∗ is a notation of Σ∗ and the identity idΣω :
Σω −→ Σω is a representation of Σω.
2. Define the binary notation νN :⊆ Σ∗ −→ N of the natural numbers by:
dom (vN) := {0} 1 {0, 1}∗and νN (ak, ..., a0) =
k∑
i=0
ai · 2i, (a0, ..., ak ∈ {0, 1}).
3. Define a notation νZ :⊆ Σ∗ −→ Z of the integers by: dom (νZ) := {0} 1 {0, 1}∗∪
−1 {0, 1}∗ , νZ (w) = νN (w) and νZ (−w) = −νN (w) for w ∈ dom (νN) \ {0}.
4. Define a notation νQ :⊆ Σ∗ −→ Q by:
dom (νQ) := {u/v | u ∈ dom (νZ) , v ∈ dom (vN) , vN (v) 6= 0} and νQ (u/v) :=
νZ (u) /νN (v). Later we will abbreviate νQ (w) by w.
Definition 5. (Signed digit notation and representation) [22, Def. 7.2.4].
Let 1 ∈ Σ abbreviate -1. Define the signed digit notation νsd of the binary ra-
tional numbers and the signed digit representation ρsd :⊆ Σ∗ −→ R of the real
numbers as follows:
dom (ρsd) :=

all an...a0  a−1a−2... ∈ Σω such that n ≥ −1,
ai ∈
{
1, 0, 1
}
for i ≤ n,
an 6= 0, if n ≥ 0 and anan−1 /∈
{
11, 11
}
if n ≥ 1,
dom (νsd) := {u  v | u, v ∈ Σ∗, u  v0w ∈ dom (ρsd)} ,
ρsd (an...a0  a−1a−2...) :=
−∞∑
i=n
ai · 2i,
νsd (u  v) := ρsd (u  v0w) .
For p := an...a0  a−1a−2... ∈ dom (ρsd) and k ∈ N define p [k] := an...a0 
a−1...a−k ∈ Σ∗.
3 Fractional Positional Notation for Periodic Rational
Numbers
Define the notation νsdZ for codifying integer numbers in signed digit notation:
νsdZ :⊂ Σ∗ −→ Z,
dom
(
νsdZ
)
:=
all an . . . a0 ∈ Σ
∗ for n ≥ 0,
ai ∈ Σ for i ≤ n,
an 6= 0, if n ≥ 0 and anan−1 /∈
{
11, 11
}
, if n ≥ 1,
νsdZ (en . . . e0) := ρsd (en . . . e00
w) .
(9)
102 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora
Now, define a fractional positional notation νnsdQ of signed digit rational num-
bers, such as for a ∈ Q then 0 ≤ a < 1.
νnsdQ :⊂ Σ∗ −→ A = {a | 0 ≤ a < 1} ∩Q
dom
(
νnsdQ
)
:= {u  v | u = 0, v ∈ Σ∗, u  v0w ∈ dom (ρsd)} ,
νnsdQ (u  v) := ρsd (u  v0w) .
(10)
Finally, define the notation νdmQ for normalized fractional positional periodic
rational numbers with double mantissa (fixed and periodic) as:
νdmQ :⊂ Σ∗ −→ Q
dom
(
νdmQ
)
:=
{
ιw (e) ιw (mf ) ιw (mp) | e,mp ∈ dom
(
νsdZ
)
and
mf ∈ dom
(
νnsdQ
)}
,
νdmQ (uvw) :=
{
q = ab | a = νsd (2e) · νsd (mfmp −mf ) with
e = ιu (u) ,mf = ιu (v) ,mp = ιu (w) , and b = {1}p {0}f with
f = |mf | , p = |mp|} ,
(11)
for a signed digit notation in radix-2.
Theorem 1. The notation νdmQ induces the same concept of computability as
the standard notation νQ (νdmQ ≡ νQ)
Proof. (νQ ≤ νdmQ ). The translation from the notation νQ into νdmQ notation can
be done straight away by applying a modified version of the conventional school
division algorithm in which, when the fractional part is being obtained, we check
the repetition of partial remainders. Notice that this key issue is used for the
online-division algorithm proposed in Section 4. Therefore, we wrap the partial
remainders pr ∈ dom (νZ) and then successively concatenate them with a given
word r ∈ Σ∗# ∪ {λ} , which we initialize to λ (r = rιw (pr)). On every iteration
related with the calculation of the fractional part, we check if the wrapped partial
remainder ιw (pr) . In that case, we find the position of the subword by means
of a subword count function c : Σ∗# ∪{λ}×Σ∗# → Z, which outputs the number
of words counted i ∈ N when the partial remainder subword was found and then
the algorithm stops. Notice that, as usual, the algorithm can also stop when the
partial remainder is zero (pr = 0). If the partial remainder is not found (pr is
not a subword of r), then output −1.
Now, let q ∈ dom (νQ) be the quotient obtained from the division, mf ∈
dom (νsd) the fixed mantissa and mp ∈ dom
(
νsdZ
)
the periodic mantissa. Then
mf and mp are the following prefix and suffix from q, which are now translated
into signed digit based notations:
mf = νsd (q0 . . .  . . . qi) ,
mp = νsdZ (qi+1 . . .  . . . qj) ,
(12)
where j = |q| − 1.
Now, in order to normalize the rational number obtained:
Online-division with Periodic Rational Numbers 103
Define the count function c : dom (νsd)→ dom (νN) , which outputs a natural
number corresponding to the position of the dot ”  ” in the input word. If the
dot ”  ” is not found, then the function outputs the length of the word.
Define the function a : Σ∗ × dom (νN) → Σ∗ × dom (νZ) which removes the
0s at the beginning of an input string u ∈ Σ∗ and successively decrements the
input n ∈ dom (νN). This function outputs the remaining string m ∈ Σ∗ and the
decremented input n, which can be negative (z ∈ dom (νZ)).
Finally, define the function i : Σ∗ × dom (νZ)→ dom (νsd) as
i (u, z) :=

0  {0}z u if z − 1 ≤ 0,
u0 . . . uz−1  uz . . . uk if 0 < z − 1 < νZ (k) ,
u0 . . . u  {0}z−νZ(k)−1 if z − 1 ≥ νZ (k) ,
(13)
where u = u0 . . . uk ∈ Σ∗, z ∈ dom (νZ) and k ∈ N.
Then, the exponent is obtained in the following way:
1. Count the number of positions from the beginning of the string until the dot
character ”  ” is found with the function c : dom (νsd)→ dom (νN). This will
provide the initial exponent.
2. Remove the dot character ”  ” from the string and then remove the 0s at
the beginning of it. At the same time, while removing the 0s, successively
decrement the exponent by applying the function a : Σ∗×dom (νN)→ Σ∗×
dom (νZ). The mantissa and the exponent at the output are the remaining
chunk of the string and the decremented exponent, respectively.
3. Apply the wrapping function in order to wrap the exponent the fixed man-
tissa and the periodic mantissa:
ιw(e) := u = ##e0#e1#...#ek##,
ιw(mf ) := v = ##mf,0#mf,1#...#mf,k##,
ιw(mp) := w = ##mp,0#mp,1#...#mp,k##,
and then concatenate u and v such as w = uv ∈ dom (νnsd) .
Proof. (νdmQ ≤ νQ). The translation from νdmQ into νQ can be directly achieved
by using TTE-computable string manipulation functions.
Define the search function s : Σ∗# ×N→ Σ∗ such as for a word w ∈ Σ∗# and
j ∈ N it obtains the corresponding unwrapped subword from w :
s (w, j) :=
{
ιu (u) | u is the subword j in w with u ∈ Σ∗# and j ∈ N
}
. (14)
1. For w ∈ dom (νdmQ ) , apply the search function s : Σ∗# × N → Σ∗ to obtain
the exponent e = s (w, 0) , the fixed mantissa mf = s (w, 1) and the periodic
mantissa s (w, 2).
2. Define q = νZ(a)νN(b) where a = νsd (2
e) · νsd (mfmp −mf ) ∈ dom (νsd) , and
b = {1}p {0}f with f = |mf | , p = |mp| .
Then, as νQ is reducible to νdmQ (νQ ≤ νdmQ ) and also νdmQ is reducible to νQ
(νdmQ ≤ νQ) it can be concluded that νdmQ is equivalent to νQ (νdmQ ≡ νQ).
104 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora
4 Online Division with Periodic Rational Numbers
The computability of the general algorithm for developing online-arithmetic op-
erators is absolutely concerned with the possibility to reduce the residual re-
currence equation to additions and shifts [9, Chap. 9]. Among all the basic
algorithms (addition, subtraction, multiplication and division) the case of the
division outlines a great variety of cases. The extended online division algorithm
for normalized signed digit operands X,D (0 < X,D < 1) with 4 delay cycles
for the initialization is the following:
Online Division Algorithm
REGQ, REGD, v, w := 0 ’Initialize
(start CP,modul) :=MaxOverlap(X,D)
For it := 0 To precision
x := ObtainDigit(. . . .); d := ObtainDigit(. . .)
REGD := AppendOnline(REGD, d)
If it < kDELAY DIV SD Then ’Initialization stage
v := 2w + x · 2−kDELAY DIV SD
w := v
Else ’For the rest of iterations
v := 2w + x · 2−kDELAY DIV SD−REGQ · d · 2−kDELAY DIV SD
q := SELD(v)
w := v − q ·REGD
exit := CheckPeriod(. . .)
If exit = True Then
Exit For;
Else
REGQ := AppendOnline(REGQ, q)
EndIf
EndIf
Next it
The extension of the algorithm is based on two functions. The function
MaxOverlap(X,D) calculates the iteration (start CP ) at which the residual is
stored for a later identification of the periodic mantissa with a predictable itera-
tion modulus (modul). The function CheckPeriod(. . .) evaluates every matching
residue w at a given iteration so that to identify the repetition of a periodic man-
tissa. For the latter function, the following cases have to be considered:
1. X,D are non-periodic. After storing the residual value res := w at the iter-
ation given (it = start CP ), the residual is checked iteration after iteration
so that to find either w = 0 (non-periodic result) or res = w (periodic result
of with mantissa length lmp = it− start CP ).
2. X is periodic andD is non-periodic. After storing the residual value res := w
at the iteration given (it = start CP ), the residual is checked at the itera-
tions which fulfill the condition mod (it,modul)= 0 so that to find res = w
(periodic result of with mantissa lengthlmp = it− start CP ). The value of
Online-division with Periodic Rational Numbers 105
modul assigned inMaxOverlap(X,D) is the length of the periodic mantissa
of X.
3. X is non-periodic andD is periodic. After storing the residual value res := w
at the iteration given (it = start CP ), the residual is checked at the itera-
tions which fulfill the condition mod (it,modul)= 0 so that to find res− w <
ulpres (periodic result of with mantissa length lmp = it− start CP ). The
value of modul assigned in MaxOverlap(X,D) is the length of the periodic
mantissa of D.
4. X,D are periodic. The case is similar to the previous one, except that the
value of modul assigned in MaxOverlap(X,D) is the maximum of length
of the periodic mantissas X and D, and the result is periodic with mantissa
length lmp = it− start CP.
The algorithm initializes the internal variables REGQ, REGD, v and w
which hold the quotient, the divisor as well as the estimator and residual, re-
spectively. In the main loop, the following leftmost input bits from the operands
X and D are obtained (x and d, respectively). The divisor digit obtained is con-
catenated to the divisor register REGD. Then, two stages are considered: first,
initialization of the residual (it < kDELAY DIV SD, the first 4 cycles) and
second, period check and/or digit output estimation. For the second stage, the
resulting digit at this iteration is obtained with the selection function SELD(.)
applied to the estimation value v and the new value of the residual is calculated.
Then the conditions for checking the period are analyzed in CheckPeriod(. . .).
If the residual is not repeated, the resulting digit is appended to the output
quotient register REGQ with the function AppendOnline(...) and the algorithm
performs a new iteration, unless it reaches the maximum iterations fixed.
5 Conclusions
A computable fractional positional notation for periodic rational numbers has
being proposed. It aims to improve the comparison operations as well as to reduce
memory storage by managing periodic mantissas. With this approach, the space
of representation for a fixed precision is increased, as it is possible to emulate the
infinite digits of a periodic rational number with a fixed length periodic mantissa.
The algorithm proposed for the division identifies the periodic mantissa of the
result before its first repetition by exploiting the concept of the residual w as the
inner state of the algorithm. Similar results can be obtained with the addition,
subtraction and multiplication. The complexity of the operations introduced
implies: obtaining the algorithm iteration at which the singular residual has to
be stored, calculating the iteration modulus step at which the next residuals have
to be checked and subtracting and comparing at the iteration check established.
The encouraging experimental results obtained outline the interest of formal-
izing the periodic mantissa identification using the congruence relationship based
on the Fermat’s Little Theorem (generalized by Euler) and also of extending the
algorithms to high-radix number bases as well as to other arithmetic operations.
106 G. de Miguel Casado, J.M. Garc´ıa Chamizo, and H. Mora Mora
References
1. Belski, A.A. and Kaluzhnin L.A.: Divisio´n inexacta, Lecciones populares de
matema´ticas. Ed. Mir, Moscow (Spanish Translation), (1980)
2. Blanck, J.: Exact real arithmetic systems: Results of competition, Computability
and Complexity in Analysis, LNCS, Vol. 2064, (2001) 390–394
3. Bourbaki, N.: E´le´ments de mathe´matique: Alge`bre. Reprinted as Elements of Math-
ematics: Algebra I. Berlin: Springer-Verlag, (1998)
4. Buchberger, B.: Groebner Bases in MATHEMATICA: Enthusiasm and Frustation,
Programming Environments for High-level Scientific Problem Solving, (1991) 80–
91
5. Brezinski, C.: History of Continued Fractions and Pade Approximants. Springer-
Verlag, (1980)
6. de Miguel Casado, G. and Garc´ıa Chamizo, J.M.: The Role of Algebraic Models
and Type-2 Theory of Effectivity in Special Purpose Processor Design, LNCS, Vol.
3988, (2006) 137–146
7. de Miguel Casado, G.; Garc´ıa Chamizo, J.M. and Signes Pont, M.T.: Algebraic
Model of an Arithmetic Unit for TTE-Computable Normalized Rational Numbers,
LNCS, Vol. 4497, (2007) 218–227
8. Dummit, D. S. and Foote, R. M., ”Abstract Algebra, 2nd ed.,” Englewood Cliffs,
NJ: Prentice-Hall, 1998.
9. Ercegovac, M.D. and Lang, T.: Digital Arithmetic. M. Kaufmann, (2004)
10. Gowland, P. and Lester, D.: A Survey of Exact Arithmetic Implementations, LNCS,
Vol. 2064, (2001) 30–47
11. Guelfond, A.O.: Resolucio´n de ecuaciones en nu´meros enteros, Lecciones populares
de matema´ticas- Ed. Mir, Moscow (Spanish Translation), (1979)
12. Hehner, E.C.R. and Horspool, R.N.S.: A New Representation of the Rational Num-
bers for fast Easy Arithmetic, SIAM J. Computing, Vol. 8(2), (1979)
13. Matula, D. and Kornerup, P.: Finite Precision Rational Arithmetic: An Arithmetic
Unit, IEEE Transactions on Computers, Vol. C-32, (1983) 378–387
14. Kornerup, P. and Matula, D.: Algorithms for Arbitrary Precision Floating Point
Arithmetic, Proceedings of the 10th IEEE Symposium on Computer Arithmetic,
(1991)
15. Kunkle, D.: Type-2 computability on spaces of integrable functions, Math. Log.
Quart., Vol. 50, (2004) 417–430
16. Lynch, T. and Schulte, M.: A High Radix On-line Arithmetic for Credible and
Accurate Computing, Journal of UCS, Vol. 1, (1995) 439–453
17. Mencer, O: Rational Arithmetic Units in Computer Systems, PhD Thesis, Stanford
University, (2000)
18. Mora, H.: Procesadores Aritme´ticos Especializados. Computacio´n Racional Exacta,
Ph.D Dissertation, University of Alicante, (2003)
19. Patterson, D.A. and Hennessy J.L.: Computer Architecture a quantitative ap-
proach. M. Kaufmann, (2002)
20. Schro¨der, M.: Admissible Representations in Computable Analysis, LNC,Vol. 3998,
(2006) 471–480
21. Vuillemin, J.E.: Exact Real Computer Arithmetic with Continued Fractions, IEEE
Transactions on Computers, Vol. 39, (1990) 1087–1105
22. Weihrauch, K.: Computable Analysis. Springer-Verlag, (2000)
23. Zhong, N. and Weihrauch, K.: Computability Theory of Generalized Functions,
Journal of the ACM, Vol. 50, 2003 (469–505)
Abstract Geometrical Computation with
Accumulations: Beyond the Blum, Shub and
Smale Model
Je´roˆme Durand-Lose
Laboratoire d’Informatique Fondamentale d’Orle´ans, Universite´ d’Orle´ans,
B.P. 6759, F-45067 ORLE´ANS Cedex 2.
http://www.univ-orleans.fr/lifo/Members/Jerome.Durand-Lose,
Jerome.Durand-Lose@univ-orleans.fr
Abstract. Abstract geometrical computation (AGC) naturally arises
as a continuous counterpart of cellular automata. It relies on signals (di-
mensionless points) traveling and colliding. It can carry out any Turing
computation, but since it works with continuous time and space, some
analog computing capability exists. In Abstract Geometrical Computa-
tion and the Linear BSS Model (CiE 2007, LNCS 4497, p. 238-247), it
is shown that AGC without any accumulation has the same computing
capability as the linear BSS model.
An accumulation brings infinitely many time steps in a finite duration.
This has been used to implement the black-hole model of computation
(Fundamenta Informaticae 74(4), p. 491-510). It also makes it possible
to multiply two variables, thus simulating the full BSS. Nevertheless a
BSS uncomputable function, the square root, can also be implemented,
thus proving that the computing capability of AGC with isolated accu-
mulations is strictly beyond the one of BSS.
Key words: Abstract geometrical computation, Accumulations, Ana-
log computation, BSS model, Signal machine.
1 Introduction
There is no agreed continuous/analog/R counterpart of the Church-Turing the-
sis. Relating the numerous models is crucial to understand the differences be-
tween their computing capabilities. For example, Bournez et al. [BH04,BCGH06]
as well as others [Kaw05] have related Moore’s recursion theory on R [Moo96],
computable analysis [Wei00] and the general purpose analog computer [PE74].
The aim of the present paper is to relate two models. One, abstract geometrical
computation (AGC) deals with regular and automatic drawing on the Euclidean
plane, while the second, the Blum, Shub and Smale model [BCSS98] relies on
algebraic computations over R. Bournez [Bou99] has already provided some re-
lations between linear BSS and Piecewise constant derivative systems which also
generates Euclidean drawings. In [DL07], AGC without accumulation was proved
equivalent to the linear BSS model (i.e. BSS restricted to multiplication only by
constants) with an unbounded number of variables. In the present paper, the
full BSS is related to AGC with isolated accumulations.
108 Je´roˆme Durand-Lose
Let us note that AGC relies on intersection of lines (to find the collisions),
another model considers also intersections with circles [Huc89,Huc91]; AGC con-
siders all collisions while in Huckenbeck’s geometrical machines the programs
chooses which geometric object to build and to consider.
Abstract geometrical computation (ACG) arises from the common use in
cellular automata (CA) literature of Euclidean settings to explain an observed
dynamics or to design a CA for a particular purpose. While CA operate in
discrete time over discrete space, Euclidean geometry deals with both continuous
time and space. This switch of context is justified by the scaling invariance of CA
and relates to our preference and ability for thinking in continuous rather than
discrete terms. Abstract geometrical computation works in a continuous setting:
discrete signals/particles are dimensionless points; the local function of CA –
computing the next state of a cell according to the states of neighbouring cells–
is replaced by collision rules: which signals emerge from a collision of signals.
Signals and rules define signal machines (SM).
This recent model, restricted to rational numbers, is able to carry out any
(discrete) Turing-computation [DL05b]; even with the additional restriction of
reversibility and conservativeness [DL06c]. With continuous time, comes Zeno
effect (infinitely many discrete steps during a finite duration): not only are ac-
cumulations possible, but they can be used to decide recursively enumerable
problems by using the black-hole scheme [DL05a,DL06a]. Although accumula-
tions can easily be generated, they can hardly be foreseen [DL06b].
In the Blum, Shub and Smale model (BSS), machines compute over any ring.
Roughly speaking, polynomial functions can be performed on variables as well as
tests (according to some order) for branching. The dimension of the input is not
bounded, a shift operator is provided in order to access any variable (finitely
many variables are considered since only finitely many are accessed in finite
time).
In [DL07], AGC (without accumulation) and linear (multiplying two variables
is forbidden) BSS over R with an unbounded number of variables are proved
equivalent. This is done through linear real number unlimited register machines
(the arguments of [Nov95] for the equivalence of URM and BSS translate to the
linear case).
With a reasonable handling of accumulations, restrictions on multiplication
can be lifted thus achieving the full BSS computing capability. They are handled
in the following way: there is no second (or higher) order accumulation; only
finitely many signals leave any accumulation; and one signal appears where an
accumulation takes place. Multiplication is embedded in the same context and
the same encoding of real numbers as in [DL07], so that addition, multiplication
by constants and test do not have to be implemented again. Only the basis is
recalled: a real number is encoded as the distance between two signals.
Multiplication of two real numbers, x and y, is done by producing the bi-
nary extension of y and according to yn adding or not x2
n. With integers, n is
increasing from 0, with real numbers, n goes down to −∞ which explains the
use of an accumulation. Each iteration divides x by 2, computes the next bit of
Abstract Geometrical Computation with Accumulations 109
y and updates the partial product accordingly. All the values are geometrically
decreasing to zero and so are the time and space used by an iteration, so that
there is an accumulation. The signal left by the accumulation is located exactly
at the value of the product.
To prove that AGC with accumulation is, as expected, strictly more powerful
than the BSS model, it is explained how to implement the square root. Each
iteration only uses addition and multiplication by constants.
Since the reader might be more familiar with BSS than with ACG, more care
and illustrations are given to ACG. Section 2 provides all the needed definitions.
Section 3 recalls basic encoding and geometric constructions for BSS simulation.
Section 4 provides the multiplication between variables. Section 5 explains how
to build a signal machine able to compute the square root with an accumulation.
Conclusion, remarks and perspectives are gathered in Section 6.
2 Definitions
Abstract geometrical computation. In this model, dimensionless objects are
moving on the real axis. When a collision occurs they are replaced according to
rules. This is defined by the following machines:
Definition 1 A signal machine with accumulation is defined by (M,S,R, µa)
where M (meta-signals) is a finite set, S (speeds) a function from M to R, R
(collision rules) a partial function from the subsets of M of cardinality at least
two into subsets of M (all these sets are composed of signals of distinct speed)
and µa is a meta-signal, the one that comes out of any accumulation.
Each instance of a meta-signal is a signal. The function S assigns speeds to
meta-signals. They correspond to the inverse slopes of the segments in space-time
diagrams. The collision rules, denoted ρ−→ρ+, define what emerge (ρ+) from
the collision of two or more signals (ρ−). Since R is a function, signal machines
are deterministic. The extended value set, V , is the union of M and R plus two
symbols: one for void, ⊘, and one for accumulation j. A configuration, c, is a
total function from R to V such that the set {x ∈ R | c(x) 6= ⊘} is finite.
A signal corresponding to a meta-signal µ at a position x, i.e. c(x) = µ, is
moving uniformly with constant speed S(µ). A signal can only start in a collision,
except for µa that can also be generated by an accumulation. A signal can only
end in a collision or an accumulation. This corresponds to condition 2 in Def. 2.
At a ρ−→ρ+ collision signals corresponding to the meta-signals in ρ− (resp.
ρ+) must end (resp. start) and no other signal should be present (condition
3). Condition 4 deals with accumulations, the first line implies that sufficiently
close to the accumulation, outside of the light cone, there is nothing but a µa
signal leaving the accumulation. The second line expresses that there is indeed
an accumulation (this is not formalized since it would be too ponderous).
Let Smin and Smax be the minimal and maximal speeds. The causal past,
or backward light-cone, arriving at position x and time t, J−(x, t), is defined by
all the positions that might influence the information at (x, t) through signals,
formally:
J−(x, t) = { (x′, t′) | x− Smax(t−t′) ≤ x′ ≤ x− Smin(t−t′) } .
110 Je´roˆme Durand-Lose
Definition 2 The space-time diagram issued from an initial configuration c0
and lasting for T , is a function c from [0, T ] to configurations (i.e. a function
from R× [0, T ] to V ) such that, ∀(x, t) ∈ R× [0, T ] :
1. ∀t∈[0, T ], {x ∈ R | ct(x) 6= ⊘} is finite,
2. if ct(x)=µ then ∃ti, tf∈[0, T ] with ti<t<tf or 0=ti=t<tf or ti<t=tf=T s.t.:
– ∀t′ ∈ (ti, tf ), ct′(x+ S(µ)(t′ − t)) = µ ,
– ti=0 or ( cti(xi) = ρ
−→ρ+ and µ ∈ ρ+ ) or ( cti(xi)=j and µ=µa )
where xi=x+ S(µ)(ti − t) ,
– tf=T or ( ctf (xf ) = ρ
−→ρ+ and µ ∈ ρ− ) or ctf (xf )=j
where xf=x+ S(µ)(tf − t) ;
3. if ct(x)=ρ
−→ρ+ then ∃ε, 0<ε, ∀t′∈[t−ε, t+ε] ∩ [0, T ], ∀x′∈[x− ε, x+ ε],
– (x′, t′) 6= (x, t) ⇒ ct′(x′) ∈ ρ−∪ρ+ ∪ {⊘},
– ∀µ∈M , ct′(x′)=µ ⇔ or
{
µ ∈ ρ− and t′ < t and x′ = x+ S(µ)(t′ − t) ,
µ ∈ ρ+ and t < t′ and x′ = x+ S(µ)(t′ − t) .
4. if ct(x) = j then
– ∃ε > 0, ∀t′∈[t−ε, t+ε] ∩ [0, T ], ∀x′∈[x− ε, x+ ε],
(x′, t′) /∈ J−(x, t) ⇒ or
{
ct′(x) = ⊘ and x′ 6= x+ S(µa)(t′ − t)
ct′(x) = µa and x
′ = x+ S(µa)(t
′ − t) ,
– ∀ε>0, there are infinitely many collisions in J−(x, t) ∩ R× [t−ε, t].
On space-time diagrams, the traces of signals are line segments whose direc-
tions are defined by (S(.), 1) (1 is the temporal coordinate). Collisions correspond
to the extremities of these segments. This definition can easily be extended to
T = ∞. In the space-time diagrams, time increases upwards. To simplify, the
same name is used for a signal throughout the computation, in fact, there is
a different meta-signal for each speed. As a computing device, the input is the
initial configuration and the output is the final configuration.
Blum, Shub and Smale model. (The reader is expected to be more familiar
with the BSS model than with AGC, so this part is not very detailed.) BSS
machines operate on an unbounded array containing real numbers in exact pre-
cision. The input/output is the content of the array. Apart from start and stop,
the available instructions are: compute a polynomial function (and store the re-
sult), branch according to a sign test and shift. The machine can only access a
finite part of the array at any time, the shift operator allows it to move on the
array (like the head of a Turing machine).
3 Basic construction
Real number encoding. A Real number is encoded as the distance from a
ba signal1 to its pairing val signal. Since signal machines are scaleless, two sca
signals whose distance amounts for a scale are provided as depicted on Fig. 1.
All numbers use the same scale. For the value 0, the superposition of ba and val
is encoded as a single signal nul. This value is never considered in the rest of the
paper; the reader is invited to check that it can be easily covered.
1 xx signal always means that it is an instance of the meta-signal xx.
Abstract Geometrical Computation with Accumulations 111
sca sca
1
val val ba
or nul(0)
val val
−pi −1.5
√
2
e
Fig. 1. Encoding: scale and positions of val for values −pi, −1.5, 0,
√
2 and e.
Geometric constructions. The construction of the multiplication, like the
addition and the multiplication by a constant, relies on some basic geometric
constructions. In each picture, the slopes of the line segments are indicated. It
can easily be checked that it is scale invariant and provides the desired effect.
Signals with equal speeds result in parallel segments, the key to many geometric
properties. Figure 2(a) shows how a distance can be halved. This is done by
starting two signals. The first one is going three times slower than the second
one. The time the first one crosses half the way, the second one goes one full way
and half way back, so that they meet exactly in the middle. Doubling is done
the other way round. Figures 2(b) and 2(c) show two ways to halve a distance
while shifting it. They also work with two signals emitted that change speed
or direction at some point and meet at the desired location. Generating simple
shifts is done by modifying only one slope (2
3
by 1
2
for Fig. 2(b) and 5
3
by 1 for
Fig. 2(b)).
1
3
1
3
1
1
l
2
l
(a) Halving.
1
3
1
3
1
2 3
2
l
2
l
(b) Shifting and halving 1.
1
31
3
1
3
3
5
l
2
l
(c) Shifting and halving 2.
Fig. 2. Basic geometric constructions.
4 Multiplication
4.1 Algorithm
The multiplication of two real numbers x and y uses the (possibly) infinite binary
extension of y. This is done in two steps: normalization and then an infinite loop.
The first step starts by considering signs and special cases (i.e. multiplication
by zero). Then, while y is greater than 2, x is multiplied by 2 and y divided by
2 (so that the product remains constant).
The second step carries out the multiplication. The binary extension y =
y0.y1y2y3 . . . (the initialization ensures that 0 < y < 2) is generated bit by bit.
112 Je´roˆme Durand-Lose
The underlying formula is:
xy =
∑
0≤i
yi
( x
2i
)
.
This is computed iteratively with the updating on Table 1. The two last cases
correspond to yn = 1 and yn = 0 respectively. It is started with: p0 = 0 (prod-
uct), b0 = 1 (bit test), x0 = x and 0 < y0 = y < 2b0 = 2. The following invariants
are satisfied:
– bn = 2
−n,
– 0 ≤ yn < 2bn,
– xn = x2
−n, and
– xy = pn +
xnyn
bn
.
The last invariant is trivially true for n = 0 and preserved by the loop. Since
xn
yn
bn
< 2xn and xn = x2
−n, from the last invariant comes that lim
n→∞
pn = xy.
Table 1. Potentially infinite loop to compute the product.
pn+1 xn+1 yn+1 bn+1
if yn = 0 stop
else if bn < yn pn + xn xn/2 yn − bn bn/2
else pn xn/2 yn bn/2
4.2 Initialisation
With our encoding, detecting whether x or y is zero is trivial. Detecting the
signs and computing the sign of the product is also very easy. The following only
deals with multiplication of positive values, the other cases are generated using
absolute values and symmetry if the product is negative.
The above algorithm should be started with 0 < y0 < 2b0 = 2. So that there
is a loop that multiplies x by 2 and divides y by 2 until y is small enough. This is
illustrated on Fig. 3 (the first two space-time diagrams go one above the other).
The algorithm is sequential: signals for base (ba ), bit testers (b ), x
(x ) and y (y ) are fix unless set on movement by some bouncing initial-
ization signals (ini). All distances are from ba and are measured according to
the “official” scale (not represented). The b signal stays at position 2 for testing
the end of the loop (i.e. y < 2). The signal ini comes from the left. If it meets
y before b, this means that the loop is finished (Fig. 3(c)). Otherwise it has to
half y, to double x and to test again (figures 3(a) and 3(b)).
At the end of initialization (which is always achieved in finite time and finitely
many collisions), the b at position 2 is set at position 1 (halved as usual). The
signal p amounting for p is generated. Since p is 0 at start, it is set on ba (this
corresponds to a different meta-signal). And finally, ini turns to mul that handles
the second step of the multiplication. Everything is depicted on Fig. 3(c).
Abstract Geometrical Computation with Accumulations 113
ini ba x b y
d
o
u
b
le
x
h
a
lv
e
y
(a) First iteration.
ba ini b y x
(b) Another iteration.
p
mul
ba ini y b x
(c) Loop end.
Fig. 3. Preparing the data for multiplication.
4.3 Main loop
The multiplication follows the (possibly) infinite loop defined in Table 1. Basi-
cally, the things to do are additions, subtractions, divisions by 2 and tests. These
are easy implementable inside signal machines. But, since the loop is infinite, a
correct accumulation have to be generated. By correct, it is understood as at the
right location and there is indeed an accumulation. The second point is not to
be underestimated since, for example, if at each iteration some constant distance
would have to be crossed, then there would be an infinite duration process but
no accumulation.
The algorithm is driven by a mul signal that bounces between p and other
signals. First of all, mul has to test to know which case to consider. This is done
easily: going away from ba, if b is encountered before y this means that bn < yn
otherwise yn < bn (when they are met simultaneously, i.e. they are equal, this
leads to the end of the loop at the next iteration).
The simpler case is when yn < bn. There is nothing to do but to halve bn
and xn, that is halve the distance from b and x to ba. Signals p and y are left
untouched. This is done as depicted on the lower part of Fig. 4(b).
The other case bn < yn is depicted on Fig. 4(a) and on the upper part of
Fig. 4(b). The addition of xn to pn is done by moving p to the location of x,
meanwhile x is moved on the right by half the distance it has from p. The signal
b is also moved on the right, at a distance from the new p that is half the previous
distance. The signal y is moved to the right, at a distance from the new p that
is equal to its original distance from b.
To ensure accumulation, all lengths are geometrically scaled down and all the
computations take place by p and are shifted according to the moves of p. This
can be seen on Fig. 4. Each iteration leads to halving the distance between ba
114 Je´roˆme Durand-Lose
ba ymul b x
p
sh
if
t
a
n
d
h
a
lv
e
sh
if
t
(a) First iteration.
p ymul b x
(b) Two iterations.
Fig. 4. Multiplication.
and both x and b. Since the distance from ba to y is at most twice the distance to
b, this ensures that all distances are bounded by some Ms
2n
at the nth iteration.
Clearly the duration of an iteration is bounded proportionally to the maximum
distance between the signals. Thus it is also bounded by some Mt
2n
at the nth
iteration. There is no time lag between two consecutive iterations, so that there
is indeed an accumulation.
5 Square root
In this section, it is briefly argued why it is possible to compute the square root
with an accumulation and how. Let a be any positive real number. The construc-
tion follows the digit by digit approximation algorithm constructing a sequence
bn such that:
b2n ≤ a <
(
bn +
1
2n
)2
.
At each stage it should be tested whether
(
bn +
1
2n+1
)2 ≤ a. If it is the case
then bn+1 = bn +
1
2n+1
otherwise bn+1 = bn. Using the sequences: dn = a − b2n,
en =
bn
2n
and fn =
1
4n+1
, the test corresponds to computing the sign of
a− (bn + 12n+1 )2 = a− b2n − bn2n − 14n+1 = dn − en − fn .
The updating of all these sequences is shown on Table 2 (plus another helpful
sequence gn =
1
2n+1
). The only operations used are additions, multiplications by
constants and tests. Again the size of the computing part is decreasing geomet-
rically and computation can be done by the signal encoding the value of bn and
shifted with it.
The algorithm starts by a pretreatment that finds the initial value for n. This
value might be negative (e.g. n = −11 for a = 220 + 1).
Abstract Geometrical Computation with Accumulations 115
Table 2. Infinite loop to compute the square root.
bn+1 dn+1 en+1 fn+1 gn+1
if dn − en − fn = 0 stop
else if 0 < dn − en − fn bn + gn dn − en − fn en/2 + fn fn/4 gn/2
else bn dn en/2 fn/4 gn/2
6 Conclusion
In the present paper, AGC with accumulation is proved to be strictly more
powerful than basic BSS. This is not very surprising because it is already known
to decide in finite time any recursively enumerable problem (in the classical
discrete setting). It would be natural to extends the structure BSS works with
(i.e. R as a ring) with square rooting but many functions are computable with
an accumulation. It would be interesting to identify them. Considering
√
2, an
accumulation point of a rational signal machine can be irrational.
If the computation is stopped before the accumulation happens, then an ap-
proximation is generated. Computable analysis relies on the idea of an infinite
approximating sequence both for representing real numbers and for computing
(type-2 Turing machine needs an infinite number of iterations to compute a
function on real numbers). The next step would be to relate these two models
(the spirit of [CH99]). One problem would be to miniaturize and to ensure the
generation of an accumulation. Another one is that computable analysis only
provides continuous functions, while in ACG, there is, for example, the sign
function which is clearly not continuous. On the other side, Moore’s recursion
theory allows non continuous functions (even the characteristic function of ra-
tional numbers).
There might be many accumulations to simulate BSS, but none is of or-
der two. Another issue is to consider nth order accumulation and connect with
infinite Turing machines and ordinals [Ham07].
References
[BCGH06] O. Bournez, M. L. Campagnolo, D. S. Grac¸a, and E. Hainry. The gen-
eral purpose analog computer and computable analysis are two equivalent
paradigms of analog computation. In J.-Y. Cai, S. B. Cooper, and A. Li,
editors, Theory and Appliacations of Models of Computations (TAMC ’06),
Beijing, China, number 3959 in LNCS, pages 631–643. Springer, 2006.
[BCSS98] L. Blum, F. Cucker, M. Shub, and S. Smale. Complexity and real computa-
tion. Springer, New York, 1998.
[BH04] O. Bournez and E. Hainry. An analog characterization of elementarily com-
putable functions over the real numbers. In J. Diaz, J. Karhuma¨ki, A. Lep-
isto, and D. T. Sannella, editors, 31st Int. Col. on Automata, Languages and
Programming (ICALP ’04), Turku, Finland, number 3142 in LNCS, pages
269–280. Springer, Jul 2004.
[Bou99] O. Bournez. Some bounds on the computational power of piecewise constant
derivative systems. Theory of Computing Systems, 32(1):35–67, 1999.
116 Je´roˆme Durand-Lose
[CH99] T. Chadzelek and G. Hotz. Analytic machines. Theoret. Comp. Sci., 219(1-
2):151–167, 1999.
[DL05a] J. Durand-Lose. Abstract geometrical computation for black hole computa-
tion (extended abstract). In M. Margenstern, editor, Machines, Computations,
and Universality (MCU ’04), number 3354 in LNCS, pages 176–187. Springer,
2005.
[DL05b] J. Durand-Lose. Abstract geometrical computation: Turing-computing ability
and undecidability. In B. S. Cooper, B. Lo¨we, and L. Torenvliet, editors,
New Computational Paradigms, 1st Conf. Computability in Europe (CiE ’05),
number 3526 in LNCS, pages 106–116. Springer, 2005.
[DL06a] J. Durand-Lose. Abstract geometrical computation 1: embedding black hole
computations with rational numbers. Fund. Inf., 74(4):491–510, 2006.
[DL06b] J. Durand-Lose. Forcasting black holes in abstract geometrical computation
is highly unpredictable. In J.-Y. Cai, S. B. Cooper, and A. Li, editors, Theory
and Appliacations of Models of Computations (TAMC ’06), number 3959 in
LNCS, pages 644–653. Springer, 2006.
[DL06c] J. Durand-Lose. Reversible conservative rational abstract geometrical compu-
tation is turing-universal. In A. Beckmann and J. V. Tucker, editors, Logical
Approaches to Computational Barriers, 2nd Conf. Computability in Europe
(CiE ’06), number 3988 in LNCS, pages 163–172. Springer, 2006.
[DL07] J. Durand-Lose. Abstract geometrical computation and the linear Blum, Shub
and Smale model. In S. Cooper, B. Lo¨we, and A. Sorbi, editors, Computation
and Logic in the Real World, 3rd Conf. Computability in Europe (CiE ’07),
number 4497 in LNCS, pages 238–247. Springer, 2007.
[Ham07] J. D. Hamkins. A survey of infinite time turing machines. In J. Durand-
Lose and M. Margenstern, editors, Machines, Computations and Universality
(MCA ’07), number 4664 in LNCS, pages 62–71. Springer, 2007.
[Huc89] U. Huckenbeck. Euclidian geometry in terms of automata theory. Theor.
Comput. Sci., 68(1):71–87, 1989.
[Huc91] U. Huckenbeck. A result about the power of geometric oracle machines. Theor.
Comput. Sci., 88(2):231–251, 1991.
[Kaw05] A. Kawamura. Type-2 computability and moore’s recursive functions. Electr.
Notes Theor. Comput. Sci., 120:83–95, 2005.
[Moo96] C. Moore. Recursion theory on the reals and continuous-time computation.
Theoret. Comp. Sci., 162(1):23–44, 1996.
[Nov95] E. Novak. The real number model in numerical analysis. J. Complex.,
11(1):57–73, 1995.
[PE74] M. B. Pour-El. Abstract computability and its relation to the general purpose
analog computer (some connections between logic, differential equations and
analog computers). Trans. Amer. Math. Soc., 199:1–28, 1974.
[Wei00] K. Weihrauch. Introduction to computable analysis. Texts in Theoretical
computer science. Springer, Berlin, 2000.
Notions of Bisimulation for Heyting-Valued
Modal Languages
Pantelis E. Eleftheriou1, Costas D. Koutras2, and Christos Nomikos3?
1 Institut de Matema`tica, Universitat de Barcelona,
Gran Via de les Corts Catalanes 585, 08007 Barcelona, Spain
pelefthe@gmail.com
2 Department of Computer Science and Technology, University of Peloponnese,
end of Karaiskaki Street, 22100 Tripolis, Greece
ckoutras@uop.gr
3 Department of Computer Science, University of Ioannina,
45110 Ioannina, Greece
cnomikos@cs.uoi.gr
Abstract. We define notions of bisimulation for the family of Heyting-
valued modal logics introduced by M. Fitting. In this family of logics,
each modal language is built on an underlying space of truth values, a
Heyting algebra H. All the truth values are directly represented in the
language, which is interpreted on relational frames with an H-valued ac-
cessibility relation. We investigate the correct notion of bisimulation in
this context: we define two variants of bisimulation relations and derive
relative (to a truth value) modal equivalence results for bisimilar states.
We further investigate game semantics for our bisimulation, Hennessy-
Milner classes and other relevant properties. If the underlying algebra
H is finite, Heyting-valued modal models can be equivalently reformu-
lated to a form relevant to epistemic situations with many interrelated
experts. Our definitions and results draw from this formulation, which is
of independent interest to Knowledge Representation applications.
Key words: Modal Logic, Many-Valued Logic, Bisimulations.
1 Introduction
Bisimulation is a very rich concept which plays an important role in many ar-
eas of Computer Science, Logic and Set Theory. Its origins can be found in the
analysis of Modal Logic but it was independently rediscovered by computer sci-
entists in their efforts to understand concurrency. In Modal Logic, bisimulations
were introduced by Johan van Benthem, under the name of p-relations or zig-
zag relations, in the course of his work on the correspondence theory of Modal
Logic [vB83,vB84]. In Computer Science, bisimulations were introduced by Park
in [Par91] and Henessy and Milner in [HM85], in the course of investigating the
notion of equivalence among processes (see [San07] for a tutorial on the history
? Corresponding author
118 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos
of bisimulations). In this context, bisimulations represent a fundamental notion
of identity between process states and every language designed to capture the
essential properties of processes should be blind for bisimilar states.
On the other hand, from the Modal Logic viewpoint, bisimulation is the cor-
rect notion of similarity between two modal models: modal formulas are unable
to distinguish bisimilar points of the two models. But its importance lies far
beyond; the celebrated van Benthem characterization theorem, published in the
mid-‘70s, states that invariance for bisimulation captures the essential property
of the ‘modal fragment’ of first-order logic: a first-order formula is invariant for
bisimulation iff it is (equivalent to the) the syntactical translation of a modal
formula (see [BdRV01] for a nice exposition of this result and its consequences).
The characterization theorem has generated an important stream of research in
the analysis of logical languages. In particular, the bisimulation-based analysis of
modal languages has been extensively studied in the Amsterdam school of modal
logic [dR93,Ger99,MV03], and it has even been suggested that this notion is as
important for modal logic as the notion of partial isomorphism has been for the
model theory of classical logic (see the PhD thesis of M. de Rijke [dR93]). It is
worth mentioning that bisimulations have been used beyond the realm of classi-
cal modal logic: see for instance the variant used to analyze since-until temporal
languages [KdR97], M. Otto’s work related to Finite Model Theory [Ott99] and
J. Gerbrandy’s dissertation [Ger99]. Bisimulations have been also used as a fun-
damental tool in the area of non-well founded set theory ([Acz88], see [BdRV01]
for a few details and further references).
In this note, we address the question of what constitutes a suitable notion of
bisimulation for the family of many-valued modal languages introduced by M.
Fitting in the early ’90s [Fit92,Fit91]. Each language of this family is built on an
underlying space of truth values, a Heyting algebra H. There exist three features
that give these logics their distinctive character. The first one is syntactic: the
elements of H are directly encoded in the language as special constants and this
permits the formation of ‘weak’, uncertainty-oriented versions of the classical
modal epistemic actions [Kou03,KNP02,KP02]. The second is semantic: the lan-
guages we discuss are interpreted on H-labelled directed graphs which provide
us a form of many-valued accessibility relation. Finally, the third one concerns
the potential applicability of these logics in epistemic situations with multiple
intelligent agents. More specifically, assuming that H is a finite Heyting algebra,
these logics can be formulated in a way that expresses the epistemic consensus
of many experts, interrelated through a binary ‘dominance’ relation [Fit92]. It is
worth mentioning that, model-theoretically, every complete Heyting algebra can
serve as the space of truth values. However, apart from the equivalent multiple-
expert formulation of the logics, the finiteness assumption for H is essential for
the elegant canonical model construction of [Fit92] which leads to a complete-
ness theorem; note that this finiteness restriction seems to be also necessary for
obtaining a many-valued analog of the ultrafilter extension construction [EK05].
We provide below two notions of bisimulation and derive modal equivalence
results. The first one is a rather strong notion, that allows us to formulate sim-
Notions of Bisimulation for Many-Valued Modal Languages 119
ple, intuitive, Ehrenfeucht-Fraisse´ type bisimulation games through which one
can easily define bounded bisimulations, as in the classical case. Also, an appro-
priate notion of unravelling is given, through which one gets a form of the cele-
brated tree-model property, considered to be critical for the ‘robust decidability’
of modal logics [Var97]. A second, rather involved notion of weak bisimulation
is discussed which allows us to obtain an interesting notion of Hennessy-Milner
class of Heyting-valued modal models. Both notions of bisimulations draw inspi-
ration from the equivalent multiple-expert formulation of these logics, which is
actually a mixture of Kripke modal and Kripke intuitionistic semantics. Due to
space limitations, this semantics, along with the interpretation of our bisimula-
tion relations in this context, is left for the full paper.
2 Many-Valued Modal Languages
In this section we provide the syntax and semantics of many-valued modal lan-
guages, as introduced in [Fit92], with only minor changes in the notation. To
construct a modal language of this family, we first fix a Heyting algebra H which
will serve as the space of truth values. Thus, we first briefly expose the basic
definitions and properties of Heyting algebras, fixing also notation and terminol-
ogy. We assume that the reader already has some familiarity with the elements
of lattice theory and universal algebra. For more details the reader is referred to
the classical texts [RS70,BD74].
Heyting Algebras A lattice L is a pair 〈L,≤〉 consisting of a non-empty set L
equipped with a partial-order relation ≤, such that every two-element subset
{a, b} of L has a least upper bound or join, denoted by a∨ b, and a greatest lower
bound or meet, denoted by a ∧ b. A lattice L is complete if a join and a meet
exist for every subset of L. A least (or bottom) element of a lattice is denoted
by ⊥ and a greatest (or top) one by >. An element x ∈ L is join-irreducible if
x 6= ⊥ (in case L has a bottom element) and x = a ∨ b implies x = a or x = b.
We frequently use indexed sets and denote (possibly infinite) meets and joins by∧
t∈T
at and
∨
t∈T
at. Some fairly obvious properties of infinite joins and meets, such
as
∧
t∈T
(
a ∧ at
)
= a ∧∧
t∈T at will be used, generally without comment.
A lattice H = 〈H,≤〉 with the additional property that, for every pair of
elements 〈a, b〉, the set {x | a∧x ≤ b} has a greatest element, is called a relatively
pseudo-complemented lattice. This element is denoted by a ⇒ b and is called
the pseudo-complement of a relative to b. A relatively pseudo-complemented
lattice is always a topped ordered set [RS70]. It is not always the case that a
relatively pseudo-complemented lattice has a least element. A relatively pseudo-
complemented lattice H with a least element is called a Heyting algebra (HA)
or a pseudo-Boolean algebra. It is known that the class of HAs includes the
class of Boolean algebras and is included in the class of distributive lattices;
both inclusions are proper. For finite lattices, the second inclusion becomes an
equality: the class of finite HAs coincides with the class of finite distributive
lattices [RS70]. The following lemma gathers some useful properties of relatively
120 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos
pseudocomplemented lattices that will be used in Section 3; whenever a possibly
infinite join or meet is involved, it is assumed that it exists. The proof of its items
can be found in [RS70,BD74]. Note also that the first item of the lemma can be
equivalently considered as a definition of relative pseudo-complementation.
Lemma 1. 1. x ≤ (a⇒ b) iff (x ∧ a) ≤ b
2. (a⇒ b) = > iff a ≤ b
3. If a1 ≤ a2 then (a2 ⇒ b) ≤ (a1 ⇒ b)
4. c ∧ (a⇒ b) = c ∧ ((c ∧ a)⇒ (c ∧ b))
5.
∨
t∈T
(a ∧ bt) = a ∧
∨
t∈T
bt
6. (a ∨ b)⇒ c = (a⇒ c) ∧ (b⇒ c)
Syntax of Many-Valued Modal Languages Having fixed a complete Heyting al-
gebra H we proceed to define the syntax of the modal language. The elements
of H are directly represented in the language by special constants, called propo-
sitional constants, and we reserve lowercase letters (along with ⊥,>) to denote
them. To facilitate notation, we use the same letter for the element of H and the
constant which represents it in the language; context will clarify what is meant.
Assuming also a set Φ of propositional variables (also called propositional let-
ters) we define the many-valued modal language LH
23
(Φ) with the following BNF
specification, where t ranges over elements of H, P ranges over elements of Φ
and A is a formula of LH
23
(Φ).
A ::= t | P | A1 ∨A2 | A1 ∧A2 | A1 ⊃ A2 | 2A | 3A
As we will see below, the modal logics defined are in general bimodal, thus
we need both modal operators. Note also that ∨ and ∧ serve both as logical
connectives, as well as lattice operation symbols but it should be clear by context
what is meant. In the rest of the paper, we shall often omit Φ when possible and
speak of the language LH
23
. A (non-classical) negation ¬X can be defined as
(X ⊃ ⊥).
Semantics of Many-Valued Modal Languages LH
23
is interpreted on an interesting
variant of a relational frame, which possesses a kind of Heyting-valued accessi-
bility relation. Note that there have been other approaches in the literature for
defining many-valued modal logics, but all of them have kept the essence of clas-
sical relational semantics intact (see [Fit92] for references). Given LH
23
(Φ), we
define H-modal frames and H-modal models as follows:
Definition 1. An H-modal frame for LH
23
(Φ) is a pair F = 〈S, g〉, where S is a
non-empty set of states and g : S×S→ H is a total function mapping pairs of
states to elements of H.
An H-modal model M = 〈S, g, v〉 is built on F by providing a valuation v, that
is a function v : S × (H ∪ Φ) → H which assigns a H-truth value to atomic
formulae in each state, such that v(s, t) = t, for every s ∈ S and t ∈ H. In other
words, the propositional constants are always mapped to ‘themselves’.
Notions of Bisimulation for Many-Valued Modal Languages 121
In the sequel, we shall often omit the adjective ‘modal’ and talk simply of
H-frames and H-models.
The valuation v extends to all the formulae of LH
23
(Φ) in a standard recursive
fashion:
Definition 2. Let M = 〈S, g, v〉 be an H-model and s a state of S. The exten-
sion of the valuation v to the whole language LH
23
(Φ) is given by the following
clauses;
– v(s, A ∧B) = v(s, A) ∧ v(s, B)
– v(s, A ∨B) = v(s, A) ∨ v(s, B)
– v(s, A ⊃ B) = v(s, A)⇒ v(s, B)
– v(s,2A) =
∧
t ∈S
(
g(s, t) ⇒ v(t, A))
– v(s,3A) =
∨
t ∈S
(
g(s, t) ∧ v(t, A))
The operators of necessity (2) and possibility (3) are not each other’s dual,
unless H is a Boolean algebra [Fit92]. Note also that all the definitions above
collapse to the familiar ones from the classical case, in the case of the classical
language L2
23
, where 2 is the lattice of two-valued classical logic.
3 Bisimulations for Many-Valued Modal Languages
In this section, we define two suitable general notions of bisimulation for a lan-
guage LH
23
of the family defined in the previous section. Before proceeding, we
have to define a refined notion of modal truth invariance which fits our aims and
which also has an interesting interpretation in the multiple-expert context. Note
that the following notion is trivial for t = ⊥.
Definition 3 (t-invariance). Let M = 〈S, g, v〉 and M′ = 〈S′, g′, v′〉 be H-
models for LH
23
(Φ), s ∈ S and s′ ∈ S′ two states and t ∈ H a truth value (t 6= ⊥).
We say that modal truth is t-invariant for the transition between s and s′ if for
every X ∈ LH
23
(Φ)
t ∧ v(s, X) = t ∧ v′(s′, X)
3.1 Strong Bisimulation for Many-Valued Modal Languages
The following definition captures the idea of moving ‘back and forth’ between
two H-models by matching steps (‘modulo’ t) in both directions.
Definition 4 (t-bisimulations). Given two H-models M = 〈S, g, v〉 and
M′ = 〈S′, g′, v′〉 and a truth value t ∈ H (t 6= ⊥), a non-empty relation
Z ⊆ S×S′ is a t-bisimulation between M and M′ if for any pair 〈s, s′〉 ∈ Z
122 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos
(base) t ∧ v(s, P ) = t ∧ v′(s′, P ) for every P ∈ Φ
(forth) for every r ∈ S such that t ∧ g(s, r) 6= ⊥,
there exists an r′ ∈ S′ such that t ∧ g(s, r) = t ∧ g′(s′, r′) and rZr′
(back) for every r′ ∈ S′ such that t ∧ g′(s′, r′) 6= ⊥,
there exists an r ∈ S such that t ∧ g′(s′, r′) = t ∧ g(s, r) and rZr′
Two states s and s′ are called t-bisimilar (notation s ↔t s′ or M, s ↔t M′, s′)
if there is a t-bisimulation Z between M and M′ such that sZs′.
We can now prove the basic theorem which states that t-bisimulation implies
t-invariance.
Theorem 1. If M, s ↔t M′, s′, then t ∧ v(s, X) = t ∧ v′(s′, X) for every
X ∈ LH
23
.
Proof. The proof is left for the full version of the paper.
EF-type games for t-bisimulation The t-bisimulation game is a simple variant
of the Ehrenfeucht-Fraisse´ game played in First-Order Logic. For the purposes
of the rest of this section call a state r a t-compatible successor state of s if
t ∧ g(s, r) 6= ⊥. Two elements a, a′ of H are called t∧-equivalent if t ∧ a =
t∧a′. We call labels the H-truth values attached to the graph’s edges and to the
propositional letters of the language in each possible world. The t-bisimulation
game is played on two pointed H-models (models with a single distinguished
state) M, s0 and M′, s′0. There exists one marked element in each H-model;
initially, the marked elements are the distinguished nodes s0 and s′0. In each
round of the game
– Player I selects one of the H-models, chooses a t-compatible successor of
the marked element and moves the marker along the edge (labelled by aI)
to its target
– Player II responds with a move of the marker in the other H-model in a
corresponding t-compatible transition (labelled by aII) such that aI and aII
are t∧-equivalent and the labels of the propositional letters in the marked
elements (states) of the models are also t∧-equivalent
The length of the game is the (finite or infinite) number of rounds and Player
II loses the match if at a certain round cannot respond with an appropriate
move. It is obvious that Player I is trying to spoil a t-bisimulation while Player
II is trying to reveal one. Player II has a winning strategy in a game of n rounds
if she can win every n-round game played on M, s0 and M′, s′0. In a classical
fashion, we can proceed to a finer analysis of t-bisimulations using the inductively
defined notion of the modal depth of a formula (the maximum number of modal
operators encountered in a subformula, [BdRV01,MV03]). The notion of a t-
bisimulation bounded by a positive integer n, or any ordinal actually, can easily
be defined (see [Ger99, Chapter 2.1] for the classical case), but we will not give
further details here, since the whole construction is identical to the classical one.
We only provide the following proposition which generalizes the known classical
results from two-valued modal logic:
Notions of Bisimulation for Many-Valued Modal Languages 123
Proposition 1. 1. Player II has a winning strategy in the n-round game played
on M, s0 and M′, s′0 iff modal truth is t-invariant in s0 and s
′
0 for every
formula up to modal depth n.
2. Player II has a winning strategy for the infinite game played on M, s0 and
M′, s′0 iff s0 ↔t s
′
0.
Proof. The proof of the first item runs by induction on n and is actually a
restatement of the proof of Theorem 1. The second item follows by the definitions
above.
The t-bisimulation games can be formulated in a simple way for the class of
languages built on finite linear orders. Assuming further that truth values are
colours, linearly ordered, and given that the meet operation is simple in finite
chains (a∧b = min(a, b)) the game can be described in an easy way that provides
also an element of fun.
t-unravellings and the tree-model property The idea of unravelling a model into a
modally-equivalent tree model is known both from modal logic and the theory of
processes. In the latter field, the states of the unravelled model represent traces
(histories) of processes, starting from a state s. The following definition provides
the many-valued analog of this notion.
Definition 5. Given a pointed model M = 〈S, g, v〉, s1, its t-unravelling is the
model Mus1 = 〈Sus1 , gus1 , vus1〉, where
1. Sus1 consists of all tuples 〈s1, · · · , sk〉 where si+1 is a t-compatible successor
of si
2. gus1(〈s1, · · · , sk〉, 〈s1, · · · , sk+1〉) = t ∧ g(sk, sk+1), and ⊥ for any other pair of
tuples
3. vus1(〈s1, · · · , sk〉, P ) = t ∧ v(sk, P ), (P ∈ Φ)
Obviously Mus1 is a tree model and the following proposition can be proved
by a careful inspection on the definition of a t-bisimulation.
Proposition 2. The graph of the function from Sus1 to S, which maps every
tuple to its last component (and 〈s1〉 to s1) is a t-bisimulation.
Thus, modal truth is t-invariant for the transition from s1 to the root 〈s1〉 of
the tree and this is a generalized version of the tree model property [Var97].
Satisfiability in Many-Valued Modal Languages The general satisfiability prob-
lem in this context can be phrased as follows: given X ∈ LH
23
and t ∈ H,
is there a state s of an H-model M in which v(s, X) ≥ t? This is equiva-
lent (by Lemma 1(2)) to t ⇒ v(s, X) = > which is equivalent (by Def. 2) to
v(s, t ⊃ X) = >. Thus, the general satisfiability problem is subsumed by the
question of finding a state in which a formula is mapped to the top element of
the lattice. By the previous paragraph, if such a state/model exists, then this
formula can be also satisfied at the root of a (>-unravelled) tree. Imitating the
124 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos
classical arguments ( [MV03]), it is easy to prove that, if H is finite, every for-
mula can be satisfied in a finite tree whose size is bounded: its depth is bounded
by the modal depth of X and its branching degree is bounded by the number of
box and diamond subformulas of x. This leads to a simple way of proving the
fact that the many-valued analog of the system K (which is determined by the
class of all H-models [Fit92]) has a decidable general satisfiability problem.
3.2 Weak Bisimulations for Many-Valued Modal Languages
We proceed now to define, a weaker, more fine-grained notion of bisimulation
that is directly inspired from (and can be better explained in the context of) the
multiple-expert semantics of these languages. We first fix some notation.
Let IH denote the set of join-irreducible elements of H. For the rest of this
section, we fix a complete Heyting algebra H that has the following property:
Every t ∈ H − IH is equal to the join of a finite number of elements in IH (1)
Define the functionDH fromH−{⊥} to 2IH , such thatDH(t) = {c ∈ IH | c ≤ t}.
Using Property (1), we see that t =
∨
c∈DH(t) c. Intuitively, DH provides a
decomposition of a value t ∈ H into join-irreducible values. In the next definition,
a bisimulation relation is defined for every truth value, but in a way that it is
“upwards (with respect to the lattice of truth values) consistent”.
Definition 6 (Weak bisimulation). Given two H-models M = 〈S, g, v〉 and
M′ = 〈S′, g′, v′〉, a function Z from H − {⊥} to 2S×S′ is a weak bisimulation
between M and M′ if it satisfies the following properties:
– for every t1, t2 ∈ H
(consistency) Z(t1 ∨ t2) = Z(t1) ∩ Z(t2)
– for every join-irreducible value t ∈ IH and any pair 〈s, s′〉 ∈ Z(t)
(base) t ∧ v(s, P ) = t ∧ v′(s′, P ) for every P ∈ Φ
(forth) for every r ∈ S such that t ∧ g(s, r) 6= ⊥
and for every c ∈ DH(t ∧ g(s, r)),
there exists an r′ ∈ S′ such that c ≤ g′(s′, r′) and 〈r, r′〉 ∈ Z(c)
(back) for every r′ ∈ S′ such that t ∧ g′(s′, r′) 6= ⊥
and for every c ∈ DH(t ∧ g′(s′, r′)),
there exists an r ∈ S such that c ≤ g(s, r) and 〈r, r′〉 ∈ Z(c)
Two states s and s′ are called weakly t-bisimilar (notation s!t s′ orM, s!t
M′, s′) if there is a weak bisimulation Z between M and M′ such that 〈s, s′〉
belongs to Z(t).
The reader can check that we have indeed defined a weaker notion than that
of a t-bisimulation: M, s ↔t M′, s′ implies M, s!t M′, s′.
The basic theorem of the previous section is still valid under this new notion,
but the proof requires some more elaboration.
Theorem 2. If M, s !t M′, s′, then t ∧ v(s, X) = t ∧ v′(s′, X) for every
X ∈ LH
23
.
Proof. The proof is left for the full version of the paper.
Notions of Bisimulation for Many-Valued Modal Languages 125
Image-finite H-models and weak t-bisimulations One of the fundamental ques-
tions in the bisimulation-based analysis of modal languages, concerns the iden-
tification of cases in which the converse of Theorem 2 is true. Much obviously, it
is not always true: the classical counterexample of two tree models, both with a
finite branch for each natural number, one of which possesses an infinite branch,
suffices (cf. [BdRV01, Chapter 2.2]). The simplest example of Hennessy-Milner
classes of modal models (classes in which modal equivalence is itself a bisimu-
lation relation) is the class of image-finite models, in which each state has only
a finite number of successors. It is natural to consider a straightforward many-
valued analog of this notion by considering H-models in which for each state
s, the set of successors of s is always finite and check whether in this case t-
invariance implies t-bisimilarity. Formally, the notion of image-finite H-models
is defined as follows.
Definition 7 (Image-finite H-models). An H-model M = 〈S, g, v〉 is called
image-finite if for every s ∈ S, the set Ss = {r ∈ S | g(s, r) 6= ⊥} is finite.
The following theorem states that for image-finiteH-models, t-invariance implies
t-bisimilarity.
Theorem 3. Let M = 〈S, g, v〉 and M′ = 〈S′, g′, v′〉 be image-finite H-models
for LH
23
(Φ). Define the function Z from H − {⊥} to 2S×S′ so that for every
s ∈ S, every s′ ∈ S′ and every t ∈ H − {⊥}, 〈s, s′〉 ∈ Z(t) iff modal truth is
t-invariant for the transition between s and s′. Then Z is a weak bisimulation
between M and M′.
Proof. The proof is left for the full version of the paper.
4 Conclusions - Related Work
In this paper, we have contributed to the extensive literature on the importance
and the fundamental nature of bisimulation. Our main aim has been to define
a fine-grained notion of bisimulation for Heyting-valued modal languages and
establish its basic facts. Our results have an interesting meaning for Knowledge
Representation situations, when interpreted in the multiple-expert context. It
remains to investigate appropriate extension of smallest and largest bisimulations
in this context and address possible applications for Knowledge Engineering in
complex epistemic situations.
References
[Acz88] P. Aczel. Non-Well-Founded Sets. CSLI Publications, 1988.
[BD74] R. Balbes and Ph. Dwinger. Distributive Lattices. University of Missouri
Press, 1974.
[BdRV01] P. Blackburn, M. de Rijke, and Y. Venema. Modal Logic. Number 53 in
Cambridge Tracts in Theoretical Computer Science. Cambridge University
Press, 2001.
126 Pantelis E. Eleftheriou, Costas D. Koutras, and Christos Nomikos
[DP90] B. A. Davey and H. A. Priestley. Introduction to Lattices and Order. Cam-
bridge University Press, 1990.
[dR93] M. de Rijke. Extending Modal Logic. PhD thesis, Institute for Logic, Lan-
guage and Computation, University of Amsterdam, 1993.
[EK05] P. Eleftheriou and C. D. Koutras. Frame constructions, truth invariance
and validity preservation in many-valued modal logic. Journal of Applied
Non-Classical Logics, 15(4):367–388, 2005.
[Fit91] M. C. Fitting. Many-valued modal logics. Fundamenta Informaticae, 15:235–
254, 1991.
[Fit92] M. C. Fitting. Many-valued modal logics II. Fundamenta Informaticae,
17:55–73, 1992.
[Ger99] J. D. Gerbrandy. Bisimulations on Planet Kripke. PhD thesis, Institute for
Logic, Language and Computation, University of Amsterdam, 1999.
[GG84] D. M. Gabbay and F. Guenthner, editors. Handbook of Philosophical Logic.
D. Reidel, Dordrecht, 1984.
[HM85] M. Hennessy and R. Milner. Algebraic laws for nondeterminism and concur-
rency. Journal of the ACM, 32:137–162, 1985.
[KdR97] N. Kurtonina and M. de Rijke. Bisimulations for temporal logic. Journal of
Logic, Language and Information, 6:403–425, 1997.
[KNP02] C. D. Koutras, Ch. Nomikos, and P. Peppas. Canonicity and completeness
results for many-valued modal logics. Journal of Applied Non-Classical Log-
ics, 12(1):7–41, 2002.
[Kou03] C. D. Koutras. A catalog of weak many-valued modal axioms and their cor-
responding frame classes. Journal of Applied Non-Classical Logics, 13(1):47–
72, 2003.
[KP02] C. D. Koutras and P. Peppas. Weaker axioms, more ranges. Fundamenta
Informaticae, 51(3):297–310, 2002.
[MV03] M. Marx and Y. Venema. Local variations on a loose theme: Modal logic
and decidability. In M. Y. Vardi and Sc. Weinstein, editors, Finite Model
Theory and its Applications. Springer Verlag, 2003. To appear.
[Ott99] M. Otto. Bisimulation-invariant Ptime and higher-dimensional mu-calculus.
Theoretical Computer Science, 224:237–265, 1999.
[Par91] D. Park. Concurrency and automata on infinite sequences. In Proceedings
of the 5th GI-Conference on Theoretical Computer Science, volume 104 of
Lecture Notes in Computer Science, pages 167–183. Springer-Verlag, 1981.
[RS70] H. Rasiowa and R. Sikorski. The Mathematics of Metamathematics. PWN -
Polish Scientific Publishers, Warsaw, third edition, 1970.
[San07] D. Sangiorgi. On the origins of Bisimulation, Coinduction, and Fixed Points.
Technical Report UBLCS-2007-24, Department of Computer Science, Uni-
versity of Bologna, 2007 (available at:
http://www.cs.unibo.it/~sangio/DOC_public/history_bis_coind.pdf).
[Var97] M. Y. Vardi. Why is modal logic so robustly decidable? volume 31 of
DIMACS Series in Discrete Mathematics and Theoretical Computer Science,
pages 561–572. AMS, 1997.
[vB83] J. van Benthem. Modal Logic and Classical Logic. Bibliopolis, Naples, 1983.
[vB84] J. van Benthem. Correspondence Theory, pages 167–247. Volume 2 of Gab-
bay and Guenthner [GG84], 1984.
Algorithmic Properties of Structures for
Languages with Two Unary Functional Symbols
Ekaterina B. Fokina?
Sobolev Institute of Mathematics
Siberian Branch of the Russian Academy of Sciences
4 Acad. Koptyug avenue
630090 Novosibirsk Russia
e fokina@math.nsc.ru
Abstract. When studying algorithmic properties of structures with in-
teresting algebraic and model–theoretic properties, one often uses known
structural properties of the structures. However, it is often the case that
results on particular kinds of structures can be transferred to structures
from many other interesting classes. One of the ways of such general-
ization involves coding of the original structure into a structure from
the given class in a way that is effective enough to preserve interesting
algorithmic properties. There are several constructions that allow us to
reduce algorithmic questions for arbitrary structures to graphs. They
also show that if we have a result for a graph, we also have it for a struc-
ture for any language containing at least one n–ary relational symbol,
where n ≥ 2. We prove that it is possible to generalize this approach
and get the same results for structures for a language with two unary
functional symbols. Thus, we get the results for structures for so–called
rich languages.
1 Introduction
When studying algorithmic properties of structures with interesting algebraic
and model–theoretic properties, one often uses known structural properties of the
structures. To find and study such connections between computability–theoretic
and algebraic properties of structures is one of the main questions of computable
model theory. However, it is often the case that results on particular kinds of
structures can be transferred to structures from many other interesting classes.
One of the ways of such generalization involves coding of the original structure
into a structure from the given class in a way that is effective enough to preserve
interesting algorithmic properties. This approach is very well illustrated in [14]
and [21]. Both papers contain a very nice survey on the known results about
? The author was partially supported by the Russian Foundation for Basic Research
(Grant 08-01-00336), the State Maintenance Program for the Leading Scientific
Schools of the Russian Federation (Grant NSh-335.2008.1.) and Russian Science
Support Foundation.
128 Ekaterina B. Fokina
different computability–theoretic properties of many classes of structures and
present several ways to transfer them to classes for various languages.
We introduce some basic definitions. We fix a computable Go¨del numbering
of a language L. Let all structures have universes contained in ω, which we think
of as computable sets of constants.
Definition 1. A structure A of the language L is computable if its domain is
a computable subset of ω and all basic operations and predicates are uniformly
computable. We identify formulas with their Go¨del numbers. Then computability
of a structure is equivalent to the condition that the atomic diagram D(A) of
A is computable, where D(A) is the set of all atomic sentences and negations
of atomic sentences with parameters in |A| true in A. A structure B has a
computable presentation if it is isomorphic to a computable structure A. In this
case we also say that B is computably presentable. A structure A is decidable if
its complete diagram Dc(A) is computable, where Dc(A) is the set of all sentences
with parameters in |A| true in A.
Definition 2. A theory T is decidable if it is a computable set of sentences. A
complete theory T is α-categorical if any two models of T of the power α are
isomorphic. A structure is α-categorical if its theory is α-categorical.
There are different approaches to study algorithmic properties of computable
structures. Here we list only some results illustrating possible ways to study
computability–theoretic properties. More detailed survey can be found in [14]
and [21].
One of the approaches is to study computable presentations of the structure
up to d–computable isomorphism.
Definition 3. Given a degree d, the d–computable dimension of a computably
presentable structure A is the number of computable presentations of A up to
d–computable isomorphism. If A has d–computable dimension 1 then it is d–
computably categorical.
There are examples of all computable dimensions 1 ≤ n ≤ ω. Moreover, many
of such examples were found in well–known classes of algebraic structures. See,
for example, [10], [11], [12], [13], [15], [18], [24], [26].
A related question is to look at the computable dimension of a computably
categorical structure when it is expanded by finitely many constants. It was
shown that in this case the computable categoricity is not always preserved.
Theorem 1 (Cholak, Goncharov, Khoussainov, and Shore in [5]). For
each k > 0 there exist a computably categorical structure A and an a ∈ |A| such
that (A, a) has computable dimension k.
Theorem 2 (Hirschfeldt, Khoussainov, and Shore in [20]). There are a
computably categorical structure A and an a ∈ |A| such that (A, a) has com-
putable dimension ω.
Algorithmic Properties of Structures 129
Definition 4. Let d be a degree. A structure A with computable domain is d–
computable if its atomic diagram is d–computable. The degree of A, denoted by
deg(A), is the least degree d (which always exists) such that A is d–computable.
An isomorphism from a structure B to a d–computable structure with computable
domain is called a d–computable presentation of B. The degree of a presenta-
tion is the degree of the image of the presentation. If B has a d–computable
presentation then it is d–computably presentable.
Definition 5. The degree spectrum of a countable structure A (denoted by
DgSp(A)) is the set of degrees of presentations of A.
Theorem 3 (Slaman in [27]; Wehner in [28]). There is a structure A that
has presentations of every degree except 0. (In other words, DgSp(A) = D−{0},
where D is the set of all degrees.)
Theorem 4 (Goncharov, Harizanov, Knight, McCoy, Miller, Solomon
in [16]). For every successor ordinal α there exists a structure with presentations
in exactly those degrees of sets X, that ∆0α(X) is not equal to ∆
0
α. In particular,
for every finite n there exists a structure with presentations in exactly non–lown
degrees.
The other approach to study the computable presentation of a structure is
to compare the images of additional relations in these presentations. Here we
can look at degree spectra of relations on structures.
Definition 6. The degree spectrum of a relation U on the domain of a com-
putable structure A (denoted by DgSpA(U)), is the set of degrees of the images
of U in all computable presentations of A.
There exist many interesting examples of computable structures and relations
on their domains that have different degree spectra (see [19], [22], etc.).
The effective transformations from [14] and [21] show that many algorithmic
properties of structures may be reduced to algorithmic properties of graphs. On
the other hand, we can pass back from graphs to classes of all structures for
languages containing at least one n–ary relational symbol, where n ≥ 2. In this
paper we show that such a transition is also possible to structures for languages
with only two unary functions. The language, which contains either at least
binary relational symbol or two unary functional symbols is called rich. Thus,
the results of the present paper completes the picture for structures for all rich
languages.
The third approach to study connections between definability and com-
putability of classes of structures involves the notion of the index set. The well–
known result of Nurtazin [25] shows that there exists a universal computable
numbering of all computable structures for a fixed relational language. In other
words, there exists a computable sequence {An}n∈ω of computable structures,
such that for every computable B for this language we can effectively find An
which is isomorphic to B. The index set of a structure B for this language is the
set I(B) of all indices of computable (isomorphic) copies of B in this numbering.
130 Ekaterina B. Fokina
For a class K of structures, closed under isomorphism, the index set is the set
I(K) of all indices for computable members of K. Index sets were widely studied
by many authors (see, for example, [1], [2], [3], [4], [6], [7] [17], etc.)
Definition 7. Let Γ be a complexity class (e.g., Σ03). I(K) is m-complete Γ if
I(K) is Γ and for any S ∈ Γ , there is a computable function f such that
n ∈ S iff f(n) ∈ I(K).
This condition is equivalent to the condition that there is a uniformly computable
sequence {Cn}n∈ω for which
n ∈ S iff Cn ∈ K.
In [8] and [9] we studied the index sets of the following important classes
structures: decidable structures, decidable structures with countably categori-
cal theories and computable structures with decidable theories. We gave the
complete characterization of degrees of complexity of the index sets for classes
of structures for the language containing at least one n–ary relational symbol,
where n ≥ 2. This paper completes the proof for all rich languages.
2 Transformation from Graphs to L0–Structures
Let L0 = 〈f, g〉 be a language containing only two unary functional symbols. In
this section we give an effective transformation of the class of graphs into the
class of L0–structures. In the next two sections we’ll show that it preserves all
algorithmic and model–theoretic properties we are interested in.
Consider a graph G = 〈|G|, P 〉. Using G, we define a new L0–structure A, in
which G will be definable in a very nice way. We build A as follows.
The structure A has its universe consisting of 3 pairwise disjoint infinite sets:
|A| = U ∪ V ∪W,
where U = {ui | i ∈ |G|}, V = {vi | i ∈ |G|}, W =
⋃
iWi = {wij | i ∈ ω, j ∈ |G|}.
Now we need to define functions f and g on |A|. We do it in the following
way.
For all ui ∈ U we let f(ui) = ui. For all vi ∈ V we let f(vi) = ui. For all
wij ∈ Wi we let f(wij) = ui. That is, the function f is identical on the set U , it
maps V 1–to–1 onto U , and for every ui ∈ U there are infinitely many elements
wij ∈W that corresponds to ui via f .
Now we define the function g. For all vi ∈ V we let g(vi) = vi. For all ui ∈ U
we let g(ui) = vi. Values of g on elements from W depends on P (i, j). If P (i, j)
is true in G, then we choose a unique index k such that in A we have g(wik) = uj .
If ¬P (i, j) is true in G, then we choose a unique index k such that g(wik) = vj .
For different js we choose different kjs corresponding to P (i, js) or ¬P (i, js),
and every wik witnesses (via g(w
i
k)) one of P (i, j) or ¬P (i, j) for some (unique)
j. The function g defined as above is identical on the set V and maps U 1–to–1
onto V . Moreover, it allows us to define P and ¬P in A via existential formulas.
The precise proof is given in the next section.
Algorithmic Properties of Structures 131
3 Algorithmic Properties of the Class of L0–Structures
To get all the results on computable categoricity and computable dimensions of
structures and degree spectra of structures and relations we use the result from
[21]. In the paper some sufficient conditions were given for a transformation to
preserve all the properties mentioned above. Here we remind definitions and the
sufficient conditions from [21].
Definition 8. A countable structure A is trivial if for some finite set S of
elements of |A|, every permutation of |A| that keeps the elements of S fixed is
an automorphism of A.
The interesting case is nontrivial structures.
Definition 9. A relation U on a structure A is invariant if for every automor-
phism f : A → A we have f(U) = U .
Definition 10. A relation U on the universe of a structure A is relatively in-
trinsically computable if for every presentation f : A → A, the image f(U) is
computable in deg(A).
Now let G be a nontrivial, countable directed graph with edge relation E and
let A be a countable structure. The general idea of the sufficient conditions given
below is to say that the transformation from graphs to structures for different
language is rather effective. That is, the graph is definable in the structure in a
nice way which allows us to preserve all algorithmic properties.
We give a more precise formalization of this idea. Let G be a graph, and
A be a structure. Assume that there exist relatively intrinsically computable,
invariant relations D(x) and R(x, y) on the universe of A and a map G 7→ AG
from the set of presentations of G to the set of presentations of A with the
following properties.
(P0) For every presentation G of G, the structure AG is deg(G)-computable.
(P1) For every presentation G of G there exists a deg(G)–computable map gG :
D(AG)
1−1−→
onto
|G| such that RAG(x, y) ⇔ EG(gG(x), gG(y)) for every x, y ∈
D(AG).
(P2) If h : D(A) 1−1−→
onto
D(A) is such that R(x, y) ⇔ R(h(x), h(y)) then h can be
extended to an automorphism of A.
(P3) For every presentation G of G there exists a deg(G)–computable set of exis-
tential formulas ϕ0(a¯, b¯0, x), ϕ1(a¯, b¯1, x), . . . such that a¯ is a tuple of elements
of |AG|, each b¯i is a tuple of elements of D(AG), each x ∈ |AG| satisfies some
ϕi, and no two elements of |AG| satisfy the same ϕi.
Proposition 1. Satisfaction of the conditions (P0)–(P3) guarantees that the
following statements hold.
1. DgSp(A) = DgSp(G).
132 Ekaterina B. Fokina
2. If G is computably presentable then
(a) for any degree d, A has the same d–computable dimension as G;
(b) if x ∈ |G| then there exists an a ∈ D(A) such that (A, a) has the same
computable dimension as (G, x);
(c) if S ⊆ |G| then there exists U ⊆ |D(A)|, such that DgSpA(U) = DgSpG(S)
and if S is intrinsically c.e. then so is U .
The proof can be found in [21].
Therefore, what we need to do is to introduce invariant, relatively intrin-
sically c.e. relations D(x) and R(x, y) defining the universe of the graph and
the edge relation correspondingly and check the conditions (P0)–(P3) for the
transformation from Sect. 2.
First of all, it is easy to see that all the sets U, V and W are definable by
quantifier–free formulas. For example, U = {x|f(x) = x}. Thus, we may use
these sets in other formulas as abbreviation for their defining formulas. Under
this convention, we set D(x)
 U(x). That is, D(A) = UA.
We define R(x, y) and its negation via two existential formulas:
〈x, y〉 ∈ P ⇔ U(x)&U(y)&∃z(W (z)&(f(z) = x)&(g(z) = y)),
〈x, y〉 /∈ P ⇔ U(x)&U(y)&∃z, t(W (z)&V (t)&(f(z) = x)&(g(z) = t)&(g(y) = t)).
It is easily seen from the definitions that D(x) and R(x, y) are invariant and
relatively intrinsically c.e. Now we check the properties (P0)–(P3).
(P0) Follows directly from the construction of the transformation.
(P1) Let G be a presentation of G. Existence of deg(G)–computable map follows
from the definition of f ,g and from defining formulas for BAG and RAG .
(P2) Let h be a 1–to-1 map ofD(A) ontoD(A) such that R(x, y)⇔ R(h(x), h(y)).
We extend h in the following way. For v ∈ V we find the corresponding u ∈ U
such that g(u) = v. After that we let h(v) = g(h(u)). For w ∈ W we first
check if g(w) is in U or in V . If g(w) = u ∈ U then we find the unique w′ ∈W
connected with h(u). Such w′ exists by properties of h. We let h(w) = w′.
The second case is similar.
(P3) Let G be a presentation of G. Every element v from V is the unique element
satisfying the formula (g(v) = v)&(g(u) = v)&(f(u) = u) for corresponding
u ∈ D(AG)(= UAG). For w ∈ W the formula depends on the value of g(w).
If g(w) ∈ U then we write that there exists exactly one u ∈ U which is the
value of g(w). Similar for g(w) ∈ V .
Thus, we have proved the following theorems.
Theorem 5. Let G be a computably presentable graph. There exists a com-
putably presentable L0–structure A such that for any degree d, A has the same
d–computable dimension as G.
Corollary 1. For every 1 ≤ n ≤ ω there exist an L0–structure A with com-
putable dimension n.
Algorithmic Properties of Structures 133
Theorem 6. Let G be a computably presentable graph. There exists a com-
putably presentable L0–structure A such that for every x ∈ |G| then there exists
an a ∈ D(A) such that (A, a) has the same computable dimension as (G, x).
Corollary 2. For every 1 ≤ n ≤ ω there exists a computably categorical L0–
structure A and an element a ∈ |A|, such that the computable dimention of
(A, a) equals n.
Theorem 7. Let G be a graph. There exists a structure A for the language L0
with only 2 unary functions such that DgSp(A)=DgSp(G).
Corollary 3. There exists a computable L0–structure with presentations in all
degrees except 0.
Theorem 8. Let G be a computably presentable graph. There exists a com-
putably presentable L0–structure A such that if S ⊆ |G| then there exists a
U ⊆ |D(A)| such that DgSpA(U) = DgSpG(S) and if S is intrinsically c.e.
then so is U .
Corollary 4. For every computable ordinal α there exists a computable L0–
structure A and a relation R on |A|, such that R is intrinsically Σ0α but not
relatively intrinsically Σ0α.
4 Results on Index Sets
Now we turn to the question about connections between computability-theoretic
and structural properties of structures in classes of L0–structures. More precisely,
instead of the language L0 = 〈f, g〉 we consider a new relational language L =
〈Pf , Pg〉. Here the predicate symbols Pf and Pg are binary. We will think of
them as representing graphs of f and g correspondingly.
According to the Nurtazin’s result, there exists a universal computable num-
bering {An}n∈ω of all computable structures for the language L consisting of 2
binary relational symbols. When talking about index sets of L0–structures with
fixed algorithmic and model–theoretic properties, in fact, we consider index sets
of L–structures with the same properties and additional requirement on Pf , Pg.
Namely, we require that these relations represent graphs of total functions in
our structures.
Under these convention, using the results from [8] and [9] we prove the fol-
lowing theorem.
Theorem 9. 1. The index set CK of the class of decidable L0–structures is
m–complete Σ03 set;
2. The index set CK0 of the class of decidable L0–structures with countably
categorical theories is m–complete Σ03 −Σ03 set (where Σ03 −Σ03 is the class
of differences of two Σ03 sets);
3. The index set DT of the class of L0–structures with decidable theories is
m–complete Σ0,∅
(ω)
2 set.
134 Ekaterina B. Fokina
Proof. First of all, we need to proof that in any of these cases the index sets
belong to the corresponding classes of complexity.
Lemma 1. 1. CK ∈ Σ03 .
2. CK0 ∈ Σ03 −Σ03 (where Σ03 −Σ03 is the class of differences of two Σ03 sets).
3. DT ∈ Σ0,∅(ω)2 .
Proof of the lemma can be found in [8] and [9]. The only one additional
condition for Pf and Pg is to represent graphs of total functions. This condition
does not change the proofs.
To prove m–completeness of the index sets, we show that the transforma-
tion from Sect. 2 preserves decidability of structures as well as decidability and
countable categoricity of their theories. Then from the results about index sets
of graphs from [8] and [9] we get the proof of the theorem.
Proposition 2. Let G be a graph and let A be an L0–structure obtained from
G by the transformation from Sect. 2. Then
1. A is computable ⇔ G is computable;
2. A is decidable ⇔ G is decidable;
3. Th(A) is countably categorical ⇔ Th(G) is countably categorical;
4. Th(A) is decidable ⇔ Th(G) is decidable.
Proof. From the construction of the transformation and observations in Sect. 3
we get the first point.
Let G be decidable. We show that in this case A is also decidable. For this
we prove that the complete diagram Dc(A) is computably axiomatizable and
complete, hence, decidable. We consider the following set of axioms.
1. Pf and Pg are graphs of total functions.
2. For every x, y, if Pf (x, y) then Pf (y, y).
3. For every x, y, if Pg(x, y) then exactly one of the following holds: either
¬Pf (x, x)&∃zPg(y, z)&Pg(z, z) or Pf (x, x)&Pg(y, y).
4. For every x, if Pf (x, x) then there exists exactly one z, such that ¬Pf (z, z)
and ¬Pg(z, z) and exactly one of 2 conditions holds:
(a) Pg(z, x) or
(b) ∃y(Pg(x, y)&Pg(z, y)).
5. For every z, if ¬Pf (z, z)&¬Pg(z, z) then there exists a unique x, such that
Pf (x, x)&Pg(z, x) or Pg(x, x)&Pg(z, x).
6. For every sentence ψ from Dc(G) we put into the set of axioms a sentence
ψ′, which we define in the following way. We consider ψ to be in the prenex
normal form. If ψ is an open formula, let ψ′ be constructed from ψ by
the substitution of positive and negative occurrences of P (x, y) for their
definitions by existential formulas using Pf , Pg. If ψ = ∃xψ1, then we let
ψ′ = ∃x(Pf (x, x)&ψ′1). If ψ = ∀xψ1, then we let ψ′ = ∀x(Pf (x, x)→ ψ′1).
Algorithmic Properties of Structures 135
To prove that the theory is complete, we consider any two of its saturated
models A1,A2 of the cardinality ω1. We want to show that they are isomorphic.
We define the predicate P on the set {x|Pf (x, x)} using its definition by existen-
tial formulas using Pf and Pg. The resulting graphs G1,G2 are saturated models
of Dc(G), hence, they are isomorphic. Let ϕ : G1 → G2 give this isomorphism.
In a way, similar to the one from Sect. 3 we can extend the isomorphism to an
isomorphism between A1 and A2. Thus, the theory Dc(A) is axiomatizable by
the decidable set of axioms and complete, hence, it is decidable.
Similar reasoning gives the other two points.
This completes the proof of the theorem.
References
1. Calvert W.: The isomorphism problem for classes of computable fields. Archive for
Mathematical Logic. 75 (2004) 327–336.
2. Calvert W.: The isomorphism problem for computable Abelian p-groups of
bounded length. Journal of Symbolic Logic 70 (2005) 331–345.
3. Calvert W., Cummins D., Knight J. F., Miller S.: Comparing classes of finite
structures. Algebra and Logic 43 (2004) 374–392.
4. Calvert W., Harizanov V., Knight J. F., Miller S.: Index sets of computable struc-
tures. Algebra and Logic 45 (2006) 306–325.
5. Cholak P., Goncharov S. S., Khoussainov B., Shore R. A.: Computably categorical
structures and expansions by constants. J. Symbolic Logic 64 (1999) 13–37.
6. Csima B. F., Montalba´n A., Shore R. A.: Boolean algebras, Tarski invariants, and
index sets/ Notre Dame J. Formal Logic 47 (2006) 1–23.
7. Dobritsa V. P.: Complexity of the index set of a constructive model. Algebra and
Logic 22 (1983) 269–276.
8. Fokina E. B.: Index Sets of Decidable Models. Siberian Math. J. 48 (2007) 939–948
(English translation).
9. Fokina E. B.: Index Sets of Computable Structures with Decidable Theories. Com-
putation and Logic in the Real World - Third Conference of Computability in
Europe, CiE 2007, Siena, Italy, June 2007, Proceedings, Lecture Notes in Com-
puter Science, 4497 (2007) 290–296.
10. Goncharov S. S.: Problem of the number of non-self-equivalent constructivizations.
Algebra and Logic 19 (1980) 401–414.
11. Goncharov S. S.: Groups with a finite number of constructivizations. Soviet Math.
Dokl. 23 (1981) 58–61.
12. Goncharov S. S.: Limit equivalent constructivizations. Mathematical Logic and the
Theory of Algorithms, vol. 2 of Trudy Inst. Mat., Nauka, Sibirsk. Otdel., Novosi-
birsk, (1982) 4–12, in Russian.
13. Goncharov S. S.: Countable Boolean Algebras and Decidability. Siberian School of
Algebra and Logic, Consultants Bureau, New York (1997).
14. Goncharov S. S.: Computability and Computable Models. Mathematical problems
from applied logic. II. Logics for the XXIst century. Edited by Dov M. Gabbay,
Sergey S. Goncharov and Michael Zakharyaschev. International Mathematical Se-
ries (New York), Springer, New York (2007) 99–216.
15. Goncharov S. S., Dzgoev V. D.: Autostability of models. Algebra and Logic 19
(1980) 28–37.
136 Ekaterina B. Fokina
16. Goncharov, S., Harizanov V., Knight J.F., McCoy C., Miller R., Solomon R.: Enu-
merations in computable structure theory. Annals of Pure and Applied Logic 136
(2005), 219-246.
17. Goncharov S. S., Knight J. F.: Computable structure and non-structure theorems.
Algebra and Logic 41 (2002) 351–373 (English translation).
18. Goncharov S. S., Molokov A. V., Romanovskii N. S.: Nilpotent groups of finite
algorithmic dimension. Siberian Math. J. 30 (1989) 63–68.
19. Harizanov V.: The possible Turing degree of the nonzero member in a two element
degree spectrum. Ann. Pure Appl. Logic 60 (1993) 1–30.
20. Hirschfeldt D. R., Khoussainov B., Shore R. A.: A computably categorical structure
whose expansion by a constant has infinite computable dimension. J. Symbolic
Logic 68 (2003) 1199–1241.
21. Hirschfeldt D. R., Khoussainov B., Shore R. A., Slinko A. M.: Degree Spectra and
Computable Dimensions in Algebraic Structures Annals of Pure and Applied Logic
115 (2002) 71–113.
22. Khoussainov B., Shore R. A.: Computable isomorphisms, degree spectra of rela-
tions, and Scott families. Ann. Pure Appl. Logic 93 (1998) 153–193.
23. Khoussainov B., Shore R. A.: Solution of the Goncharov-Ash problem and the
spectrum problem in the theory of computable models. Doklady Math. 61 (2000)
178–179, research announcement, Russian version in Dokl. Akad. Nauk 371 (2000)
30–31.
24. Lempp S., McCoy C., Miller R., Solomon R.: Computable categoricity of trees of
finite height. J. Symbolic Logic 70 (2005) 151-215.
25. Nurtazin A. T.: Computable classes and algebraic criteria for autostability. Ph.D.
Thesis, Institute of Mathematics and Mechanics, Alma-Ata (1974).
26. Remmel J. B.: Recursively categorical linear orderings. Proc. Amer. Math. Soc. 83
(1981) 387–391.
27. Slaman T. A.: Relative to any nonrecursive set. Proc. Amer. Math. Soc. 126 (1998)
2117–2122.
28. Wehner S.: Enumerations, countable structures, and Turing degrees. Proc. Amer.
Math. Soc. 126 (1998) 2131–2139.
Verification of Newman’s and Yokouchi’s
Lemmas in PVS
Andre´ Luiz Galdino1,2 and Mauricio Ayala-Rinco´n1?
1 Instituto de Cieˆncias Exatas, Universidade de Bras´ılia, Brazil
2 Departamento de Matema´tica, Universidade Federal de Goia´s
Campus de Catala˜o, Brazil
{galdino, ayala}@unb.br
Abstract. This paper shows how a formalization for Abstract Reduc-
tion Systems (ARSs) in which noetherianity was defined by the notion
of well-foundness over binary relations is used in order to prove results
such as well-known Newman’s and Yokouchi’s Lemmas. The former is
known as the diamond lemma and the latter states a property of com-
mutation between ARSs. The theory ars was specified in the Prototype
Verification System (PVS) for which to the best of our knowledge there
are no available theory for dealing with rewriting techniques in general.
In addition to proof techniques available in PVS, the verification of these
lemmas implies an elaborated use of natural and noetherian induction.
Key words: Abstract Reduction Systems, Rewriting Systems, PVS.
1 Introduction
The Prototype Verification System (PVS), developed at the SRI and widely used
by industrial and academic parties, consists of a specification language built on
higher-order logic, which supports modularity by means of parameterized theo-
ries, with a rich type-system and a prover which uses the sequent-style. A PVS
theory, ars, built over the PVS prelude libraries for sets and binary relations
that is useful for the treatment of Abstract Reduction Systems (ARS) was re-
ported in [4]. In the theory ars basic ARS notions such as reduction, derivation,
normal form, confluence, local confluence, joinability, noetherianity, etc., were
adequately specified in such a way that non elementary proof techniques such
as noetherian induction are possible. In this paper we describe the usefulness of
ars by describing proofs of Newman’s and Yokouchi’s lemmas. The former proof
is a well-known classical application of noetherian induction and the latter is of
interest because it is based on several applications of natural and noetherian
induction. The inductive proof of the Newman’s Lemma given by Huet in [6] is
a classical example of proofs in higher-order logic.
The novelty of this work in not to present mechanical proofs of ARS theorems
in PVS that were done previously in other proof assistants. In fact, well-known
? Both authors are partially supported by the Brazilian Research Council, CNPq.
138 Andre´ Luiz Galdino and Mauricio Ayala-Rinco´n
formalizations of Newman’s Lemma have been specified in several proof assis-
tants, e.g., ACL2 [15], Coq [7], Isabelle [14], Boyer-Moore [16], Otter [3], among
others. ars is presented as the basis for the formalization of an elaborated and
robust PVS theory for full Term Rewriting Systems (TRSs) [5]. The motivation
for doing this work is the fact that rewriting has been applied to the specifica-
tion and synthesis of reconfigurable hardware [9] and that the correction of these
specifications can be carried out by translating these rewriting specifications into
the language of the PVS proof assistant as logic theories ([1] introduces a proved
correct translation from ELAN rewriting specifications into PVS theories). And
robust proof rewriting based methods are necessary to deal efficiently with the
correctness of these theories that come from rewriting based specifications.
Section 2 presents analytical proofs of both lemmas. Sections 3 and 4 describe
respectively the specification and verification of both lemmas in PVS. The the-
ory ars together with proofs of Newman’s, Yokouchi’s, among other interesting
lemmas is available at www.mat.unb.br/∼ayala/publications.html.
2 Background: analytical proofs
We suppose the reader is familiar with rewriting concepts and standard notations
as presented in [2] or [17].
An abstract reduction relation is a binary relation R over a set T , denoted
also as 〈R, T 〉. The relation is identified as R, →R or simply →. R+ and R∗
respectively denote the transitive and the reflexive transitive closure of R, de-
noted in arrow notation as →+ and →∗, respectively. In the elegant arrow no-
tation, the inverse relation R−1, its transitive and its reflexive transitive clo-
sures are respectively denoted as ←, +← and ∗←. The operator of compo-
sition is denoted as usual as ◦. An abstract reduction relation → over T is
said to be: confluent whenever (∗← ◦ →∗) ⊆ (→∗ ◦∗←) and locally confluent
whenever (← ◦ →) ⊆ (→∗ ◦∗←). → satisfies the diamond property whenever
(← ◦ →) ⊆ (→ ◦ ←). Two elements of T , say x, y, are said to be joinable when-
ever ∃u.x →∗ u ∗← y. → is said to be noetherian whenever there is no infinite
sequence of the form x1 → x2 → · · · .
Lemma 1 (Newman’s Lemma [11]). Let R be a noetherian relation defined
on the set T . Then R is confluent if, and only if it is locally confluent.
proof (Sketch). The⇒-direction follows immediately by definition.⇐-direction
is proved by noetherian induction using the predicate
P (x) = ∀y, z. y ∗← x→∗ z =⇒ y and z joinable
Obviously R is confluent if P (x) holds for all x. Noetherian induction require
us to show P (x) under the assumption P (t) for all t such that x→+ t. To prove
P (x), we analyze the divergence y ∗← x →∗ z. If x = y or x = z, y and z are
joinable immediately. Otherwise we have x → y1 →∗ y and x → z1 →∗ z as
shown in the Figure 1(a), where as usual dashed arrows stand for existence. The
existence of u follows by local confluence (LC) of R, the existence of v and w
follows by induction hypothesis (Ind). 
Verification of Newman’s and Yokouchi’s Lemmas in PVS 139
x

// z1
∗ 

∗ // z
∗ 


 x
S

R
// z
R∗◦S◦R∗



 x
S

R∗
// z
R∗◦S◦R∗




LC
y1
∗ 
∗ //____ u
∗ 
 Ind D D′
Ind
y ∗ //____ v ∗ //____ w y R
∗
//___ u y R
∗
//___ u
(a) (b) (c)
Fig. 1. Proof of Newman’s Lemma, Diagram D and Generalization of D as D′
Lemma 2 (Yokouchi’s Lemma [18]). Let R and S be two relations defined
on the same set T , R being confluent and noetherian, and S having the diamond
property. Suppose moreover that the diagram D as shown in the Figure 1(b)
holds: Then the relation R∗ ◦ S ◦R∗ has the diamond property.
proof (Sketch). The proof starts by generalizing the diagram D of the lemma
as the diagram D′ in the Figure 1(c). This generalization is proved by noetherian
induction using the predicate
P (x) := ∀y, z. xR∗z ∧ xSy ⇒ ∃u.(yR∗u ∧ zR∗ ◦ S ◦R∗u)
Then, to prove that R∗ ◦S ◦R∗ has the diamond property, one also proceeds by
noetherian induction but this time using the predicate
P ′(x) := ∀y, z. xR∗◦S◦R∗y ∧ xR∗◦S◦R∗z ⇒ ∃u.(yR∗◦S◦R∗u ∧ zR∗◦S◦R∗u)
One concludes, by induction in the length of the derivation of the first R∗ in
xR∗ ◦S ◦R∗y. In other words, we distinguish between the cases xR◦R∗ ◦S ◦R∗y
and xS ◦ R∗y as is shown in the Figure 2, where C and DP stand for use of
confluence of R and diamond property of S hypotheses, respectively. 
x
R

R∗ // z1
R∗



S // z2
R∗



R∗ // z
R∗


 x
S

S // z1
S



R∗ // z
R∗◦S◦R∗



C D′ C DP D′
y1
R∗◦S◦R∗

R∗
//___ u1
R∗◦S◦R∗
//____ u2
R∗
//___ u3
R∗◦S◦R∗


 y1
R∗

S
//____ u1
R∗


 R∗
//___ u2
R∗



Ind D′ C
y
R∗◦S◦R∗
//____________ u y
R∗◦S◦R∗
//____ u3
R∗
//____ u
Fig. 2. Cases xR ◦R∗ ◦ S ◦R∗y and xS ◦R∗y
3 Specification
The PVS theory newman yokouchi presented in Table 1 specifies Newman’s and
Yokouchi’s lemmas. This theory is parameterized as newman yokouchi[T], where
(within the newman yokouchi theory) T is treated as a fixed uninterpreted type.
140 Andre´ Luiz Galdino and Mauricio Ayala-Rinco´n
Table 1. newman yokouchi PVS theory
newman_yokouchi[T : TYPE] : THEORY BEGIN
IMPORTING results_confluence[T], noetherian[T]
R, S: VAR PRED[[T,T]]
Newman_lemma: THEOREM
noetherian?(R) => (confluent?(R) <=> local_confluent?(R))
Yokouchi_lemma_ax1: LEMMA
(noetherian?(R) & confluent?(R) &
(FORALL x,y,z: (S(x,y) & R(x,z)) =>
(EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))))
=> (FORALL x,y,z: (S(x,y) & RTC(R)(x,z)) =>
(EXISTS (w:T): RTC(R)(y,w) & (RTC(R) o S o RTC(R))(z,w)))
Yokouchi_lemma: THEOREM
(noetherian?(R) & confluent?(R) & diamond_property?(S) &
(FORALL x,y,z: (S(x,y) & R(x,z)) =>
(EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))))
=> diamond_property?(RTC(R) o S o RTC(R))
END newman_yokouchi
R and S denote reduction relations over T and x, y, z, w and u elements of T.
=>, <=> and & are abbreviations for IMPLIES, IFF and AND, respectively.
Newman’s Lemma specification is straightforward and based on predicates
over reduction relations. In the specification of the Yokouchi’s Lemma RTC(R) de-
notes the reflexive transitive closure of the relation R, i.e. R∗. The composition of
relations is denoted as o. The second lemma, Yokouchi lemma ax1, corresponds
to the generalization D′ of the diagram D presented in Figure 1.
The theories results confluence[T] and noetherian[T], also components
of the whole ars theory, are imported by the newman yokouchi theory. The for-
mer contains results about confluence and the latter the definition of noetheri-
anity formulated in terms of the notion of well-foundness as it will be explained.
Specifications of rewriting properties are exemplified by the following ones.
joinable?(R)(x,y): bool = EXISTS z:RTC(R)(x,z) & RTC(R)(y,z)
local_confluent?(R): bool = FORALL x,y,z: R(x,y) & R(x,z) =>
joinable?(R)(y,z)
confluent?(R): bool = FORALL x,y,z: RTC(R)(x,y) & RTC(R)(x,z) =>
joinable?(R)(y,z)
diamond_property?(R): bool = FORALL x,y,z: R(x,y) & R(x,z) =>
EXISTS r: R(y,r) & R(z,r)
In order to make easy the use of natural induction, closures of relations are
built as unions of iterations of their compositions. For instance, the reflexive
transitive closure operator RTC of a relation R is specified as the union of the
iterations iterate(R,i), for all i ≥ 0, where iterate(R,i) specifies the relation
Verification of Newman’s and Yokouchi’s Lemmas in PVS 141
R ◦ R · · · ◦ R︸ ︷︷ ︸
i times
available in the PVS orders library. The operators iterate and
IUnion, available in the PVS prelude theory are specified as
iterate(R,i): RECURSIVE PRED[[T,T]] =
IF i = 0 THEN =[T] ELSE iterate(R,i - 1) o R ENDIF
MEASURE i
IUnion(A): set[T] = {x | EXISTS i: A(i)(x)}
and using them, RTC can be specified as
RTC(R): reflexive_transitive = IUnion(LAMBDA n: iterate(R,n))
Notice that the type of RTC is reflexive transitive. This places in the
basis of the system of types the intrinsic characteristics of the RTC construction
creating the necessary proofs obligations to be proved (PVS Type Correctness
Conditions - TCCs). This type is specified as follows.
reflexive_transitive?(R): bool = reflexive?(R) & transitive?(R)
reflexive_transitive: TYPE = (reflexive_transitive?)
Noetherianity is formalized in terms of well foundness, and noetherian in-
duction is then verified using the lemma wf induction, which expresses the
principle of well-founded induction. The notion of well-founded relations and
this principle are available in the prelude theory [12].
noetherian?(R): bool = well_founded?(converse(R))
noetherian: TYPE = (noetherian?)
wf_induction: LEMMA
(FORALL (p: pred[T]): (FORALL x: (FORALL y:
y < x => p(y)) => p(x)) => (FORALL (x:T): p(x)))
The lemma noetherian induction presented below corresponds to the prin-
ciple of noetherian induction.
noetherian_induction: LEMMA (FORALL (R: noetherian, P: PRED[T]):
(FORALL x, y: (TC(R)(x,y) => P(y)) => P(x)) => (FORALL x: P(x)))
This lemma uses the transitive closure operator TC, that is specified similarly
to RTC. Its application depends on an adequate instantiation of the predicate P.
4 Verification
The verification of Newman’s and Yokouchi’s lemmas consists of 1857 lines
(168247 bytes) of proofs. The formalizations of Newman’s and Yokouchi’s lem-
mas use 114 and 225 proof steps, respectively. Here the relevant fragment of the
proof trees, focusing on the application of noetherian induction, are presented.
142 Andre´ Luiz Galdino and Mauricio Ayala-Rinco´n
4.1 Verification of Newman’s Lemma
When the PVS prover is invoked the proof tree starts off with a root node
(sequent) having no antecedent and as succedent the theorem to be proved.
Newman_lemma :
|-------
{1} FORALL (R: PRED[[T,T]]):
noetherian?(R) => (confluent?(R) <=> local_confluent?(R))
The reduction relation R is correctly universally quantified, since it was de-
clared as a variable in the theory newman yokouchi (see Table 1). After skolem-
ization by applying the proof command (skeep), the conjunctive splitting com-
mand (split) is applied to the goal obtaining two subgoals. The first sub-
goal, Newman lemma.1, is to demonstrate that confluence implies local conflu-
ence, which is easily formalized. The second subgoal, Newman lemma.2, that is
to demonstrate that local confluence implies confluence (under noetherianity
hypothesis), is the truly interesting one.
For proving this subgoal, after disjuntive simplification with (flatten), one
introduces the noetherian induction scheme noetherian induction and instan-
tiates its predicate P as:
(LAMBDA (a:T): (FORALL (b,c:T): RTC(R)(a,b) & RTC(R)(a,c)
=> joinable?(R)(b,c)))
Then, the subgoals Newman lemma.2.1 and 2.2 presented below are obtained
by applying the command (split), that splits the implication of the instantiated
noetherian induction scheme. The first subgoal is easily verified by expanding the
definition of the predicate confluent?, skolemization and adequate instantiation
of the variables of the antecedent {-1}.
Newman_lemma.2.1 :
{-1} FORALL (x:T): FORALL (b,c:T):
RTC(R)(x,b) & RTC(R)(x,c) => joinable?(R)(b,c)
[-2] local_confluent?(R) [-3] noetherian?(R)
|-------
[1] confluent?(R)
Newman_lemma.2.2 :
[-1] local_confluent?(R) [-2] noetherian?(R)
|-------
{1} FORALL (x:T): (FORALL (y:T): TC(R)(x,y) => (FORALL (b,c: T):
RTC(R)(y,b) & RTC(R)(y,c) => joinable?(R)(b,c)))
=> (FORALL (b,c:T): RTC(R)(x,b) & RTC(R)(x,c)=> joinable?(R)(b,c))
To prove the latter subgoal, one needs to show P(x) under the assumption
P(y) for all y such that x→+ y. After skolemization, expansion of the definition
of RTC and hidding unnecessary formulas one obtains the following sequent.
Verification of Newman’s and Yokouchi’s Lemmas in PVS 143
Newman_lemma.2.2 :
[-1] FORALL (y:T): TC(R)(x,y) => (FORALL (b,c:T):
RTC(R)(y,b) & RTC(R)(y,c) => joinable?(R)(b,c))
{-2} iterate(R,i)(x,b) {-3} iterate(R,j)(x,c)
[-4] local_confluent?(R) [-5] noetherian?(R)
|-------
[1] joinable?(R)(b, c)
To prove this goal, one analyzes the cases x = b or x = c or b 6= x 6= c. To
contemplate these cases one uses the command (case-replace "i = 0") which
replaces i by 0 in the current subgoal and generates a second subgoal for the
case x = b. Similarly, the case x = c is proved. The case x 6= b and x 6= c, i.e.,
x → x1 →∗ b and x → x2 →∗ c corresponds to the following sequent obtained
after some simplifications. Compare with the diagram of Figure 1(a) (replacing
some variable symbols: u, v and w).
Newman_lemma.2.2.2.2.1.1 :
[-1] RTC(R)(x1,b) [-2] RTC(R)(x2,c)
[-3] FORALL (y:T): TC(R)(x,y) => (FORALL (b1,c1:T):
RTC(R)(y,b1) & RTC(R)(y,c1) => joinable?(R)(b1,c1))
[-4] R(x,x1) [-5] R(x,x2) [-6] RTC(R)(x1,u) [-7] RTC(R)(x2,u)
[-8] noetherian?(R)
|-------
[1] j = 0 [2] i = 0 [3] joinable?(R)(b,c)
Firstly, make a copy of the formula -3 by using (copy -3).
The existence of u follows by expanding local confluent? (instantiated
with variables x, x1 and x2), joinable?, by introducing skolem constants (u)
and by applying disjunctive simplification flatten. Then one applies the lemma
R subset TC, which states that a relation is contained in its transitive closure
and one proves that x →+ x1 and x →+ x2. Thus, the existence of v and w
follows by induction hypothesis, that is by instantiating [-3] conveniently, and
the lemma follows.
4.2 Verification of Yokouchi’s Lemma
The verification starts with the sequent.
Yokouchi_lemma :
|-------
{1} FORALL (R, S: PRED[[T,T]]):
(noetherian?(R) & confluent?(R) & diamond_property?(S) &
(FORALL x,y,z: (S(x,y) & R(x,z)) =>
(EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))))
=> diamond_property?(RTC(R) o S o RTC(R))
After skolemization and propositional flattening, one introduces the auxiliary
lemma Yokouchi lemma ax1 corresponding to the generalization D′ in Figure 1.
Then one obtains the new goal:
144 Andre´ Luiz Galdino and Mauricio Ayala-Rinco´n
Yokouchi_lemma :
{-1} FORALL (R, S: PRED[[T,T]]):
(noetherian?(R) & confluent?(R) &
(FORALL x,y,z: (S(x,y) & R(x,z)) =>
(EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))))
=> (FORALL x,y,z: (S(x,y) & RTC(R)(x,z)) =>
(EXISTS (w:T): RTC(R)(y,w) & (RTC(R) o S o RTC(R))(z,w)))
[-2] noetherian?(R) [-3] confluent?(R) [-4] diamond_property?(S)
[-5] FORALL x,y,z: (S(x,y) & R(x,z))
=> (EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))
|-------
[1] diamond_property?(RTC(R) o S o RTC(R))
Notice that the antecedents [-2], [-3] and [-5] correspond to the hypothe-
ses of {-1}. Then, after a suitable instantiation of {-1}, propositional simplifi-
cation and expansion of the definition diamond property?, one introduces the
noetherian induction scheme instantiating its predicate P as
LAMBDA (a:T):(FORALL (b,c:T):
(RTC(R) o S o RTC(R))(a,c) & (RTC(R) o S o RTC(R))(a,b)
=> (EXISTS (d:T):
(RTC(R) o S o RTC(R))(b,d) AND (RTC(R) o S o RTC(R))(c,d)))
Then, after splitting conjunctions, one obtains the following two subgoals:
Yokouchi_lemma.1 :
{-1} FORALL (x:T): FORALL (b,c:T):
(RTC(R) o S o RTC(R))(x,c) & (RTC(R) o S o RTC(R))(x,b)
=> (EXISTS (d:T):
(RTC(R) o S o RTC(R))(b,d) & (RTC(R) o S o RTC(R))(c,d))
...
[-7] (RTC(R) o S o RTC(R))(x,y) [-8] (RTC(R) o S o RTC(R))(x,z)
|-------
[1] EXISTS (r:T):
(RTC(R) o S o RTC(R))(y,r) & (RTC(R) o S o RTC(R))(z,r)
and
Yokouchi_lemma.2 :
[-1] FORALL x,y,z: (S(x,y) & RTC(R)(x,z))=>
(EXISTS (w:T): RTC(R)(y,w) & (RTC(R) o S o RTC(R))(z,w))
[-2] noetherian?(R) [-3] confluent?(R)
[-4] FORALL(x:T), (y:T), (z:T):
S(x,y) & S(x,z) => (EXISTS (r:T): S(y,r) & S(z,r))
[-5] FORALL x,y,z: (S(x,y) & R(x,z)) =>
(EXISTS (u:T): RTC(R)(y,u) & (RTC(R) o S o RTC(R))(z,u))
[-6] (RTC(R) o S o RTC(R))(x,y) [-7] (RTC(R) o S o RTC(R))(x,z)
|-------
Verification of Newman’s and Yokouchi’s Lemmas in PVS 145
{1} FORALL (x:T): (FORALL (y:T): TC(R)(x,y) =>
(FORALL (b,c:T): (RTC(R) o S o RTC(R))(y,c) &
(RTC(R) o S o RTC(R))(y,b)
=> (EXISTS (d:T): (RTC(R) o S o RTC(R))(b,d) &
(RTC(R) o S o RTC(R))(c,d))))
=> (FORALL (b,c:T):
(RTC(R) o S o RTC(R))(x,c) & (RTC(R) o S o RTC(R))(x,b)
=> (EXISTS (d:T): (RTC(R) o S o RTC(R))(b,d) &
(RTC(R) o S o RTC(R))(c,d)))
The subgoal Yokouchi lemma.1 is easily verified instantiating adequately
the antecedent [-1] and asserting. The subgoal Yokouchi lemma.2 is proved
following the scheme in Figure 2 as detailed below.
1. First step: introduce Skolem constants and consider the cases x = z1 and/or
x = y1.
2. Second step: invoke the lemma iterate RTC which states that for all n,
iterate(R,n) ⊆ RTC(R); expand the definitions of composition of relations,
confluent? and joinable?; hide irrelevant formulas; and then, apply dis-
junctive simplification.
3. Third step: conclude applying the confluence of R, the auxiliary lemma
Yokouchi lemma ax1 and induction hypothesis.
5 Conclusions and Future Work
This work illustrates that the ars theory, previously presented in [4], is in fact
adequate for formalizing (well-known) non elementary results of ARSs. The for-
malizations of Newman’s and Yokouchi’s lemmas were described focusing on the
key proof steps related with the applications of noetherian induction. Also it
should be stressed here, that although this work does not advance the state of
the art in the formalization of mathematics, since specifications of ARSs and
even of TRSs are available since the development of the Rewriting Rule Labo-
ratory (RRL) in the 1980’s [8], it is of practical interest doubtless. In fact, the
availability of rewriting proving technologies are essential in any modern proof
assistant and to the best of our knowledge before the development of the ars
theory neither rewriting theories nor rewriting libraries were available in PVS.
As current work we are developing trs, a more elaborated PVS theory for
dealing with TRSs [5], that is of interest to verify the correction of concrete
rewriting based specifications of computational objects such as reconfigurable
hardware as mentioned in the introduction. By this extension rewriting strate-
gies and new tactic-based proving techniques will be available in PVS in a natural
manner. For this purpose, the type term built over a signature of function sym-
bols is specified as an abstract data type [13] with the type of function symbols
and the type of variables as its parameters. In the trs theory the type term is
the actual parameter of the theory ars[T]. From this point, term positions
are given as usual by finite sequences of naturals, and useful operations on terms
146 Andre´ Luiz Galdino and Mauricio Ayala-Rinco´n
such as subterm at a given position and replacement of a subterm at a given
position by using recursive declarations; substitutions are functions from vari-
ables into term. All ars definitions and results hold for the reduction relation
induced over term by an specific TRS which is specified as a binary relation over
term. The induced reduction relation is given by closing the rewriting one under
substitutions and structure of terms (signature operations) as it is formalized in
the standard rewriting literature [2, 17].
References
1. M. Ayala-Rinco´n and T. M. Sant’Ana. SAEPTUM: Verification of ELAN Hardware
Specifications using the Proof Assistant PVS. In 19th Symp. on Integrated Circuits
and System Design - SBCCI06, pages 125–130. ACM Press, 2006.
2. F. Baader and T. Nipkow. Term Rewriting and All That. CUP, 1998.
3. M. Bezem and T. Coquand. Neman’s Lemma - a Case Study in proof automation
and geometric logic. Bull. of the EATCS, 79(86-100), 2003.
4. A.L. Galdino and M. Ayala-Rinco´n. A Theory for Abstract Rewriting Systems
in PVS. In 33th Latin American Conference on Informatics - CLEI07. Available:
www.mat.unb.br/∼ayala/publications.html.
5. A.L. Galdino and M. Ayala-Rinco´n. A PVS Theory for Term Rewriting Systems.
2008. Available: www.mat.unb.br/∼ayala/publications.html.
6. G. Huet. Confluent Reductions: Abstract Properties and Applications to Term
Rewriting Systems. J.ACM, 27(4):797–821, 1980.
7. G. Huet. Residual Theory in λ-calculus: A Formal development. Jornal of Func-
tional Programming, 4(3):371–394, 1994.
8. D. Kapur and H. Zhang. An overview of Rewrite Rule Laboratory (RRL). In
N. Dershowitz, editor, Proc. 3rd Int. Conf. on Rewriting techniques and Applica-
tions, LNCS, 355, 1989.
9. C. Morra, J. Becker, M. Ayala-Rinco´n, and R. W. Hartenstein. FELIX: Using
Rewriting-Logic for Generating Functionally Equivalent Implementations. In 15th
Int. Conference on Field Programmable Logic and Applications - FPL 2005, pages
25–30. IEEE CS, 2005.
10. C. Mun˜oz and M. Mayero. Real automation in the field. ICASE Interim Report
39 NASA/CR-2001-211271, NASA Langley Research Center, 2001.
11. M. H. A. Newman. On theories with a combinatorial definition of ”equivalence”.
Ann. of Math., 43(2):223–243, 1942.
12. S. Owre and N. Shankar. The PVS Prelude Library. Technical report, SRI-CSL-
03-01, Computer Science Laboratory, SRI International, Menlo Park, CA, 2003.
13. S. Owre and N. Shankar. Abstract datatypes in PVS. Technical Report SRI-
CSL-93-9R, Computer Science Laboratory, SRI International, Menlo Park, CA,
December 1993. Extensively revised June 1997.
14. O. Rasmussen. The church-rosser theorem in isabelle: A proof porting experiment.
Technical Report 364, University of Cambridge, Computer Lab., 1995.
15. J. L. Ruiz-Reina, J. A. Alonso, M. J. Hidalgo, and F.J. Mart´ın-Mateos. Formalizing
Rewriting in the ACL2 Theorem Prover. LNCS, 1930, 2001.
16. N. Shankar. A Mechanical Proof of the Church-Rosser theorem. J.ACM, 35:475–
522, 1988.
17. C. J. van Rijsbergen, editor. Term Rewriting Systems. CUP, 2003.
18. H. Yokouchi. Church-Rosser Theorem for a Rewriting System on Categorical Com-
binators. Theoretical Computer Science, 65(3):271–290, 1989.
Computation over Groups
Christine Gaßner?
Institut fu¨r Mathematik und Informatik, Ernst-Moritz-Arndt-Universita¨t,
F.-L.-Jahn-Straße 15 a, 17487 Greifswald, Germany
gassnerc@uni-greifswald.de
Abstract. We discuss the uniform model of computation over groups,
in particular the P =? NP problems. We consider relativizations of the
P =? NP question for groups, and we discuss in which setting the known
method to extend and to expand structures in order to get P = NP for
new structures over strings cannot be used.
1 Introduction
In [4] L. Blum, M. Shub, and S. Smale (BSS) introduced the uniform model of
computation over the reals where the constants, the operations, and the rela-
tions in (IR; IR; ·,+,−;≥) can be used in the instructions of programs. Ideas and
methods of classical complexity theory (cf. [1, 2], for instance) were transferred
to the BSS model (cf. [3, 4, 5]). The uniform model of computation over arbitrary
algebraic structures considered in [6–10] was developed in analogy to the BSS
model in order to investigate the computation over structures like groups. Here,
we define this model for groups only. We want to discuss some special features of
the computation over groups resulting from the existence of only one constant,
which is neutral with respect to computations, and from properties of groups
which are countable, which contain an element of infinite order or all elements
of which have finite order. We define NP-complete problems, in particular those
which are similar to the classical setting. We construct oracles O in a uniform
way for some classes of groups such that there holds POS = NP
O
S or P
O
S 6= NPOS
for several kinds of S-machines over groups. Moreover, we explain why it is not
possible to construct new relations over extensions of any group G in the known
way in order to get PS = NPS for new structures S over strings if not permitting
additional parameters.
For groups (G; e; ◦; =) we consider the computation over (G; e; ◦; =) without
parameters and over (G;G; ◦; =) with parameters. The computation without
additional parameters means that only the unit element e can be a machine
constant. In the other case, we allow any element to be a machine constant. Any
deterministic (BSS) machine restricted to a group (G; e; ◦; =), (for instance, to
(IR; 0;+;=)) cannot compute new values in G \ {e} from an input of the form
(e, . . . , e). For this reason, neither the old form of machines in [4] nor the new
form of machines in [3] is suitable for computation over groups. Therefore, in the
? I thank R. Bialowons, M. Kla¨re, R. Schimming, and the referees for helpful hints.
148 Christine Gaßner
definition of our model some features of the BSS model have to be modified so
that the model becomes suitable for groups. Additionally to an infinite number of
registers for the elements of the structure, each machine is provided with a finite
number of index registers. The index registers can be used in order to simulate
the usual while and loop instructions in processing all input values x1, . . . , xn for
any n. In the model over the reals introduced in [4], two additional index registers
are sufficient since auxiliary calculation, necessary to compute new indices, can
be executed by means of the elements of the ring. In [3] the BSS machines assume
a new form which is similar to the form of special Turing machines whose cells
can contain real numbers. A blank symbol and index registers are not used. The
length of the inputs (x1, . . . , xn) is determined by storing the unary code of n in
additional registers on the left side of the point.
. . . , 0, 0, 1, . . . , 1︸ ︷︷ ︸
n×
. x1, x2, . . . , xn, 0, 0, . . .
Hence, for this model, at least two constants are necessary.
2 The Model of Computation over Groups
For each group G = (G; e; ◦; =) where |G| ≥ 2, let G¯ = (G;G; ◦; =). For the
structures S ∈ {G, G¯}, every S-machine M is provided with registers Z1, Z2, . . .
for the elements of G and with a fixed number of registers I1, I2, . . . , IkM for
indices in IN+ = IN \ {0}. For an input (x1, . . . , xn) ∈ G∞ =df
⋃∞
i=1G
i, the se-
quence x1, . . . , xn−1, xn, xn, xn, . . . is assigned to the registers Z1, Z2, . . . and the
index registers get the content n. The labelled computation, copy, and branching
instructions have the form Zj := Zj1 ◦ Zj2 ;, Zj := e; (resp. Zj := g; for any
machine constant g ∈ G), ZIj := ZIk ;, and if Zj = Zk then goto l1 else goto l2;.
The index registers are used in the copy instructions. For copying, we also allow
Ij := 1;, Ij := Ij + 1;, and if Ij = Ik then goto l1 else goto l2;. If the output
instruction is reached, then (Z1, . . . , ZI1) is the output and the machine halts.
Note that the machines over another structure S can use all operations and
all relations of S in the computation and branching instructions, respectively.
The non-deterministic machines are able to guess an arbitrary number of
arbitrary elements y1, . . . , ym ∈ G in one step after the input and to assign the
guesses to ZI1+1, . . . , ZI1+m. Note that we make no restriction of the domain
for m. To simplify matters, m is independent of n. However, a machine can use
at most t guesses within t steps. In any case, the size of an input (x1, . . . , xn)
is, by definition, its arity n. The digital (or binary) non-deterministic machines
cannot guess any elements. They only can execute additional instructions of the
form goto l1 or goto l2;. Semi-decidability can be defined as usual. If e is the
only constant, then we say that the decidability of a problem A ⊆ G∞ results
from the semi-decidability of A and of G∞ \ A. Moreover, oracle machines can
execute if (Z1, . . . , ZI1) ∈ O then goto l1 else goto l2; for some oracle O ⊆ G∞.
Let MS(O) and M[D]NS (O) be the sets of deterministic and [digital] non-
deterministic S-machines, respectively, which can use the oracle O.
Computation over Groups 149
Further, let POS , DNP
O
S , and NP
O
S denote the usual complexity classes of deci-
sion problems A ⊆ G∞ decided or non-deterministically recognized by an oracle
machine inMS(O), inMDNS (O), or inMNS (O) in polynomial time (where an input
is in A iff this machine halts on this input: without guesses, for some guessed se-
quence of instructions, or for some guesses), and let PS = P∅S , DNPS = DNP
∅
S ,
and NPS = NP∅S . Note, that, for groups G containing at least two elements,
we have PO¯G ⊆ DNPO¯G ⊆ NPO¯G and POG ⊆ DNPOG ⊆ NPOG . For groups with a
finite universe G = {g1, . . . , gk} where k ≥ 2 we get DNPO¯G = NPO¯G since a
DNPO¯G -machine can select one of the instructions Zj := g1; . . . ;Zj := gk; non-
deterministically.
3 A Finite Group with PG 6= NPG
For all infinite abelian groups G, there holds PG 6= DNPG and PG¯ 6= DNPG¯
(cf. [13, 6, 14, 12]). Moreover, it is known that there are infinite abelian groups
G satisfying DNPG¯ = NPG¯ [11]. On the other hand there are infinite abelian
groups G satisfying DNPG 6= NPG and DNPG¯ 6= NPG¯ . For instance, there holds
{x ∈ ZZ | (∃y ∈ ZZ)(x = y + y)} ∈ NP(ZZ;0;+;=) \DNP(ZZ;ZZ;+;=).
In order to demonstrate the difference between the computation with and
without parameters let us consider the dihedral group D2,4 = (D; e; ·; =) of
order 8, the elements of which are the isometric transformations of a square
onto itself. The group can be described by two generating elements r (a rotation
of a square) and s (a reflection of a square) where r4 = e , s2 = e, and sr = r3s
hold. We have D = {e, r, r2, r3, s, rs, r2s, r3s}. The center of the group which
is, by definition, the set C = {x ∈ D | (∀y ∈ D)(x · y = y · x)} consists of the
elements e and r2. The elements e, r2, s, rs, r2s, r3s are of order 2, the elements
r and r3 are of order 4.
Proposition 1. For D2,4 = (D; e; ·; =), there holds PD2,4 6= NPD2,4 .
Proof. For D2,4 = (D; e; ·; =), we have D \ C ∈ NPD2,4 since for any x ∈ D \ C,
an element y can be guessed for which x · y 6= y · x holds.
On the other hand, if M is any deterministic D2,4-machine, then all inputs
x ∈ D (which have the arity 1) of order 2 have the same computation path.
Thus, M accepts all inputs x ∈ D of order 2 or, in the other case, it rejects
all inputs x ∈ D of order 2. Consequently, no deterministic D2,4-machine can
decide the problem D \ C, since r2 ∈ C and s ∈ D \ C are of the same order.
Therefore, D \ C is not in PD2,4 . uunionsq
For D¯2,4 = (D;D; ·; =), the simulation of any D¯2,4-machine by a Turing machine,
and vice versa, is possible. Since any element of D can be encoded by a tuple
over {0, 1}, a Turing machine can simulate any computation over D, especially, it
can simulate the instruction Zj := d; by using the code of d. On the other hand,
any machine can simulate a Turing machine if it possesses two constants. If, in
contrast to the D2,4-machines, any element of D is permitted to be a machine
constant, we get compatible results to the classical setting.
150 Christine Gaßner
Proposition 2. PD¯2,4 = NPD¯2,4 holds if and only if P = NP holds.
4 NP-Complete Problems
For any group G = (G; e; ◦; =), we can define a universal non-deterministic G¯-
machine which is able to simulate each step of any non-deterministic G¯-machine
M on an input x in polynomial time if it gets x and a suitable code of M as
input.
Let G∪{a, b} be an alphabet where a 6= b and {a, b} ⊆ G or a = 0 and b = 1.
In order to define such a universal problem, we first encode the programs of
G¯-machines by strings in (G ∪ {a, b})∗ and then we transform these strings into
suitable tuples. We distinguish between strings and tuples since the strings of
the length k have to be transformed in k-tuples in order to store it in k registers.
Definition 1. For every non-empty string s = c1 · · · ck ∈ (G∗)(=k) ⊂ G∗, let
dse be the representation of s in the form of a tuple (c1, . . . , ck) ∈ Gk ⊂ G∞,
that means that dc1 · · · cke = (c1, . . . , ck).
Definition 2. Let Code∗a,b be an injective mapping of the set of all determin-
istic and non-deterministic G¯-machines into (G ∪ {a, b})∗ such that, with the
exception of the machine constants in G \ {e}, every character of the program is
unambiguously translated into a string in {a, b}∗ by this mapping and such that
the codes contain the sub-string b2 as prefix only and the last character of the
codes is a. The machine constants are encoded by themselves. For any G¯-machine
M and a, b ∈ G, let Codea,b(M) = dCode∗a,b(M)e.
Note that (x, dc1 · · · cke) stands for (x1, . . . , xn, c1, . . . , ck), and the like. The
following problems can be recognized by a universal machine.
Definition 3. For G = (G; e; ◦; =) with a, b ∈ G and a 6= b and for any O ⊆
G∞, let the Universal NPO¯G -Problem UNIG¯(O) be the set
{(x, Codea,b(M), dbte) | x ∈ G∞ & M∈ MNG¯ (O) & M(x) ↓t}
where M(x) ↓t means that M accepts x within t steps.
Proposition 3. For G-machines where |G| ≥ 2, UNIG¯(∅) is NPG¯-complete.
Moreover, UNIG¯(O) is NPO¯G -complete.
Definition 4. For G = (G; e; ◦; =) and any O ⊆ G∞, let the Universal NPOG -
Problem UNIG(O) be the set⋃
a,b∈G
a 6=b
{(x, Codea,b(M), dbte) | x ∈ G∞ & M∈ MNG (O) & M(x) ↓t}
∪ {(e, e, e, . . . , e︸ ︷︷ ︸
c(n,C(M),t)×
) | M ∈ MNG (O) & M(dene) ↓t}
where c(n1, n2, n3) = Cantor(Cantor(n1, n2), n3) is the number of (n1, n2, n3)
given by the Cantor diagonal method and C(M) is the number whose binary
representation is Code∗0,1(M).
Computation over Groups 151
If an input x contains a component xi 6= e, a reduction machine can compute
(x, Codexi,e(M), dete). In the other case a reduction to the latter subset is pos-
sible.
Proposition 4. For G-machines, UNIG(∅) is NPG-complete. UNIG(O) is NPOG -
complete.
5 Some Oracles O with POS = NPOS
We transfer the ideas given in [1] and [5] and modify the definitions, appropri-
ately. The tuples which can occur in the oracles O(G)1 and O(G)2 have the same
form as the elements of a universal problem. Whereas the definition in [5] is
recursive on the number of steps occurring in these tuples, we define the oracle
recursively on the length (size) of tuples as in [1].
Definition 5. Let O1(= O(G)1 ) and O2(= O(G)2 ) (For a given G, we omit the
index G.) be universal oracles defined by
O1 =
⋃
k∈IN Vk, V0 = ∅, Vk = Gk ∩UNIG¯(
⋃
j<k Vj),
O2 =
⋃
k∈INWk, W0 = ∅, Wk = Gk ∩UNIG(
⋃
j<kWj).
UNIG¯(O1) ∩Gk ⊆ Vk and UNIG(O2) ∩Gk ⊆ Wk hold since we assume that the
codes of the oracle machines do not depend on the used oracle. This implies
UNIG¯(O1) = O1 and UNIG(O2) = O2, respectively.
Proposition 5. For any group G, there holds PO2G = NPO2G . If |G| ≥ 2, then
there holds PO1G¯ = NP
O1
G¯ .
6 Some oracles Q with PQS 6= NPQS
In order to get the inequality between the corresponding relativized complexity
classes with respect to some oracles for groups G, we want to define oracles
Qi(= Q(G)i ) recursively. The ideas particularly go back to [1] and [5]. As T.
Baker, J. Gill, and R. Solovay and T. Emerson we use diagonalization techniques
in order to construct these oracles and several problems Li(= L
(G)
i ) such that
L1 ∈ NPQ1G \ DNPQ1G , L2 ∈ (NPQ2G ∩ DNPQ2G¯ ) \ PQ2G¯ , and Li ∈ NPQiG¯ \ DNPQiG¯
for i ∈ {3, 4} for several kinds of machines.
6.1 Constructions for Countable Sets of Machine Constants
The first definition works for all infinite groups. Since groups are structures of
finite signature, we can take the positive integers in order to encode all programs
of digital non-deterministic oracle (G; e; ◦; =)-machines using one given oracle.
Consequently, the set of all couples of all polynomials and of these programs
152 Christine Gaßner
whose form (including the oracle queries) is independent of the used oracle can
be enumerated. Let i ∈ IN+ be the code of the ith pair (pi, Pi) which determines
a class of digital non-deterministic oracle machines {NBi | B ⊆ G∞} by the
following.
(a) For any input in Gn, the machine NBi executes pi(n) steps of the program
Pi.
(b) If NBi queries an oracle, then NBi uses the oracle B.
(c) NBi simultaneously counts the number of steps which are determined by Pi
by means of an additional index register.
(d) For any input in Gn and for any guessed sequence of instructions, the ma-
chine NBi makes an output after at most pi(n) steps of the execution of Pi
if the output of Pi is reached in this time. If the output instruction of Pi is
not reached in this time, then NBi does not halt for the guessed sequence of
instructions and it does not compute new values after the first pi(n) steps.
Then, for any digital non-deterministic G-machine M, which recognizes some
problem in DNPBG , there is some i ∈ IN+ such that M and NBi recognize the
same problem.
The Construction of Q1. Let V0 = ∅ and m0 = 0. We construct the set Q1 in
stages. For i ≥ 1, let ni some integer such that ni > mi−1 and pi(ni)+ni < 2ni .
Moreover, let
Wi =
⋃
j<i Vj ,
Vi = {x ∈ Gni | NWii does not accept (e, . . . , e) ∈ Gni non-deterministically
& x is not queried by NWii on input (e, . . . , e) ∈ Gni},
mi = 2ni .
Finally, let Q1 =
⋃
i∈IN+ Wi and L1 = {y | (∃i ∈ IN+)(y ∈ Gni & Vi 6= ∅)}.
Since the set L1 belongs to NPQ1G , we get the following analogously to [1].
Lemma 1. For any infinite group G, there holds L1 ∈ NPQ1G \DNPQ1G .
Proof. The property, that G is infinite, implies that Vi 6= ∅ if NWii does not
accept (e, . . . , e) ∈ Gni non-deterministically. The computation of NWii and
NQ1i is the same on input (e, . . . , e) ∈ Gni since the length of the tuples in the
oracle queries are bounded by pi(ni) + ni and the queries of NWii and NWi+1i ,
with respect to Vi, are the same. Since, for any i ≥ 1, the machine NWii accepts
(e, . . . , e) ∈ Gni iff L1 ∩ Gni = ∅ and it does not accept (e, . . . , e) ∈ Gni iff
L1 ∩Gni = Gni , there holds L1 6∈ DNPQ1G . uunionsq
Proposition 6. For any infinite group G, there is an oracle Q such that
DNPQG 6= NPQG .
Corollary 1. For any infinite group G, there is an oracle Q such that PQG 6=NPQG .
Computation over Groups 153
For any group G defined on a countable (finite or infinite) universe G where
|G| ≥ 2, the construction can be transferred to deterministic G¯-machines. Let us
assume that the machines N˜Bi are the deterministic oracle machines over G¯ and
that there is some g ∈ G \ {e}. Then, the sets V˜i, W˜i, Q2, and L2 can be defined
in the same manner and L2 is in (NPQ2G ∩DNPQ2G¯ )\PQ2G¯ since there are machines
in MNG (Q2) and in MDNG¯ (Q2) which can guess or select every combination of e’s
and g’s non-deterministically in order to use it in a query. For a group of two
elements, the construction is the same as in [1]. Here, pi(ni) < 2ni implies that
V˜i 6= ∅ if N˜ W˜ii does not accept (e, . . . , e) ∈ Gni .
Lemma 2. For any group G containing at least two elements, if G is countable,
there holds L2 ∈ DNPQ2G¯ \ PQ2G¯ and L2 ∈ NPQ2G .
Proposition 7. If a group G contains at least two elements and the universe is
countable, then there is an oracle Q such that PQG¯ 6= DNPQG¯ and PQG 6= NPQG .
Corollary 2. If a group G contains at least two elements and the universe is
countable, then there is an oracle Q such that PQG¯ 6= NPQG¯ .
6.2 Further Constructions for Infinite Sets of Machine Constants
Now we consider structures of the form G¯ = (G;G; ◦; =) where the universe is
infinite and does not need to be countable. We know that any oracle machine
over G¯ can be encoded by elements of G∞ where, for any oracle, any query is
encoded by the same code. Let U ⊆ G∞ be the set of codes for all couples of
all polynomials and of all programs of digital non-deterministic oracle machines
over G¯ (independent of the used oracle). Then, any u ∈ U is the code for a pair
(pu, Pu) which determines a class of digital non-deterministic oracle machines
{NBu | B ⊆ G∞} by properties analogously to (a), (b), (c), and (d).
The Construction of Q3. Let us assume that G contains an element of infinite
order, a. Let V0 = ∅. We construct the set Q3 in stages. For i ≥ 1, let
Ki = {u ∈ U | (∀j > i)(∀B ⊆ G∞)
(NBu does not compute or use the value aj on input u)},
Wi =
⋃
k<i Vk,
Vi = {(ai+1,u) | u ∈ Ki & NWiu does not accept u non-deterministically}.
Finally, let Q3 =
⋃
i∈IN+ Wi and L3 = {y | (∃n ≥ 2)((an,y) ∈ Q3)}.
Lemma 3. For groups G containing an element a for which ai 6= aj iff i 6= j,
L3 ∈ NPQ3G¯ \DNPQ3G¯ .
Proof. Let us assume that there is some machine NQ3u with property (A).
(A) NQ3u recognizes L3.
154 Christine Gaßner
In any case we have u ∈ L3 or u 6∈ L3. If u ∈ L3, then there is some i such
that (ai+1,u) ∈ Vi. By the definition of Ki the machines NQ3u and NWiu do not
compute or use any value aj on input u if j > i. Hence, NQ3u works like NWiu
on u and, consequently, it does not accept u non-deterministically such that
u 6∈ L3 because of (A). If u 6∈ L3, then because of (A), NQ3u does not accept u.
However, by definition of K1,K2, . . . there is some i0 such that u ∈ Ki for all
i ≥ i0. Thus, (ai0+1,u) ∈ Q3 and hence u ∈ L3. uunionsq
Proposition 8. For groups G containing an element of infinite order, there is
an oracle Q such that DNPQG¯ 6= NPQG¯ .
Corollary 3. For groups G containing an element of infinite order, there is an
oracle Q such that PQG¯ 6= NPQG¯ .
Similar constructions are possible for any infinite group. For any infinite
abelian group which does not contain an element of infinite order, there is an
infinite sequence of elements such that any new element is independent of its
predecessors. For these groups we can choose the following oracle.
The Construction of Q4. Let (Gi)i∈IN+ be an infinite sequence of subgroups
such that Gi ⊂ Gi+1. Let V0 = ∅. We construct the set Q4 in stages. For i ≥ 1,
let
Ki = {u ∈ U | (∀j > i)(∀B ⊆ G∞)
(NBu does not compute or use values in Gj \Gi on input u)},
Wi =
⋃
k<i Vk,
Vi = {(g,u) | g ∈ Gi+1 \Gi & u ∈ Ki & NWiu does not accept u}.
Let Q4 =
⋃
i∈IN+ Wi and L4 = {y | (∃n ≥ 2)(∃g ∈ Gn)((g,y) ∈ Q4)}.
Lemma 4. For any infinite abelian group G which contains an infinite sequence
of subgroups Gi ⊂ Gi+1, L4 ∈ NPQ4G¯ \DNPQ4G¯ .
Proposition 9. For any infinite abelian group G which does not contain an
element of infinite order, there exists an oracle Q such that DNPQG¯ 6= NPQG¯ .
Corollary 4. For any infinite abelian group G which does not contain an ele-
ment of infinite order, there exists an oracle Q such that PQG¯ 6= NPQG¯ .
6.3 Open Questions
In order to get PQS 6= NPQS for some structures S in {G, G¯}, we showed one of the
inequalities PQS 6= DNPQS and DNPQS 6= NPQS by uniform constructions for some
classes of groups. Of course, for special structures we even know P∅S 6= DNP∅S
and DNP∅S 6= NP∅S . If the set of constants of any structure S is not countable,
then no uniform construction of oracles Q is known which imply the inequalities
PQS 6= DNPQS . Moreover we do not know any uniform construction of oracles Q
which imply the inequalities PQS 6= DNPQS for infinite structures containing only
one constant or which imply the inequalities PQS 6= DNPQS for finite structures
containing only one constant.
Computation over Groups 155
7 Expansions of Extensions with PS = NPS?
Let us explain the meaning of padding in the known constructions of structures S
over strings with PS = NPS and its bounds for groups. As in [9], for every group
G = (G; e; ◦; =), where G contains two elements a and b, we can embed the struc-
ture G¯ into a new structure of the form G¯∗ = (G∗;G ∪ {ε}; ◦, add, subl, subr; =)
and afterward into a new structure of the form
G¯∗R1 = (G∗;G ∪ {ε}; ◦, add, subl, subr;R1,=)
such that we get PG¯∗
R1
= NPG¯∗
R1
. Here, ε is the empty string, add is a binary
operation for adding a character to a string, and subr and subl are unary opera-
tions for computing the last character and the remainder of a string, respectively.
For the strings s ∈ G∗, r ∈ G∗ \ G, and c ∈ G, the operations are defined by
add(s, c) = sc, subl(sc) = s, subr(sc) = c, add(s, r) = ε, subl(ε) = ε, subr(ε) = ε,
s ◦ r = ε, and r ◦ s = ε. For G¯∗, the relation R1 can be derived from some
universal oracle O1 = UNIG¯∗(O1) by using, for example, a ∈ G for padding such
that, for any r = 〈x〉Code∗a,b(M)bt (where, for x ∈ (G∗)∞, 〈x〉 is a suitable code
for x in G∗), the string ra|r| satisfies R1 if and only if
(x, Codea,b(M), dbte) ∈ O1
holds. Then, any problem in NPG¯∗
R1
can be reduced to the universal oracle O1
and any oracle query can be replaced by a branching instruction defined by
R1 by transforming each tuple of the form (x, Codea,b(M), dbte) used in oracle
queries into a single string. The definition of the additional relation R1 on padded
codes of the elements of O1 guarantees that any input, any computed values,
and any guesses can be replaced by short strings without changing the path of
computation, and that the short strings x1, x2, . . . in a query can be compressed
into one single string in polynomial time. Since R1 has the properties (1) and
(2), the strings of the form sw for which the test conditions R1(sai) are satisfied
for some i, can be replaced by strings s′w for which s′ai satisfying R1.
(∀s ∈ G∗)(∀i ∈ IN)(∀j ∈ IN)(R1(sai) & R1(saj)→ i = j), (1)
(∀s ∈ G∗)(∃r ∈ G∗)(R1(s)→ s = rba|r|+1). (2)
If G contains the neutral element e and a second element g, then we can also
transform G into a structure of the form (G∗; e, g, ε; ◦, add, subl, subr;R,=) in a
similar way.
If we do not allow the additional parameter g 6= e as constant, the same
definition ofR for the construction of some G∗R = (G∗; e, ε; ◦, add, subl, subr;R,=)
with PG∗
R
= NPG∗
R
is not possible. The reason is the following. Let us assume
that the definition of an additional relation R2 is done on padded codes of the
elements of some oracle O2 and that the tuples of the form
(x, Codea,b(M), dbte) (3)
dec(n,C(M),t)e (4)
156 Christine Gaßner
used in oracle queries could be transformed into single strings in polynomial
time in order to check it by R2. Then, any problem in NPG∗
R2
could be reduced
to O2 containing tuples of the form (3) and (4) and any oracle query could be
replaced by a branching instruction defined by R2. In this case the padding of
the codes of the tuples of the form (3) and (4) by adding an element a are
necessary since the properties (1) and (2) are the base for shortening. But, if the
components in an input are only e, then only e can be used for padding since
any other element in G cannot be computed. That means that the elements of
the form (4) could be padded by e only. Then, for any string s = ek there are
several i implying that R2(sei) is true and we do not know all i which satisfy
this property. Consequently, the condition (1) cannot be true.
Remark 1. A natural extension of groups yielding structures S with PS = NPS
can be realized by embedding groups in structures over trees. The ideas are given
in [10].
References
1. Baker, T., J. Gill, and R. Solovay (1975). Relativizations of the P =? NP
question. SIAM J. Comput. 4, 431–442.
2. J. Balca´zar, J. D´ıaz, and J. Gabarro´ (1988/90). Structural Complexity I and
Structural Complexity II. Springer-Verlag.
3. Blum, L., F. Cucker, M. Shub, and S. Smale (1998). Complexity and Real
Computation. Springer-Verlag.
4. Blum, L., M. Shub, and S. Smale (1989). On a theory of computation and com-
plexity over the real numbers: NP-completeness, recursive functions and universal
machines. Bulletin of the Amer. Math. Soc. 21, 1–46.
5. Emerson, T. (1994). Relativizations of the P =? NP question over the reals (and
other ordered rings). Theoretical Computer Science 133, 15–22.
6. Gassner, C. (2001). The P-DNP problem for infinite abelian groups. Journal of
Complexity 17, 574–583.
7. Gassner, C. (2006). A structure with P = NP. CiE 2006. Computer Science
Report Series of the University of Wales Swansea CSR 7, 85–94.
8. Gassner, C. (2006). Expansions of structures with P = NP. CiE 2006. Computer
Science Report Series of the University of Wales Swansea CSR 7, 95–104.
9. Gassner, C. (2007). P = NP for Expansions Derived from Some Oracles. CiE
2007. Technical report no. 487 June 2007.
10. Gassner, C. (2004). NP ⊂ DEC und P=NP fu¨r Expansionen von Erweiterungen
von Strukturen endlicher Signatur mit Identita¨tsrelation. Preprint-Reihe Mathe-
matik, E.-M.-Arndt-Universita¨t Greifswald, Preprint 13/2004.
11. Koiran, P. (1994). Computing over the reals with addition and order. Theoretical
Computer Science 133, 35–47.
12. Meer, K. (1992). A note on a P 6= NP result for a restricted class of real machines.
Journal of Complexity 8, 451–453.
13. Poizat, B. (1995), Les Petits Cailloux. Ale´as.
14. Prunescu, M. (2002) A model-theoretical proof for P 6=NP over all infinite abelian
groups. J. Symb. Logic 67, 235-238.
Singularities of Holomorphic Functions
in Subsystems of Second Order Arithmetic
Yoshihiro Horihata1 and Keita Yokoyama2
1 Mathematical Institute, Tohoku University, Sendai 980-8578, Japan.
sa6m31@math.tohoku.ac.jp or higurashi3873@yahoo.co.jp
2 Department of Mathematics, Tokyo Institute of Technology,
2-12-1 Oh-okayama, Meguro-ku, Tokyo 152-8551, Japan.
yokoyama@math.titech.ac.jp or k yoko tautology@infoseek.jp
Abstract. We study complex analysis in the context of weak subsys-
tems of second order arithmetic. We are mainly concerned with inte-
grability and singularities of holomorphic functions. Then, we develop a
part of complex analysis concerned with Picard’s little theorem. We show
that Picard’s little theorem is provable from WKL0 plus a version of the
Riemann mapping theorem. Since a full version of the Riemann map-
ping theorem is provable in ACA0, we can prove Picard’s little theorem
in ACA0.
Key words: Reverse Mathematics, Picard’s theorem, Riemann map-
ping theorem, second order arithmetic
1 Introduction
This paper is a contribution to the program of Reverse Mathematics. Reverse
Mathematics was pioneered by Harvey Friedman and Stephan Simpson in the
1970s. Its goal is to develop large parts of ordinary mathematics in second order
arithmetic and determine which axioms are exactly required to prove theorems.
In this paper, we aim to develop some parts of complex analysis especially related
to singularities.
We mainly consider subsystems RCA0, WWKL0, WKL0 and ACA0 in this pa-
per. RCA0 is a system of recursive comprehension axioms. This is the weakest
system we consider and we can prove basic theorems for analysis such as the
intermediate value theorem, the mean value theorem and Taylor’s theorem for
holomorphic functions within RCA0. WithinWKL0, we can prove the Heine/Borel
theorem, and then, we can integrate a continuous function on a closed interval.
If a continuous function is bounded, then, we can integrate it within a weaker
system WWKL0, in which we can show a weak version of the Heine/Borel com-
pactness (see [7]). Within ACA0, we can prove the Riemann mapping theorem.
Actually, the Riemann mapping theorem is equivalent to ACA0 over WKL0 (see
[6]).
Analysis in second order arithmetic is carried forward by many people (see,
e.g. [3]). Complex analysis in second order arithmetic is carried forward in [5,
6]. We aim to expand these into studies for singularities or studies for Riemann
surfaces.
158 Yoshihiro Horihata and Keita Yokoyama
2 Preliminaries
In RCA0, we can define the real number system R, the Euclidean space Rn and
continuous functions on Rn in the usual way. We first define the complex number
system and holomorphic functions.
Definition 1 (the complex number system). The following definitions are
made in RCA0. We identify a complex number, an element of C, with an element
of R2, and we define +C, ·C and | · |C by:
(x1, y1) +C (x2, y2) = (x1 + x2, y1 + y2);
(x1, y1) ·C (x2, y2) = (x1x2 − y1y2, x1y2 + x2y1);
|(x, y)|C = ‖(x, y)‖R2 =
√
x2 + y2.
We write (0, 1) = i and (x, y) = x + iy = z, where x, y ∈ R and z ∈ C. We
usually leave out the subscript C. A continuous (partial) function from C to C
is a continuous (partial) function from R2 to R2.
Definition 2 (holomorphic functions). The following definition is made in
RCA0. Let D be an open subset of C, and let f , f ′ be continuous functions from
D to C. Then a pair (f, f ′) is said to be holomorphic if
∀z ∈ D lim
w→z
f(w)− f(z)
w − z = f
′(z).
Informally, we write f for a holomorphic function (f, f ′).
Within RCA0, we can show Taylor’s theorem, i.e., a holomorphic function is an
analytic function. Thus, we can prove basic properties for holomorphic functions
in RCA0.
Next, we define line integrals. Let a, b be elements of C and let r be a positive
real number. Then we define
[a, b] := {a+ (b− a)x ∈ C | 0 ≤ x ≤ 1},
∂B(a; r) := {z ∈ C | |z − a| = r}.
Definition 3 (line integral). Let D be an open or closed subset of C, and let
f be a continuous function from D to C. Then the following definitions are made
in RCA0.
1. Let γ be a continuous function from [0, 1] to D. Then, we define
∫
γ
f(z) dz,
the line integral of f along γ, as∫
γ
f(z) dz = lim
|∆|→0
S∆γ (f)
if this limit exists. Here, ∆ is a partition of [0, 1], i.e. ∆ = {0 = x0 ≤ ξ1 ≤
x1 ≤ · · · ≤ ξn ≤ xn = 1}, S∆γ (f) =
∑n
k=1 f(γ(ξk))(γ(xk) − γ(xk−1)) and
|∆| = max{xk − xk−1 | 1 ≤ k ≤ n}.
Singularities of Holomorphic Functions 159
2. If [a, b] ⊆ D, we define γ(t) = a+ (b− a)t and define ∫
[a,b]
f(z) dz as∫
[a,b]
f(z) dz =
∫
γ
f(z) dz.
3. If ∂B(a; r) ⊆ D, we define γ(t) = a+ r exp(2piit) and define ∫
∂B(a;r)
f(z) dz
as ∫
∂B(a;r)
f(z) dz =
∫
γ
f(z) dz.
Let f be a continuous function from D ⊆ C to C, and let [a, b] ⊆ D. A modulus
of integrability along [a, b] for f is a function h[a,b] from N to N such that for all
n ∈ N and for all partitions ∆1,∆2 of [0, 1] ⊆ R, if |∆1|, |∆2| < 2−h[a,b](n) then
|S∆1[a,b](f)− S∆2[a,b](f)| < 2−n+1. We say that f is effectively integrable on D when
for every [a, b] ⊆ D, we can find a modulus of integrability along [a, b].
Theorem 1. Let D ⊂ C. Then, the following assertions are equivalent over
RCA0.
1. WKL0.
2. Every continuous function on D is effectively integrable.
For the proof, see [3, Theorem IV.2.7].
Next, we define subsystem WWKL0 which is introduced in [7].
Definition 4. WWKL0 is the system which consists of RCA0 plusWWKL, where
the WWKL is weak-weak Ko¨nig’s lemma;
if an infinite tree T ⊆ 2<N has no path, then limn→∞ |{σ ∈ T | lh(σ) =
n}|/2n = 0.
The Heine/Borel theorem is not provable in WWKL0, but a weak version of
the Heine/Borel theorem is provable.
Theorem 2. The following assertions are equivalent over RCA0.
1. WWKL0.
2. Weak Heine/Borel covering theorem; if
⋃
n∈NB(an; rn) is a covering of [0, 1],
then
∃〈(bij , cij)j≤li | i ∈ N, li ∈ N〉s.t.
[0, 1] ⊆ ⋃n<iB(an; rn) ∪⋃j≤li(bij , cij) ∧ limi→∞∑j<li |cij − bij | = 0.
For the proof, see [7].
By the above therem, we only needWWKL0 to integrate a bounded function.
Theorem 3. Let D ⊂ C. Then, the following assertions are equivalent over
RCA0.
1. WWKL0.
2. Every bounded continuous function on D is effectively integrable.
For the proof, see [4, Theorem 3].
160 Yoshihiro Horihata and Keita Yokoyama
3 Complex analysis in weak second order arithmetic
We review some recent studies on complex analysis in weak second order arith-
metic.
We first state two basic theorems for holomorphic functions contained in [5].
Definition 5 (analytic function). The follwoing definition is made in RCA0.
Let D be an open subset of C. An analytic function on D is defined to be a triple
(f, {an, rn}n∈N, {αnk}n∈N,k∈N), where f is a continuous function from D to C,
an, αnk ∈ C and rn ∈ R+, satisfying the following conditions:
1.
⋃
n∈NB(an; rn) = D;
2. ∀n ∈ N ∀z ∈ B(an; rn) f(z) =
∑
k∈N αnkz
k.
Theorem 4 (Taylor’s theorem). The following is provable in RCA0. Holo-
morphic functions are analytic, i.e. given a holomorphic function (f, f ′) on an
open subset D ⊆ C, we can effectively find an analytic function (f, {an, rn}n∈N,
{αnk}n∈N,k∈N) on D. In particular, the derivative of a holomorphic function is
holomorphic.
Theorem 5 (Cauchy’s integral theorem). The following assertions are equiv-
alent over RCA0.
1. WKL0.
2. Cauchy’s integral theorem for triangles: if f is a holomorphic function on an
open subset D ⊆ C, then for all 4abc ⊆ D, ∫
∂4abc f(z) dz exists and∫
∂4abc
f(z) dz = 0.
To prove Taylor’s theorem, we need to use Cauchy’s integral theorem ‘locally’,
i.e., we need to find a good neighborhood for Cauchy’s theorem at each point.
Note that a holomorphic function is locally effectively integrable in RCA0. As we
stated, we can show many theorems for holomorphic functions such as maximal
value principle or mean value principle in RCA0. By Theorem 4, we can apply
Cauchy’s integral theorem to a holomorphic function f if f is effectively inte-
grable within RCA0. However, Cauchy’s integral theorem as itself is not provable
in RCA0.
Based on the line integrability in WKL0 or WWKL0 (Theorems 1 and 3) and
the previous two theorems, we develop some more complex analysis in second
order arithmetic. Proofs of the following theorems can be found in [1]. We first
study Laurent expansion.
Theorem 6 (Laurent expansion). The following is provable in RCA0. Let f
be an effectively integrable holomorphic function on D = {z | 0 ≤ R1 < |z−a| <
R2}. Then, for all z ∈ D,
f(z) =
∞∑
n=1
a−n
(z − a)n +
∞∑
n=0
an(z − a)n
Singularities of Holomorphic Functions 161
where R1 < r < R2 and
a−n =
1
2pii
∫
∂B(a;r)
f(ζ)(ζ − a)n−1 dζ,
an =
1
2pii
∫
∂B(a;r)
f(ζ)
(ζ − a)n+1 dζ.
Using effective integrability, we can show this theorem as usual. However, as for
Cauchy’s integral theorem, we cannot omit the assumption that f is effectively
integrable. Actually, the following theorem holds.
Theorem 7. The following assertions are equivalent over RCA0.
1. WKL0.
2. If f is a holomorphic function on D = {z | 0 ≤ R1 < |z − a| < R2}, then,
there exists {an}n∈Z such that f(z) =
∑
n∈Z an(z − a)n for all z ∈ D.
We next study isolated singularities.
Definition 6 (isolated essential singularity). The follwing definition is made
in RCA0. Let f be a holomorphic function on D = {z | 0 < |z − a| < R}. Then
a is said to be an isolated essential singularity if there exists {an}n∈Z such that
f(z) =
∑
n∈Z an(z − a)n for all z ∈ D and ∀m ∈ N ∃k ≥ m a−k 6= 0.
We can prove the following two theorems within WWKL0.
Theorem 8 (Riemann’s theorem on removable singularities). The fol-
lowing is provable in WWKL0. Let f be a holomorphic function on D = {z |
0 < |z − a| < r}. If there exists r′ > 0 such that r′ < r and f is bounded on
{z | 0 < |z − a| < r′}, then there exists a holomorphic function f˜ on D ∪ {a}
such that f˜(z) = f(z) for all z ∈ D.
The following theorem is a special case of Picard’s theorem.
Theorem 9 (Casorati/Weierstraß theorem). The following is provable in
WWKL0. Let f be a holomorphic function on D = {z | 0 < |z− a| < r} and a be
an isolated essential singularity. Then, f(D) is dense in C.
To prove these theorems, we need to apply Cauchy’s theorem for bounded holo-
morphic functions. This can be done inWWKL0 by Theorem 3. Thus, we can im-
itate usual proofs within WWKL0. We can also prove the following two theorems
and many other theorems in WWKL0 by the same reasoning. These theorems
play key roles in the proof of Picard’s theorem.
Theorem 10 (Liouville theorem). The following is provable in WWKL0. If
f is a bounded entire function, then it is a constant function.
Theorem 11 (Schwarz reflection principle). The following is provable in
WWKL0. Let D ⊆ C+ = {x+ iy | y > 0} be an open set and let L = (a, b) ⊆ R
be an open interval such that L = ∂D ∩ R. Let f be a continuous function on
D ∪ L such that f is holomorphic on D and f(z) ∈ R for all z ∈ L. Then there
exists a holomorphic function f˜ on D˜ = D ∪ {z ∈ C | z¯ ∈ D ∪ L} such that
f˜(z) = f(z) for all z ∈ D ∪ L.
162 Yoshihiro Horihata and Keita Yokoyama
Finally, we recall the Riemann mapping theorem contained in [6].
Theorem 12 (Riemann mapping theorem). The following assertions are
equivalent over RCA0.
1. ACA0.
2. If D ⊆ C is a simply connected open set D 6= C, then there exists a conformal
map f : D → B(0; 1).
4 Picard’s little theorem
To study singularities, we need covering spaces.
Definition 7 (covering space). Let X,D ⊆ C be open sets, and let pi be a
continuous surjective function from X to D. Let {Uij}i∈I,j∈J and {Vi}i∈I be
sequences of open sets, and let piij be homeomorphic functions from Uij to Vi.
Then, pi is said to be a covering map and a hextuple (X,D, pi, Uij , Vi, piij) is said
to be a covering space of D if they satisfy the following:
each of Uij and Vi is simply connected;
D =
⋃
i∈I
Vi;
∀i ∈ I pi−1(Vi) =
⋃
j∈J
Uij ,
∀i ∈ I ∀j ∈ J pi|Uij = piij .
The following lemma plays a key role of the proof of Picard’s little theorem.
Lemma 1 (lifting). The following assertions are equivalent over RCA0.
1. WKL0.
2. Let D0, D ⊆ C be open sets, and let (X,D, pi, Uij , Vi, piij) be a covering space
of D. If D0 is simply connected and f is a continuous function from D0
to D, then, there exists a continuous function fˆ from D0 to X such that
pi ◦ fˆ = f .
Proof. We first show 1 → 2 in case D0 = [0, 1]. We reason within WKL0. Let
D0 = [0, 1] and aij = j/2i+1 for all j ≤ 2i+1. By WKL0, we can define Wi as
follows:
Wi =
⋃
∃kf(B(aij ;2−i))⊆Vk
B(aij ; 2−i).
ThenWi ⊆Wi+1 for all i ∈ N and [0, 1] ⊆
⋃
i∈NWi. By the Heine/Borel theorem,
there exists N ∈ N such that [0, 1] ⊆WN . Take a sequence 〈kj | j ≤ 2N+1〉 such
that f(aNj) ∈ Vkj for all j ≤ 2N+1. Then, by the definition of the covering
space, we can take a sequence 〈lm | m ≤ 2N+1〉 such that
pi−1km,lm(f(aN,m+1)) ∈ Ukm+1,lm+1
for all m < 2N+1. Thus we can define a lifting fˆ as follows:
Singularities of Holomorphic Functions 163
fˆ(x) = pi−1kj ,lj ◦ f(x) if x ∈ [aN,j , aN,j+1].
Similarly, we can construct a lifting in case D0 = [0, 1]× [0, 1]. The general case
can be proved easily by the previous two cases.
Next, we show ¬1→ ¬2. We assume ¬WKL0. Let D0 = {(x, y) ⊆ R2 | −1 ≤
x ≤ 1,−1 ≤ y ≤ 1}, D = ∂D0, and X = R. Then we can take pi : X → D
as a covering map by winding X on D. By ¬WKL0, there exists a continuous
function f : D0 → D such that f |D = idD (by Shioji and Tanaka[2]). However,
there is no lifting of f .
The complete proof of the previous lemma is in [1].
Now we study Picard’s little theorem. For this, we consider a version of the
Riemann mapping theorem:
(∗) Let D be an interior of a simple closed curve on the Riemann sphere. Then,
D is conformally equivalent to the unit open ball B(0; 1) and a conformal map
h : D → B(0; 1) can be expanded into a homeomorphism h¯ : D → B(0, 1).
We prove Picard’s little theorem, which asserts that an entire function is a
constant if it omits two points, within WKL0 + (∗).
We reason within WKL0. Let f be an entire function which omits two points.
Without loss of generality, we assume that f : C → C \ {0, 1}. By (∗) and
the Schwarz reflection principle, we can construct a holomorphic covering map
pi : B(0; 1) → C \ {0, 1} in the usual way. Then, by Lemma 1, there exists a
holomorphic function fˆ : C → B(0; 1) such that pi ◦ fˆ = f . By the Liouville
theorem, fˆ is constant. Thus, f is also constant and this completes the proof of
Picard’s little theorem.
Since a full version of Riemann mapping theorem is provable in ACA0, we
can prove Picard’s theorem within ACA0.
Theorem 13 (Picard’s theorem). The following is provable in ACA0. Let f
be a holomorphic function from C to C. If the range of f omits two points, then,
f is a constant function.
We conjecture that (∗) is provable withinWKL0, which implies thatWKL0 proves
Picard’s theorem. We have not managed the reversal of Picard’s little theorem.
References
1. Y. Horihata. Reverse mathematics of complex analysis and general topology
(in Japanese), Feb. 2008. Master’s Thesis, Tohoku University, February 2008.
2. N. Shioji and K. Tanaka. Fixed point theory in weak second-order arithmetic.
Annals of Pure and Applied Logic, 47:167–188, 1990.
3. S. G. Simpson. Subsystems of Second Order Arithmetic. Springer-Verlag, 1999.
4. K. Yokoyama. Reverse mathematics for Fourier expantion. In the local proceedings
of the fourth conference on computability in Europe 2008.
5. K. Yokoyama. Complex analysis in subsystems of second order arithmetic. Arch.
Math. Logic, 46:15–35, 2007.
164 Yoshihiro Horihata and Keita Yokoyama
6. K. Yokoyama. Non-standard analysis in ACA0 and Riemann mapping theorem.
Math. Logic Quart., 53(2):132–146, 2007.
7. X. Yu and S. G. Simpson. Measure theory and weak Ko¨nig’s lemma. Arch. Math.
Logic, 30:171–180, 1990.
Modelling Linear Cellular Automata with the
Minimum Stage Corresponding to CCSG based
on LFSR ?
Yoon-Hee Hwang1, Sung-Jin Cho2, Un-Sook Choi3, and Han-Doo Kim4
1 Department of Information Security, Graduate School
Pukyong National University
Busan 608-737, Korea, yhhwang@pknu.ac.kr
2 Division of Mathematical Sciences, Pukyong National University
Busan 608-737, Korea, sjcho@pknu.ac.kr
3 Department of Multimedia Engineering, Tongmyong University
Busan 626-847, Korea, choies@tu.ac.kr
4 Institute of Mathematical Sciences and School of Computer Aided Science
Inje University, Gimhae 621-749, Korea, mathkhd@inje.ac.kr
Abstract. Cellular Automata(CA) with simple, regular, modular and
cascadable structures, which the VLSI design community prefer, was
made an alternative proposal of LFSRs. Pseudorandom sequences were
produced by generators which accompany several LFSRs joined by non-
linear functions or irregular clocking techniques. Clock-Controlled Shrink-
ing Generators(CCSGs) are a class of clock-controlled sequence gener-
ators. Clock-controlled LFSRs have become important building blocks
for keystream generators in stream cipher applications, because they are
known to produce sequences of long period and high linear complexity.
In this paper, we propose a method of modelling linear CA with the
minimum stage corresponding to CCSGs based on LFSR using the Cho
et al.’s synthesis algorithm.
1 Introduction
The VLSI era has ushered in a new phase of activities into the research of linear
machines, and specially the local neighborhood CA structures. The VLSI design
community prefer simple, regular, modular and cascadable structures with local
interconnections. The CA provides a wonderful solution in all these respects
[1]. CA have the characters of simplicity of basic components, locality of CA
interactions, massive parallelism of information processing, and exhibit complex
global properties. These ensure that CA have higher speed and more potential
applications than LFSR. The locality of signal path of CA contributes more
higher speed than LFSR. So in the form of VLSI implementation, CA have more
speed advantages than LFSR [2].
? This work was supported by grant No. (R01-2006-000-10260-0) from the Basic Re-
search Program of the Korea Science and Engineering Foundation.
166 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
Pseudorandom sequences were produced by generators which accompany sev-
eral LFSRs joined by nonlinear functions or irregular clocking techniques. The
theory for CA based pseudorandom number generator is well developed [1] and
n-stage linear CA can be designed to generate sequences with desirable prop-
erties: maximum period 2n − 1, uniform distribution of n-tuples and balanced
distribution of 1 and 0 ([3],[4]).
Pseudorandom sequence generators intend to be used in a stream cipher.
Especially, the Shrinking Generator(SG) proposed by Coppersmith et al. [5] is
a popular form of pseudorandom sequence generators that employ the irregular
clocking. It has one or more LFSRs whose clocking is controlled by the output
sequence of one. Such a sequence is called a clock-controlled sequence [6]. The
SG generally uses two sources of pseudorandom sequences to create the third
source of pseudorandom sequence, having better cryptographic quality(long pe-
riod, high linear complexity, good statistical properties, etc.) than the original
sources.
Clock-controlled LFSRs have become important building blocks for keystream
generators in stream cipher applications, because they are known to produce se-
quences of long period and high linear complexity ([7], [8]).
In [9], they showed that CCSGs can be described in terms of linear CA con-
figurations by using mirror image and the Cattell and Muzio synthesis algorithm
[10]. Since the CA obtained by the Sabater et al.’s method has the maximum
stage, the method has a waste of space. Also the sequence obtained by CA is
not secure because the rule of this CA is symmetrical.
In this paper, we propose a new method of modelling linear CA with the
minimum stage corresponding to CCSGs based on LFSR using the Cho et al.’s
synthesis algorithm to overcome these weak points [11].
2 90/150 CA Preliminaries
CA are considered to be a good model of complex systems in which an in-
finite one-dimensional array of finite state machines(cells) updates itself in a
synchronous manner according to a uniform local rule. In the long history of the
study of CA, generally speaking, the number of internal states of each cell is
finite and the local state transition rule is defined in a such way that the state
of each cell depends on the previous states of itself and its neighboring cells
[12]. CA consists of a number of cells. In a 3-neighborhood dependency, the next
state qi(t+ 1) of a cell is assumed to be dependent only on itself and on its two
neighbors (left and right), and is denoted as
qi(t+ 1) = f(qi−1(t), qi(t), qi+1(t))
where qi(t) represents the state of the i-th cell at the t-th instant of time and f is
the next-state function. The cells evolve in discrete time steps according to some
deterministic rule that depends only on logical neighborhood. The CA structure
Modelling Linear Cellular Automata 167
investigated by Wolfram [13] can be viewed as a discrete lattice of sites (cells),
where each cell can assume either the value 0 or 1. In effect, each cell consists
of a storage element (D flip-flop) and a combinatorial logic implementing the
next-state function.
The next-state function of a cell is called its rule. If the rule of a CA cell
involves only XOR logic, then it is called a linear rule. A CA with all the cells
having linear rules is called a linear CA. If all the CA cells obey the same
rule, then the CA is said to be a uniform CA; otherwise, it is a hybrid CA. A
CA is said to be a Null Boundary CA(NBCA) if the left(right) neighbor of the
leftmost(rightmost) terminal cell is connected to logic 0-state.
If the next-state function of a cell is expressed in the form of a truth table,
then the decimal equivalent of the output is conventionally called the rule number
for the cell [13].
Table 1. Rule 90 and rule 150
Neighborhood state 111 110 101 100 011 010 001 000
Next state 0 1 0 1 1 0 1 0 rule 90
Next state 1 0 0 1 0 1 1 0 rule 150
In Table 1, the top row gives all eight possible states of the three neighboring
cells (the left neighbor of the i-th cell, the i-th cell itself, and its right neighbor)
at the time instant t. The second and third rows give the corresponding states
of the i-th cell at the time instant t + 1 for two CA rules. The corresponding
combinatorial logic for the above rules can be specified as
rule 90 : qt+1i = q
t
i−1 ⊕ qti+1
rule 150 : qt+1i = q
t
i−1 ⊕ qti ⊕ qti+1
In this paper, CA are NBCA with rule 90 and 150. A natural form for the
specification is an n-tuple < d1, d2, · · · , dn >, called the rule vector, where
di =
{
0, if cell i uses rule 90
1, if cell i uses rule 150
Now we define the state qt of a CA at time t to be the n-tuple
qt = (qt1, q
t
2, · · · , qtn)Tn
where AT is the transpose of the matrix A. Hence
qt+1 = f(qt)(:= Tnqt)
168 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
where the product is a matrix-vector multiplication over GF (2). We call this
matrix Tn the CA state-transition matrix. If C is a CA whose rule vector is
< d1, d2, · · · , dn >, then the state-transition matrix Tn of C is a tridiagonal
matrix
Tn =

d1 1 0 0 0 · · · 0 0 0
1 d2 1 0 0 · · · 0 0 0
0 1 d3 1 0 · · · 0 0 0
...
...
...
...
...
. . .
...
...
...
0 0 0 0 0 · · · 1 dn−1 1
0 0 0 0 0 · · · 0 1 dn

The characteristic polynomial ∆n of C is defined by
∆n = |Tn − xI|
where x is an indeterminate, I is the n× n identity matrix and Tn is the state-
transition matrix of C.
For any n-stage 90/150 CA whose state-transition matrix is Tn, the minimal
polynomial for Tn is the same as the characteristic polynomial for Tn [15].
A polynomial is said to be a CA-polynomial if it is the characteristic poly-
nomial of some CA [10]. All irreducible polynomials are CA-polynomials([10],
[11]).
In [10], authors proposed a method for the synthesis of one-dimensional
90/150 Linear Hybrid Group Cellular Automata(LHGCA) for irreducible CA-
polynomial.
In [11], Cho et al. proposed a new method for the synthesis of one-dimensional
90/150 LHGCA for any CA-polynomial. In this case CA-polynomial need not
irreducible. This algorithm is efficient and suitable for all practical applica-
tions. Table 2 shows an algorithm for finding the 90/150 CA for the given
CA-polynomial. In this paper we propose a new method of modelling linear
CA with the minimum stage corresponding to CCSGs based on LFSR using this
algorithm.
3 Clock-Controlled Shrinking Generator
Two LFSRs are used, both clocked regularly. If the output of the first LFSR is
1, the output of the second LFSR becomes the output of the generator. If the
output of the first LFSR is 0, however, the output of the second is discarded. This
mechanism suffers from timing attacks on the second generator, since the speed
of the output is variable in a manner that depends on the second generator’s
state. This can be alleviated by buffering the output.
CCSGs are a class of clock-controlled sequence generators [14]. They have
applications to cryptography, error correcting codes and digital signature. A
CCSG consists of two LFSRs A(control register) and B(generating register).
The A is clocked normally, but the B is clocked by one plus the integer value
Modelling Linear Cellular Automata 169
Table 2. Cho et al.’s Synthesis Algorithm
Algorithm Cho et al.’s Synthesis Algorithm
Input : CA-polynomial f(x)
Output : 90/150 group/nongroup CA
Step 1 : Make the matrix B which is the n× n matrix
obtained by reducing the n polynomials
xi−1 + x2i−1 + x2i (mod f(x)) (i = 1, 2, · · · , n).
Step 2 : Solve the equation Bv = (0, · · · , 0, 1)T .
Step 3 : Construct a Krylov matrix H = K(CT , v) by the seed vector v
which is a solution of the equation in Step 2.
Step 4 : Compute the LU factorization H = LU .
Step 5 : Compute CA for f(x) by the matrix U .
represented in selected w fixed stages of the A. The output bits of the system are
produced by shrinking the output of B under the control of A as the following.
At any time t the output of B is taken if the current output of A is 1, otherwise
it is discarded. Suppose as the following Table 3.
Table 3. LFSRs A and B
LFSR stage characteristic polynomial initial state
A m R(x) A0
B n S(x) B0
F is a function that acts on the state of A at a given time t to determine the
number of times which B is clocked such that
F (At) = 1 + 20Ai0(t) + 2
1Ai1(t) + · · ·+ 2w−1Aiw−1(t)
for w < m, and distinct integers i0, i1, · · · , iw−1 ∈ {0, 1, · · · ,m − 1}, At is the
state at the time instant t. If no stages are selected (i.e. w = 0), define F (At) = 1.
In this way, the output sequence of a CCSG is obtained from a double dec-
imation. First, the sequence {bi} of B is decimated by F (At) giving rise to
the sequence {b′i}. Next, if the output of A is 1, b′i becomes the output of the
generator, otherwise b′i is discarded.
Example 3.1 LetA be the 4-stage LFSR with the characteristic polynomial
R(x) = x4 + x+ 1 and the initial state (0, 0, 0, 1). The sequence {ai} generated
by A is
{ai} = {0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, · · ·}
with period 24 − 1 = 15. And let B be the 5-stage LFSR with the characteristic
polynomial S(x) = x5 + x2 + 1 and the initial state (0, 0, 0, 0, 1). The sequence
170 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
{bi} generated by B is
{bi} = {0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
0, 0, 0, 0, 1, · · ·}
with period 25 − 1 = 31. If w = 1, then
F (At) = 1 + 20Ai0(t)
Thus {Xti} is produced by F (At) as the following:
{Xti} = {1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, · · ·}
In [14], they defined the cumulative function GA of A to be
GA(Xti) = 2
m−1(2w + 1)− 1
Then GA(Xti) = 2
4−1(21 + 1)− 1 = 23. That is, 1 + 1 + 1 + 2+ 1+ 1+ 2+ 2+
1 + 2 + 1 + 2 + 2 + 2 + 2 = 23. In brief, after clocking A 24 − 1(= 15) times, B
is clocked 23 times.
According to the following,
{
b′0 := b0
b′i+1 := bj , j =
∑i
k=0Xti
the underlined bits 0 or 1 of {bi} are outputted in order to produce the
sequence {b′i}.
{bi} 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, · · ·
{Xti} 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, · · ·
{b′i} 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, · · ·
Then the output sequence {zi} of the CCSG is given by shrinking {b′i} with
{ai}
{ai} 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, · · ·
{b′i} 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, · · ·
{zi} 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, · · ·
The underlined bits 0 or 1 of b′i are outputted.
Modelling Linear Cellular Automata 171
4 90/150 CA-based CCSG
In this section, we analyze the period of sequences generated by CCSG based on
LFSR.
Definition 4.1 Let {a′i} be the sequence obtained by concatenations of
{Ci}’s.
C0 := 1
Ci :=

1, Xti = 1,
(
k︷ ︸︸ ︷
0, · · · , 0, 1), Xti = k, (k ≥ 2).
Example 4.2 {b′i} in Example 3.1 can be obtained by shrinking {bi} with
{a′i}. That is, {Xti} in Example 3.1 can be represented by {a′i}.
{a′i} = {1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, · · ·}
If a′i is 1, bi becomes the output of the generator, otherwise the output of bi is
discarded. This is just b′i. The decimated sequence {b′i} is given by
{a′i} 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, · · ·
{bi} 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, · · ·
{b′i} 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, · · ·
Theorem 4.3 Let A (resp. B) be an m (resp. n)-stage LFSR whose charac-
teristic polynomial is primitive. And let {a′i} be the sequence obtained by con-
catenations of {Ci}’s in Definition 4.1. The period of {a′i} is 2m−1(2w + 1) − 1
for a given w.
Proof. Because GA(Xti) = 2
m−1(2w+1)−1 for a given w, the period of {a′i}
is 2m−1(2w + 1)− 1 by Definition 4.1.
Theorem 4.4 Let A (resp. B) be an m (resp. n)-stage LFSR whose charac-
teristic polynomial is primitive. And let {b′i} be a sequence given by shrinking
{bi} with {a′i}. The period of {b′i} is
(2m − 1)lcm(GA(Xti), 2n − 1)
GA(Xti)
Proof. The period of the output sequence {bi} of B is 2n − 1. {a′i} repeats
GA(Xti) period sequences
lcm(GA(Xti ),2
n−1)
GA(Xti )
times and (2m − 1) 1’s occurs in a
full period of {a′i}. Thus the period of {b′i} is
(2m − 1)lcm(GA(Xti), 2n − 1)
GA(Xti)
172 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
Theorem 4.5 Let A (resp. B) be an m (resp. n)-stage LFSR whose charac-
teristic polynomial is primitive. And let {zi} be a sequence given by shrinking
{b′i} with {ai}. The period of {zi} is
2m−1lcm(GA(Xti), 2
n − 1)
GA(Xti)
Proof. The period of the output sequence {b′i} is (2m−1) lcm(GA(Xti ),2
n−1)
GA(Xti )
(:=
h). {ai} repeats lcm(GA(Xti ),2
n−1)
GA(Xti )
period sequences lcm(2
m−1,h)
2m−1 times and (2
m−1)
1’s occurs in a full period of {ai}. Thus the period of {zi} is
2m−1 lcm(2m − 1, lcm(GA(Xti ),2
n−1)
GA(Xti )
)
2m − 1
= 2m−1lcm(GA(Xti), 2
n − 1)/GA(Xti)
Remark If 2n − 1 and GA(Xti) are relatively prime, lcm(GA(Xti), 2n −
1)/GA(Xti) = 2
n − 1. Therefore in this case, the period of the output sequence
by CCSG is 2m−1(2n − 1). Thus the characteristic polynomial of such output
sequence is of the form F (x) = (n stage primitive polynomial)N , 2m−2 < N ≤
2m−1.
In [9], they proposed the algorithm that converts a given CCSG into a CA-
based linear model using mirror image and the Cattell and Muzio synthesis
algorithm. Therefore N = 2m−1 is the maximum stage. Also this CA-based
linear model is symmetrical and has a waste of space.
5 Modelling Linear Cellular Automata with the minimum
stage
In this section, we propose a method that converts a given CCSG into a CA-
based linear model by using Cho et al.’s Synthesis Algorithm [11].
According to the previous results, the following algorithm that converts a
given CCSG into a CA-based linear model is introduced in Table 5.
The following example shows modelling of linear CA with the minimum stage
corresponding to CCSG based on LFSR using the above algorithm.
Example 5.1 Let A be the 4-stage LFSR with a primitive polynomial of de-
gree 3, and B be the 5-stage LFSR with a primitive polynomial of degree 5. Let
w = 1. Then CCSG with A and B has the characteristic polynomial F (x) = (5-
stage primitive polynomial)(2
4−2−1) = (x5+x2+1)3. By the proposed algorithm,
we can compute a CA with T15 =< 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1 >. If the al-
gorithm in [9] is used, they must compute a CA with T20 =< 1, 1, 1, 1, 1, 1, 1, 1, 1,
0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1 > corresponding to F (x) = (x5 + x2 + 1)4.
Modelling Linear Cellular Automata 173
Table 5. Algorithm for modelling 90/150 CA
Algorithm ModellingOf90/150CA
Input : A CCSG characterized by: The stages m of LFSR(A) and n of LFSR(B), w
(2n − 1 and GA(Xti) = 2m−1(2w + 1)− 1 are relatively prime.)
Output : Linear CA with the minimum stage corresponding to CCSG based on LFSR
Step 1 : Compute the characteristic polynomial F (x) for the given CCSG,
where F (x) = (n stage primitive polynomial)N , N = 2m−2 + 1.
Step 2 : Compute CA by Algorithm “Cho et al.’s Synthesis Algorithm”.
6 Conclusion
In this paper we proposed a new method of modelling linear CA with the mini-
mum stage corresponding to CCSGs based on LFSR to overcome the weak points
of the Sabater et al.’s method.
References
1. P.P. Chaudhuri, D.R. Chowdhury, S. Nandy and S. Chattopadhyay, Additive Cel-
lular Automata Theory and Applications 1, IEEE Computer Society Press, Cali-
fornia, (1997)
2. Z. Chauanwu and L. Libin, “VLSI characteristic of cellular automata as LFSR,”
Communications and Information Technology, 2005. ISCIT 2005, IEEE Interna-
tional Symposium. 2 (2005) 1031-1034
3. A.J. Menezes, P.C. van Oorschot and S.A. Vanstone, Handbook of Applied Cryp-
tography, CRC Press, (1997)
4. P.H. Bardell, W.H. McAnney and J. Savir, Built-In Test for VLSI: Pseudorandom
Techniques, A WILEY-INTERSCIENCE PUBLICATION, (1987)
5. D. Coppersmith, H. Krawczyk and Y. Mansour, “The shrinking generator,” Lecture
Notes in Computer Science , 773 (1994) 22-39
6. G. Gong, “Theory and applications of q-ary interleaved sequences,”IEEE Trans-
action on Information Theory, 41-2 (1995) 400-411
7. J.D. Golic, “Toward fast correlation attacks on irregularly clocked shift registers,”
Advances in Cryptology - EUROCRYPT’95, Lecture Notes in Computer Science,
921 (1995) 248-262
8. D. Gollmann and W.G. Chambers, “Clock controlled shift registers: a review,”
IEEE J. Sel. Ar. Commun., 7-4 (1989) 525-533
9. A.F. Sabater and D.G. Martinez, “Modelling nonlinear sequence genterators in
terms of linear cellular automata,” Applied Mathematical Modelling, 31 (2007)
226-235
174 Yoon-Hee Hwang, Sung-Jin Cho, Un-Sook Choi, and Han-Doo Kim
10. K. M. Cattell and Jon C. Muzio, “Synthesis of One-Dimensional Linear Hybrid
Cellular Automata,” IEEE Trans. Comput-Aided Des. Integr. Circuits Syst., 15-3
(1996) 325-335
11. S.J. Cho, U.S. Choi, H.D. Kim, Y.H. Hwang, J.G. Kim and S.H. Heo, “New syn-
thesis of one-dimensional 90/150 linear hybrid group cellular automata,” IEEE
Trans. Comput-Aided Des. Integr. Circuits Syst., 26-9 (2007) 1720-1724
12. H. Umeo, T. Yanagihara and M. Kanazawa, “State-Efficient Firing Squad Syn-
chronization Protocols for Communication-Restricted Cellular Automata,” Lecture
Notes in Computer Science , 4173 (2006) 169-181
13. S. Wolfram, “Statistical mechanics of cellular automata,” Rev. Mod. Phys. 55
(1983) 601-644
14. A. Kanso, “Clock-controlled shrinking generators,” Lecture Notes in Computer
Science , 2727 (2003) 443-451
15. M. Serra, T. Slater. J.C. Muzio and D.M. Miller, “The analysis of one dimensional
linear cellular automata and their aliasing properties,” IEEE Trans. Comput-Aided
Des. Integr. Circuits Syst., 9 (1990) 767-778
Multitape Ordinal Machines and Primitive
Recursion
Bernhard Irrgang and Benjamin Seyfferth
University of Bonn, Mathematical Institute
Beringstraße 1, D-53115 Bonn, Germany
irrgang@math.uni-bonn.de, benjamin.seyfferth@gmx.de
Abstract. We introduce a multitape version of the ordinal Turing ma-
chines which are defined in [4] as Turing machines computing on tapes
of transfinite length in transfinite time. These machines are used to com-
pute the primitive recursive ordinal functions which have a classical the-
ory developed by Jensen and Karp [3]. Making use of that theory we
are able to 1. identify the ∆1(Lα)-definable subsets of α as computable
(if α is closed with respect to primitive recursive functions) and 2. char-
acterize admissible ordinals by the means of transfinite computations.
Similar results linking α-recursion theory and ordinal computability are
contained in [6].
Key words: Ordinal computability, multitape Turingmachine, prim-
itive recursive ordinal function, primitive recursive set function, admis-
sible ordinal.
1 Introduction
Ordinal computability studies machine models generalized to perform computa-
tions on ordinals. Such machines have been used to compute Gödel's hierarchy
of constructible sets L ([4], [5]) and can provide a computational approach to
α-recursion theory ([6]). The α-Turing machine is a standard Turing program
computing on tapes of length a limit ordinal α using a lim inf-rule to determine
the machine configuration at limit times. The machine is said to terminate if it
runs for less than α many steps, otherwise it diverges.
In [3] Jensen and Karp established a connection between the primitive
recursive ordinal functions (PrimO, a generalization of the usual primitive re-
cursive functions on natural numbers) and the primitive recursive set functions
(PrimS) that are used in the theory of Gödel's constructible universe ([2]).
The multitape α-Turing machines introduced in this paper are a straightfor-
ward multitape version of the α-Turing machine and are well suited to handle
the calculus of PrimO functions, i.e. every PrimO function is also multitape α-
computable (Theorem 1).
Using a result from [3] that the PrimO functions are exactly the PrimS func-
tions that map ordinals to ordinals we prove Theorem 2: If α is an ordinal closed
under PrimO functions then the ∆1(Lα[B]) definable subsets of α are exactly
176 Bernhard Irrgang and Benjamin Seyfferth
the ones that are multitape α-computable in B. Furthermore we are able to give
a characterization of admissible ordinals by the means of multitape α-Turing
machines (Theorem 3): A limit ordinal is admissible iff there is no multitape α-
computable function mapping some β < α cofinally into α. Admissible ordinals,
which play an important role in generalizing recursion theory, are classically
defined by the means of definability over a constructible level Lα or axioma-
tized through the axioms of Kripke-Platek set theory. We hence provide an
alternative approach to admissibility from a computational perspective.
Similar theorems are already contained in [6] but obtained in a different
way: A (single-tape) α-computable truth function for Lα is employed to transfer
definability over Lα to the computational context. The computability of this
truth function however requires α to be sufficiently closed with respect to ordinal
arithmetic. Seeing this in a talk the second author gave in a seminar on ordinal
computability in Bonn in November 2007, the first author suggested to make
use of the well developed theory of primitive recursive ordinal and set functions
and applied them in the proofs of Theorems 2 and 3. We kindly thank Peter
Koepke for his suggestions and support of this work.
2 Multitape α-Turing Machines
A program P for a standard Turing-machine with k tapes (each with an inde-
pendent read-write head) can be seen as a finite subset of {0, 1}k×ω×{0, 1}k×
ω×{−1,+1}k. An element (a, s, a′, s′, d) ∈ P codes the following instruction: If
the k-many read write heads read the symbols corresponding to the entries of
the vector a ∈ {0, 1}k and the machine is in state s ∈ ω then have the heads
write the entries of a′ ∈ {0, 1}k to the respective tapes, change the machine state
to s′ ∈ ω and move the read-write heads according to the vector d ∈ {−1,+1}k.
Similarly to previous studies ([4, 6]) this can be used as a basis for the following
notion of a transfinite computation according to P :
At successor times the program P is used as in the standard Turingmachine
case, with the single exception that when dealing with tapes of transfinite length
a convention has to be found what should happen when a read-write-head is
being moved left from a cell indexed by a limit ordinal. In this situation we want
the head to be reset to the beginning of the tape (cell number `0').
For limit times the Turing-program cannot determine the tape content,
head positions and program state so we have to define them in a sensible way.
Following the lines of [4] we use inferior limits: We want each single cell of every
tape to contain the lim inf of its previous values, the machine state to be the
lim inf of the previous machine states and every tape's read/write-head to be
located on the cell indexed by the lim inf over the positions it previously assumed
in the limit machine state, i.e. the least cell that was read cofinally often while
the machine was in the same state as at the limit time.
More formally:
Definition 1. Let α be a limit ordinal or α = Ord. Let P ⊆ {0, 1}k × ω ×
{0, 1}k×ω×{−1, 1}k be finite and let T0 = (T0,0, T1,0, . . . , T(k−1),0) ∈ (α{0, 1})k
Multitape Ordinal Machines and Primitive Recursion 177
be the initial tape content of the k-many tapes of length α. A triple
(Tθ,Hθ, Sθ)θ≤Θ
is a k-tape α-Turing computation by P on input T0 if the following conditions
hold:
− Θ ≤ α;
− Sθ ∈ ω for θ ≤ Θ ;
− Hθ = (H0,θ,H1,θ, . . . ,H(k−1),θ)
where Hi,θ ∈ α for 0 ≤ i < k and for θ ≤ Θ ;
− Tθ = (T0,θ, T1,θ, . . . , T(k−1),θ)
where Ti,θ : α −→ {0, 1} for 0 ≤ i < k and for θ ≤ Θ ;
− (Tθ,Hθ, Sθ)θ≤Θ is defined recursively in P and the initial tape contents Ti,0
in the following way:
Termination: Let θ ≤ Θ < α and let (Tθ′ ,Hθ′ , Sθ′)θ′≤θ be already defined.
If there is no (a, s, a′, s′, d) ∈ P where (Ti,θ(Hi,θ))i<k = a and Sθ = s then
the computation terminates, i.e. θ = Θ.
Successor step: Let θ ≤ Θ, let (Tθ′ ,Hθ′ , Sθ′)θ′≤θ be already defined and
let there be a c = (a, s, a′, s′, d) ∈ P where (Ti,θ(Hi,θ))i<k = a and Sθ = s.
Choose c minimally with respect to some fixed well-order on P . As usual
we want the configuration (Tθ+1,Hθ+1, Sθ+1) to be derived from (Tθ,Hθ, Sθ)
according to the instruction c.
Let a′ = (a′0, a
′
1, . . . , a
′
k−1). For all i < k we require:
Ti,θ+1(ξ) =
{
ai , if ξ = Hi,θ
Ti,θ(ξ) , else
Hi,θ+1 =

Hi,θ + 1 , if d = +1
Hi,θ − 1 , if d = −1 and Hi,θ is a successor ordinal
0 , if d = −1 and Hi,θ is a limit ordinal
Si,θ+1 = s′.
Limit step: Now let θ ≤ Θ be a limit ordinal and let (Tθ′ ,Hθ′ , Sθ′)θ′<θ be
already defined. For i < k and ξ < α set
Sθ = lim inf
θ′<θ
Sθ′
Hi,θ = lim inf
θ′<θ,Sθ=Sθ′
Hi,θ′
Ti,θ(ξ) = lim inf
θ′<θ
Ti,θ′(ξ).
Note that the machine configuration at limit times is always defined whenever
the configurations at all previous stages are defined. If θ = Θ = α we say
that the computation diverges.
The primitive recursive ordinal functions we want to compute are n-ary func-
tions mapping ordinals < α to α. So we define:
178 Bernhard Irrgang and Benjamin Seyfferth
Definition 2. Let B ⊆ α and n ∈ ω. A function f : αn −→ α is called k-α-
computable in B if there is a k-tape Turing program P and a finite sequence
of ordinal parameters pi = (pi1, pi2, . . . , pim) ∈ αm s.t. k ≥ n+m+ 2 and for all
ξ = (ξ1, ξ2, . . . , ξn) ∈ αn the k-tape α-Turing computation by P with input T0
(Tθ,Hθ, Sθ)θ≤Θξ is of the form
− Θξ < α, i.e. the computation terminates;
− for every i < n there is a read-only tape containing χ{ξi};
− for every j < m there is a read-only tape containing χ{pij};
− there is a read-only tape containing χB;
− all other tapes are initially empty;
− there is a tape that at time Θξ contains χ{f(ξ)}.
If B = ∅ we call f k-α-computable.
A function g : αn −→ α is called multitape α-computable if there is a k such
that g is k-α-computable.
The question arises which closure properties an ordinal α must have so that
the notions of α-computable in B defined in [6] and multitape α-computable in
B coincide. Clearly this is true for admissible ordinals, but much weaker closure
properties will certainly suffice.
Lemma 1. Let f : αn −→ α be k-α-computable by the program P in parameters
pi. Let (Tθ,Hθ, Sθ)θ≤Θξ be the k-α-computation by P for f(ξ). Then the function
Timef : αn −→ α which maps ξ 7→ Θξ is (k + 1)-α-computable.
Proof. We extend P to a (k + 1)-tape Turing program which still computes f
but where every instruction in P additionally moves the (k + 1)-st tape's head
to the right. The computation terminates after Θξ many steps, i.e. there is no
command for the final configuration. Add a new instruction for this configuration
that writes a `1' at the current head position of the (k+1)-st tape. Since the k+1-
st tape's head now points to a `1' instead of a `0' the computation terminates
with this new instruction. It follows that Timef is (k + 1)-α-computable.
3 Primitive Recursive Functions
The familiar primitive recursive functions on natural numbers are those recursive
functions generated by a weak recursion scheme that allows recursive definitions
using the supremum over the previous function values. A generalization of this
concept to functions operating on the universe of sets has for instance been used
in the study of the constructible hierarchy ([2]). In [3] Jensen and Karp defined
the notion of primitive recursiveness for functions mapping ordinals to ordinals
and developed their theory. Following [3] we define:
Definition 3. The class PrimO(B) of primitive recursive ordinal functions in
B ⊆ Ord is defined as the minimal set containing all the functions of type (1)
to (5) and closed under the schemes for substitution (a) and (b) and recursion
(R).
Multitape Ordinal Machines and Primitive Recursion 179
(1) f(ξ) = χB(ξ)
(2) prn,i(ξ) = ξi, for all n ∈ ω, ξ = (ξ1, . . . , ξn) and 1 ≤ i ≤ n.
(3) f(ξ) = 0
(4) f(ξ) = ξ + 1
(5) c(ξ, ζ, γ, δ) =
{
ξ, if γ < δ
ζ, else
(a) f(ξ, ζ) = g(ξ, h(ξ), ζ)
(b) f(ξ, ζ) = g(h(ξ), ζ)
(R) f(ζ, ξ) = g(sup{f(η, ξ) | η < ζ}, ζ, ξ)
We write PrimO instead of PrimO(∅).
If B ⊆ α and α is an ordinal that is closed under PrimO(B) functions then
we call a function f : αn −→ α PrimO(B) iff it is the restriction of a PrimO(B)
function.
Definition 3 is a special case of the following definition of primitive recursive
set functions also taken from [3]:
Definition 4. Let X be a one-place set function. The class PrimS(X) of prim-
itive recursive set functions in X is defined as the minimal set containing all the
functions of type (1) to (5) and closed under the schemes for substitution (a)
and (b) and recursion (R).
(1) F (x) = X(x)
(2) Prn,i(x) = xi, for all n ∈ ω, x = (x1, . . . , xn) ∈ αn and 1 ≤ i ≤ n.
(3) F (x) = 0
(4) F (x, y) = x ∪ {y}
(5) C(x, y, u, v) =
{
x, if u ∈ v
y, else
(a) F (x,y) = G(x,H(x),y)
(b) F (x,y) = G(H(x),y)
(R) F (y,x) = G(sup{F (z,x) | z < y}, y,x)
We write PrimS instead of PrimS(∅).
If B is a set and X is the unary function X(x) = x ∩ B then we may write
PrimS(B) for PrimS(X).
If B ⊆ α and α is an ordinal that is closed under PrimO(B) functions then
we call a function f : αn −→ α PrimS(B) iff it is the restriction of a PrimS(B)
function.
An n-ary relation R ⊆ V n is PrimS(X) if there is a PrimS(X) function
FR : V n −→ V s.t. FR(x) = 0 iff x ∈ R.
Theorem 1. If α is closed under PrimO functions, then every PrimO(B) func-
tion f : αn −→ α is multitape α-computable in B. Furthermore there is a PrimO
function Mf that majorizes Timef , i.e. ∀ξ ∈ αn M(ξ) > Timef (ξ).
180 Bernhard Irrgang and Benjamin Seyfferth
Proof. Functions (1) to (4) are easily seen to be multitape α-computable with
majorizing functions the maximum input ordinal plus 1.
(5) We define a program to compute C. Move the heads of the tapes con-
taining γ and δ to the right to decide whether γ < δ. According to the outcome
move the output tape's head together with the head of the tape containing ξ
or ζ to the right until a `1' is read. Copy the `1' to the output tape and stop.
This program runs at most MC(γ, δ, ξ, ζ) = max{γ, δ} + max{ξ, ζ} + 1-many
steps. Since α is closed under PrimO functions and ordinal addition is PrimO
the program terminates and C is multitape α-computable.
(a) Let g be k-α-computable by a program Pg with parameters pig and ma-
jorizing functionMg, h l-α-computable by Ph with parameters pih and majorized
by function Mh. We define a (k+ l)-tape Turing program that computes f us-
ing pig pˆih as parameters. The program first runs Ph to compute h(ξ). Note that
any program computing f in parameters pig pˆih uses tapes containing the com-
ponents of ξ, ζ, pih, pig and one tape containing the characteristic function of B;
these can be used as input tapes for Ph and later on for Pg respectively. After
resetting all heads to position zero the program continues with running Pg with
the output tape of Ph, now containing h(ξ), as additional input tape. Resetting
of heads takes only finitely many machine steps thanks to the well-foundedness
of ordinals (to recognize head position `0' we may assume additional parameter
tapes containing χ{0}). So the new Program will run in less than α many steps
since addition of ordinals is PrimO and α is closed under PrimO functions. We
can set Mf = (Mg · 2) + (Mh · 2).
(b) Similarly to (a).
(R) Let g be k-α-computable by Pg in parameters pig and majorized by Mg.
We describe a (k + 2)-tape Turing program that computes f with parameters
pig. The algorithm runs through ζ-many stages. In stage η < ζ the new (k + 1)-
st tape contains the current value of sup{f(η′, ξ) | η′ < η}. Pg is called once
to compute f(η, ξ) = g(sup{f(η′, ξ) | η′ < η}, η, ξ). If necessary the value of
sup{f(η′, ξ) | η′ ≤ η} has to be updated. The stage concludes by erasing all the
work tapes used by Pg and resetting all heads to zero. We have to ensure that
for all η < ζ the following conditions hold:
(i) At the time of the call of Pg to compute f(η, ξ) the (k+1)-st tape contains
in fact sup{f(η′, ξ) | η′ ≤ η}.
(ii) The number of machine steps the program uses before entering stage η is
less than α.
For (i) use the (k + 2)-nd tape to save the value of f(η, ξ) = γ not as χ{γ}
but as χγ+1. At the beginning of every stage use tape (k+2) to write a `1' to the
sup{f(η′, ξ) | η′ ≤ η}-th cell of tape (k+1) and use this as input to Pg. (i) holds
inductively at successor stages. So let η be a limit ordinal. By the lim inf-rule
tape (k + 2) contains in fact χsup{f(η′,ξ)|η′≤η}. So (i) holds.
Stage η consists of the following operations:
− Find the first 0 on tape (k+2) and write a 1 to the respective cell of tape (k+
1) (note that the the first `0' occurs in cell number sup{f(η′, ξ) | η′ < η}+1):
Multitape Ordinal Machines and Primitive Recursion 181
As seen above the number of machine steps β
find
= sup{f(η′, ξ) | η′ < η}+2
is less than α.
− Reset tape (k + 1)'s head: This needs at most βreset(k+1) = sup{f(η′, ξ) |
η′ < η many steps.
− Run Pg to compute f(η, ξ) = g(sup{f(η′, ξ) | η′ < η}, η, ξ), the number
of steps βg = Timeg(sup{f(η′, ξ) | η′ < η}, η, ξ) is (k + 1)-α-computable
therefore < α.
− Reset the head of Pg's output tape and update tape (k+2) if necessary: This
can be decided and done in at most β
update
= f(η, ξ) · 2 < α many steps.
− Erase the work tapes used by Pg. WLOG Pg maintains a `timer' tape as in
Lemma 1 which we can use to determine up to which cell the tapes have to
be erased. Since only cells up to index βg may have been used by Pg (all
heads were at position 0 when Pg was called) this takes again at most βg · 2
many steps (·2 since we have to reset the heads before erasing the tapes).
− Reset all heads. This takes at most max{βfind, βg} many steps.
We define a majorizing function Mf for Timef by PrimO recursion:
Mf (η) = sup
η′<η
Mf (η′)
+ (sup{f(η′, ξ) | η′ ≤ η}+ 2) · 2
+Mg(sup{f(η′, ξ) | η′ ≤ η}, η)
+ f(η, ξ) · 2
+Mg(sup{f(η′, ξ) | η′ ≤ η}, η) · 2
+ max{sup{f(η′, ξ) | η′ ≤ η},Mg(sup{f(η′, ξ) | η′ ≤ η}, η, ξ)}
It follows from inductive analysis of the algorithm above that Timef < Mf . Note
that as supremum of the first η many values of a PrimO function supη′<ηMf (η′)
is PrimO and therefore < α. So (ii) holds.
4 Applications
The following theorems are similar to Theorem 7 and 9 in [6] which provide a
connection between α-recursion theory and ordinal computability. The proofs
found in [6] explicitely give a truth function for bounded formulas in Lα which
is α-computable if α is sufficiently closed with respect to ordinal arithmetic.
Instead we use facts from the classical theory of PrimS functions to obtain these
results.
Theorem 2. If α is an ordinal closed under PrimO functions then A ⊆ α is
∆1(Lα[B]) iff A is multitape α-computable in B.
Proof. `⇐' follows from the recursion theorem as in [6].
`⇒' Let A ⊆ α be ∆1(Lα[B]) by
γ ∈ A↔ Lα[B] |= ∃xφ[x, γ,p]
γ /∈ A↔ Lα[B] |= ∃xψ[x, γ, q].
182 Bernhard Irrgang and Benjamin Seyfferth
where φ, ψ are ΣB0 formulas. So we have:
γ ∈ A↔ Lα[B] |= ∃xφ[x, γ,p]
↔ ∃x ∈ Lα[B] φ[x, γ,p]
In [3], Lemma 3.2, it is shown that there is a one-one PrimS function N mapping
the ordinals onto the constructible sets s.t. N  α : α bij−−→ Lα. It is easily seen
that there is also a one-one PrimS(B) function N ′ mapping the ordinals onto
L[B] s.t. F = N ′  α : α bij−−→ Lα[B]. So we can write:
↔ ∃ξ ∈ α φ[F (ξ), γ,F (pi)]
Like in [2], Lemma I.2.4, we see that every ΣB0 relation is PrimS(B). Hence there
is a PrimS(B) function G s.t. G(z) = 0 iff φ[z]:
↔ ∃ξ ∈ α G(F (ξ), γ,F (pi)) = 0
↔ ∃ξ ∈ α g(ξ, γ,pi) = 0.
Where
g(ξ, γ,pi) =
{
0 , if G(F (ξ), γ,F (pi)) = 0
1 , else.
Since G is PrimS(B) so is g by (a) and (5). g mapping ordinals to ordinals,
B ⊂ α, and we see like in Theorem 3.5 in [3] that g is PrimO(B). Similarly we
obtain a function h for ψ. Now we can describe the following algorithm which
computes the characteristic function of A:
WHILE (g(ξ, γ,pi) = 1 AND h(ξ, γ,η) = 1) DO ξ ++;
IF g(ξ, γ,pi) = 0 THEN STOP = 0;
IF h(ξ, γ,pi) = 0 THEN STOP = 1;
We analyse the algorithm into its stages ξ < α, each consisting mainly of one
computation for g and one for h plus some erasing of work tapes and resetting of
heads. We have to make sure that this algorithm behaves nicely at limit stages,
i.e. actually reaches every limit stage δ < α. Again it will be neccessary to store
the counter ξ as χξ+1 and to decode this into χξ at the beginning of every stage.
Similar to the proof of Theorem 1 we see that the algorithm up to stage ξ uses a
number of steps majorized by a PrimO function M(ξ). Again also supζ<δM(ζ)
for δ < α is PrimO so the algorithm reaches every stage.
We can now give a characterization of admissible ordinals solely based on
ordinal computations.
Theorem 3. A limit ordinal α is admissible iff there is no multitape α-computable
function mapping some β < α cofinally into α.
Multitape Ordinal Machines and Primitive Recursion 183
Proof. α is admissible iff there is no Σ1(Lα)-definable total function that maps
some β < α cofinally into α (cf. [1], Lemma II.7.2).
If α is already closed under PrimO functions then any multitape α-computable
function f is ∆1(Lα) (and therefore Σ1(Lα)) by the recursion theorem as in [6]
(cf. Theorem 2). Conversely let α be not admissible and let f : β
cof−−→ α be a
Σ1(Lα)-definable total function on β. So we have:
f(γ) = δ ↔ Lα |= ∃xφ[x, γ, δ,p]
↔ ∃x ∈ Lα φ[x, γ, δ,p]
Where φ is a Σ0 function. With F : α
bij−−→ Lα PrimS from Lemma 3.2 in [3]:
↔ ∃ξ ∈ α φ[F (ξ), γ, δF (pi)]
Since φ is Σ0 it is PrimS ([2], Lemma I.2.4):
↔ ∃ξ ∈ α G(F (ξ), γ, δ,F (pi)) = 0
↔ ∃ξ ∈ α g(ξ, γ, δ,pi) = 0.
Where
g(ξ, γ, δ,pi) =
{
0 , if G(F (ξ), γ, δ,F (pi)) = 0
1 , else.
Since G is PrimS so is g by (a) and (5). g is mapping ordinals to ordinals and
is therefore PrimO by Theorem 3.5 in [3]. The desired algorithm goes through
less than α many stages to compute f(γ) given γ as input. In every stage η < α
the algorithm computes the values of g(ξ, γ, δ,pi) for all ξ, δ < η. g is PrimO so
Timeg is multitape α-computable majorized by Mg which in turn is PrimO. So
supξ,δ<ηM(ξ, γ, δ,pi) is less than α and every stage η is reached by the algorithm.
At some stage one computation for g will return 0 and the value δ = f(γ) is
found. So f is multitape α-computable as required.
In the case that α is not closed under PrimO functions we will define β < α
and a multitape α-computable total function f : β
cof−−→ α.
Any limit ordinal is closed under (1)-(4) so assume in the following closure
under (1)-(4). If α is not closed under (5) then also for one instance of (5)
f(ζ0, ζ1, ζ2, ζ3) the canonical algorithm will not terminate in less than α many
steps. Analysing the algorithm in the proof of Theorem 1 we can extract two
ordinals γ, δ < α with γ + δ ≥ α (set γ = max{ζ2, ζ3} and δ = max{ζ0, ζ1}).
So there is a β ≤ δ such that the multitape α-computable function f : β −→ α,
ξ 7→ γ + ξ is cofinal in α.
If α is closed under two functions f and g so it is also closed under f(ξ, ζ) =
g(ξ, h(ξ), ζ) and under f ′(ξ, ζ) = g(h(ξ), ζ). So if α is closed under (1)-(5) but
not closed under PrimO functions it has to be not closed under (R).
Assume α closed under (1)-(5),(a),(b) but not closed under (R). Since the
PrimO functions are defined by recursion, there are PrimO functions f, g s.t.
184 Bernhard Irrgang and Benjamin Seyfferth
f(ζ, ξ) = g(sup{f(η, ξ) | η < ζ}, ζ, ξ) is an instance of (R) and α closed un-
der g but not closed under f . Choose β minimally s.t. f(β, ξ) /∈ α. f(β, ξ) =
g(supγ<β f(γ), ξ), β, ξ) and since α is closed under g we have that supγ<β f(γ, ξ) =
α. Now f  β : β −→ α is cofinal in α.
References
1. Keith Devlin. Constructibility. Perspectives in Mathematical Logic.
Springer-Verlag, Berlin, 1984.
2. Keith Devlin. Aspects of Constructibility. Perspectives in Mathematical
Logic. Springer-Verlag, Berlin, Heidelberg, 1973.
3. Ronald B. Jensen and Carol Karp. Primitive recursive set functions.
In: Axiomatic Set Theory, Proceedings of Symposia in Pure Mathematics,
Volume XIII, Part I. American Mathematical Society, Providence, Rhode
Island, 1971.
4. Peter Koepke. Turing computations on ordinals. The Bulletin of Symbolic
Logic 11 (2005), 377397.
5. Peter Koepke and Martin Koerwien. Ordinal computations. Mathe-
matical Structures in Computer Science (2006), 867884.
6. Peter Koepke and Benjamin Seyfferth. Ordinal Machines and Admis-
sible Recursion Theory. Submitted to: Annals of Pure and Applied Logic,
CiE 2007 special volume, to be published 2008.
Prescribed Learning of Indexed Families?
Sanjay Jain1, Frank Stephan2, and Nan Ye3
1 Department of Computer Science,
National University of Singapore, Singapore 117590, Republic of Singapore.
Email: sanjay@comp.nus.edu.sg .
2 Department of Computer Science and Department of Mathematics,
National University of Singapore, Singapore 117543, Republic of Singapore.
Email: fstephan@comp.nus.edu.sg .
3 Department of Computer Science and Department of Mathematics,
National University of Singapore, Singapore 117543, Republic of Singapore.
Email: u0407028@nus.edu.sg .
Abstract. This work extends studies of Angluin, Lange and Zeugmann
on how learnability of a language class depends on the hypothesis space
used by the learner. While previous studies mainly focused on the case
where the learner chooses a particular hypothesis space, the goal of this
work is to investigate the case where the learner has to cope with all
possible hypothesis spaces. In that sense, the present work combines the
approach of Angluin, Lange and Zeugmann with the question of how a
learner can be synthesized. The investigation for the case of uniformly r.e.
classes has been done by Jain, Stephan and Ye [6]. This paper investigates
the case for indexed families and gives a special attention to the notions
of conservative and non U-shaped learning.
1 Introduction
The goal of inductive inference [1, 2, 4] is to model the process of learning rig-
orously. Following many real-world scenarios, the learner observes more and
more data which in the limit uniquely determines the concept to be learnt. The
learner is supposed to determine the target concept from the data it observes.
Following the model of linguistics, the concept to be learnt is always consid-
ered to be an (often infinite) set of finite items which can be coded as natural
numbers. The language to be learnt is chosen from a concept class {L0, L1,
L2, . . .} and the learner is using an explicit hypothesis space {H0,H1,H2, . . .}.
This hypothesis space may be either the same as {L0, L1, L2, . . .} (exact learning
[1]) or chosen by the learner (class-preserving and class-comprising learning [8,
13, 14]) or imposed on the learner (prescribed and uniform learning [6]). Angluin
[1] considered the important case that the concept class and hypothesis class are
both given by an indexed family, that is, the class is uniformly recursive. She
? The work for this paper is supported in part by NUS grants number R252-000-212-
112 and R252-000-308-112. A full version is available as Technical Report TRB9/07,
School of Computing, National University of Singapore, 2007.
186 Sanjay Jain, Frank Stephan, and Nan Ye
has given a characterization when such a class is explanatorily learnable and
introduced also important variants like consistent and conservative learning.
The goal of the present work is to study prescribed and uniform learning
and to contrast the results obtained for them to the well-studied cases of exact,
class-preserving and class-comprising learning. The idea that the learner has to
accept a given choice of the hypothesis class is not completely new; besides the
case of exact learning (for which the results would be equivalent to the (not con-
sidered case of) class-preserving prescribed learning), it has also been considered
under the framework of synthesis of learners. But the models like those consid-
ered by Zilles [15, 16] differ from the scenario in the present work. Jain, Stephan
and Ye [6] have studied the more general case of uniformly r.e. concept and
hypothesis spaces in a separate paper. The main difference to the setting in the
r.e. case is that there it is more reasonable to consider class-preserving-uniformly
and class-preserving-prescribed learning instead of uniform and prescribed learn-
ing. Furthermore, the relation between non U-shaped learning and conservative
learning depends crucially on the indexed family nature of the hypothesis space.
The study of prescribed and uniform learning is done for the criteria of finite
learning (Section 2), conservative learning (Section 3), non U-shaped learning
(Section 4) and the various notions of monotonic learning (Section 5).
In the following, we will provide more details, but have to introduce some formal
notations first. Let N be the set of natural numbers and 〈·, ·〉 is a fixed pairing
function: a recursive bijective mapping from N2 to N. Furthermore, |S| denotes
the cardinality of set S. ϕ0, ϕ1, ϕ2, . . . denotes a fixed acceptable numbering of
the partial recursive functions from N to N. In some cases, we use ϕi as a function
of two arguments. In such cases one implicitly assumes a pairing function being
used to code the inputs: thus, ϕi(x, y) means ϕi(〈x, y〉). The setWe is the domain
of ϕe. The set K = {e : e ∈ We} is the diagonal halting problem which is used
as a standard example of an r.e. but nonrecursive set. Let Kt denote the set
of elements enumerated into K within t steps, via some standard enumeration
procedure. We assume without loss of generality that K0 = ∅.
Definition 1. A learner is a mapping from (N ∪ {#})∗ to N ∪ {?}. Let M be
a given learner, {L0, L1, L2, . . .} be a language class and {H0,H1,H2, . . .} be a
hypothesis space. In this paper, M is a partial-recursive function and {L0, L1,
L2, . . .}, {H0,H1,H2, . . .} are indexed families of subsets of the natural numbers,
that is, the mappings e, x 7→ Le(x) and e, x 7→ He(x) are recursive functions from
N×N to {0, 1}. Let σ, τ, ρ range over (N∪{#})∗. Furthermore, let σ ⊆ τ denote
that τ is an extension of σ as a string. T is a text if T is a total function which
maps N to N ∪ {#} and T is a text for La iff the numbers occurring in T are
exactly those in La.
A learner converges [4] on T to b iff there is an n with M(T [m]) = b for all
m ≥ n; here T [m] is the finite string consisting of the first m members of T .
The learnerM is total ifM(σ) is defined for all finite strings σ in (N∪{#})∗.
For the learning criteria in this paper, a given learner can always be effectively
converted to a total leaner which learns the same classes. Hence learners are
assumed to be total here onwards.
Prescribed Learning of Indexed Families 187
The learner M is finite [4] if for every text T there is one index e such that
for all n, either M(T [n]) = ? or M(T [n]) = e.
The learner M is confident [10] if M is total and converges on every text T
to a hypothesis.
The learner M is conservative [1] if for all σ, τ with HM(σ) 6= HM(στ) there
is an x occurring in στ such that x /∈ HM(σ). M is non U-shaped [3] if there
are no a and σ, τ, ρ ∈ (La ∪ {#})∗ such that HM(σ) = HM(στρ) = La and
HM(στ) 6= La. In other words, M never changes from a correct to an incorrect
and then back to a correct hypothesis. M is decisive [3] if there are no σ, τ, ρ
such that HM(στρ) = HM(σ) and HM(στ) 6= HM(σ). In other words, M never
returns to a once abandoned hypothesis (even semantically).
The learnerM ismonotonic [5] if for every La and for every σ, τ ∈ (La∪{#})∗
the inclusion La ∩HM(σ) ⊆ La ∩HM(στ) holds. M is strong-monotonic [5] if for
all σ, τ ∈ (N ∪ {#})∗ the inclusion HM(σ) ⊆ HM(στ) holds.
Here note that ? is not considered as a conjecture and thus the constraints
in conditions like conservative, monotonic, strong-monotonic, non U-shaped and
decisive refer only to inputs where M makes a conjecture and does not output ?:
so, more formally, a learner would be strong-monotonic iff for all σ, τ , M(σ) 6=?
and M(στ) 6=? implies HM(σ) ⊆ HM(στ). Similarly for the other criteria.
Finite learning is quite restrictive since the learner has to make up its mind with-
out having viewed all of the available infinite information. Learning in the limit
(or just “learning”) is more powerful since the learner can revise its hypothesis
a finite but arbitrary number of times.
In this paper we will only be concerned about learning indexed families and
using hypothesis spaces which are also indexed families. Angluin, Kapur, Lange
and Zeugmann [1, 7–9, 12–14] studied how learnability of the family to be learned
depends on the hypothesis space {H0,H1,H2, . . .} used by the learner. To for-
malize this, they introduced the notions of exact, class-preserving and class-
comprising learning. In addition to this we consider notions like uniform and
prescribed learning [6]. Here I ranges over properties of the learner as defined in
Definition 1, so I stands for “conservative”, “finite”, “monotonic” and so on.
Definition 2. In the following, let {L0, L1, L2, . . .} and {H0,H1,H2, . . .} be
indexed families.
{L0, L1, L2, . . .} is explanatorily learnable [4] with hypothesis space {H0,H1,
H2, . . .} iff there is a learner M which converges on every text of a language La
to a hypothesis b such that Hb = La.
For a property I from Definition 1, {L0, L1, L2, . . .} is I learnable with hy-
pothesis space {H0,H1,H2, . . .} iff there is a learner M which explanatorily
learns {L0, L1, L2, . . .} using hypothesis space {H0,H1,H2, . . .} and furthermore
satisfies the requirement I.
{L0, L1, L2, . . .} is class-comprisingly I learnable iff it is I learnable with some
hypothesis space {H0,H1,H2, . . .}; note that learnability automatically implies
{L0, L1, L2, . . .} ⊆ {H0,H1,H2, . . .}.
188 Sanjay Jain, Frank Stephan, and Nan Ye
{L0, L1, L2, . . .} is class-preservingly I learnable iff it is I learnable with
some hypothesis space {H0,H1,H2, . . .} satisfying {H0,H1,H2, . . .} = {L0, L1,
L2, . . .}.
{L0, L1, L2, . . .} is exactly I learnable iff it is I learnable with {L0, L1, L2, . . .}
itself taken as the hypothesis space.
{L0, L1, L2, . . .} is prescribed I learnable iff it is I learnable with respect to
every hypothesis space {H0,H1,H2, . . .} such that {L0, L1, L2, . . .} ⊆ {H0,H1,
H2, . . .}.
{L0, L1, L2, . . .} is uniformly I learnable iff there is a recursive enumera-
tion of partial-recursive functions M0,M1,M2, . . . such that whenever ϕe is a
decision-procedure b, x 7→ Hb(x) for an indexed family {H0,H1,H2, . . .} ⊇ {L0,
L1, L2, . . .} then Me is an I learner for {L0, L1, L2, . . .} using this hypothesis
space {H0,H1,H2, . . .}.
{L0, L1, L2, . . .} is class-preserving-uniformly I learnable iff there is a recur-
sive enumeration of partial-recursive functions M0,M1,M2, . . . such that when-
ever ϕe is a decision-procedure b, x 7→ Hb(x) for an indexed family {H0,H1,
H2, . . .} = {L0, L1, L2, . . .} then Me is an I learner for {L0, L1, L2, . . .} using
this hypothesis space {H0,H1,H2, . . .}.
Remark 3. For explanatory learning, class comprising learning and exact learn-
ing are the same as shown in [13]. This can be easily extended to the equivalence
between uniform learning and exact learning, by noting that given a language
class {L0, L1, L2, . . .}, then for any hypothesis space {H0,H1,H2, . . .} ⊇ {L0,
L1, L2, . . .}, one can find in the limit a b such that Ha = La for each a.
Exact finite learning and class comprising finite learning are the same [13].
For strong-monotonic, monotonic and conservative learning, there is a proper hi-
erarchy for learning from exact, class preserving and class comprising hypothesis
spaces [13]. For every criterion I, the following implications hold:
– Every uniformly I learnable family is also class-preserving-uniformly I learn-
able and prescribed I learnable.
– Every class-preserving-uniformly I learnable family and every prescribed I
learnable family is also exactly I learnable.
– Every exactly I learnable family is also class-preservingly I learnable.
– Every class-preservingly I learnable family is also class-comprisingly I learn-
able.
It depends on the actual choice of I what other implications hold (besides the
transitive ones).
For example, for confident learning, the class containing all {x} where |Wx| <∞
and {x, y} where x 6= y is not class-preservingly but class-comprisingly confi-
dently learnable. Although confident learnability becomes more general in the
class-comprising case, one can show that it coincides for all other criteria from
Definition 2. Suppose that N is an exact confident learner for the class {L0, L1,
L2, . . .} and e is given such that ϕe is total and the hypothesis space {H0,H1,
H2, . . .} satisfies Hd = {x : ϕe(〈d, x〉) = 1} for all d and {L0, L1, L2, . . .} ⊆ {H0,
Prescribed Learning of Indexed Families 189
H1,H2, . . .}. Then Me simulates the learner N as follows: if N on text T con-
verges to a then Me on text T converges to the least b such that Hb = La.
From the definition of uniform learning, we can easily obtain the following
useful lemma.
Lemma 4. Let L be a uniformly I learnable indexed family. If H0,H1,H2, . . .
is a recursive enumeration of indexed family hypothesis spaces for L, then there
exists a recursive enumeration of learners M0,M1,M2, . . . such that Mn I learns
L with respect to Hn.
We will often make use of the following simple set in our proofs.
Definition 5. Define S = ∪n=0,1,2,...Jn, where Jn contains for each e < n the
first element, if any, ofWe enumerated from In = {2n−1, 2n, 2n+1, . . . , 2n+1−2}.
Then: S is recursively enumerable; S intersects with every infinite recursively
enumerable set; for every n there is an m in In which is not in S. In other words,
S is a simple set [11]. Let St be the set of elements enumerated into S within t
steps via some standard procedure. Here we take S0 = ∅.
In the following sections, without loss of generality we assume for i, j < |{L0,
L1, L2, . . .}| that Li = Lj implies i = j.
2 Finite Learning
Finite learning or one-shot learning requires the learner to make a correct guess
using only finite amount of information. So it is not a surprise that this criterion
turns out to be very restrictive for prescribed and uniform learning, as shown
below. The following theorem gives some characterization results, and separates
various notions of finite learning. It can be shown that class comprising finitely
learnable classes are also exact finitely learnable [13].
Theorem 6. Let {L0, L1, L2, . . .} be exactly finitely learnable.
(a) {L0, L1, L2, . . .} is not uniformly finitely learnable.
(b) {L0, L1, L2, . . .} is class-preserving-uniformly finitely learnable.
(c) {L0, L1, L2, . . .} is prescribed finitely learnable iff the class is finite and for
all i, j < |{L0, L1, L2, . . .}|, either i = j or Li 6⊂ Lj.
Hence, the class {{0}, {1}, {2}, . . .} is exactly finitely learnable but not pre-
scribed finitely learnable; furthermore, a class is prescribed finitely learnable iff
it is finitely learnable and finite.
3 Conservative Learning
Conservative learning is non-trivial: there is an infinite indexed family which is
uniformly conservatively learnable. This is shown in the next example.
Example 7. Let La = N−{a} for all a ∈ N. Then {L0, L1, L2, . . .} is uniformly
conservatively learnable.
190 Sanjay Jain, Frank Stephan, and Nan Ye
The class used in above example consists of co-finite sets only. The next result
shows that this is necessary for uniform conservative learning.
Theorem 8. If {L0, L1, L2, . . .} is uniformly conservatively learnable then every
set La is cofinite. Moreover, there is a recursive function r bounding the non-
elements of La for all a < |{L0, L1, L2, . . .}|.
Proof. Let S be as in Definition 5. Furthermore, let Ga = La if a < |{L0, L1,
L2, . . .}| and Ga = N otherwise.
We define a sequence of hypothesis spaces H0,H1,H2, . . ., where for each
n ∈ N the space Hn = {Hn0 ,Hn1 ,Hn2 , . . .} is defined as follows:
Hn〈i,j〉 =
Gi, if j /∈ S and j > n;Gi ∪ {t+ 1, t+ 2, t+ 3, . . .}, if j ∈ St+1 − St and j > n;N, j ≤ n.
H0,H1,H2, . . . is a recursive enumeration of indexed families. Since {L0, L1,
L2, . . .} is uniformly conservatively learnable, there exists a recursive enumera-
tion of learnersM0,M1,M2, . . . such that for all n,Mn conservatively learns {L0,
L1, L2, . . .} with respect to Hn.
For all a < |{L0, L1, L2, . . .}| and n ∈ N, let e = 〈v(a, n), w(a, n)〉 be the
first number found (in some dovetailing search) such that Mn outputs e on the
canonical text Ta of La and one of the following conditions hold:
(a) w(a, n) ∈ S and La ⊆ Hn〈v(a,n),w(a,n)〉 (note that this can be verified by
finding a t with w(a, n) ∈ St and checking La(x) ≤ Hn〈v(a,n),w(a,n)〉(x) for all
x ≤ t);
(b) v(a, n) = a;
(c) w(a, n) ≤ n.
First note that for every a < |{L0, L1, L2, . . .}| there is an n such that either
w(a, n) ≤ n or w(a, n) ∈ S: Otherwise the set {w(a, n) : n ∈ N} would be
an infinite r.e. set disjoint to S which does not exist as S is simple. Hence
there is a recursive function u which searches for this n; that is, w(a, u(a)) ≤
u(a) ∨ w(a, u(a)) ∈ S for all a < |{L0, L1, L2, . . .}|.
It is easy to see that there is a further recursive function r such that either
r(a) = 0 ∧ w(a, u(a)) ≤ u(a) or r(a) > 0 ∧ w(a, u(a)) ∈ Sr(a). Note that La ⊆
H
u(a)
〈v(a,u(a)),w(a,u(a))〉 and {r(a), r(a)+1, r(a)+2, . . .} ⊆ Hu(a)〈v(a,u(a)),w(a,u(a))〉. Since
eachMn is conservative, it follows that La = H
u(a)
〈v(a,u(a)),w(a,u(a))〉 and thus N−La
contains only elements below r(a) for all a < |{L0, L1, L2, . . .}|. 
For prescribed conservative learning, we have a less stringent necessary condition
as compared to uniform conservative learning.
Theorem 9. If {L0, L1, L2, . . .} is prescribed conservatively learnable then al-
most every set in {L0, L1, L2, . . .} is cofinite. Moreover, there is a recursive func-
tion r bounding the non-elements of the cofinite La.
Prescribed Learning of Indexed Families 191
Proof. Let S and In be as in Definition 5. Define a hypothesis space {H0,H1,
H2, . . .} as follows. For m ∈ N, define Hm as follows: suppose n is such that
m ∈ In; then let
Hm =
{
Ln, if m /∈ S;
Ln ∪ {m+ t,m+ t+ 1, . . .}, if m ∈ St+1 − St for some t ∈ N.
{H0,H1,H2, . . .} is an indexed family which is a superclass of {L0, L1, L2, . . .}.
LetM be a learner for {L0, L1, L2, . . .} with respect to {H0,H1,H2, . . .}. Letmn
be the first index output byM on the canonical text Tn for Ln such thatmn ∈ In
(mn may not always be defined). Then the set {mn : n ∈ N and mn exists} is
recursively enumerable. Let e be an index for this set. If there are infinitely
many coinfinite sets in {L0, L1, L2, . . .}, then there is an a > e such that La is
coinfinite. Since La is coinfinite,ma exists (as only indices in In can be indices for
coinfinite Ln). Furthermore, ma ∈ S because ma is the only element in We ∩ Ia.
But this implies Hma ⊃ La. Hence M cannot be conservative. The function r
can be found by techniques similar to those in the proof of Theorem 8. 
The above is not a characterization as the class of all cofinite sets is not learnable
in the limit. The next example shows that there is also a learnable class of
cofinite sets which is not prescribed conservatively learnable; this class is still
class-comprising conservative learnable.
Example 10. Let {L0, L1, L2, . . .} contain all sets of the form N− {a} and all
sets of the form N − {a, b} where a < b, a ∈ K − Kb. Then {L0, L1, L2, . . .}
is class-preservingly conservatively learnable but not prescribed conservatively
learnable.
The following theorem shows that class-preserving-uniformly conservative learn-
ability and prescribed conservative learnability are not comparable.
Theorem 11. (a) There exists {L0, L1, L2, . . .} which is class-preserving-uni-
formly conservatively learnable, but not prescribed conservatively learnable.
(b) There exists {L0, L1, L2, . . .} which is prescribed conservatively learnable but
not class-preserving-uniformly conservatively learnable.
4 Non U-Shaped Learning
Every conservative learner is clearly non U-shaped. Furthermore, one can modify
a conservative learner to be decisive by only changing to a new hypothesis if it
is consistent with the input.
The following theorem thus shows that non U-shaped learning is equivalent to
conservative learning in the case of exact, class-preserving and class-preserving-
uniform learning.
Theorem 12. Assume that {L0, L1, L2, . . .} is class-preserving-uniformly non
U-shaped learnable. Then {L0, L1, L2, . . .} is already class-preserving-uniformly
conservatively learnable by the same learner. The same applies for exact and
class-preserving learning.
192 Sanjay Jain, Frank Stephan, and Nan Ye
Proof. We show that if M non U-shaped learns {L0, L1, L2, . . .} with re-
spect to a class-preserving hypothesis space {H0,H1,H2, . . .}, then M con-
servatively learns {L0, L1, L2, . . .} with respect to {H0,H1,H2, . . .}. Assume
M is not conservative, then there exist τ, σ such that HM(τ) 6= HM(τσ), but
content(τσ) ⊆ HM(τ). Since {H0,H1,H2, . . .} is class-preserving, there exists an
n such that Ln = HM(τ). Let T be a text for Ln, then τσT is a text for Ln.
However, M is not non U-shaped on τσT , as it first outputs a correct hypothe-
sis HM(τ) = Ln and then abandons it. The same argument applies for an exact
hypothesis space. 
However, for class-comprising learning, non U-shaped learning is more powerful
than conservative learning, as shown by the following theorem.
Theorem 13. Assume that {L0, L1, L2, . . .} contains all sets {x, x+1, x+2, . . .}
and all finite sets D such that there is an s with min(D) ∈ Ks+1 − Ks and
0 < |D| < s. Then {L0, L1, L2, . . .} has a non U-shaped class-comprising learner
but not a conservative class-comprising learner.
The following theorem gives a sufficient condition for uniform non U-shaped
learnability. Furthermore, this condition helps us to separate uniform non U-
shaped learnability from prescribed conservative learnability.
Theorem 14. If the class {L0, L1, L2, . . .} is exactly finitely learnable then {L0,
L1, L2, . . .} is uniformly non U-shaped learnable. In particular, there are classes
which are uniformly non U-shaped learnable but not prescribed conservatively
learnable.
Proof. Let M be an exact finite learner for {L0, L1, L2, . . .}, we define a recur-
sive enumeration of non U-shaped learnersM0,M1,M2, . . . which uniformly learn
{L0, L1, L2, . . .}. For each i ∈ N, Mi(T [t]) is defined as follows: if M(T [t]) = ?,
then output ?; if M(T [t]) = e, then for each j ≤ t, define rj = min{x : x > t or
Le(x) 6= ϕi(j, x)}. Output the minimal j which maximizes rj . It can be easily
verified that the above learners witness that {L0, L1, L2, . . .} is uniformly non
U-shaped learnable.
Take any exactly finitely learnable language collection with infinitely many
coinfinite languages, then it is uniformly non U-shaped learnable but not pre-
scribed conservatively learnable. An example is the language collection {L0, L1,
L2, . . .} where Ln = {0, 2, 4, 6, . . .} ∪ {2n+ 1}. 
With the following result we can see that non U-shaped learning and decisive
learning are equivalent for prescribed learning and uniform learning.
Theorem 15. If {L0, L1, L2, . . .} is prescribed non U-shaped learnable then {L0,
L1, L2, . . .} is also prescribed decisively learnable. If {L0, L1, L2, . . .} is uniformly
non U-shaped learnable then {L0, L1, L2, . . .} is also uniformly decisively learn-
able.
Prescribed Learning of Indexed Families 193
Proof. It suffices to show that if M non U-shaped learns {L0, L1, L2, . . .} with
respect to a given hypothesis space {H0,H1,H2, . . .}, then we can effectively
build another learner M ′ which decisively learns {L0, L1, L2, . . .} with respect
to {H0,H1,H2, . . .}. The desired M ′ can be defined as follows. Given a text T ,
let M ′(T [0]) =M(T [0]). For t > 0, M ′(T [t]) =M(T [t]) if for all t′ < t, for some
x ≤ t, HM(T [t])(x) 6= HM ′(T [t′])(x); and M ′(T [t]) = M ′(T [t − 1]) otherwise. It
can be easily verified that M ′ is decisively learning {L0, L1, L2, . . .} using {H0,
H1,H2, . . .}. 
As in the case of conservative learning, class-preserving-uniform non U-shaped
learnability and prescribed non U-shaped learnability are not comparable as
well.
Theorem 16. (a) There exists an {L0, L1, L2, . . .} which is class-preserving-
uniformly non U-shaped learnable but not prescribed non U-shaped learnable.
(b) There exists an {L0, L1, L2, . . .} which is prescribed non U-shaped learnable
but not class-preserving-uniformly non U-shaped learnable.
5 Monotonic Learning
The prescribed and uniform versions of strong-monotonic and monotonic learn-
ing are very restrictive.
Theorem 17. (a) {L0, L1, L2, . . .} is prescribed strong-monotonically learnable
iff {L0, L1, L2, . . .} is finite.
(b) {L0, L1, L2, . . .} cannot be uniformly strong-monotonically learnable.
As in the case of uniform conservative learning, there also exists an infinite class
which is uniformly monotonically learnable.
Example 18. Let La = {a}. Then {L0, L1, L2, . . .} is an infinite class which is
uniformly monotonically learnable.
The following result shows that in fact it is necessary for a class to contain only
finite sets in order to be uniformly monotonically learnable.
Theorem 19. If {L0, L1, L2, . . .} is uniformly monotonically learnable, then
{L0, L1, L2, . . .} contains only finite sets.
For prescribed monotonic learning, finitely many sets in the language class can
violate the above necessary condition for uniform monotonic learning.
Theorem 20. If {L0, L1, L2, . . .} is prescribed monotonically learnable, then
{L0, L1, L2, . . .} contains only finitely many infinite sets.
Theorem 21. (a) There exists a class {L0, L1, L2, . . .} which is class-preserv-
ing-uniformly strong-monotonically learnable but not prescribed monotonically
learnable.
(b) There exists a class {L0, L1, L2, . . .} which is prescribed monotonically learn-
able but not class-preserving-uniformly monotonically learnable.
194 Sanjay Jain, Frank Stephan, and Nan Ye
(c) Every prescribed strong-monotonically learnable class is also class-preserving-
uniformly strong-monotonically learnable.
Acknowledgements: We thank the anonymous referees for helpful comments.
References
1. Dana Angluin. Inductive inference of formal languages from positive data. Infor-
mation and Control, 45:117–135, 1980.
2. Lenore Blum and Manuel Blum. Toward a mathematical theory of inductive in-
ference. Information and Control, 28:125–155, 1975.
3. Ganesh Baliga, John Case, Wolfgang Merkle, Frank Stephan and Rolf Wiehagen.
When unlearning helps. Information and Computation, to appear.
4. E. Mark Gold. Language identification in the limit. Information and Control,
10:447–474, 1967.
5. Klaus-Peter Jantke. Monotonic and Non-monotonic Inductive Inference. New
Generation Computing, 8:349–360, 1991.
6. Sanjay Jain, Frank Stephan and Nan Ye. Prescribed Learning of R.E. Classes. In
M. Hutter, R. Servedio and E. Takimoto, editors, Algorithmic Learning Theory,
18th International Conference, ALT’ 07, Sendai, Japan, October 2007, pages 64–
78. Lecture Notes in Artificial Intelligence 4754. Springer Verlag, 2007.
7. Steffen Lange. Algorithmic Learning of Recursive Languages. Habilitationsschrift,
der Fakulta¨t fu¨r Mathematik und Informatik der Universita¨t Leipzig eingereichte,
Mensch and Buch Verlag, Berlin, 2000.
8. Steffen Lange and Thomas Zeugmann. Language learning in dependence on the
space of hypotheses. Proceedings of the Sixth Annual Conference on Computational
Learning Theory, Santa Cruz, California, United States, pages 127–136, 1993.
9. Steffen Lange, Thomas Zeugmann and Shyam Kapur. Monotonic and dual mono-
tonic language learning. Theoretical Computer Science, 155:365–410, 1996.
10. Daniel Osherson, Michael Stob and Scott Weinstein. Systems That Learn, An
Introduction to Learning Theory for Cognitive and Computer Scientists. Bradford
— The MIT Press, Cambridge, Massachusetts, 1986.
11. Emil Post. Recursively enumerable sets of positive integers and their decision
problems, Bulletin of the American Mathematical Society, 50:284–316, 1944.
12. Thomas Zeugmann. Algorithmisches Lernen von Funktionen und Sprachen. Ha-
bilitationsschrift, Technische Hochschule Darmstadt, 1993.
13. Thomas Zeugmann and Steffen Lange. A guided tour across the boundaries of
learning recursive languages. Algorithmic Learning for Knowledge-Based Systems,
final report on research project Gosler, edited by Klaus P. Jantke and Steffen
Lange, Springer Lecture Notes in Artificial Intelligence 961:193–262, 1995.
14. Thomas Zeugmann, Steffen Lange and Shyam Kapur. Characterizations of mono-
tonic and dual monotonic language learning, Information and Computation,
120:155–173, 1995.
15. Sandra Zilles. Separation of uniform learning classes. Theoretical Computer Sci-
ence, 313:229–265, 2004.
16. Sandra Zilles. Increasing the power of uniform inductive learners. Journal of
Computer and System Sciences, 70:510–538, 2005.
Lower Bounds for Syntactically Multilinear
Algebraic Branching Programs
Maurice J. Jansen
Centre for Theory in Natural Science
Department of Computer Science, University of Aarhus
IT-Parken, Aabogade 34, DK-8200 Aarhus N, Denmark.
mjjansen@daimi.au.dk
Abstract. It is shown that any weakly-skew circuit can be converted
into a skew circuit with constant factor overhead, while preserving either
syntactic or semantic multilinearity. This leads to considering syntacti-
cally multilinear algebraic branching programs (ABPs), which are de-
fined by a natural read-once property. A 2n/4 size lower bound is proven
for ordered syntactically multilinear ABPs computing an explicitly con-
structed multilinear polynomial in 2n variables. Without the ordering
restriction a lower bound of level Ω(n3/2/ logn) is observed, by consid-
ering a generalization of a hypercube covering problem by Galvin [1].
Key words: Computational complexity, arithmetical circuits, lower
bounds, multilinear polynomials, algebraic branching programs.
1 Introduction
It is not known whether polynomial size arithmetical circuits (VP) are com-
putationally more powerful than polynomial size arithmetical formulas (VPe).
For the former, we have a surprising construction by Valiant, Skyum, Berkowitz
and Rackoff, which shows that VP = VNC2 [2]. For the latter, we know by a
result of Brent that VPe = VNC1 [3]. Recently, Raz made a breakthrough by
showing that polynomial size multilinear circuits are strictly more powerful than
polynomial size multilinear formulas. Raz proved that
Theorem 1 ([4]). s-mlin-VNC1 6= s-mlin-VNC2.
Here “s-mlin-” denotes syntactic multilinearity. Technically, multilinearity
comes in two flavors: syntactic and semantic (See Section 2). For formulas these
two notions are equivalent, but this is not known to be true for circuits.
Intermediate between circuits and formulas we have so-called skew and
weakly-skew circuits (See [5]). Letting VPs and VPws stand for the classes of p-
families of polynomials that have skew circuits or weakly-skew circuits of polyno-
mial size, respectively, Malod an Portier prove that VPs = VPws. The situation
can be summarized as follows:
Theorem 2 ([5, 2]). VNC1 ⊆ VPs = VPws ⊆ VNC2 = VP.
196 Maurice J. Jansen
A priori it is not clear whether the equality VPs = VPws holds up when
passing to the multilinear variants of these classes, as the proofs in [5] appeal to
the completeness of the determinant or trace of iterated matrix multiplication
for the class VPws. For the determinant, currently no polynomial size multilinear
circuits are known. Furthermore, multilinearity is not necessarily preserved under
Valiant projections.
1.1 Results
It will be demonstrated one can convert any weakly-skew circuit into a skew-
circuit with constant factor overhead, while maintaining either syntactic or se-
mantic multilinearity. Further, it will be observed that the conversion without
multilinearity restrictions can be done with constant factor overhead as well1.
Note, that the conversions indicated in [5] suffers an at least cubic blow-up in
size, as an appeal is made to either polynomial size skew circuits for the deter-
minant [6], or for the trace of iterated matrix multiplication. As a consequence
one obtains that
Theorem 3.
s-mlin-VNC1 ⊆ s-mlin-VPs = s-mlin-VPws ⊆ s-mlin-VNC2 = s-mlin-VP.
In the above, the rightmost equality was proven in [7]. Looking at Theorem
3, the question is raised whether perhaps the techniques used to prove Theorem
1 can be strengthened to show that s-mlin-VPs 6= s-mlin-VNC2, or that we can
prove s-mlin-VNC1 6= s-mlin-VPs, by showing, in the terminology of [7, 8], some
full rank polynomial has polynomial size skew circuits.
As a skew circuit can be transformed into an algebraic branching program
(ABP, See [9]) with relatively little overhead, we turn to algebraic branching
programs to investigate the above questions. If the initial skew circuit is syntac-
tically multilinear, this results in an ABP B which is syntactically multilinear
in the following natural sense: on any directed path in B, any variable can ap-
pear at most once. This can be thought of as the algebraic analog of a Boolean
read-once branching program. In the latter model we know of tight exponential
lower bounds [10]. Also exponential lower bounds are known for ABPs in the
non-commutative case [9]. Bryant introduced so-called ordered binary decision
diagrams (OBDDs), for which he proved exponential lower bounds [11]. These
are read-once Boolean branching programs in which variables are restricted to
appear in the same order on all directed paths. This restriction can naturally
also be considered for ABPs, leading to the following result:
Theorem 4. Let X be a set of 2n variables. For any field F , there exists an ex-
tension field G of F and explicitly constructed multilinear polynomial f ∈ G[X],
such that any ordered algebraic branching program over X and G computing f
has at least 2n/4 nodes.
1 This observation has simultaneously been made by Kaltofen and Koiran in an un-
published paper, which was unknown to the author at the time of this research.
They do not consider the preservation of multilinearity.
Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 197
Finally, the problem of proving lower bounds for unrestricted ABPs and
(unordered) multilinear ABPs is explored. For any fixed constant 0 < α < 1,
it will be shown that any unrestricted ABP requires size Ω(n3/2α
√
1− α) to
compute the elementary symmetric polynomial of degree bαnc in n variables.
Next a relation between proving lower bounds for multilinear ABPs and the
generalization of a hypercube covering problem by Galvin will be established
[1]. By straightforward counting this yields a lower bound for multilinear ABPs
of level Ω( n
3/2
logn ), for computing any full rank polynomial. Potentially however,
this technique yields up to quadratic lower bounds, provided linear lower bounds
can be proven for certain generalizations of Galvin’s Problem.
2 Preliminaries
For non-negative integer n, [n] denotes the set {1, 2, . . . , n}. Let F be a field and
X = {x1, x2, . . . , xn} be a set of variables. An arithmetical circuit Φ over F and
X is a directed acyclic graph with nodes of in-degree zero or two. Nodes with in-
degree zero are called inputs and are labeled by variables or field elements. Nodes
with in-degree two are called gates and have labels ∈ {×,+}. For each gate g
in Φ, one has associated the polynomial computed by g, denoted by Φg, which
is defined in the obvious manner. Also let Φg stand for the subcircuit rooted at
gate g. It will be made clear from the context which meaning is intended. Denote
by Xg the set of variables used by the subcircuit Φg. The size of Φ, denoted by
|Φ|, is taken to be the number gates. If the underlying graph of Φ is a tree, Φ is
called a formula. For a polynomial f , C(f) and L(f) denote the smallest size of
a circuit or formula, respectively, computing f .
An arithmetical circuit Φ is called weakly-skew if at every multiplication gate
g with inputs g1 and g2, one of Φg1 and Φg2 is disjoint from the rest of Φ. Φ is
called skew if for each multiplication gate at least one g1 and g2 is an input gate
(See [5]). For a polynomial f , Cws(f) and Cs(f) denote the smallest size of a
weakly-skew or skew circuit computing f , respectively.
A polynomial f is called multilinear if for any monomial of f , every vari-
able has degree at most one. A circuit Φ is semantically multilinear if every
polynomial computed at any gate of Φ is multilinear. Φ is called syntactically
multilinear if for each multiplication gate g with inputs g1 and g2, Xg1 and Xg2
are disjoint. For a polynomial f , Csyn(f) and Csem(f) denote syntactic and
semantic multilinear circuit size, respectively. Similarly, Lsyn(f) and Lsem(f)
denote syntactic and semantic multilinear formula size. These definitions will be
combined in the obvious manner. For example, Csyn,ws(f) denotes the smallest
size of a syntactically multilinear weakly-skew circuit computing f . Standard
notation for arithmetical circuit classes will be followed (See e.g. [12]). For any
class C ∈ {VNC1,VNC2,VPs,VPws,VP}, let syn-mlin-C and mlin-C stand for
the class of p-families of polynomials that have C-circuits which additionally are
required to be syntactically or semantically multilinear, respectively.
Definition 1 (See [9]). An algebraic branching program (ABP) over a field F
and a set of variables X is a 4-tuple B = (G,w, s, t), where G = (V,E) is a
198 Maurice J. Jansen
directed acyclic graph for which V can be partitioned into levels L0, L1, . . . , Ld,
where L0 = {s} and Ld = {t}. Vertices s and t are called the source and sink of
B, respectively. Edges may only go between consecutive levels Li and Li+1. The
weight function w : E → F [X] assigns homogeneous linear forms to the edges of
G. For a path p in G, we extend the weight function by w(p) =
∏
e∈p w(e). For
i, j ∈ V , let Pi,j be the collection of all path in G from i to j. The program B
computes the polynomial
∑
p∈Ps,t w(p). The size of B is taken to be |V |.
For a linear form L(x) =
∑n
i=1 cixi, define coef[L, xi] = ci. An ABP B is called
syntactically multilinear if for any directed path p in B, for all i, there is at most
one edge e on p with coef[w(e), xi] 6= 0. B(f) denotes the smallest size of an ABP
computing f , and Bsyn(f) denotes such size with the additional restriction of
syntactic multilinearity.
3 Circuit Transformations
It is convenient to work with the following data structure: a skew schedule is a
directed acyclic graph G with weights on the edges ∈ F ∪ X, where the out-
degree of a vertex is either zero, one or two, and where for any vertex v with
distinct edges e1 = (v, w) and e2 = (v, u), the labels of e1 and e2 equal 1. For a
directed acyclic graph G with node s ∈ V [G], a path p in G is called a maximal
path with starting point s, if the first vertex of p is s and the last vertex of p has
no outgoing edges.
Lemma 1. For any polynomial f , Csyn,s(f) ≤ 5Csyn,ws(f).
Proof. First process Φ so that any addition gate has its input coming in from
different gates by inserting dummy addition gates. This at most doubles the size.
Let e′ be the new size. Let g1, g2, . . . , ge′ be a topological sort of the gates of Φ,
where wlog. we assume Φge′ = f , and that ge′ is the only gate with out-degree
zero. Let g−m+1, g−m+2, . . . , g0 be the set of inputs of Φ. Sequentially for stages
k = 1, 2, . . . , e′, we construct a skew schedule Gk from Gk−1. To initialize, let G0
consists of m distinct directed edges. For each input g of Φ, we select a unique
edge among E[G0] and put the label of g on it. Let B be the set of vertices in G0
with out-degree zero. The set B will remain as a subset of vertices in each Gk.
We will never change the in-degree of vertices in B. At the beginning of stage
k, the skew schedule Gk−1 will satisfy:
1. Each node g = gk′ with k′ < k will correspond one-to-one with a vertex
vg ∈ V [Gk−1]\B.
2. Let Gk−1 be the set of nodes gk′ with k′ < k, that are used by gates gk′′
for some k′′ ≥ k. For any g ∈ Gk−1,
∑
p∈P
∏
e∈p w(e) = Φg, where P is the
collection of all maximal paths with starting point vg in Gk−1,
3. On any directed path in Gk−1 no variable appears more than once,
4. For any node g = gk′ with k′ < k, the set of nodes not in B reachable in
Gk−1 from vg, is precisely {vg′ : g′ ∈ Φg}.
Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 199
At the beginning of stage k = 1, we have that G0 is the set of all input gates.
For each input gate g, vg is defined to be the starting vertex of the unique edge
we have selected for it. Properties (1)-(4) can now be verified to hold. At stage
k we do the following:
Case I: gk = +(gi, gj). We have that gi, gj ∈ Gk−1. We construct Gk from
Gk−1 by adding one new vertex w with edges of weight 1 from w to vgi and vgj .
No parallel edges are created since i 6= j. Let us verify the needed properties.
Property (3) is clear. It is also clear that if we let P we the collection of all
maximal paths starting in w that
∑
p∈P
∏
e∈p w(e) = Φgk . If we are at the last
iteration, i.e. k = e′, then this is all we are required to verify. Otherwise, gk
will be used later on, i.e. gk ∈ Gk. Observe that Gk = Gk−1 ∪ {gk} − S, where
S ⊆ {gi, gj}. We define vgk = w. By our above observation for the vertex w, and
the fact that we do not modify connectivity for the other vertices, Property (2)
holds. Property (4) is clear.
Case II: gk = ×(gi, gj). Wlog. assume Φgj is disjoint from the rest of Φ.
We have that gi, gj ∈ Gk−1. For s ∈ {i, j}, let Ws be the set of vertices in Gk−1
reachable from vgs . Wi and Wj are disjoint. Namely, suppose w ∈ Wi ∩Wj . If
w /∈ B then Property (4) implies there exists a shared node in Φgi and Φgj ,
which is a contradiction. In case w ∈ B, then since we do not add edges into w,
we have a vertex w′ = vg′ for some input gate g′ with w′ ∈ Wi ∩Wj . Hence we
again have a contradiction.
Let E ⊆ Wj be the set of vertices reachable from vgj with out-degree zero.
We define Gk to be the graph Gk−1 modified by adding an edge (v, vgi) of weight
1 for each v ∈ E. We add2 a new vertex w and edge (w, vgj ) with weight 1, and
let vgk = w. SinceWi∩Wj = ∅, no vertex from E is reachable from vgi . Hence Gk
is an acyclic graph. Observe Gk is a skew schedule. We will now verify Properties
(1)-(4).
Let P be the set of maximal paths starting in vgk in Gk. Let Ps be the set of
maximal paths in Gk−1 starting in vgs , for s ∈ {i, j}. For p ∈ Pi and q ∈ Pj , let
q#p denote the path in Gk that is (vgk , vgj ), followed by q, followed by the edge
with weight 1 into vgi , followed by p. We have that P = {q#p : q ∈ Pj , p ∈ Pi}.
This means
∑
r∈P
∏
e∈r w(e) =
∑
p∈Pi
∏
e∈p w(e) ·
∑
q∈Pj
∏
e∈q w(e) = Φgi ·
Φgj = Φgk . In case this was the last iteration, i.e. k = e
′, this is all we need
together with Property (3) to be verified below. Otherwise, since gk will be used
later again, gk ∈ Gk. Observe that Gk = Gk−1 − S ∪ {gk}, where S is the set of
nodes in Φgj .
By what we observed above, Property (2) holds for gk. For g 6= gk in Gk, the
only way Property (2) can be disturbed is if some vertex w ∈ E is reachable
from vg in Gk−1. This means some w′ /∈ B is reachable from both vg and vgj in
Gk−1, but then Φg and Φgj share a vertex by ’previous’ Property (4). Since Φ is
weakly-skew, g must be a node in Φgj , but that is a contradiction since g ∈ Gk.
To check Property (3), we note that the only edges with variables are of the
form (v, b) with b ∈ B and v = vg, for some input g ∈ Φ. Property (3) can be
violated only, if in Gk−1 we have for such (v, b) that some vertex in E can be
2 This is not strictly necessary, but we do so to simplify the proof.
200 Maurice J. Jansen
reached from b, and that in Gk−1 there exists a path starting in vgi going over an
edge (v′, b′) with b′ ∈ B and v′ = vg′ , for some input gate g′, that has the same
variable label as (v, b). This means that g′ ∈ Φgi . Similar as above, by Property
(4), it must be that g ∈ Φvgj . By syntactic multilinearity, we conclude the labels
of (v, b) and (v′, b′) must be different. Property (4) clearly holds.
This completes the description the graphs G1, G2, . . . , Ge′ . Ge′ is a skew
schedule of size at most 2m + e′ ≤ 5e′. We can evaluate it node by node in a
bottom-up fashion. This yields a syntactically multilinear skew circuit computing
f of size at most 5e′ ≤ 10e gates. To optimize the constant to be 5 instead of
10, we observe adding dummy addition gates is not required. uunionsq
Lemma 2. For any polynomial f , Csem,s(f) ≤ 5Csem,ws(f).
Proof. Modify the multiplication case in the proof of Lemma 1 as follows. For
each variable xi appearing in the polynomial Φgi , replace any edge weight xi
in the induced subgraph Gk−1[Wj ] by zero. This does not alter the polynomial
represented at vertex vgj , since it cannot contain variable xi. Polynomials repre-
sented at other vertices in Gk[Wj ] can have changed, but cannot be used at later
stages. The substitution has forced all these polynomials to be multilinear. uunionsq
Removing Property (3) in the proof of Lemma 1 immediately yields a proof
that for any polynomial f , Cs(f) ≤ 5Cws(f). We put together the basic facts
about the measures that are being considered.
Lemma 3. For a homogeneous polynomial of degree d,
1. C(f) ≤ Cws(f) ≤ L(f) and Csyn(f) ≤ Csyn,ws(f) ≤ Lsyn(f).
2. Cws(f) ≤ Cs(f) ≤ 5Cws(f) and Csyn,ws(f) ≤ Csyn,s(f) ≤ 5Csyn,ws(f).
3. B(f) ≤ d · (4Cs(f) + 2) and Bsyn(f) ≤ d · (4Csyn,s(f) + 2).
4. α
√
Cs(f)/n ≤ B(f) and α
√
Csyn,s(f)/n ≤ Bsyn(f), for some constant
α > 0.
4 Ordered ABPs
Let B = (G,w, s, t) be an ABP over a field F and set of variables X =
{x1, x2, . . . , xn}. Say a directed path p from s to t respects a permutation
pi : [n] → [n], if whenever an edge e1 appears before an edge e2 on p and
coef[w(e1), xpi(i)] 6= 0 and coef[w(e2), xpi(j)] 6= 0, one has that i < j. B is called
ordered, if there exists a permutation pi that is respected by all directed s, t-paths.
For a polynomial f , we denote ordered ABP size by Bord(f). We first observe
that lower bounds for non-commutative ABPs of Nisan [9] transfer to the or-
dered model. This follows by evaluating the ordered ABP over non-commutative
variables. For example, one obtains that
Theorem 5. Any ordered algebraic branching program B = (G,w, s, t) comput-
ing the permanent or determinant of an n × n matrix of variables has size at
least 2n.
Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 201
The above bound for the permanent and determinant is of level 2Ω(
√
N),
where N is the number of variables. In [9] a bound of 2Ω(N) is proven for the
non-commutative model, but this is for a polynomial that is not multilinear. In
order to obtain a bound of level 2Ω(N), we now turn to the aforementioned full
rank polynomials [4, 7, 8].
Let X = {x1, x2, . . . , x2n}, Y = {y1, y2, . . . , yn} and Z = {z1, z2, . . . , zn} be
sets of variables. Following [4, 7, 8], for a multilinear polynomial g ∈ F [Y, Z],
we define the 2n × 2n partial derivatives matrix Mg, by taking Mg(m1,m2) =
coefficient of monomial m1m2 in g, where m1 and m2 range over all multilinear
monic monomials in Y and Z variables, respectively. A partition of X into Y and
Z is any bijection A : X → Y ∪ Z. For f ∈ F [X] denote by fA the polynomial
obtained from f by substitution of xi by A(xi), for all i ∈ [2n]. f is of full rank
if for every partition A, rank MfA = 2n. For multilinear g ∈ F [Y, Z], let Yg and
Zg be Y and Z variables appearing in g.
Proposition 1 ([8]). Let g, g1, g2 ∈ F [Y, Z] be multilinear polynomials. Then
1. rank Mg ≤ 2min(|Yg|,|Zg|),
2. rank Mg1+g2 ≤ rank Mg1 + rank Mg2 , and
3. rank Mg1·g2 = rank Mg1 ·rank Mg2 , provided Yg1∩Yg2 = ∅ and Zg1∩Zg2 = ∅.
We obtain the following lower bound:
Theorem 6. Let X be a set of 2n variables, and let F be a field. For any full
rank homogeneous polynomial f of degree n over X and F , Bord(f) ≥ 2n/4.
A proof of above theorem will appear in the full version of the paper.
To indicate the idea, suppose that B respects the permutation pi : [2n] →
[2n]. In this case consider any partition A that assign all n y-variables to
{xpi(1), xpi(2), . . . , xpi(n)} and all n z-variables to {xpi(n+1), xpi(n+2), . . . , xpi(2n)}.
Applying this partition to B, we obtain an ABP computing fA. Partition A has
been selected to frustrate “progress” of the rank measure rank MgA , for gates
g of B. Regardless of that, the final output fA must still have rank MfA = 2n.
Using Proposition 1, it is then possible to argue this implies the middle level
Ln/2 must of size at least 2n/4.
We need to modify the construction of a full rank polynomial from [7] to
yield a homogeneous full rank polynomial. This can be done, provided we work
over a suitable extension field of the underlying field F .
Let W = {ωi,l,j}i,l,j∈[2n] be sets of variables. For each interval [i, j] ⊆ 2n
of even length, we define a polynomial fi,j ∈ F [X,W] inductively as follows: if
|[i, j]| = 0, then define fi,j = 1. If |[i, j]| > 0, define fi,j = (xi + xj)fi+1,j−1 +∑
l ωi,l,jfi,lfl+1,j , where we sum over all l such that |[i, l]| is even. Finally, f is
defined to be f1,2n. It follows inductively that fi,j is multilinear and homogeneous
of degree |[i, j]|/2 in the X-variables. It can be verified that f is indeed a full
rank polynomial when considered as a polynomial in G[X], where G = F (W) is
the field of rational functions over the field F and variables W. Theorem 4 then
follows from Theorem 6. More details will appear in the full version.
202 Maurice J. Jansen
5 Unrestricted and Multilinear ABPs
Consider the following observation by Kristoffer Hansen: if for an ABP B the
number of edges between any two consecutive levels Ld and Ld+1 is at most
K < n, then the polynomial f(x1, x2, . . . , xn) computed by B vanishes on a
linear space of dimension at least n − K. Provided this is a contradiction for
f , one concludes that max(|Ld|, |Ld+1|) ≥
√
K. For example, for the elementary
symmetric polynomial of degree d in n variables defined by
∑
S⊂[n],|S|=d
∏
i∈S xi,
using Theorem 1.1 in [13], this yields the following theorem:
Theorem 7. Let α be a constant with 0 < α < 1, and assume αn is integer.
Over fields of characteristic zero, for the elementary symmetric polynomial Sαnn
of degree αn is n variables, it holds that B(Sαnn ) = Ω(n
3/2α
√
1− α).
Above argument is limited to yield O(n3/2) lower bounds, for polynomials of
degree Θ(n). In order to overcome this, we turn to a hypercube covering problem.
Consider H2n = {−1, 1}2n and B2n = {0, 1}2n over the real numbers. Define
inner product x · y = ∑2ni=1 xiyi. Let 1 denote the vector 12n. Let He2n = {x ∈
H2n : x · 1 = 0}. Of interest are minimal size coverings of He2n by hyperplanes,
where coefficients for the defining equations are taken from particular subsets of
B2n. More precisely, for W ⊂ [2n], define BW2n = {b ∈ B2n : wt(b) ∈ W}, where
wt(b) = b · 1. Define
Q(n,W, d) = min{|E| : E ⊆ BW2n, (∀x ∈ He2n), (∃e ∈ E), |x · e| ≤ d}.
Finding the value of Q(n,W, d) becomes interesting only for certain weight
sets W . For example, if 2n ∈ W , then E = {1} trivially covers all of He2n.
The discrepancy parameter d should also be small w.r.t. min(W ), e.g. trivially
Q(n,W,min(W )) = 1. Also, in case W does not contain an even number and
d = 0, we have that Q(n,W, d) is ill-defined. In all other cases Q(n,W, d) is well-
defined. Namely, say 2` ∈W . By taking E to be all 2n cyclic shifts of 12`02n−2`,
we have that for each x ∈ He2n, there must exist some e ∈ E, x · e = 0. Similarly,
this set E works, in case W contains no even numbers, but d ≥ 1. The crucial
question is whether for cases that avoid trivialities, one can do significantly better
than |E| being linear in n.
The special case of finding m(k) := Q(2k, {2k}, 0) is a problem posed origi-
nally by Galvin (See [14]). For an upper bound, note one requires only half of
all 4k cyclic shifts, i.e. m(k) ≤ 2k. For odd k, Frankl and Ro¨dl proved the linear
lower bound m(k) > k, for fixed  > 0 [1]. The proof relies on a strong result in
extremal set theory they proved, which resolved a $250 problem of Erdo˝s. Later
the bound was improved to m(k) ≥ 2k, for odd k [15].
Consider Q(n, [0n, (1 + 1)n], 2 log n), for fixed 0 < 0 < 1 < 1. From
Theorem 8 below it will follow, that proving an Ω˜(n) lower bounds on this
quantity would yield an Ω˜(n2) multilinear ABP lower bound. In light of the
result by Frankl and Ro¨dl such a linear lower bound appears plausible. Also
note the linear lower bound by Alon et al. for covering the entire hypercube, in
Lower Bounds for Syntactically Multilinear Algebraic Branching Programs 203
case the defining equations have coefficients in {−1, 1} instead of in {0, 1} [14].
They define for n ≡ d (mod 2), K(n, d) = min{|E| : E ⊆ Hn, (∀x ∈ Hn), (∃e ∈
E), |x · e| ≤ d}, and prove K(n, d) = dn/(d+ 1)e.
Theorem 8. Let X be a set of 2n variables, and let F be a field. For any
full rank homogeneous polynomial f of degree n over X and F , Bsyn(f) =
Ω
(∑n−1
r=1 min(n,Q(n, [r, n+ r], 2 log n))
)
.
Proof. Let B = (G,w, s, t) be a multilinear ABP computing f . Let
L0, L1, . . . , Ln be the levels of B. For v, w ∈ V [G], let fv,w denote the poly-
nomial computed by the subprogram of B with source v and sink w. Let Xv,w
denote the set of all variables appearing on directed paths from v to w. Consider
r such that 0 < r < n. Write f = fs,t =
∑
v∈Lr fs,vfv,t.
By syntactic multilinearity, we have that |Xs,v| ≥ r and |Xv,t| ≥ n− r. The
latter implies |Xs,v| ≤ n + r, again by syntactic multilinearity. Let χ(Xs,v) ∈
B
[r,n+r]
2n denote the characteristic vector of Xs,v.
Suppose that |Lr| < Q(n, [r, n+ r], 2 log n). Then there exists γ ∈ He2n such
that for every b ∈ {χ(Xs,v) : v ∈ Lr}, |γ · b| > 2 log n. Let A : X → Y ∪Z be any
partition that assigns a Y variable to xi, if γi = 1, and a Z variable otherwise,
for all i ∈ [2n].
Let B′ be the branching program obtained from B by substituting according
to A. For nodes v and w, we let Yv,w and Zv,w denote the sets of y and z
variables, respectively, appearing on paths from v to w in B′. Then for any
v ∈ Lr, min(Ys,v, Zs,v) ≤ (Ys,v + Zs,v)/2 − log n ≤ |Xs,v|/2 − log n. Hence, by
Item 1 of Proposition 1, we have that rank MfAs,v ≤ 2|Xs,v|/2−logn. By syntactic
multilinearity, none of the variables appearing on paths from s to v can appear
on paths from v to t. So |Xv,t| ≤ 2n − |Xs,v|. By Item 1 of Proposition 1 we
get rank MfAv,t ≤ 2n−|Xs,v|/2. Using multiplicativity (Proposition 1, Item 3), we
conclude rank MfAs,vfAv,t ≤ 2n−logn, and thus using subadditivity (Proposition 1,
Item 2) and since f =
∑
v∈Lr fs,vfv,t that rank MfA ≤ |Lr|2n−logn. Since f is
of full rank, rank MfA = 2n. We conclude that |Lr| ≥ n. uunionsq
Counting yields that Q(n, [r, n + r], d) = Ω((d + 1)−1
√
r(n−r)
2n ). Applying
Theorem 8, and summing for r ∈ [0n, 1n], for fixed 0 < 0 < 1 < 1 yields
Theorem 9. Let X be a set of 2n variables. For any field F , there exists an ex-
tension field G of F and explicitly constructed multilinear polynomial f ∈ G[X],
such that any multilinear algebraic branching program over X and G computing
f has Ω(n
√
n
logn ) nodes.
6 Conclusions
It should be noted the ABP-model can be quite powerful. The prime example
being that, given two n × n matrices X and Y of variables, one can compute
f =
∑
i,j∈[n](XY )ij with a syntactically multilinear ABP with O(
√
N) many
204 Maurice J. Jansen
nodes, where N = 2n2 is the number of input variables. This is an example of
a polynomial, for which the multilinear ABP-model is at least “quadratically
more efficient” than the syntactic multilinear circuit model. In the latter model,
the currently best know lower bound for an explicit function is of level Ω( n
4/3
log2 n
)
[8]. Theorem 8 supplies a lower bound strategy, which yielded an Ω( n
3/2
logn ) lower
bound for multilinear ABPs. By resolving a certain generalization of Galvin’s
Problem this method can yield an Ω˜(n2) lower bound for syntactically multilin-
ear ABPs.
Acknowledgments I thank Peter Bro Miltersen, Kristoffer Hansen and Oded
Lachish for helpful discussions.
References
1. P. Frankl and V. Ro¨dl. Forbidden intersections. Trans. Amer. Math. Soc.,
300(1):259–286, 1987.
2. L. Valiant, S. Skyum, S. Berkowitz, and C. Rackoff. Fast parallel computation of
polynomials using few processors. SIAM J. Comput., 12:641–644, 1983.
3. R. Brent. The parallel evaluation of general arithmetic expressions. J. Assn. Comp.
Mach., 21:201–206, 1974.
4. R. Raz. Separation of multilinear circuit and formula size. Theory of Computing,
2(6):121–135, 2006.
5. Guillaume Malod and Natacha Portier. Characterizing Valiant’s algebraic com-
plexity classes. In MFCS, pages 704–716, 2006.
6. S. Berkowitz. On computing the determinant in small parallel time using a small
number of processors. Inf. Proc. Lett., 18:147–150, 1984.
7. R. Raz and A. Yehudayoff. Balancing syntactically multilinear arithmetical cir-
cuits. Journal of Computational Complexity (to appear).
8. R. Raz, A. Shpilka, and A. Yehudayoff. A lower bound for the size of syntacti-
cally multilinear arithmetic circuits. In Proc. 48th.Annual IEEE Symposium on
Foundations of Computer Science, pages 438–448, 2007.
9. N. Nisan. Lower bounds for non-commutative computation: extended abstract. In
Proc. 23rd Annual ACM Symposium on the Theory of Computing, pages 410–418,
1991.
10. A. Andreev, J. Baskov, E. Clementi, and R. Rolim. Small pseudo-random sets
yield hard functions: new tight lower bounds for branching programs. In Proc.
26th Annual International Conference on Automata, Languages, and Programming,
volume 1644 of Lect. Notes in Comp. Sci., pages 179–189, 1999.
11. R.E. Bryant. On the complexity of vlsi implementations and graph representa-
tions of boolean functions with application to integer multiplication. IEEE Trans.
Computers, 40(2):205–213, 1991.
12. P. Bu¨rgisser, M. Claussen, and M.A. Shokrollahi. Algebraic Complexity Theory.
Springer Verlag, 1997.
13. A. Shpilka. Affine projections of symmetric polynomials. In Proc. 16th Annual
IEEE Conference on Computational Complexity, pages 160–171, 2001.
14. N. Alon, E. Bergmann, D. Coppersmith, and A. Odlyzko. Balancing sets of vectors,
1988.
15. H. Enomoto, P. Frankl, N. Ito, and K. Nomura. Codes with given distances. Graphs
and Combinatorics, 3:25–38, 1987.
The Use of Information Affinity in Possibilistic
Decision Tree Learning and Evaluation
Ilyes Jenhani
1
, Salem Benferhat
2
, and Zied Elouedi
1
1
LARODEC, Institut Supérieur de Gestion de Tunis, Tunisia.
2
CRIL, Université d'Artois, Lens, France.
ilyes.j@lycos.com, benferhat@cril.univ-artois.fr, zied.elouedi@gmx.fr
Abstract. This paper investigates the issue of building decision trees
from data with imprecise class values where imprecision is encoded in
the form of possibility distributions. The Information Affinity similarity
measure is introduced into the well-known gain ratio criterion in order to
assess the homogeneity of a set of possibility distributions representing
instances's classes belonging to a given training partition. For the exper-
imental study, we proposed an information affinity based performance
criterion which we have used in order to show the performance of the
approach on well-known benchmarks.
1 Introduction
Machine learning and data mining researches have rapidly emerged in the last
decade. Especially, classification is considered as one of the most successful
branches of Artificial Intelligence and it is playing a more and more important
role in real-world applications.
Classification tasks are ensured by several approaches such as: discriminant
analysis, artificial neural networks, k-nearest neighbors, Bayesian networks, de-
cision trees. etc. The latter, namely, decision trees, is considered as one of the
most popular classification techniques. They are able to represent knowledge in
a flexible and easy form which justifies their use in decision support systems,
intrusion detection systems, medical diagnosis, etc.
For many real-world problems and particulary for classification problems,
imprecision is often inherent in modeling these applications and should be con-
sidered when building classifiers. For example, for some instances, an expert or
a sensor may be unable to give the exact class value: an expert in ballistics in
the scientific police who is unable to provide the exact type of a gun used in a
crime, a mechanic who is unable to provide the exact fault of an engine, a doctor
who cannot specify the exact disease of a patient, etc.
Hence, in these different examples, the expert can provide imprecise classi-
fications expressed in the form of a ranking on the possible classes. Obviously,
rejecting these pieces of information in a learning process is not a good practice.
A suitable theory dealing with such situations is possibility theory which is a
non-classical theory of uncertainty proposed by [12] and developed by [4].
206 Ilyes Jenhani et al.
In this paper, we propose a new decision tree approach that allows the induc-
tion of decision trees from imprecisely labeled instances, i.e., whose class labels
are given in the form of possibility distributions. We introduced the concept of
similarity into the attribute selection step of the proposed approach.
It is important to mention that existing possibilistic decision trees do not deal
with uncertainty in classes, except, the work we have proposed in [7] using the
concept of non-specificity in building possibilistic decision trees. A work proposed
by Borgelt and al. [2] deals with crisp (standard) training sets: the authors encode
the frequency distributions as possibility distributions (an interpretation which
is based on the context model of possibility theory [2]) in order to define a
possibilistic attribute selection measure. The possibilistic decision tree approach
proposed by Hüllermeier [5] uses a possibilistic branching within the lazy decision
tree technique. Again, this work does not deal with any uncertainty in the classes
of the training objects. A work by Ben Amor et al. [1] have dealt with the
classification of objects having possibilistic uncertain attribute values within the
decision tree technique.
This paper is organized as follows. Section 2 gives necessary background on
possibility theory. Section 3 describes some basics of decision trees. Then, in
Section 4, we present our approach, so-called Aff-PDT. Section 5 presents and
analyzes experimental results carried out on modified versions of commonly used
data sets from the U.C.I. repository [9]. Finally, Section 6 concludes the paper.
2 Possibility Theory
Possibility distribution
Given a universe of discourse Ω = {ω1, ω2, ..., ωn}, a fundamental concept of
possibility theory is the possibility distribution denoted by pi. pi corresponds to
a function which associates to each element ωi from the universe of discourse Ω
a value from a possibilistic scale [0,1]. This value is called a possibility degree: it
encodes our knowledge on the real world.
By convention, pi(ωi) = 1 means that it is fully possible that ωi is the real
world, pi(ωi) = 0 means that ωi cannot be the real world (is impossible). Flexi-
bility is modeled by allowing to give a possibility degree from ]0,1[. In possibility
theory, extreme cases of knowledge are given by:
-Complete knowledge: ∃ωi, pi(ωi) = 1 and ∀ ωj 6= ωi, pi(ωj) = 0.
- Total ignorance: ∀ ωi ∈ Ω, pi(ωi) = 1 (all values in Ω are possible).
Normalization
A possibility distribution pi is said to be normalized if there exists at least one
state ωi ∈ Ω which is totally possible. In the case of sub-normalized pi,
Inc(pi) = 1−max
ω∈Ω
{pi(ω)} (1)
is called the inconsistency degree of pi. It is clear that, for normalized pi,
maxω∈Ω{pi(ω)} = 1, hence Inc(pi)=0. The measure Inc is very useful in assessing
the degree of conflict between two distributions pi1 and pi2 which is given by
Inc(pi1 ∧ pi2). We take the ∧ as the minimum operator. Obviously, when pi1 ∧ pi2
Information Affinity in Possibilistic Decision Trees 207
gives a sub-normalized possibility distribution, it indicates that there is a conflict
between pi1 and pi2 (Inc(pi1 ∧ pi2) ∈]0, 1]).
Information Affinity: a possibilistic similarity measure
After a deep study of existing possibilistic similarity measures, we have proposed
in a recent work [6], a new similarity index satisfying interesting properties.
The information affinity index, denoted by InfoAff takes into account a
classical informative distance, namely, the Manhattan distance along with the
well known inconsistency measure. InfoAff is applicable to any pair of nor-
malized possibility distributions.
Definition 1 Let pi1 and pi2 be two possibility distributions on the same universe
of discourse Ω. We define a measure InfoAff(pi1, pi2) as follows:
InfoAff(pi1, pi2) = 1− d(pi1, pi2) + Inc(pi1 ∧ pi2)2 (2)
where d(pi1, pi2) = 1n
∑n
i=1 |pi1(ωi) − pi2(ωi)| represents the Manhattan distance
between pi1 and pi2 and Inc(pi1 ∧ pi2) tells us about the degree of conflict between
the two distributions (see Equation (1)).
Two possibility distributions pi1 and pi2 are said to have a strong affinity (resp.
weak affinity) if InfoAff(pi1, pi2) = 1 (resp. InfoAff(pi1, pi2) = 0).
For sake of simplicity, in the rest of the paper, a possibility distribution pi on
a finite set Ω = {ω1, ω2, ..., ωn} will be denoted by pi[pi(ω1), pi(ω2), ..., pi(ωn)].
3 Decision trees
Decision trees, also called classification trees, are graphical models with a tree-
like structure: they are composed of three basic elements: decision nodes cor-
responding to attributes, edges or branches which correspond to the different
possible attribute values. The third component consists of leaves including ob-
jects that typically belong to the same class or that are very similar.
Several algorithms for building decision trees have been developed, e.g., ID3
[10] and its successor C4.5 "the state-of-the-art" algorithm developed by Quin-
lan [11]. These algorithms have many components to be defined:
a) Attribute selection measure generally based on information theory,
serves as a criterion in choosing among a list of candidate attributes at each de-
cision node, the attribute that generates partitions where objects are distributed
less randomly, with the aim of constructing the smallest tree among those con-
sistent with the data. The well-known measure used in the C4.5 algorithm of
Quinlan [11] is the gain ratio. Given an attribute Ak, the information gain rela-
tive to Ak is defined as follows:
Gain(T,Ak) = E(T ) − EAk(T ) (3)
where
E(T ) = −
n∑
i=1
n(Ci, T )
|T | log2
n(Ci, T )
|T | (4)
208 Ilyes Jenhani et al.
and
EAk(T ) =
∑
v∈D(Ak)
|TAkv |
|T | E(T
Ak
v ) (5)
n(Ci,T ) denotes the number of objects in the training set T belonging to the
class Ci, D(Ak) denotes the finite domain of the attribute Ak and |TAkv | denotes
the cardinality of the set of objects for which the attribute Ak has the value
v. Note that
n(Ci,T )
|T | corresponds to the probability of the class Ci in T . Thus,
E(T ) corresponds to the Shannon entropy of the set T . The gain ratio is given
by:
Gr(T,Ak) =
Gain(T,Ak)
SplitInfo(T,Ak)
(6)
where SplitInfo(T,Ak) represents the potential information generated by
dividing T into n subsets. It is given by:
SplitInfo(T,Ak) = −
∑
v∈D(Ak)
|TAkv |
|T | log2
|TAkv |
|T | (7)
b) Partitioning strategy consisting in partitioning the training set accord-
ing to all possible attribute values (for symbolic attributes) which leads to the
generation of one partition for each possible value of the selected attribute. For
continuous attributes, a discretization step is needed.
c) Stopping criteria stopping the partitioning process. Generally, we stop
the partitioning if all the remaining objects belong to only one class, then the
node is declared as a leaf labeled with this class value. We, also, stop growing
the tree if there is no further attribute to test. In this case, we take the majority
class as the leaf's label.
4 Affinity based possibilistic decision trees
An affinity based possibilistic decision tree (Aff-PDT) has the same representa-
tion of a standard decision tree, i.e., it is composed of decision nodes for testing
attributes, branches specifying attribute values and leaves dealing with classes
of the training set.
4.1 Imperfection in classification problems
As models of the real world, databases, or more specifically, training sets are often
permeated with forms of imperfections, including imprecision and uncertainty.
The topic of imperfect databases is gaining more and more attention the last
years [8] since commercial database management systems are not able to deal
with such kind of information.
Examples of imperfect class values include the exact type of an attack in an
intrusion detection system, the exact cancer class of a patient in cancer diagnosis
Information Affinity in Possibilistic Decision Trees 209
applications, the exact location or type of a detected aerial engine in military
applications, etc. These imperfections might result from using unreliable infor-
mation sources, such as faulty reading instruments, or input forms that have
been filled out incorrectly (intentionally or inadvertently).
In order to deal with such kind of imperfection, in this work, we used a
convenient mathematical model, namely, possibility theory [4, 12]. More formally,
a possibility degree will be assigned to each possible class value indicating the
possibility that the instance belongs to a given class [3]. These possibility degrees
can be obtained from direct expert's elicitation, i.e., each expert is asked to
quantify by a real number between 0 and 1 the possibility that a training instance
belongs to each one of the different classes of the problem.
4.2 Building procedure
In the possibilistic setting, instances classes in the training set will be represented
by possibility distributions over the different classes of the problem instead of
exact classes. Hence, one must find a way to assess homogeneity of a given
training partition. The idea consists in measuring the entropy of each partition
weighted by the mean similarity degree of the possibility distributions in the
corresponding partition. Let us define the basic components for the Aff-PDT
approach:
a) Attribute selection measure
Given a training set T (the initial partition) containing n instances and given
the set of attributes, let us denote by pii the possibility distribution labeling the
class of the instance i in T .
In standard decision trees, homogeneity of a partition is determined by the
entropy of that partition. However, in our context, pii's are most of the time
very different, so it has no sense to directly compute their frequencies in order to
determine the entropy of T (the entropy will be equal to 1). Moreover, one cannot
simply view each pii as a new class. First, because the number of classes will be
exponential. Second, there are similar distributions that should be considered
as globally expressing same or similar pieces of information. For instance, we
cannot simply consider the distributions [1, 0.2] and [1, 0.21] as two different
exclusive classes, but we will consider them as similar.
Hence, we need a finite set of Meta-ClassesMCj=1..m. EachMCj corresponds
to a meta-class which gathers together all possibility distributions similar to a
predefined wrapper possibility distribution (say WDj). More precisely, wrapper
possibility distributions are binary possibility distributions (i.e., ∀ω ∈ Ω, pi(ω) ∈
{0, 1}) representing special cases of complete knowledge, partial ignorance and
total ignorance representing the set of reference distributions.
After specifying the set of Meta-Classes MC, we will assign to each possibil-
ity distribution pii labeling an instance i a meta-class MCj such that: MCj =
argmaxmj=1{InfoAff(pii,WDj)} where m is the total number of meta-classes
and InfoAff corresponds to the information Affinity index (Equation (2)). Note
that, as in standard decision trees, ties are broken arbitrarily.
210 Ilyes Jenhani et al.
After mapping the different pii's to their corresponding MCj 's , it becomes
possible to assess the discriminative power of each attribute in partitioning a set
into homogeneous subsets by extending the well-known gain ratio criterion [11].
First, we define the Affinity-Entropy Gain (AGain) of an attribute Ak by:
AGain(T,Ak) = AE(T ) − AEAk(T ) (8)
where
AE(T ) = −
m∑
j=1
(AvgAff(MCj)) ∗ ( |MCj ||T | log2
|MCj |
|T | ) (9)
and
AEAk(T ) =
∑
v∈D(Ak)
|TAkv |
|T | SE(T
Ak
v ) (10)
where |MCj | in Equation (9) denotes the number of objects in the training set T
belonging to the meta-class MCj . Obviously, to compensate for the information
loss resulting from grouping resemblant pii's into their corresponding MCj 's,
we have introduced the AvgAff(MCj) factor which corresponds to the average
similarity between the original possibility distributions pip=1..n assigned toMCj :
AvgAff(MCj) =
∑n−1
p=1
∑n
q=p+1 InfoAff(pip, piq)
n∗(n−1)
2
(11)
Proposition 1 When dealing with crisp training sets, i.e., with precise classes
(MCj ≡ Cj), we will always have AvgAff(MCj) = 1 and |MCj | will correspond
to number of instances labeled by the same class Cj, thus we recover the standard
C4.5 approach.
Then, the Affinity-gain ratio is expressed in the same way as the classical
gain ratio using SplitInfo (Equation (7)):
AGr(T,Ak) =
AGain(T,Ak)
SplitInfo(T,Ak)
(12)
Obviously, the attribute maximizing AGr will be assigned to the decision
node at hand.
Example 1 Let us use a modified version of the golf data set [9] to illustrate the
notion of wrapper distributions. Let T be the training set composed of fourteen in-
stances i=1..14. A possibility distribution was given for each possible class of each
instance of T. The set of wrapper distributions relative to this example is WD =
{[1, 0], [0, 1], [1, 1]}. Consequently, MC = {MC1,MC2,MC3} such that MC1 =
{i4, i9, i10, i11, i12, i13}, MC2 = {i1, i2, i6, i8, i14} and MC3 = {i3, i5, i7, }. The
Affinity-Entropy of the set T is computed (using Equation (9)) as follows :
AE(T ) = −0.956∗( 614 ∗log2 614 )−0.95∗( 514 ∗log2 514 )−0.966∗( 314 ∗log2 314 ) = 1.464.
Information Affinity in Possibilistic Decision Trees 211
Table 1. Imprecisely labeled Training Set
Outlook Temp Humidity Wind C1 C2
i1 sunny hot high weak 0.2 1
i2 sunny hot high strong 0.4 1
i3 overcast hot high weak 1 0.7
i4 rainy mild high weak 1 0
i5 rainy cool normal weak 1 0.8
i6 rainy cool normal strong 0.4 1
i7 overcast cool normal strong 1 0.9
i8 sunny mild high weak 0.3 1
i9 sunny cool normal weak 1 0.3
i10 rainy mild normal weak 1 0
i11 sunny mild normal strong 1 0.2
i12 overcast mild high strong 1 0
i13 overcast hot normal weak 1 0.3
i14 rainy mild high strong 0 1
b) Partitioning strategy
Since we only deal with nominal attributes, the partitioning strategy will be the
same as with standard decision trees.
c) Stopping criteria
We will stop growing the tree if:
1. There is no further attribute to test.
2.AGain ≤ 0, i.e., no information is gained.
3. |Tp|=0, i.e., the generated partition does not contain any instance.
d) Structure of leaves
Leaves of our induced Aff-PDT trees will be labeled by possibility distributions
on the different classes rather than crisp classes. In fact, when the above stopping
criterion 1 or 2 is satisfied for a training partition Tp containing n possibility
distributions, we will declare a leaf labeled by the representative possibility dis-
tribution of that set (piRep), that is, the possibility distribution which corresponds
to the closest distribution to all the remaining distributions in the set Tp:
piRep(Tp) = argmaxni=1{
∑
j 6=i InfoAff(pii,pij)
(n−1) }
Hence, when considering the special case of a leaf with only certain possibility
distributions, if we take the fully possible class of (piRep(Tp)) as a final decision,
we join the solution of majority class adopted by standard decision trees.
Finally, when stopping criterion 3 is satisfied, we declare an empty leaf labeled
by a randomly chosen wrapper possibility distribution from WD.
4.3 Classification procedure
Once the Aff-PDT is constructed, we can classify any new object given values
of its attributes. We start with the root of the constructed tree and follow the
path corresponding to the observed value of the attribute in the interior node
of the tree. This process is continued until a leaf is encountered. As mentioned
212 Ilyes Jenhani et al.
above, each leaf of our decision tree will be labeled by a possibility distribution
over the different class values. Hence, to make a decision about the class of a
given object, the decision maker can take the fully possible class label (i.e. the
class having a possibility degree equal to 1).
5 Experimental results
Our experimental studies are divided in two parts. First, we evaluate our Aff-
PDT approach. Second, we compare our results with those of the C4.5 algorithm
if we ignored uncertainty. Note that we do not intend to compare Aff-PDT with
C4.5 since this latter do not deal with uncertainty: the aim of the comparison is
to show whether ignoring uncertainty in training data is a good practice or not.
The experimental study is based on several data sets selected from the U.C.I
repository [9]: (1) Wisconsin Breast cancer (699 instances, 8 attributes, 2 classes),
(2) Voting (497, 16, 2), (3) Balance scale (625,4,3), (4) Solar Flare (1389, 10, 3),
(5) Nursery (12960, 8, 5). We have modified these data sets by transforming the
original crisp classes by possibility distributions over the different classes. We
have used levels of uncertainty (L%) when generating these possibilistic training
sets: for each training instance from the L% randomly chosen instances, we
have assigned a possibility degree equal to 1 to the original class and a random
possibility degree to the remainders in an uniform way. To each one of the
remaining (100− L)% instances, we have assigned a completely sure possibility
distribution corresponding to the original crisp instance's class.
In order to determine the accuracy of the induced trees, we have used two
criteria, the first is relative to the percentage of correct classification (PCC =
number of well classified instances
total number of classified instances ×100) and the second corresponds to a similar-
ity based criterion (PCC_Aff) which we have proposed in [6] as a new criterion
that is more appropriate to the possibilistic context:
PCC_Aff =
∑n
j=1 InfoAff(pi
res, pij)
total_nbr_classified_inst
× 100 (13)
Let us recall that the output of a possibilistic decision tree is given in the
form of a possibility distribution (pires). Thus, the standard PCC is computed
by choosing for each instance to classify the class having the highest possibility
degree (equal to 1). If more than one class is obtained, then one of them is
chosen randomly. Finally, this class label is compared with the true class label.
Table 2. Aff-PDT (PCC_Aff and standard deviation)
L% 0% 30% 50%
W.B.cancer 93.37(1.3) 91.2(1.7) 88.71(2.1)
Voting 96.76 (1.7) 95.35(1.9) 94.11(1.9)
Solar Flare 87.26(2.2) 84.90(1.8) 83.77(1.6)
Balance 83.54 (1.5) 73.83 (1.2) 72.88(1.2)
Nursery 98.74 (0.8) 96.96(1.1) 95.95(1.4)
Information Affinity in Possibilistic Decision Trees 213
Table 2 reports the different obtained results after varying the training sets'
level of uncertainty L% for each database. PCC_Aff values of the induced
Aff-PDT trees are complemented by standard deviations after the use of a 10-
fold cross validation testing process. Note that high values of the PCC_Aff
criterion do not only imply that the induced trees are accurate but also imply
that the possibility distributions provided by the induced Aff-PDT trees are of
high quality and faithful to the original possibility distributions. From Table
2, we can see that PCC_Aff values decrease when L% increases. This can
be explained by the fact that the higher the level of uncertainty (L%), the
less informative the training set becomes (consequently, the harder the learning
becomes), and therefore the less accurate the predictions are.
Now, let us see what happens when ignoring imprecisely labeled training
instances when building decision trees. To respond to this question, we have
conducted our experimentations as follows: for each training set and for each
uncertainty level L%, we have induced an Aff-PDT tree. On the other hand,
a C4.5 tree was induced from the corresponding training set, i.e., the standard
training set from which we have discarded the L% instances to which we have
assigned imprecise class labels since the C4.5 algorithm cannot deal with such
instances. Then, both approaches are evaluated on the same testing sets: stan-
dard testing sets for C4.5 trees have been used and their corresponding testing
sets (with completely sure possibility distributions on the original class labels)
for Aff-PDT trees: this corresponds to one iteration of the 10-fold cross val-
idation process used for the evaluation of the approach. Table 3 reports the
different obtained results after varying the training sets' level of uncertainty L%
for each database. MPCC denotes the mean PCC (complemented by standard
deviation) of the induced decision trees for the 10-fold cross validation process.
Table 3. C4.5 and Aff-PDT (MPCC and standard deviation)
Database Method L = 0% L = 30% L = 50%
W.B.cancer C4.5 94.54(1.1) 91.05(2.5) 90.11(3.2)
Aff-PDT 94.54(1.1) 91.63(2.3) 90.73(2.6)
Voting C4.5 94.56(3.2) 90.15(3.8) 87.27(4.6)
Aff-PDT 94.56(3.2) 91.62(3.5) 88.52(4.0)
Solar flare C4.5 81.96(3.3) 77.03(3.7) 74.37(3.9)
Aff-PDT 81.96(3.3) 80.57(3.7) 78.48(3.9)
Balance C4.5 78.48(4.2) 74.78(5.3) 70.38(5.7)
Aff-PDT 78.48(4.2) 77.06(4.9) 74.82(5.4)
Nursery C4.5 98.78(0.8) 94.45(1.6) 92.81(2.6)
Aff-PDT 98.78(0.8) 97.11(1.2) 94.37(2.2)
Table 3 shows that the Aff-PDT approach gives interesting results when com-
pared with the C4.5 algorithm. Again, we can see that classification accuracies of
both approaches decrease when the level of uncertainty increases (for the same
explanation provided above for Table 2). In spite of this decrease in accuracy, we
can see that the classification rate of Aff-PDT is always (even slightly) greater
214 Ilyes Jenhani et al.
than the one of C4.5. Note that the aim of this comparison is not to directly
compare the two approaches. In fact, the C4.5 is used only in certain environ-
ments: it is trained from reduced training sets (imprecisely labeled instances are
omitted) while the Aff-PDT approach deals with both certain and uncertain en-
vironments: it is trained from complete training sets (including both precisely
and imprecisely labeled instances). Besides, Table 3 confirms Proposition 1. In
fact, our approach recovers the C4.5 one when dealing with crisp instances (with
precise labels, i.e., L%=0).
From the results given in this table, we can conclude that, generally, rejecting
training instances, classes of which are imprecisely defined, is not a good practice
and reduces the accuracy of the induced classifier. This issue can be avoided and
well handled by the use of the proposed Aff-PDT approach which can exploit
the information contained in imprecise labels.
6 Conclusion
This paper proposes a generalization of the C4.5 approach to the imprecise
setting. The new approach has the advantage of allowing the induction of decision
trees from training instances having possibilistic class labels. The proposed Aff-
PDT approach blends information affinity with entropy in order to asses the
homogeneity of a given training partition. Experiments have shown that rejecting
training instances, classes of which are imprecisely defined, is not a good practice
and reduces the accuracy of the induced classifier. We plan to add an automatic
clustering phase for the specification of the wrapper distributions which could
enhance the performance of the approach.
References
1. N. Ben Amor, S. Benferhat, Z. Elouedi: Qualitative classification and evaluation in
possibilistic decision trees, (FUZZ-IEEE'04), Hungary, 2004, 653-657.
2. C. Borgelt, J. Gebhardt, R. Kruse: Concepts for Probabilistic and Possibilistic In-
duction of Decision Trees on Real World Data. (EUFIT'96), 1996, 1556-1560.
3. T. Denoeux and L. M. Zouhal. Handling possibilistic labels in pattern classification
using evidential reasoning. Fuzzy Sets and Systems, 122(3), 2001, 47-62.
4. D. Dubois and H. Prade: Possibility theory: An approach to computerized processing
of uncertainty, Plenum Press, New York, 1988.
5. E. Hüllermeier. Possibilistic Induction in decision tree learning. ECML'02, Helsinki,
Finland, 2002, 173-184.
6. I. Jenhani, N. Ben Amor, Z. Elouedi, S. Benferhat and K. Mellouli: Informa-
tion Affinity: a new similarity measure for possibilistic uncertain information, EC-
SQARU'07, Hammamet, Tunisia, 2007, 840-852.
7. I. Jenhani, N. Ben Amor, Z. Elouedi: Decision Trees as Possibilistic Classifiers,
International Journal of Approximate Reasoning, Elsevier, 2007, to appear.
8. A. Motro: Sources of Uncertainty, Imprecision and Inconsistency in Information
Systems. Uncertainty Management in Information Systems: From Needs to Solutions,
1996, 9-34.
9. P. M. Murphy, D. W. Aha: UCI repository of machine learning databases, 1996.
10. J. R. Quinlan: Induction of decision trees, Machine Learning, 1, 1986, 81-106.
11. J. R. Quinlan: C4.5: Programs for machine learning, Morgan Kaufmann, 1993.
12. L. A. Zadeh: Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets ans Systems,
1, 1978, 3-28.
Ordering Finite Labeled Trees
Herman Ruge Jervell
Department of Computer Science, Pb 1080
University of Oslo
N-0317 Oslo, Norway
herman@ifi.uio.no
Abstract. Previously — in CiE 2005 [2005] — we have given an order-
ing of finite trees and showed that it is a well ordering reaching up to
the small Veblen ordinal. Here we extend this ordering to finite labeled
trees — similar to Takeutis ordinal diagrams [1975] — and show that
this extended ordering is also a well ordering.
Key words: ordinals, ordinal notation, finite labeled trees
1
We have given a wellordered set of labels Λ and consider finite trees with labels
at the nodes. In the trees the branches are ordered — from left to right. Below
there are four examples of finite labeled trees
0
0 0 0
1 2 2
0 2 1
0
Here the smallest label is 0, then 1, 2 . . . . The first tree is the ordinal 0.
The second tree — consisting of only one node with label 1 — is the smallest
tree larger than all trees with just 0 as label. It corresponds to the small Veblen
ordinal. The third tree corresponds to the Howard ordinal. The fourth tree is
larger than the three others and is just an example of the more general type of
labeled trees considered.
2
In CiE 2005 [2005] we gave an ordering of finite trees. The ordering was given
by
A < B ⇔ A ≤ 〈B〉 ∨ (〈A〉 < B ∧ 〈A〉 < 〈B〉)
where
216 Herman Ruge Jervell
A ≤ 〈B〉 : There is an immediate subtree Bi of B such that either A < Bi or
A = Bi
〈A〉 < B : For all immediate subtrees Aj of A we have Aj < B
〈A〉 < 〈B〉 : The inverse lexicographical ordering of the immediate subtrees —
we first check which sequence have smallest length, and if they have equal
length we look at the rightmost immediate subtree where they differ
We then prove by induction over subtrees
– The relation is transitive
– The relation is total: A < B ∨ A = B ∨ B < A where A = B means the
ordinary equality between trees and the three cases are mutually exclusive
– The relation is decidable
We used Π11 − CA to prove that the relation is a well order. There is a 1-
1 correspondence between the finite trees and the ordinals less than the small
Veblen ordinal.
3
In the generalization to finite labeled trees we shall
– Consider more orderings — an ordering <n for each label n and an ordering
<∞
– Have a proof of the well ordering going beyond Π11 − CA
– Consider a new notion — visibility in a labeled tree
First the new notion — visibility. For each label n we define n-visibility. Say
that we are in a labeled tree S and at node ν. Then from node ν a node µ is
n-visible if
– Node µ has label ≥ n
– All nodes strictly between ν and µ have labels > n
So from node ν we can n-see all nodes which can be reached through nodes
with labels ≥ n and up to the first nodes with label n. A node with label ≤ n
will block the n-visibility for all nodes above it. We use the visibility to define
the n-subtrees of a tree S
〈S〉n = the sequence of all n-subtrees n-visible from the root of S
Now we are ready to define the orderings
S <j T ⇔ S ≤j 〈T 〉j ∨ (〈S〉j <j T ∧ S <j+ T )
S <∞ T ⇔ lexicographical ordering
Here we use abbreviations like for the finite trees
Ordering Finite Labeled Trees 217
– ≤j means either ordinary = or <j
– S ≤j 〈T 〉j : There exists a j-subtree T0 of T with S ≤j T
– 〈S〉j < T : For all j-subtrees S0 of S , S0 <j T
– j+ is the smallest label in S and T larger than j if it exists, else it is ∞
– The lexicographical ordering is such that we compare in priority
• The labels at the root of S and the root of T
• For the same label i: The lengths of 〈S〉i and 〈T 〉i
• The rightmost place where the two sequences differ in the >i-ordering
As for the finite trees we get that the orderings are transitive, total and
decidable. This is proved by induction over the subtrees and using the finite set
of labels occurring in the trees. The following differs from our theory of finite
trees
– We consider many orderings
– In the theory of finite trees to each ordinal there corresponded a unique finite
tree. This is not so any more. The two finite labeled trees below is equal in
the ordering
1
0
1
0
0
Both trees have the same 1-immediate subtrees — they have none, and to
get equality this is what we compare.
4
To make the proof of the well ordering more perspicuous we go through the
following cases
– Only label 0
– Finite number of labels
– A well ordered set of labels
Trees with only label 0 is the same as finite trees without labels. Our proof
here is not the same as in [2005]. The proof is a variant of the usual minimal
bad argument. We remind the reader
Bad tree: A tree is bad if there is an infinitely descending sequence starting
with the tree
Minimal tree: A tree is minimal if no immediate subtree is bad
218 Herman Ruge Jervell
Now consider the case where the only label is 0. We must prove that there
are no bad trees. Assume we have a bad tree S0. If S0 is not minimal, then by
going to an immediate subtree we get a smaller tree which is bad. And then we
can continue this going to immediate subtrees until we get a bad tree where all
its immediate subtrees are not bad. We call this minimal bad tree for
S00
Consider now the bad sequence starting with it. We shall construct a sequence
of minimal bad trees. Say we have constructed the minimal bad trees
S00 >0 S
0
1 >0 · · · >0 S0n
The sequence can be continued with bad trees
S00 >0 S
0
1 >0 · · · >0 S0n >0 Tn+1 >0 Tn+2 >0 · · ·
Observe now that for any immediate subtree Un+1 of Tn+1 we have Tn+1 >0
Un+1 and by transitivity S0n >0 Un+1. Hence by continuing taking immediate
subtrees of Tn+1 we get a minimal bad tree S0n+1 with
S00 >0 S
0
1 >0 · · · >0 S0n >0 S0n+1
and we can continue the construction to get an infinite sequence of minimal
bad trees
S00 >0 S
0
1 >0 · · · >0 S0n >0 S0n+1 >0 S0n+2 >0 · · ·
We next observe that from the definition of the ordering
S <j T ⇔ S ≤j 〈T 〉j ∨ (〈S〉j <j T ∧ S <j+ T )
we cannot use the first condition. If 〈S0m〉 ≥0 S0m+1 then S0m would have an
immediate subtree which is bad — contradicting the minimality of S0m. We must
use the second condition and get
S00 >∞ S
0
1 >∞ · · · >0 S0n >∞ S0n+1 >∞ S0n+2 >∞ · · ·
Now look at the lexicographical ordering. Here our trees have only labels 0.
From some stage off all lengths of sequences of immediate subtrees must be the
same, and we get an immediate subtree which starts an infinitely >0-descending
sequence and a bad immediate subtree — contradicting that we had a sequence
of minimal trees.
Ordering Finite Labeled Trees 219
5
Now we come to the case where we have a finite number of labels. We must
generalize the notions of bad and of minimal
n-bad: There is an infinite >n-descending sequence
n-minimal: No m-immediate subtree is m-bad for any m ≤ n
Now assume we have a 0-bad tree. As in the previous section we construct a
0-minimal 0-bad sequence
S00 >0 S
0
1 >0 S
0
2 >0 · · ·
We get as above
S00 >1 S
0
1 >1 S
0
2 >1 · · ·
This is a 1-bad sequence. We must construct a sequence which is also 1-
minimal. Observe that going to the 1-immediate subtrees does not destroy the
0-minimality. By going to the 1-immediate subtrees we may get rid of some 0-
immediate subtrees but we do not create any new 0-immediate subtrees. There-
fore we get a 1-minimal 1-bad sequence
S10 >1 S
1
1 >1 S
1
2 >1 · · ·
And then we continue this line for line until we have the sequence
S∞0 >∞ S
∞
1 >∞ S
∞
2 >∞ · · ·
which is k-minimal for all k. Then from some stage in the sequence we have
the same label n at the root and the same length of the n-immediate subtrees.
But then we get an n-immediate subtree which is n-bad — contradicting the
n-minimality of all trees in the sequence.
6
In the first construction of minimal bad sequence we used Π11 -CA. We had to
decide whether an immediate subtree is bad or not. This is more problematic
for the case where we have more labels. The construction of a 0-minimal 0-bad
sequence of trees useΠ11 -CA as before. But we had to do more in the construction
of a 1-minimal 1-bad sequence. So say we have the 0-minimal 0-bad sequence
S00 >0 S
0
1 >0 S
0
2 >0 · · ·
And we get from the 0-minimality
S00 >1 S
0
1 >1 S
0
2 >1 · · ·
220 Herman Ruge Jervell
Now we had to show that this sequence can also be made 1-minimal. But then
we had to decide whether an immediate subtree is starting an infinite sequence
of >1 descending 0-minimal trees. This goes beyond Π11 -CA. Further down in
the construction we do the same — we had to decide whether an immediate
subtree is starting an infinite sequence of >n+1 descending n-minimal trees.
7
We now consider the general case where the labels are taken from a well ordered
set. As before we assume we have a 0-bad tree, and then construct trees
Snm for each label n and each number m
Each row is a sequence of n-minimal n-bad trees
Sn0 >n S
n
1 >n S
n
2 >n · · ·
The construction goes row for row. We must say how the construction goes
at limit rows. Remember that in the construction row n and n+1 may be quite
similar. The first place m where they differ Sn+1m is a subtree of S
n
m. We then
prove that in the columns the trees are mostly as the tree above — in each
column there are only a finite number of changes. For assume not. Call the
leftmost column m with an infinite number of changes for the critical column.
Then from some row n there are no changes to the left of the critical column m.
That means that from row n all changes in the critical column comes from going
from a tree to a subtree. But this can only be done a finite number of times —
and the critical column was not critical. In each column there are only a finite
number of changes. And we have the obvious construction for limit rows — just
take the limit along each column.
Now look at a limit row
Tλ0 >λ T
λ
1 >λ T
λ
2 >λ · · ·
Here we have
– The row gives a λ-bad sequence
– All elements are n-minimal for each n < λ.
Now we can use the construction above to also get a λ-bad sequence which
is λ-minimal. And then we continue as before. The contradiction comes at row
∞.
Theorem 1. If the labels Λ is well ordered, then <0 is a well order.
Ordering Finite Labeled Trees 221
8
We shall now prove that all the orderings <n are well orderings. Assume we have
proved it for all labels < n. But then
– No tree is m-bad for m < n
– All trees are m-minimal for m < n.
Assume now we have an n-bad tree. So we get an n-bad sequence
Tn0 >n T
n
1 >n T
n
2 >n · · ·
This sequence is m-minimal for all m < n since all trees are. But then we go
through the construction as above and get an n-bad n-minimal sequence
Sn0 >n S
n
1 >n S
n
2 >n · · ·
and we can continue the construction as above to get the last row
S∞0 >∞ S
∞
1 >∞ S
∞
2 >∞ · · ·
which is ∞-bad and m-minimal for all labels m. And then we have a contra-
diction as above and can conclude
Theorem 2. All orderings <k and <∞ are well orderings.
9
We now consider trees with labels up to some finite number N . Then consider
the treeclasses
– TN — trees with label N at the root
– TN−1 — trees with label N − 1 or N at the root
– TN−2 — trees with label N − 2, N − 1 or N at the root
– . . .
– S0 — trees with only label 0
– S1 — trees with only label 0 or 1
– S2 — trees with only label 0, 1 or 2
– . . .
We then have
Theorem 3. The following are isomorphic orderings
– TN and <N — S0 and <0
– TN−1 and <N−1 — S1 and <0
– TN−2 and <N−2 — S2 and <0
– . . .
222 Herman Ruge Jervell
And we have
Theorem 4. The one node trees
– 1 is the supremum of S0 under <0
– 2 is the supremum of S1 under <0
– 3 is the supremum of S2 under <0
– . . .
10
Let us now compare our finite labeled trees with the ordinal diagrams of Takeuti.
It is easiest to compare them with the variant of ordinal diagrams of finite order
made by Levitz [1970]. We use the following variant of the ordinal diagrams O(n)
– In O(n) we consider labeled finite trees which are unordered
– We have given a pair of two numbers as the labels
• The first number is from 0 to n and is called the order
• The second number is a natural number called the degree
– In our labeled trees we defined sequences 〈T 〉k for each order k
– In the ordinal diagrams O(n) we define similarly multisets [T ]k for each order
k
– The orderings <k are defined similarly for the labeled finite trees and the
ordinal diagrams using the gap condition
– The orderings<∞ are defined lexicographically but this comes out differently
for our ordered labeled trees and the unordered ordinal diagrams
– In the ordinal diagrams the elements of the multisets [T ]k all have the same
order at the root but they may have different degrees
– Then for the multisets we first compare the elements of largest degree and
so on
Levitz have the connection between the finite ordinal diagrams and iterated
inductive definitions
Theorem 5 (Levitz). The ordinal diagram O(n) gives the ordinals connected
with IDn−1. In particular the ordinal diagrams with order 0 and 1 give the
ordinal below the Howard ordinal.
On the other hand we give connections between the unordered ordinal di-
agrams and our ordered finite labeled trees. We can use the degrees to embed
the ordered trees into the unordered trees and use the possibility of having large
branchings to embed ordered trees with large degrees. We get
Theorem 6. The finite labeled trees Sn corresponds to the ordinal diagrams
O(n). In particular the labeled tree with the single node 2 under <0 corresponds
to the Howard ordinal.
Helmut Pfeiffer [1972] has generalized Levitz connections between ordinal
diagrams and Schu¨ttes notation system to arbitrary wellordered set of orders.
As above we also get connections to finite labeled trees where the labels come
from a wellordered set.
Ordering Finite Labeled Trees 223
References
[1970] Levitz, Hilbert: On the relationship between Takeuti’s ordinal diagrams O(n)
and Schu¨tte’s system of ordinal notations Σ(n). Intuitionism and Proof Theory. (Eds
Myhill, Vesley, Kino). North-Holland
[1972] Pfeiffer, Helmut: Vergleich zweier Bezeichnungssysteme fu¨r Ordinalzahlen. Arch.
math. Logik 15, 1972.
[1975] Takeuti, Gaisi: Proof theory. North-Holland
[2005] Jervell, Herman Ruge: Finite trees as ordinals. CiE 2005, Springer Lecture
Notes.
Towards Reverse Proofs-as-Programs
Reinhard Kahle
CENTRIA and Departamento de Matema´tica, Universidade Nova de Lisboa,
2829-516 Caparica, Portugal
kahle@mat.uc.pt
Abstract. In this programmatic paper we renew the well-known ques-
tion “What is a proof?”. Challenged by computer based theorem provers,
we argue that a (good) proof is not the formalized one. Looking for the
notion of equality of proofs, we propose to investigate proofs in relation
to Moschovakis’s theory of algorithms. This should be carried out by
a “reverse engineering” of program extraction functions underlying the
proofs-as-programs paradigm.
Key words: Proof; equality of proofs; algorithm; proofs-as-programs
1 What is a proof?
The birth of science is often attributed to the Ancient Greeks since they in-
troduced the concept of proof in mathematical arguments. While the success of
mathematics since the ancient world is an everlasting story, it is surprising that
the central notion of proof in its own only recently got in the focus of research.
Although one might find a lot of controversial “proofs” in the history of mathe-
matics, the question of what is a proof had a direct impact on mathematics only
in the last 150 years.
A first example seems to be Hilbert’s proof of his finite basis theorem which
caused a sensation due to its non-constructive character. In a first reaction,Gor-
dan labeled it as theology rather than mathematics. From a modern perspec-
tive, it was not the notion of proof which was changed by Hilbert, but rather
the (traditional) meaning of a particular mathematical concept: the existential
quantifier. In fact, most of the controversies about certain proofs may actually
be regarded as controversies about the concepts which are involved in these
proofs. However, historically Hilbert’s non-constructive proof was important
since it gave rise to a profound discussion of the foundations of mathematics,
which eventually led to the introduction of proof theory by Hilbert himself as
a new discipline within mathematical logic. This discussion was accompanied by
the rigid formalization of mathematics in the work of Boole, Peano, Frege,
Whitehead and Russell (to mention the most important ones). As a result
we nowadays have at hand a quite clear defined notion of formalized proof in
mathematics. But these formalized proofs have a specific aim in the foundations
of mathematics, and, as a matter of fact, a working mathematician nearly never
formalizes a proof in such a rigid way.
Towards Reverse Proofs-as-Programs 225
Formalized proofs play, however, the central role in computer-aided theorem
proving. But, at present, computer generated proofs do not meet the standards
we have for (good) mathematical proofs. They also do not help (yet) to decide the
question of equality of proofs. Based on the close relation of (constructive) proofs
and algorithms, we propose to investigate (constructive) proofs as inverse images
of algorithms under a program extraction function. Using an existing notion
of equality for algorithms, as given, for instance, in Moschovakis’s theory of
algorithms [26], one can expect to obtain a corresponding notion of equality for
(constructive) proofs which should meet certain requirements we expect for such
an equality.
2 The computer challenge
The use of computers in mathematical theorem proving is an issue which is
becoming more and more important. In this field one can distinguish two different
directions: Proof search and Proof check. Proof search suffers from the well-
known complexity problems and is not very promising for general mathematical
problems.1 While it might be very hard to find a proof, to check a proof for
correctness is, in general, of lower complexity, although it can be rather technical
and long. Thus, proof check seems to be a perfect application for computers.
In a recent book [36] (see also our review [18]), edited byWiedijk, seventeen
theorem provers are presented how they prove the irrationality of
√
2. This book
gives an interesting insight into the state of the art of theorem proving, showing
that in particular the proof representation is still far from being satisfactory for
a human reader. In the introduction the editor presents a six line proof of a
theorem, taken from the textbook of Hardy and Wright [15, p. 39f]:
“The traditional proof ascribed to Pythagoras runs as follows. If
√
2 is rational,
then the equation
a2 = 2b2
is soluble in integers a, b with (a, b) = 1. Hence a2 is even, and therefore a is
even. If a = 2c, then 4c2 = 2b2, 2c2 = b2, and b is also even, contrary to the
hypothesis that (a, b) = 1.”
This proof should be understandable for everybody with basic mathematical
knowledge. The interesting thing is that the editor seems to see the overall
objective of proofs in proof check when he writes after this proof [36, p. 3]:
“Ideally, a computer should be able to take this text as input and check it for
its correctness.”
We think that this perspective is misdirecting—at least with respect to math-
ematical proofs. Of course, the correctness of a proof is essential; but, in most
cases, it is not the primary objective of the proof. Only if a proof is the very first
1 But, of course, proof search has its merits for specialized problems which are properly
modeled, in particular, with respect to applications.
226 Reinhard Kahle
of a theorem, its correctness is the first question. Of course, we do not like to
see faulty proofs later on, but when a theorem is accepted in the mathematical
community as true, most of its proofs have the role to convince the reader of the
correctness of the theorem. Here, the representation of a proof is essential. Over
centuries, mathematicians developed a rather sophisticated, however implicit,
standard to represent proofs which allow to convince other mathematicians. It
is just a matter of fact, that theorem provers still do not have the ability to
represent proofs in a way that they can compete with usual textbook proofs. To
guarantee correctness, they have to take into account too many details, details
which a mathematician does not like to see exposed in the proof. This was for-
mulated by Scott [33, p. ixf] as follows: “[F]or verification (. . . ) checkable proofs
have to be generated and archived. Computers are so fast now that hundreds of
pages of steps of simplifications can be recorded even for simple problems. Hence,
we are faced with the questions, ‘What really is a proof?’ and ‘How much detail
is needed?’ ”.
The two questions of Scott can be described as the computer challenge.
Let us mention here one particular shortcoming of proofs given by computers.
They seem to be inadequate for an exchange between different proof systems.
Thus, we would like to challenge the theorem prover community to provide
interfaces which allow to transfer proofs performed in one theorem prover to
another. Of course, many systems differ significantly in the underlying logic,
the internal representation of datatypes, automatic components, user defined
extensions, etc. However, a proof—in the sense we look for—of the irrationality of√
2 should not depend on any of these particularities. Thus, if there is something
like an abstract proof of this theorem, the theorem provers should be able to
give such one, and they should be able to exchange it among each other. We
conjecture that a very lot of the disturbing details, which most likely depend
on the specific implementation of a theorem prover, would be filtered out in the
proof representation suitable for computer interaction.2
At present, we consider that computers are still not able to give us a sat-
isfactory representation of proofs, as we would like to have for mathematical
theorems.3 The critique can be even extended to formalized proofs in general.4
We will not discuss this issue at length here, but let us mention only two indica-
tions for it: First, mathematical textbooks as well as research papers still avoid
formalized proofs as much as possible; they serve, as everybody knows, rarely for
didactical reasons or to convince the reader of the correctness of an argument.
Secondly, one should have a look how we examine proofs: We would hardly be
2 Avigad [3] proposes to use methods (or tactics) as available in theorem provers
as Isabelle for such a purpose. This is definitely a step in the right direction, but
we think, that the the methods have still not the sufficient level of abstraction. In
particular, they depend too much on the particulars of the implementation.
3 We are aware, that this opinion is not shared by everybody, in particular, several
advocates of computer-aided theorem provers. Let us clarify, that here we do not
refer to the question of the correctness of proofs, but only to the question of (human
understandable) representation of proofs.
4 Cf., for example, [3, p. 129].
Towards Reverse Proofs-as-Programs 227
satisfied if a student would give us the raw text of a formalized proof. But, our
the principle argument against formalized proofs is that it fails to provide a
satisfactory notion of equality of proofs, which goes beyond the literal equality.5
3 Equality of proofs
It seems to be a deficiency of proof theory to not be able, up to today, to provide
a satisfying criterion for equality of proofs.
The only proposal which is generally discussed is Prawitz’s normalization
conjecture, which proposes to identify two proofs with the same normal form,
[30, 31]. A good discussion of the proposal can be found in Dosˇen [12]6. It also
includes the “rather neglected” [12, p. 477] generalization proposal of Lambek,
which is stated in terms of category theory. Both are of interest in logic and
category theory; however, they failed to apply to mathematical proofs. This is
admitted by Dosˇen when he writes [12, p. 500]: “Faced with two concrete proofs
in mathematics—for example, two proofs of the Theorem of Pythagoras, or some-
thing more involved—it could seem pretty hopeless to try to decide whether they
are identical just armed with the Normalization Conjecture or the Generality
Conjecture.” Then, however, he expresses some hopes: “But this hopelessness
might just be the hopelessness of formalization. We are overwhelmed not by
complicated principles, but by sheer quantity.”
But, the normalization conjecture suffers from a conceptional problem: al-
though it might be an interesting technical logical approach, it definitely is not
the notion of equality of proof we can accept in mathematics. By the opposite,
the introduction of a lemma is often considered as the specific achievement in a
new proof of a theorem.7
About the general claims that category theory and/or proof nets are able
to solve the problem of equality of proofs, cf., for instance, [11, 35], we like to
remark that these approaches are mainly concerned with the logical structure
of a proof, but not with mathematical arguments. This is often even admitted,
or at least posed as a challange, as in the citation of Dosˇen given above, or by
Straßburger [35, §4]: “Let me finish with mentioning some of the questions
that are still waiting for an answer: [. . . ] Are these identifications useful from
the point of view of mathematics (i.e., can we use them for identifying real
mathematical proofs)?”
To sharpen the question of equality a fundamental distinction should be made
first: whether we consider proofs of one theorem, or proofs of different theorems.
5 Of course, there are proposals around for other notions of equality of proofs, first
of all, Prawitz’s normalization conjection. However, we do not consider them as
satisfactory; see the discussion below.
6 This article also contains comprehensive references to the normalization conjecture
and to the generalization proposal which we don’t repeat here.
7 Moschovakis (personal communication) suggested that one should identify the nor-
malform of a proof with its denotation rather with its meaning. For our question,
however, it is the meaning we would like to capture.
228 Reinhard Kahle
For two different theorems (most likely with some structural similarities) we
might have proofs which are considered “essentially the same”, in the sense that
they are substitutional variants of each other or that they follow the same pattern
etc. This case is, for instance, investigated by use skeletons of (formal) proofs,
i.e., proof trees in which the formulas at the leafs are replaced by variables, cf. e.g.
[19, 5]. With certain unification methods, Baaz [4] showed how different proofs
can be identified as instances of the same skeleton, and, in this respect they can
be considered as equal. However, there are more informal notions of equality of
proofs in different areas. It is a standard experience that mathematicians, as soon
as they realize any equality, tend to abstract from the concrete cases and to build
a theory which encapsulates the common part of the different instances. This
is also the case for proofs: Baaz8 pointed to the example of universal algebra
which grows out of the experience that the proof of the homomorphism theorem
is “always the same” in the different algebras. This might be a very good example
to show that mathematicians indeed already have a tool to “define” equality of
proofs, just by introducing an abstract theory which identifies the “different”
instances.9
The question when two proofs of one theorem are equal seems to be more
subtle. Very often we can easily decide that two proofs are different, just by
identifying some clear differences. But to give a theoretical account for criteria
of equality is much more complicated. One might separate two levels here: the
structure of a proof, and the demonstration of single steps within a large proof.
The problem is that there is no clear way to separate these two aspects. One
attempt could be to relate them to the differentiation made by theorem provers
between tactics (methods) and the actual proof terms. The tactic should repre-
sent the general structure of a proof, while single steps can (often) be given to an
automatic problem solver unit. As the separation might be, we now have even
two questions: what does it mean that the structure of two proofs is equal? And,
what does it mean that two demonstrations of a single proof step are equal?
In the following we will compare the question of equality of proofs with the
question of equality for algorithms.
4 What is an algorithm?
It exists a field very closely related to proofs with analogous questions: there
is no commonly accepted abstract notion of an algorithm. In the next section
we will discuss how the field of algorithms can be directly linked to the field of
proofs via the proofs-as-programs paradigm. Before, we like to review shortly an
8 Personal communication.
9 It is an interesting observation that this form of unifying different theories implies
that there are common patterns in the proofs, not only in the theorems: since the
abstract theorem should have one proof in the abstract theory, it will be possible to
give the “same” proof as instantiations of this abstract one for all instances, at least
ex post.
Towards Reverse Proofs-as-Programs 229
answer to the question what an algorithm is given by Moschovakis, [21, 22,
24–26].
In [26, §3], entitled The Insufficiency of Machine Models, Moschovakis ar-
gues convincingly on the basis of the mergesort algorithm, given by a recursive
specification, that machine models are not good for defining algorithms. Assum-
ing that algorithms are machines, he emphasizes two problems [26, p. 924f]:
“Now the first obvious problem is that there are many compilation procedures,
and so we don’t get one, but many machines with competing claims ‘to be’
the mergesort algorithm. Moreover, there are essential differences among these
machines, . . . . On an intuitive level, these machines are, of course, equivalent,
but it is hard to see how to make this notion of equivalence precise, . . . ; and if
we could make the relevant equivalence relation precise, then one could argue
that the mergesort algorithm is the appropriate equivalence class (which is
much wider than machine isomorphism), and not any particular member of it.
“One might try to get out of this dilemma by choosing some one, ‘natural’,
‘most general’ machine which implements the mergesort, . . . . It is not clear
how that could be done in a systematic way for all recursive algorithms, but in
any case, it would not suffice: because we want to know that the conclusion of
Prop. 3.1 [some formal properties of mergesort] holds for all ‘implementations
of the mergesort’, not just the most general one, . . . .
“The second problem is that the details of particular implementations are
irrelevant for the elementary proof of Prop. 3.1, . . . .”
It is striking that these problems are the same we encounter for mathemati-
cal proofs, if we would like to identify them with formal proofs: there are many
formal systems around, as Hilbert-style calculi, sequent calculi, natural deduc-
tion, dialogues, etc., and neither we can distinguish one as the ‘natural’ or ‘most
general’ one nor we have any reasonable idea what could be the equivalence
relation over different calculi. The second problem relates to the fact that for-
mal proofs—in particular the ones given by theorem provers—lavish us with
irrelevant details.
In contrast to machines, Moschovakis proposes to define algorithms in
terms of recursors which are associated with recursion equations. He stresses
the non-syntactic character of recursors saying “. . . I have avoided the word ‘def-
inition’ in their name since it suggests syntactical objects, which algorithms are
not” and adding the footnote: “Recursors are related to systems of recursive
equations in the same way that differential operators are related to differential
equations.” [26, p. 925, footnote 11]. Even if he is quite cautious with claiming
to have found a definite answer to the question what an algorithm is,10 to our
knowledge, this is the first and only proposal which provides a—non-trivial and
non-useless—approach to equality of algorithms, [26, §8.2], (which, concededly,
still needs further elaboration).
10 He writes: “I would not claim, however, that mine is the only approach, or the best
approach — or, perhaps, even an adequate approach: my chief goal is to convince
the reader that the problem of founding the theory of algorithm is important, and
that it is ripe for solution.” [26, p. 920].
230 Reinhard Kahle
In view of the similarity of the situation with respect to proofs and algo-
rithms, it seems to be promising to investigate whether, and if so, to which
extend, Moschovakis’s abstract notion of algorithm can give rise to an ab-
stract notion of proof. In the following section we discuss in which way such a
relation could be established.
5 Reverse Proofs-as-programs
The proofs-as-programs paradigm can be taken directly from the Curry-Howard-
isomorphism. Writing (constructive) proofs as λ terms, they are in principle
already functional programs.11
Based on this theoretic relation, the practical implementation of proofs-as-
programs is already quite advanced. As one of the first implementations, we
just mention Bates and Constable’s system PRL, [6]. Nowadays, every good
proof system implements a proofs-as-programs component.12 In this process, the
main emphasize is put on the question how constructive content, needed for the
programs, can be extracted from proofs using classical logic. For our purpose the
particular logical framework is not important; all we need is that the programs
are generated from a proof by use of a given program extraction function.
Our main suggestion is to attack the questions of what is a proof and how
equality of proofs can be defined by use of reverse engineering of the extraction
function:
Given Moschovakis’s notion of algorithm, we have to study the inverse
images of such algorithms under a program extraction function.
The hope is to find (one-to-one) correspondences between concepts about algo-
rithms and about proofs. From the “non-reverse” proofs-as-program perspective,
some interesting examples concerning the principle of mathematical induction
can already be found in [20]. A very concrete instance of a particular concept
we are interested is, is given in [6, p. 115]: “Another possibility in problems of
this sort is that some property of A(n, p) can be generalized to add an extra
parameter, say A(m,n, p), and then we can use induction on m. This technique
is called generalization in Polya [29]; Dijkstra [10], Gries [14], and Reynolds [32]
call it weakening in the context of programming.”
If one focus on Moschovakis’s notions introduced for algorithms, at best,
this should lead in a natural way to a concept which corresponds to recursors
on the side of the proofs. Also, one can hope to get an analog of the equality
relation given for recursors.
In its first step, this analysis applies to proofs only which give rise to al-
gorithms, first of all, constructive proofs of Π02 statements. However, in any
11 There exists a lot of literature about the Curry-Howard-isomorphism which we will
not include here. As a standard reference, we can refer to [34].
12 We restrict ourselves to refer to the book [27], which contains a lot of additional
references.
Towards Reverse Proofs-as-Programs 231
reasonable account to equality of proof, however a notion of equality for such
constructive proofs will look like, this notion has to be part of the notion of equal-
ity of proofs in general. Therefore, we do not see this approach as restrictive,
but, on the contrary, as a step which provides, firstly a necessary condition for a
general notion of equality, and secondly as a heuristic guideline which probably
can give rise to an extension to general proofs.
In any case, there are already several questions which have to be checked:
neither it is clear whether the inverse image of a given algorithm is unique—
one might even wonder if there always is one—, nor whether a given extraction
function is canonical or whether the same proof can give rise to different algo-
rithms.13
A negative answer to the first of these questions would mean that we have
two proofs which are considered as different, but the extracted algorithms are
equal. In such a case, it would be interesting to see on which parts the difference
of these proofs is based.
A negative answer to the second question would mean that the extraction
function itself has to be analyzed more carefully, because it would add compu-
tationally relevant components to a proof.
All in all, our proposal of reverse engineering on program extraction should
not only provide insights in the notion of proof, but also contribute to a better
understanding of program extraction, algorithms, and verification proofs.
Note added in proof. A referee drew our attention to the paper Problems in the
logic of provability of Beklemishev and Visser [7] of which we were not aware
while writing this text. In fact, their consideration about an Informal concept
of proof in section 2, concide to a large extend with our view. Beklemishev
and Visser propose to relate the notion of proof with the notion of algorithm
as given in the theory of Abstract State Machines by Gurevich with references
to [8, 9]. Of course, our proposal should work, in principle, with any theory of
algorithms which provide a notion of equality. We just proposedMoschovakis’s
theory because we are more familiar with it. A comparison of both, Gurevich’s
and Moschovakis’s, theories would be desirable in any case, but also with
respect to its implications for a notion of equality of proofs. About the abstract
theory of provability (or also the more specific logic of proofs of Arte¨mov, [2])
which is proposed as an account in this context by Beklemishev and Visser
we are rather sceptical, since it seems to be formulated on a “too abstract” level.
However, the fruitfulness of it is still subject to research.
6 The broader view: Sense and Denotation as Proof and
Truth
It is an old question why the two equations a = a and a = b should have a
different epistemological status, if a and b refer to (denote) the same object.
13 Moschovakis raised the question of the relation between (constructive) proofs and
algorithms already in [25, §3.4].
232 Reinhard Kahle
This question, still asked by Poincare´ [28], gave reason to Frege’s seminal in-
vention of the distinction between sense and denotation, [13]. This distinction is
discussed in length in philosophical logic, and there are several attempts to for-
malize sense within intensional logic, cf. e.g., [1]. Up to today, these approaches,
however, did not succeed to provide a satisfactory analysis.
An alternative approach was suggested byMoschovakis who analyses sense
and denotation as algorithm and value, [23]:
“In contemporary computing terms, we have defined an algorithm which com-
putes the truth value of χ. . . . This algorithm is the referential intension of just
intension of χ,
int(χ) = the algorithm which computes the truth value of χ,
and we propose to model the sense of χ by its referential intension.”
Of course, for the underlying notion of algorithm,Moschovakis suggests to use
the notion discussed above.
At other places we considered to relate intentional phenomena to proofs while
the extensional counterpart is related to truth. Thus our reading of a sense of
a formula φ would be a proof of φ; in this context, obviously, a formula can
have different senses if it has different proofs. This approach depends on the
underlying axiom system (or better: the underlying axiom system is implicitly
part of the sense of formula). But, with this reading the epistemological difference
of, for instance, 2 = 2 and 2 =
√
4 is obviously on the level of senses: the first
one has a trivial proof—it is an axiom (in any reasonable axiom system)—; the
second one requires some deduction steps.
Some first applications of our proposal are given in [17] with respect to ne-
cessity, and in [16] with respect to belief revision. However, these approaches are
still informal, and a better theory of proofs is needed, a theory which we are
looking for in this paper.
References
1. C. Anthony Anderson. General intensional logic. In D. Gabbay and F. Guenthner,
editors, Handbook of Philosophical Logic, volume II, pages 355–385. Kluwer, 1984.
2. S. Artemov. Logic of proofs. Annals of Pure and Applied Logic, 67:29–59, 1994.
3. J. Avigad. Mathematical method and proof. Synthese, 153:105–149, 2006.
4. M. Baaz. Note on the generalization of calculations. Theoretical Computer Science,
224(1–2):3–11, 1999.
5. M. Baaz and P. Pudla´k. Kreisel’s conjecture for l∃1. In P. Clote and J. Kraj´ıcˇek,
editors, Arithmetic Proof Theory and Computational Complexity, pages 30–49. Ox-
ford University Press, 1993.
6. J. L. Bates and R. L. Constable. Proofs as programs. ACM Transactions on
Programming Languages and Systems, 7(1):113–136, 1985.
7. L. Beklemishev and A. Visser. Problems in the logic of provability. In D. M.
Gabbay, S. S. Goncharov, and M. Zakharyaschev, editors, Mathematical Problems
from Applied Logic, volume I, pages 77–136. Springer, 2006.
Towards Reverse Proofs-as-Programs 233
8. A. Blass and Y. Gurevich. Algorithms vs. machines. Bulletin of the European
Association for Theoretical Computer Science, 77:96–118, 2002.
9. A. Blass and Y. Gurevich. Algorithms: A quest for absolute definitions. Bulletin
of the European Association for Theoretical Computer Science, 81:195–225, 2003.
10. E. W. Dijkstra. A Discipline of Programming. Prentice-Hall, 1976.
11. K. Dosˇen and Z. Petric´. Proof-Theoretical Coherence. KCL Publications, 2004.
12. K. Dosˇen. Identity of proofs based on normalization and generality. The Bulletin
of Symbolic Logic, 9(4):477–503, 2003.
13. G. Frege. U¨ber Sinn und Bedeutung. Zeitschrift fu¨r Philosophie und philosophische
Kritik, (NF 100):25–50, 1892.
14. D. Gries. The Science of Programming. Springer, 1982.
15. G. H. Hardy and E. M. Wright. An Introduction to the Theory of Numbers. Oxford,
4th edition, 1960.
16. R. Kahle. Structured belief bases. Logic and Logical Philosophy, 10:49–62, 2002.
17. R. Kahle. A proof-theoretic view of necessity. Synthese, 148(3):659–673, 2006.
18. R. Kahle. Review of: Freek Wiedijk (editor), The Seventeen Provers of the World.
Springer, 2006, [36]. Studia Logica, 87:369–374, 2007.
19. J. Kraj´ıcˇek and P. Pudla´k. The number of proof lines and the size of proofs in first
order logic. Archive for Mathematical Logic, 27:69–84, 1988.
20. Z. Manna and R. J. Waldinger. Toward automatic program synthesis. Communi-
cations of the ACM, 14:151–165, 1971.
21. Y. Moschovakis. The formal language of recursion. The Journal of Symbolic Logic,
54:1216–1252, 1989.
22. Y. Moschovakis. A mathematical modeling of pure, recursive algorithms. In A. R.
Meyer and M. A. Taitslin, editors, Logic at Botik ’89, pages 208–229. Springer,
1989.
23. Y. Moschovakis. Sense and denotation as algorithm and value. In J. Oikkonen and
J. Va¨a¨na¨nen, editors, Logic Colloquium ’90, pages 210–249. Springer, 1994.
24. Y. Moschovakis. A game-theoretic, concurrent and fair model of the typed lambda-
calculus, with full recursion. In M. Nielsen and W. Thomas, editors, CSL ’97, pages
341–359. Springer, 1998.
25. Y. Moschovakis. On founding the theory of algorithms. In H. D. Dales and G. Oliv-
eri, editors, Truth in mathematics, pages 71–104. Clarendon Press, 1998.
26. Y. Moschovakis. What is an algorithm? In B. Engquist and W. Schmid, editors,
Mathematics unlimited — 2001 and beyond, pages 919–936. Springer, 2001.
27. I. H. Poernomo, J. N. Crossley, and M. Wirsing. Adapting Proofs-as-Programs.
Springer, 2005.
28. H. Poincare´. la Science et l’Hypothe´se. Flammarion, 1902.
29. G. Polya. How To Solve It. Princeton University Press, 1945.
30. D. Prawitz. Natural Deduction. Almquist and Wiksell, 1965.
31. D. Prawitz. Ideas and results in proof theory. In J. E. Fenstad, editor, Proceedings
of the Second Scandinavian Logic Symposium, pages 235–307. North-Holland, 1971.
32. J. C. Reynolds. The Craft of Programming. Prentice-Hall, 1981.
33. D. Scott. Foreword. In F. Wiedijk, editor, The Seventeen Provers of the World,
pages vii–xii. Springer, 2006.
34. M. H. So¨rensen and P. Urzyczyn. Lectures on the Curry-Howard isomorphism.
Elsevier, 2006.
35. L. Straßburger. What is a logic, and what is a proof? In Jean-Yves Beziau, editor,
Logica Universalis, pages 135–145. Birkha¨user, 2005.
36. F. Wiedijk, editor. The Seventeen Provers of the World. Springer, 2006.
Plotkin Definability Theorem
for Atomic-Coherent Information Systems
Basil A. Kara´dais
Mathematisches Institut, Ludwig-Maximilians-Universita¨t
Theresienstraße 39, 80333 Mu¨nchen, Germany
karadais@math.lmu.de
Abstract. By a Plotkin definability theorem we mean here a statement
of the following sort: a PCF-like language can be extended to a lan-
guage so that a functional will be definable by a term if and only if it
is computable [1–4]. We prove this result directly for a class of Scott
information systems [5] that feature “atomicity” and “coherence”.
Key words: Atomic-Coherent Information Systems, Partial Computable
Functionals, PCF Definability.
1 Partial Continuous Functionals
In late seventies, Gordon Plotkin [1] proved PCF-definability of computable func-
tions on flat domains. What we prove in section 2 directly is an analogous result
in the slightly but interestingly different setting of “atomic-coherent information
systems”. These were introduced in [6] to serve as a better yet interpretation
of data types than general information systems: they induce domains which are
nonflat, in a way that allows constructors to be injective and to have disjoint
ranges. Moreover, and interestingly enough, it can be shown that these domains
are exactly the “coherent” ones (see Appendix), a notion that also goes back to
Plotkin and the late seventies [7].
In this section we list notions and facts that form the raw material for the
main result, drawing mainly from [6, §2.2]; for further preliminary material see
Appendix. In the second section we proceed to our main theorem, namely, a
direct proof of Plotkin’s definability result for atomic-coherent information sys-
tems; contrary to what one might expect, we will see that the proof goes through
in a substantially more intricate way than in the flat case.
Basic notions and facts. The information systems we will use are simple in
that all discussion of consistence and entailment of information can be conducted
on the primary level of atomic pieces of information, just by using binary rela-
tions. An atomic-coherent information system—from now on acis1—is a triple
α = (Tα,3α,α), where Tα is a nonempty countable set of atomic pieces of
1 We will be using the term as a proper english noun, hence allowing for the forms
acises for the plural and acis’s for the possessive.
Plotkin Definability Theorem for Atomic-Coherent Information Systems 235
information or just atoms (or tokens), 3α is the consistency, a reflexive and
symmetric binary relation on Tα and α is the entailment, a reflexive and tran-
sitive binary relation on Tα, so that consistency propagates through entailment,
that is,
∀
a,b,c∈Tα
. a 3α b ∧ b α c→ a 3α c .
We will drop the subscripts in places we can afford to. For U, V ⊆ T adopt
the following shorthands: U 3 a := ∀b∈U b 3 a, U 3 V := ∀a∈V U 3 a,
a  U := ∀b∈U a  b, U  a := ∃b∈U b  a, U  V := ∀a∈V U  a. Call
U ⊆f Tα a (formal) neighborhood (or finite approximation or consistent set),
and write U ∈ Conα, if a 3α b for all a, b ∈ U . Call u ⊆ Tα an ideal (set) (or
element or point), and write u ∈ Ideα, if a 3α b for all a, b ∈ u, and, whenever
a α b for some a ∈ Tα, then b ∈ u. In the following we will try to keep
the notation fixed: we will write a, b, . . . for atoms, U, V, . . . for consistent sets,
X,Y, . . . for arbitrary finite sets of atoms, and u, v, . . . for infinite sets of atoms,
mainly ideals.
Lemma 1. Let α = (T,3,) be an acis. For a, b ∈ T , U, V,W ∈ Con, u, v ∈ Ide,
the following hold.
1. a  b→ a 3 b,
2. a1  b1 ∧ a2  b2 ∧ a1 3 a2 → b1 3 b2,
3. U1  V1 ∧ U2  V2 ∧ U1 3 U2 → V1 3 V2,
4. U 3 V ∧ V W → U 3W ,
5. u  v ↔ v ⊆ u.
Denote the empty ideal ∅ ∈ Ide by ⊥. For a finite set of atoms X ⊆f T ,
define its (deductive) closure by X := {a ∈ T | X  a} and the cone (of ideals)
above it by ∇X := {u ∈ Ide | X ⊆ u}. Denote by Con the class of all closures of
formal neighborhoods and by Kgl the class of all cones in the acis.
Lemma 2. Let X,Y ⊆ T and U, V ∈ Con.
1. X ∈ Con↔ X ∈ Ide,
2. X ∪ Y = X ∪ Y ,
3. X 3 Y ↔ X 3 Y ,
4. X  Y ↔ X ⊇ Y ,
5. U 3 V → ∇U ∩∇V = ∇(U ∪ V ).
Let α and β be two acises. Define their function space α → β by Tα→β :=
Conα × Tβ , (U, a) 3α→β (V, b) when U 3α V → a 3β b, and (U, a) α→β (V, b)
when V α U∧a β b. Define their cartesian product α×β, by Tα×β := TαunionmultiTβ ,
a 3α×β b when either a, b belong to different acises or are consistent in the same
one, and a α×β b when a entails b in either α or β; in this way we are able to
approximate any (u, v) ∈ Ideα×β separately in each component.
Both the function space and the cartesian product of two acises can be proven
to be again acises. Moreover, one can show that the pairs (u, v) ∈ Ideα × Ideβ
are in a bijective correspondence with the ideals u ∪ v ∈ Ideα×β , as well as that
236 Basil A. Kara´dais
the Scott continuous mappings from Ideα to Ideβ (see Appendix) are exactly the
ideals of α→ β [6].
In the following we will be largely concerned with “application of ideals”. In
general, define (set) application · : P(Tα→β)× P(Tα)→ P(Tβ), by
{(Xi, bi)}i∈I · Y :=β {bi | Y α Xi} .
Lemma 3. For the application operation the following hold.
1. If U ∈ Conα→β and V ∈ Conα, then U · V ∈ Conβ. Furthermore, if U 3α→β
U ′ and V 3α V ′ then U · V 3β U ′ · V ′.
2. For all V ∈ Conα, it is U α→β U ′ if and only if U · V β U ′ · V .
3. For all U ∈ Conα→β, if V α V ′ then U · V β U · V ′.
4. It is U · V := U · V .
5. For X ⊆f Tα→β, Y ⊆f Tα and Z ⊆f Tβ, where α and β are fixed, the
relation Z = X · Y is Σ01 -definable.
Arithmetical and boolean acises. We define the acis of lazy natural numbers
as follows. Let ∗ be a preatom, intuitively meaning least information. Natural
numbers given as usual by the constructors 0 and S give rise to an acis N as
follows: a ∈ TN if a = ∗, or a = 0, or a = Sa′ for some a′ ∈ TN; a 3N b if a = ∗,
or b = ∗, or a = b = 0, or a = Sa′ ∧ b = Sb′ for a′ 3N b′; a N b if b = ∗, or
a = b = 0, or a = Sa′ ∧ b = Sb′ for a′ N b′. Similarly, the boolean numbers
tt and ff give rise to an acis B as follows: b ∈ TB if b = ∗, or b = tt, or b = ff;
b1 3B b2 if b1 = ∗, or b2 = ∗, or b1 = b2 = tt, or b1 = b2 = ff; b1 B b2 if b2 = ∗, or
b1 = b2 = tt, or b1 = b2 = ff. Note that the least info atom ∗ is not considered to
be a proper atom and that we will count it out whenever we talk about atoms,
unless otherwise stated; on the level of ideals, least info is expressed by ⊥.
Acises which are function spaces whose target is N will be called arithmetical
acises. In particular, we think of ideals of N as partial numbers and ideals of an
arithmetical function space as partial continuous functionals. Write ∞ for the
ideal {. . . , Sn∗, . . . , S∗} and n for the ideal {Sn0}. Furthermore, we take the
risk of overloading the symbols tt and ff to mean the corresponding ideals of B
as well. We refer to function spaces consisting of both arithmetical and boolean
acises as arithmetical-boolean acises.
Define the total ideals Gα ⊆ Ideα, in an arithmetical acis α, inductively as
follows: u ∈ GN if u = 0, or u = Sv for some v ∈ GN; u ∈ Gα→β if u(v) ∈ Gβ
for all v ∈ Gα. In what follows, the total numbers 0, 1, . . . will mainly serve as
indices.
2 Computable Functionals
Call a partial continuous functional computable if it is Σ01 -definable as a set of
atoms. It is direct to check (cf. Appendix) that evaluation and currying func-
tionals are computable, that composition, application and cartesian products of
computable functionals are computable, that projections are computable, and,
consequently, that the functional [[M ]] is computable for any term M .
Plotkin Definability Theorem for Atomic-Coherent Information Systems 237
Recursion on parallel conditionals and existentials. We pave the road for
the definability statement by introducing the standard fixed point and parallel
functionals. The proofs of the corresponding statements found in [3] are easy to
adapt to our nonflat setting.
Let α be an arbitrary acis and f : α→ α a continuous mapping. An ideal set
u ∈ Ideα is said to be the least fixed point of f if f(u) = u and f(v) = v → u ⊆ v
for all v ∈ Ideα. One can prove that the mapping f has a least fixed point
given by the equation Y(f) =
⋃
n∈GN f
n(⊥), and that the fixed point functional
Y : (α→ α)→ α is continuous and computable for any α.
From now on we restrict our attention to arithmetical-boolean acises. Define
the parallel conditional functional pcond : B→ N→ N→ N by
pcond(p, u, v) :=

u p = tt
v p = ff
u ∩ v p = ⊥
.
One can prove that the parallel conditional functional is continuous and com-
putable.
Define the existential functional exist : (N→ B)→ B by
exist(u) =

ff u(⊥) = ff
tt ∃n∈GN u(n) = tt
⊥ otherwise
.
For the first clause of the definition, notice that whenever u(⊥) = ff, it should
be v(⊥) = ff for all ideals v N→B u. Again, one can prove that the existential
functional is continuous and computable.
Call a partial continuous functional u ∈ Ideα1→···→αp→N recursive in pcond
and exist if it can be defined explicitly for all arguments v1, . . . , vp by an equation
u(v1, . . . , vp) = t(v1, . . . , vp)
where t is a simply-typed lambda term (cf. Appendix) built up from variables
v1, . . . , vp, algebra constructors, fixed point functionals, parallel conditional func-
tionals and existential functionals.
A functional we will need in what follows is the disjunction functional or :
B→ B→ B defined by
or(p, q) := pcond(p, tt, q)
which unfolds to
or(p, q) :=

tt p = tt ∨ q = tt
ff p = q = ff
⊥ otherwise
.
This is a continuous and computable functional, since it is defined by pcond. Its
p-ary generalization we will denote by ORpi=1.
238 Basil A. Kara´dais
Enumeration and inconsistency functionals. We begin with two lemmas.
Lemma 4. The following hold.
1. For all a, b ∈ TN, we have the following conditional dichotomy property:
a 3N b→ (a N b ∨ b N a).
2. For an arithmetical acis α, if {(Ul, bl)}l≤n ∈ Conα→N and u ∈ Ideα, then
{(Ul, bl)}l≤n(u) ∈ ConN.
We introduce a notion of “relative height” between partial numbers. Let
u, v ∈ IdeN and n ∈ GN. Say that u is above n, and write u  n, if it contains
information built from at least n constructors, that is, if either u N Sn−10 or
u N Sn∗. Say further that u is above v, and write u  v, if it is above any
index below v, that is, if v  n → u  n for all n ∈ GN. Note that aboveness
is not antisymmetric, since Sn0  Sn+1⊥ and Sn+1⊥  Sn0 for all n. It is also
obvious that aboveness between ideals is a total preorder with single maximum
element ∞ and single minimum element ⊥, as well as that, for total ideals, it
reduces to the standard ≥ relation.
This straightforward notion, which is merely based on the simple tree struc-
ture of the lazy natural numbers, is nevertheless the necessary and sufficient
step beyond the techniques used in the flat case. The calculus it provides is
summarized in the following.
Lemma 5. Concerning aboveness  ⊆ IdeN × IdeN the following hold.
1. If u N v then u  v.
2. If v  w and w  v then v = w for all v, w ⊆ u, u ∈ IdeN.
3. If u 3N v and u  v then u N v.
4. If u  v and u 63N v then v ∈ GN.
For every arithmetical-boolean acis we fix an enumeration of consistent sets
{Un}n∈GN so that U0 = ∅ holds, and the following are primitive recursive rela-
tions: Un 3α Um, Un α Um, Uα→βn · Uαm = Uβk and Un ∪ Um = Uk, with k = 0
if Un 63α Um. In the following we will lift the notational convention of the first
section and write b for singleton neighborhoods, whenever we want to stress that
they behave as atoms.
We will use inconsistency functionals incnsα : α→ GN → B, defined by
incnsα(u, n) :=

tt u 63α Un
ff u α Un
⊥ otherwise
.
having application in mind: let β be an arbitrary acis and (Un, bn) an atom of
a partial continuous functional v of type α → β; the only case where the atom
contributes information concerning the value of v(u) is for incnsα(u, n) to be ff,
so bn will belong to the value.2
2 To make case distinctions in subsequent proofs less branching, we adopt the following
convention. Let α and β be arbitrary arithmetical-boolean acises. An ideal mapping
f : α→ N→ β that satisfies f(u, x, v) = ⊥ for x 6∈ GN, will be informally typed by
α→ GN → β, and be treated only on its arguments where x is total.
Plotkin Definability Theorem for Atomic-Coherent Information Systems 239
In order to define the functionals incnsα, we will further need to define an
appropriate enumeration functional enα : GN → N→ α to be able to enumerate
all finitely generated extensions of Um, for Um ∈ Conα.
Proposition 6. Let α be an acis over N and B.
1. There exists a functional enα : GN → N→ α, with the following properties.
enα(m,x) = Um, when x 6∈ GN ,
enα(m,n) = Un, when Un α Um ,
which is recursive in pcond and exist.
2. The inconsistency functional incnsα is recursive in pcond and exist.
Proof. By induction on α.
For the enumeration functionals. Let α = α1 → · · · → αp → N, Um ∈
Conα1→···→αp→N and j, k, and h be primitive recursive functions such that
Um =
{
(Uj(m,1,l), . . . , Uj(m,p,l), bk(m,l))
}
l<h(m)
,
with
l > l′ → bk(m,l)  bk(m,l′) . (1)
In this representation we have Uj(m,i,l) ∈ Conαi and bk(m,l) ∈ ConN, for all
l < h(m) and all 1 ≤ i ≤ p, while h(m) denotes the number of elements of
Um.3 Consider the collection {Um,l}l of progressive approximations to Um; in
particular, for l ∈ GN, define
Ux,l := ∅ if x 6∈ GN ,
Um,0 := ∅ ,
Um,l+1 := Um,l ∪
{
(Uj(m,1,l), . . . , Uj(m,p,l), bk(m,l))
}
.
Observe that Um,h(m) = Um.
For u ∈ Ideα1→···→αp , let qu,m,l express whether the application Um,l+1(u)
does not contribute information to the value of Um,l(u), that is,
qu,m,l :=
p
OR
i=1
incnsαi(ui, j(m, i, l)) =

tt ∃pi=1 ui 63αi Uj(m,i,l)
ff ∀pi=1 ui αi Uj(m,i,l)
⊥ otherwise
3 Observe that, since j, k, and h are primitive recursive, the respective generated
ideals are total, and so all values (m, i, l), k(m, l), and h(m) will be total for total
arguments. Based on this observation, we write j(m, i, l) for (m, i, l), and so on,
with the convention that m, i, and l, as well as n (for m, ı, l, and n respectively),
will denote total ideals in N; for not necessarily total ones, together with u and v,
we reserve x and y.
240 Basil A. Kara´dais
Observe that, in the last case, it is ∃pi=1 ui 6αi Uj(m,i,l), but we still have
∀pi=1 ui 3αi Uj(m,i,l). Similarly, let
qy,m,l := incnsN(y, k(m, l)) =

tt y 63N bk(m,l)
ff y N bk(m,l)
⊥ otherwise
.
Observe again that in the last case, by the dichotomy property, we have bk(m,l) N
y. Define now an auxiliary functional Ψ : α1 → · · · → αp → N → N → GN → N
by
Ψu,x(y, l) := y if x 6∈ GN ,
Ψu,m(y, 0) := y ,
Ψu,m(y, l + 1) := pcond
(
qu,m,l, Ψu,m(y, l),
bk(m,l) ∪ pcond
(
qΨu,m(y,l),m,l,⊥, Ψu,m(y, l)
) )
.
Using Lemma 5 and Lemma 4 respectively, one can prove the following.
Lemma 7. It is Ψu,x(⊥, l) = Ux,l(u).
Lemma 8. If ∀l<h(m) . qu,m,l 6= tt → qy,m,l = qu,m,l, then ∀l<h(m) Ψu,m(y, l) =
y.
Let
Φ(u, x, y) := Ψu,x
(
y, h(x)
)
,
en(m,x)(u) := Φ (u,m, Φ(u, x,⊥)) .
One can now easily show how the desired properties of en hold. The first one
follows from Lemma 7. For the second one, suppose that Un α1→···→αp→N
Um, for n ∈ GN; we get en(m,n)(u) = Ψu,m(Un(u), h(m)) by Lemma 7, with
Un(u) N Um(u) by Lemma 3; let l < h(m); if qu,m,l = ff, then ui αi Uj(m,i,l)
for all i, so Um,l(u) = bk(m,l), which gives Um(u) N bk(m,l); by hypothesis and
transitivity, we have Un(u) N bk(m,l), which yields qUn(u),m,l = ff; if, on the
other hand, qu,m,l = ⊥, then, by hypothesis, it is ui 3αi Uj(m,i,l) for all i and
p
∀
i=1
Uj(m,i,l) αi Uj(n,i,l′) ∧ bk(n,l′) N bk(m,l)
for some l′ < h(n); by propagation of consistency through entailment and the
definition of consistency in a function space, we get
Un(u) 3N bk(n,l′) ∧ bk(n,l′) N bk(m,l) ,
which, by propagation again, gives Un(u) 3N bk(m,l), and so, qUn(u),m,l = ⊥; so
Lemma 8 applies and we are done.
For the inconsistency functionals. Omitted. uunionsq
Plotkin Definability Theorem for Atomic-Coherent Information Systems 241
Definability. We are now in the position to prove the main result.
Theorem 9. A partial continuous functional of type α → N over N and B is
computable if and only if it is recursive in pcond and exist.
Proof. Since all λ-terms are computable and computable functionals are closed
under application, it follows that any functional is computable if it is recursive
in pcond and exist. For the converse, we consider here arrow types only. Let
Ω : α1 → · · · → αp → N be a computable functional. It will be represented as a
primitive recursively enumerable set of atoms, that is,
Ω =
{
(Uf1(n), . . . , Ufp(n), bg(n))
}
n∈GN ,
where, for each i = 1, . . . , p, Ufi(n) follows an enumeration of Conαi , bg(n) follows
an enumeration of ConN, and f1, . . . , fp, g are fixed primitive recursive functions.
For arbitrary u ∈ Ideα1→···→αp , v ∈ IdeN, let
qu,n :=
p
OR
i=1
incnsαi(ui, fi(n)) =

tt ∃pi=1 ui 63αi Ufi(n)
ff ∀pi=1 ui αi Ufi(n)
⊥ otherwise
,
qv,n := incnsN(v, g(n)) =

tt v 63N bg(n)
ff v N bg(n)
⊥ otherwise
,
and define a functional ω : α1 → · · · → αp → (N→ N)→ GN → N by
ωu(ψ)(n) := pcond
(
qu,n, ψ(n+ 1), bg(n) ∪ pcond
(
qψ(n+1),n,⊥, ψ(n+ 1)
))
.
We show that Ω(u) = Y(ωu)(0). In particular, will show that both recursion on
ωu at 0 and Ω(u) entail the very same information, that is,
∀
n∈GN
. Ω(u) N bg(n) ↔ Y(ωu)(0) N bg(n) .
For the right direction, suppose that there exists an index n ∈ GN such that
Ω(u) N bg(n). This means that
∃
m∈GN
.
p
∀
i=1
ui αi Ufi(m) ∧ bg(m) N bg(n) . (2)
We claim that
∀
k≤m
ωk+1u (λx. ⊥)(m− k) N bg(m) (3)
and we prove it by induction on k. For k = 0, by (2), it is ωu(λx. ⊥)(m) =
bg(m) N bg(m). For brevity, let v := ωk+2u (λx. ⊥)(m − k − 1) and v0 :=
ωk+1u (λx. ⊥)(m− k); the induction hypothesis is that v0 N bg(m). For k+1 we
have
v = pcond
(
qu,m−k−1, v0, bg(m−k−1) ∪ pcond (qv0,m−k−1,⊥, v0)
)
.
242 Basil A. Kara´dais
We argue by cases on the outer boolean argument. If qu,m−k−1 = tt, then v :=
v0 N bg(m) by induction hypothesis.
If qu,m−k−1 = ff, then ui αi Ufi(m−k−1) for all i, so Ω(u) N bg(m−k−1).
By (2) and Lemma 1 we get bg(m) 3N bg(m−k−1), and by Lemma 4 this yields
bg(m) N bg(m−k−1) ∨ bg(m−k−1) N bg(m) (4)
while v = bg(m−k−1) ∪ pcond (qv0,m−k−1,⊥, v0)). We distinguish cases on the
inner boolean now. If qv0,m−k−1 = tt, then v0 63N bg(m−k−1); if it were bg(m) N
bg(m−k−1), by induction hypothesis, transitivity of entailment would yield v0 N
bg(m−k−1), which is absurd; the dichotomy property (4) gives bg(m−k−1) N
bg(m), and
v := bg(m−k−1) ∪ ⊥ = bg(m−k−1) N bg(m) .
If qv0,m−k−1 = ff, then v0 N bg(m−k−1), and
v := bg(m−k−1) ∪ v0 = v0 N bg(m)
by induction hypothesis. If qv0,m−k−1 = ⊥, then v0 6N bg(m−k−1) ∧ v0 3N
bg(m−k−1); if it were bg(m) N bg(m−k−1), we would again have a contradiction
due to transitivity of entailment and induction hypothesis, so dichotomy (4)
gives bg(m−k−1) N bg(m), and
v := bg(m−k−1) ∪ ⊥ = bg(m−k−1) N bg(m) .
If now it is qu,m−k−1 = ⊥, then ui 3αi Ufi(m−k−1) for all i, so by (2)
and propagation we get Ufi(m−k−1) 3αi Ufi(m). By definition of consistency
in a function space, we get bg(m−k−1) 3N bg(m), and this yields the dichotomy
property (4) again, so we distinguish cases on the inner boolean and argue as
before.
We proved that v N bg(m) in all cases. Letting k = m in (3), we get
ωn+1u (λx. ⊥)(0) N bg(m) ⇒ Y(ωu)(0) N bg(n)
by (2) and the definition of the fixed point functional. So, Y(ωu)(0) entails all
information of Ω(u).
Conversely, suppose that
Y(ωu)(0) N bg(n) . (5)
We claim that
∃
m∈GN
. Ω(u) N bg(m) ∧ bg(m) N bg(n) . (6)
Suppose on the contrary that this is not the case. Then, for all m ∈ GN, if
Ω(u) N bg(m) then bg(m) 6N bg(n). Now, the conditional clause gives qu,k =
ff ∧ bg(k) N bg(m) for some k ∈ GN, which a fortiori gives
ωu(λx. ⊥)(k) N bg(k) ∧ bg(k) N bg(m) .
Plotkin Definability Theorem for Atomic-Coherent Information Systems 243
By transitivity we get ωu(λx. ⊥)(k) N bg(m), and so, all put together give
Y(ωu)(k) N bg(m) → bg(m) 6N bg(n)
for some k. But for m = n and k = 0 we have a contradiction to (5). By
transitivity of entailment and (6) we get Ω(u) N bg(n). So, Ω(u) entails all
information of Y(ωu)(0). uunionsq
Acknowledgements
Many thanks go to Helmut Schwichtenberg for his impressive knack of coming up
with insightful hints in minimal time. Sincere thanks are also due to the referees.
The author was supported by a Marie Curie Early Stage Training fellowship
(MEST-CT-2004-504029).
References
1. Plotkin, G.D.: LCF considered as a programming language. Theoret. Comput.
Sci. 5(3) (1977/78) 223–255
2. Escardo´, M.H.: PCF extended with real numbers: a domain-theoretic approach to
higher-order exact real number computation. PhD thesis, University of London,
Imperial College of Science, Technology and Medicine, Department of Computing
(1997)
3. Schwichtenberg, H.: Classifying recursive functions. In Griffor, E., ed.: Handbook
of computability theory. Volume 140 of Stud. Logic Found. Math. North-Holland,
Amsterdam (1999) 533–586
4. Normann, D.: Computability over the partial continuous functionals. J. Symbolic
Logic 65(3) (2000) 1133–1142
5. Scott, D.S.: Domains for denotational semantics. In: Automata, languages and pro-
gramming (Aarhus, 1982). Volume 140 of Lecture Notes in Comput. Sci. Springer,
Berlin (1982) 577–613
6. Schwichtenberg, H.: Recursion on the partial continuous functionals. In Dimi-
tracopoulos, C., Newelski, L., Normann, D., Steel, J., eds.: Logic Colloquium ’05.
Volume 28 of Lecture Notes in Logic., Association for Symbolic Logic (2006) 173–
201
7. Plotkin, G.D.: Tω as a universal domain. J. Comput. System Sci. 17(2) (1978)
209–236
8. Stoltenberg-Hansen, V., Lindstro¨m, I., Griffor, E.R.: Mathematical theory of do-
mains. Volume 22 of Cambridge Tracts in Theoretical Computer Science. Cam-
bridge University Press (1994)
9. Kara´dais, B.A.: Atomicity, coherence of information, and pointfree structures.
Submitted (2008)
10. Winskel, G.: Event structures. In: Advances in Petri nets 1986, Part II (Bad
Honnef, 1986). Volume 255 of Lecture Notes in Comput. Sci. Springer, Berlin
(1987) 325–392
11. Zhang, G.Q.: dI-domains as prime information systems. Inform. and Comput.
100(2) (1992) 151–177
12. Santocanale, L.: A nice labelling for tree-like event structures of degree 3. In:
CONCUR 2007 – Concurrency Theory. Volume 4703 of Lecture Notes in Computer
Science. Springer, Berlin (2007) 151–165
244 Basil A. Kara´dais
Appendix
Scott topology, cartesian closure, syntax and semantics. We collect here
material which is preliminary to the previous exposition and hints at the the-
oretical background of the present work. Again, one may further refer to [6]
and [3] (corresponding results of the latter, which refers to the flat case, are easy
to adapt to our setting).
The set of ideals in an acis carries a natural topology based on the cones
above its formal neighborhoods. In particular, for every acis α, Kglα forms the
basis of a topology on Ideα, called Scott topology. A collection of ideals U ⊆ Ideα
is an open set in the Scott topology if and only if u ⊆ v implies v ∈ U for all u ∈ U
(Alexandrov condition), and for all u ∈ U there is a U ⊆f u so that U ∈ U (Scott
condition), or equivalently, if U = ⋃U∈U ∇U . Let f : Ideα → Ideβ be a mapping
between ideals, or an ideal mapping. It is monotone when it preserves inclusion,
that is, u ⊆ v → f(u) ⊆ f(v). It is (Scott) continuous if and only if either
of the following equivalent conditions holds: (a) the mapping is monotone and
satisfies the principle of finite support, that is, if b ∈ f(u) then b ∈ f(U) for some
U ⊆f u; (b) the mapping is monotone and commutes with directed unions, that
is, f
(⋃
u∈D u
)
=
⋃
u∈D f(u) for all directed sets D of ideals of α. Easy examples
of continuous ideal mappings are identity and constant mappings, compositions
of continuous mappings, application of ideals and the standard projections piα,
piβ of cartesian products α× β.
Let r ∈ Ideα→β be an ideal and f : Ideα → Ideβ a continuous ideal mapping.
Define a continuous mapping cm(r) : Ideα → Ideβ and an ideal set is(f) ∈ Ideα→β
respectively by
b ∈ cm(r)(u) :⇔ ∃
U∈Conα
. U ⊆f u ∧ (U, b) ∈ r ,
(U, b) ∈ is(f) :⇔ b ∈ f(U).
Indeed, through these assignments we obtain that the continuous mappings from
Ideα to Ideβ are exactly the ideals of α → β [6]. Since is (cm(r)) = r and
cm (is(f)) = f , we identify r with cm(r) and f with is(f), and rely on the
context to prevent ambiguity. Moreover, we allow ourselves to write f : α → β
meaning f : Ideα → Ideβ , or equivalently, f ∈ Ideα→β .
Let α, β and γ be acis with Tα∩Tβ = ∅. For every pair f : γ → α, g : γ → β of
continuous mappings, there exists a unique continuous mapping h : γ → α × β
such that f = piα ◦ h and g = piβ ◦ h, given by h(u) := (f(u), g(u)) for all
u ∈ Ideγ (universal property of cartesian products). Define the cartesian product
f × g : α × β → γ × δ of two continuous mappings f : α → γ and g : β → δ,
where Tα ∩ Tβ = ∅, by f × g(u, v) := (f(u), g(v)). A mapping f : α× β → γ is
continuous if and only if it is continuous in each component, that is, if and only
if all sections fvα : α → γ, v fixed and all sections fuβ : β → γ, u fixed, defined
by fvα(u) := f(u, v) and f
u
β (v) := f(u, v) are continuous.
Let α, β, and γ be acises. Define the evaluation mapping eval : (α→ β)×α→
β by eval(f, u) := f(u) and the currying mapping curry : (α× β → γ)→ (α→
Plotkin Definability Theorem for Atomic-Coherent Information Systems 245
(β → γ)) by curry(f)(u, v) := f(u, v); it is direct to see that the currying
mapping is bijective. The evaluation and currying mappings are continuous. All
of the above facts together provide that the acises together with continuous
mappings between their function spaces form a cartesian closed category, with
the triple (∅,∅,∅) being the terminal information system.
The types we consider are N and B as base types, function types α → β
and product types α × β. We build simply-typed λ-terms based on typed vari-
ables uα, vα, wα, . . . and typed constants cα by application (Mα→βNα)β and
λ-abstraction (λuα. Mβ)α→β . As expected, we associate each type to the cor-
responding acis. For every λ-term Mα whose variables are among uα11 , . . . , u
αn
n
there is an ideal [[M ]] : α1 × · · · × αn → α such that
[[ui]](u1, . . . , un) = ui ,
[[MN ]](u1, . . . , un) = [[M ]](u1, . . . , un) ([[N ]](u1, . . . , un)) ,
[[λu. M ]](u1, . . . , un)(v) = [[M ]](u1, . . . , un, v) .
Acises vs. other structures. Let α = (T,Con,`) be a Scott information
system in the sense of, say, [8]. Call it atomic if, whenever U ` b, it is {a} ` b
for some a ∈ U , and coherent if for all U ⊆f T , it is U ∈ Con if and only if
{a, b} ∈ Con for all a, b ∈ U . It is an easy exercise to see that a Scott information
system which is atomic and coherent is exactly an acis in our sense. Moreover,
let D = (D,≤,⊥) be a domain (in the sense again of [8]). Call it coherent if
lubi∈I ui ∈ Dc exactly when lub {ui, uj} ∈ Dc for all i, j ∈ I. One can show
the following [9]. (a) Given an arbitrary information system α, one can define
an atomic information system αa and a coherent information system αc, so
that Ideα ∼= Ideαa and Ideα ⊆ Ideαc . So, to the extend that one is concerned
with the ideals induced by an information system, one can choose to work with
their more practical atomic version at no cost. On the other hand, coherent
information systems are generally idealwise richer. (b) The domains induced by
coherent information systems are exactly the coherent domains.
As pointed out by a referee, a structure that holds a strong resemblance to
acises—more accurately, to Scott information systems in general—is an event
structure in the sense of [10], a fact that has already been noticed elsewhere in
the literature. Apart from the obvious difference in terms of axiomatization (for
instance, in an event structure there is no ‘propagation of consistency through
enabling’ required) the difference really lies in their quite disjoint perpective
and intended meaning: information systems describe computation “logically”
whereas event structures do it “temporally” [11]. Bringing the relation down to
coherence, it is interesting that coherent event structures have also been consid-
ered in the literature (for example in [12]); these seem to correspond exactly to
our acises. It is an interesting question whether these could interrelate in any
fruitful way in the future.
Safety Properties Verification for Pfaffian
Dynamics
Margarita Korovina1 and Nicolai Vorobjov2
1 The University of Manchester, UK, and IIS SB RAS, Novosibirsk, Russia
korovina@brics.dk,
http://www.brics.dk/~korovina
2 Department of Computer Science, University of Bath, UK
nnv@cs.bath.ac.uk,
http://www.bath.ac.uk/~masnnv
Abstract. In this paper we propose an algorithm for verification of
safety properties of Pfaffian dynamical systems. We also show that this
algorithm has an elementary (doubly-exponential) upper complexity bound.
1 Introduction
One of the important problems in the theory of dynamical systems is understand-
ing of the behavior of a dynamical system with respect to safety properties. In
other words it would be desirable for a given dynamical system to verify a safety
property which states that ”something bad does never happen”, for examples,
the power plant will never blow up, the reactor temperature will never exceed
100o C.
In mathematical settings this problem is formalised in the following way. We
consider a continuous dynamical system γ : G1 × T → G2, where G1 ⊆ IRk1 is
a set of control parameters, T is an interval of time and G2 ⊆ IRk2 is a state
space. Let U be a set of control parameters. A safety property is formalised by
an invariant which given by a condition Φ on the states and requires that Φ
holds for all reachable states under the control U , i.e. ∀x ∈ U∀t ∈ TΦ(γx(t)).
In this case we say the subset U ⊆ G1 satisfies the invariant Φ and the dynamic
γ is safety under the control U. We assume that dynamical systems, invariants
and sets of control parameters are defined by Pfaffian functions, either implic-
itly (via triangular systems of ordinary differential equations) or explicitly (by
means of equations and inequalities involving Pfaffian functions). Such functions
naturally arise in applications as real analytic solutions of triangular first order
partial differential equations with polynomial coefficients, and include polyno-
mials, algebraic functions, exponentials, and trigonometric functions in appro-
priate domains. Pfaffian functions form the largest natural class of real analytic
functions which have a uniform description and an explicit characterisation of
complexity of their representations in terms of formats.
Our goal is to characterise the subsets of control parameter space which sat-
isfy a given invariant. In order to achieve our goal we use encoding trajectories
Safety Properties Verification for Pfaffian Dynamics 247
of a Pfaffian dynamical system by finite words [3, 7] and cylindrical cell decom-
position for semi-Pfaffian sets [10, 4]. Based on this technique we construct an
algorithm for safety properties verification for Pfaffian dynamical systems with
an elementary exponential upper bound.
2 Basic Definitions and Notions
2.1 Pfaffian functions and related sets
In this section we overview the theory of Pfaffian functions and sets definable
with Pfaffian functions. The detailed exposition can be found in the survey [4].
Definition 1. A Pfaffian chain of the order r ≥ 0 and degree α ≥ 1 in an open
domain G ⊂ IRn is a sequence of real analytic functions f1, . . . , fr in G satisfying
differential equations
∂fj
∂xi
= gij(x, f1(x), . . . , fj(x)) (1)
for 1 ≤ j ≤ r, 1 ≤ i ≤ n. Here gij(x, y1, . . . , yj) are polynomials in x =
(x1, . . . , xn, y1, . . . , yj) of degrees not exceeding α.
A function
f(x) = P (x, f1(x), . . . , fr(x)),
where P (x, y1, . . . , yr) is a polynomial of a degree not exceeding β ≥ 1, the se-
quence f1, . . . , fr is a Pfaffian chain of order r and degree α, is called a Pfaffian
function of order r and degree (α, β).
It is worth noting, apart from polynomials, the class of Pfaffian functions
includes real algebraic functions, exponentials, logarithms, trigonometric func-
tions, their compositions, and other major transcendental functions in appropri-
ate domains (see [4, 5]). Now we introduce classes of sets definable with Pfaffian
functions. In the case of polynomials they reduce to semialgebraic sets whose
quantitative and algorithmic theory is treated in [1].
Definition 2. A set X ⊂ IRn is called semi-Pfaffian in an open domain G ⊂ IRn
if it consists of the points in G satisfying a Boolean combination of some atomic
equations and inequalities f = 0, g > 0, where f, g are Pfaffian functions having
a common Pfaffian chain defined in G. A semi-Pfaffian set X is restricted in G
if its topological closure lies in G.
Definition 3. A set X ⊂ IRn is called sub-Pfaffian in an open domain G ⊂ IRn
if it is the image of a semi-Pfaffian set under a projection into a subspace.
In the sequel we will be dealing with the following subclass of sub-Pfaffian
sets.
248 Margarita Korovina and Nicolai Vorobjov
Definition 4. Suppose I¯ ⊂ IR is a closed interval. Consider the closed cube
I¯m+n in an open domain G ⊂ IRm+n and the projection map pi : IRm+n → IRn.
A subset Y ⊂ I¯n is called restricted sub-Pfaffian if Y = pi(X) for a restricted
semi-Pfaffian set X ⊂ I¯m+n.
Note that a restricted sub-Pfaffian set need not be semi-Pfaffian.
Definition 5. Consider a semi-Pfaffian set
X :=
⋃
1≤i≤M
{x ∈ IRn| fi1 = 0, . . . , fili = 0, gi1 > 0, . . . , giji > 0} ⊂ G, (2)
where fis, gis are Pfaffian functions with a common Pfaffian chain of order r and
degree (α, β), defined in an open domain G. Its format is the tuple (r,N, α, β, n),
where N ≥ ∑1≤i≤M (li + ji). For n = m + k and a sub-Pfaffian set Y ⊂ IRk
such that Y = pi(X), its format is the format of X.
Remark 1. In this paper we are concerned with complexities of computations,
as functions of the format. In the case of Pfaffian dynamical systems these sizes
and complexities also depend on the domain G. So far our definitions imposed
no restrictions on an open set G, thus allowing it to be arbitrarily complex and
to induce this complexity on the corresponding semi- and sub-Pfaffian sets. To
avoid this we will always assume in the context of Pfaffian dynamical systems
that G is “simple”, like IRn, or In for open I ⊆ IR.
Remark 2. In this paper we construct and examine complexities of algorithms
for verification of safety properties. In order to estimate the “efficiency” of a
computation we need to specify more precisely a model of computation. As such
we use a real number machine which is an analogy of a classical Turing machine
but allows the exact arithmetic and comparisons on the real numbers. Since we
are interested only in upper complexity bounds for algorithms, there is no need
for a formal definition of this model of computation (it can be found in [2]). In
some of our computational problems we will need to modify the standard real
number machine by equipping it with an oracle for deciding feasibility of any
system of Pfaffian equations and inequalities. An oracle is a subroutine which
can be used by a given algorithm any time the latter needs to check feasibility.
We assume that this procedure always gives a correct answer (“true” or “false”)
though we do not specify how it actually works. An elementary step of a real
number machine is either an arithmetic operation, or a comparison (branching)
operation, or an oracle call. The complexity of a real number machine is the
number of elementary steps it makes in the worst case until termination, as a
function of the format of the input.
In the special case of semialgebraic sets, the oracle can be replaced by a
proper real number machine, so the algorithm for checking of satisfiability of an
invariant can be realized as a standard real number machine.
Safety Properties Verification for Pfaffian Dynamics 249
2.2 Cylindrical Cell Decompositions
Now we define cylindrical decompositions of semi- and sub-Pfaffian sets in a
cube I¯n, where I¯ is a closed interval.
Definition 6. A cylindrical cell in I¯n is defined by induction as follows.
1. A cylindrical 0-cell in I¯n is an isolated point.
2. A cylindrical 1-cell in I¯ is an open interval (a, b) ⊂ I¯.
3. For n ≥ 2 and 0 ≤ k < n a cylindrical (k+1)-cell in I¯n is either a graph of a
continuous bounded function f : C → IR, where C is a cylindrical (k+1)-cell
in I¯n−1 and k < n− 1, or else a set of the form
{(x1, . . . , xn) ∈ I¯n| (x1, . . . , xn−1) ∈ C and
f(x1, . . . , xn−1) < xn < g(x1, . . . , xn−1)},
where C is a cylindrical k-cell in I¯n−1, and f, g : C → I¯ are continuous
bounded functions such that f(x1, . . . , xn−1) < g(x1, . . . , xn−1) for all points
(x1, . . . , xn−1) ∈ C.
Definition 7. A cylindrical cell decomposition D of a subset A ⊂ I¯n with re-
spect to the variables x1, . . . , xn is defined by induction as follows.
1. If n = 1, then D is a finite family of pair-wise disjoint cylindrical cells (i.e.,
isolated points and intervals) whose union is A.
2. If n ≥ 2, then D is a finite family of pair-wise disjoint cylindrical cells in I¯n
whose union is A and there is a cylindrical cell decomposition of pi(A) such
that pi(C) is its cell for each C ∈ D, where pi : IRn → IRn−1 is the projection
map onto the coordinate subspace of x1, . . . , xn−1.
Definition 8. Let B ⊂ A ⊂ I¯n and D be a cylindrical cell decomposition of
A. Then D is compatible with B if for any C ∈ D we have either C ⊂ B or
C ∩B = ∅ (i.e., some subset D′ ⊂ D is a cylindrical cell decomposition of B).
Definition 9. For a given finite family f1, . . . , fN of Pfaffian functions in an
open domain G we define its consistent sign assignment as a non-empty semi-
Pfaffian set in G of the kind
{x ∈ G | fi1 = 0, . . . , fiN1 = 0, fiN1+1 > 0 . . . , fiN2 > 0, fiN2+1 < 0, . . . , fiN < 0},
where i1, . . . , iN1 , . . . , iN2 , . . . , iN is a permutation of 1, . . . , N .
Theorem 1. [5, 9] Let f1, . . . , fN be a family of Pfaffian functions in an open
domain G ⊂ IRn, G ⊃ I¯n having a common Pfaffian chain of order r, and degrees
(α, β). Then there is an algorithm (with the oracle) producing a cylindrical cell
decomposition of I¯n which is compatible with each consistent sign assignment of
f1, . . . , fN . Each cell is a sub-Pfaffian set represented as a projection of a semi-
Pfaffian set in DNF. The number of cells, the components of their formats and
the complexity of the algorithm are less than
N (r+n)
O(n)
(α+ β)(r+n)
O(n3)
.
250 Margarita Korovina and Nicolai Vorobjov
We summarize main properties of Pfaffian functions in the following propositions.
• Pfaffian functions can be considered as generalisation of algebraic functions.
• Pfaffian functions have the uniform description and the explicit characteri-
zation of complexity of their representations.
• The class of Pfaffian functions includes exp, trigonometrical functions defined
in appropriate domains, and more generally solutions of a large class of
differential equations.
• The structure IR = 〈IR,+, ∗, 0, 1, <, {f1, . . . , fn}〉, where f1, . . . , fn are Pfaf-
fian, is o-minimal, i.e. definable sets have only a finite number of connected
definable components, in the other words, it has finiteness property.
3 Pfaffian Dynamical Systems
3.1 Pfaffian Dynamics and Related Sets
We now recall definitions concerning Pfaffian dynamical systems.
Definition 10. Let G1 ⊂ IRk1 and G2 ⊂ IRk2 be open domains. A Pfaffian
dynamical system is a map
γ : G1 × (−T, T )→ G2
with a semi-Pfaffian graph, where G1 is a set of control parameters, (−T, T ) is
an interval of time, and G2 is a state space.
For a given x ∈ G1 the set
Γx = {y|∃t ∈ (−T, T ) (γ(x, t) = y)} ⊂ G2
is called the trajectory (or evolution) determined by x, and the graph
Γ̂x = {(t,y)| γ(x, t) = y} ⊂ (−T, T )×G2
is called the integral curve determined by x.
Definition 11. Let U ⊆ G1. A set Inv ⊆ G2 is called invariant under the
dynamical system γ and the control U if for all x ∈ U and for all t ∈ T ,
γx(t) ∈ Inv.
In the next sections we investigate the behavior of a Pfaffian dynamical system
with respect to a given semi-Pfaffian invariant.
3.2 Encoding Trajectories by Words
We now introduce, following [3, 7], a technique of encoding trajectories of dynam-
ical systems by words. Consider a Pfaffian dynamical system γ : G1×(−T, T )→
G2, where G1 ⊂ IRk1 and G2 ⊂ IRk2 are open domains, and a partition P :=
{P1, . . . , Ps} of G2 into s semi-Pfaffian sets Pj . Let the graph of γ and each set
Safety Properties Verification for Pfaffian Dynamics 251
Pj have a format (r,N, α, β, n), where n ≥ k1+k2+1, and all Pfaffian functions
involved have a common Pfaffian chain. Fix x ∈ G1. Define the set of points and
open intervals in IR:
Fx := {J | J is a point or an interval in (−T,T) maximal w.r.t. inclusion for the
property ∃i ∈ {1, . . . , s}∀t ∈ J (γ(x, t) ∈ Pi)}.
Let the cardinality |Fx| = r and y1 < · · · < yr be the set of representatives of Fx
such that γ(x, yj) ∈ Pij . Then define the word ω := Pi1 · · ·Pir in the alphabet
P. Informally, ω is the list of names of elements of the partition in the order they
are visited by the trajectory Γx. In our setting ω is called the type of trajectory
Γx. Introduce the set of words Ω := {ω| x ∈ G1}.
Theorem 2. [3, 7] The set Ω is finite and the number of different trajectory
types of γ with respect to the partition P is less than
(sN)(r+n)
O(n)
(α+ β)(r+n)
O(n3)
(3)
Theorem 3. There is a cell decomposition of the control parameter space G1
such that if x1 and x2 belong to the same cell then Γx1 and Γx2 are labelled by
the same word.
Proof. Consider the family F = {f1, . . . , fk} of Pfaffian functions in the domain
G1 × (−T, T ) × G2 consisting of all functions in variables x, t,y involved in
the defining formulas for the graph of the map γ : (x, t) 7→ y, and for all
sets Pj . According to Theorem 1, there is a cylindrical decomposition D of
G1 × (−T, T ) × G2 with respect to the variables x, t,y having the following
properties.
1) D is compatible with each consistent sigh assignment of f1, . . . , fk.
2) There are at most (3) cylindrical cells.
3) Each of these cells is sub-Pfaffian.
4) D induces a cylindrical decomposition on G1 which we denote by E .
We claim that for any cell C ∈ E and any two points x1,x2 ∈ C the trajec-
tories Γx1 , Γx2 ∈ G2 are intersecting sets P1, . . . , Ps in the same order (i.e., are
encoded by the same word from Ω). Indeed, let pi : G1 × (−T, T )×G2 → G1 be
the projection on G1. The decomposition D induces cylindrical decompositions
D1 and D2 on pi−1(x1) and pi−1(x2) respectively. In particular, each of the in-
tegral curves Γ̂x1 and Γ̂x2 is decomposed into a sequence of alternating points
and open intervals. Due to basic properties of cylindrical decomposition, there
is a natural bijection ψ : D1 → D2 such that
(i) the restriction of ψ to the set of all cells in Γ̂x1 is a bijection onto the set of
all cells in Γ̂x2 ;
(ii) for each 1 ≤ j ≤ s the restriction of ψ to the set of all cells in (−T, T )×Pj ∩
pi−1(x1) is a bijection onto the set of all cells in (−T, T )× Pj ∩ pi−1(x2).
252 Margarita Korovina and Nicolai Vorobjov
(iii) the bijection ψ preserves the order in which cells appear in the trajectories.
It follows that if a cell B ∈ D1 is a subset of Γ̂x1∩((−T, T )×Pj) for some 1 ≤ j ≤
s, then ψ(B) ⊂ Γ̂x2∩((−T, T )×Pj). Moreover, if for cells B1, B2 ∈ D1 there exist
t1, t2 ∈ (−T, T ) such that t1 < t2 and γ(x1, t1) ∈ B1 ∧ γ(x1, t2) ∈ B2 then there
exist t′1, t
′
2 ∈ (−T, T ) such that t′1 < t′2 and γ(x2, t′1) ∈ ψ(B1)∧γ(x2, t′2) ∈ ψ(B2).
The claim is proved.
It follows that the cardinality of Ω does not exceed the cardinality of E which
does not exceed the cardinality of D which in turn is at most (3).
4 An Algorithm for Safety Properties Verification
Consider a Pfaffian dynamical system γ : G1 × (−T, T ) → G2, a semi-Pfaffian
subset of control parameters U ⊆ G1 and a semi-Pfaffian invariant Inv ⊆ G2.
Let the graph of γ and the sets U, Inv have a format (r,N, α, β, n), and all
Pfaffian functions involved have a common Pfaffian chain.
Theorem 4. There is an algorithm which checks whether the control U satisfies
the invariant Inv. The complexity of this algorithm does not exceed
(sN)(r+n)
O(n)
(α+ β)(r+n)
O(n3)
(4)
Proof. First the algorithm produces the set of wordsΩ corresponding to the Pfaf-
fian dynamical system γ : G1 × (−T, T ) → G2 and the partition P = {P1, P2},
where P1 = Inv and P2 = G2 \ Inv. Consider the family of Pfaffian functions
in the domain G1 × (−T, T ) × G2 consisting of all functions in variables x, t,y
involved in the defining formulas for the graph of the map γ : (x, t) 7→ y, for the
set U , and for the partition P. According to Theorem 1, there is a cylindrical
decomposition D with respect to (x, t,y) which is compatible with this family
and consists of at most (4) cylindrical cells.
This cell decomposition D induces the cell decomposition E (see the proof of
Theorem 3). Using the oracle, which decides feasibility of any system of Pfaffian
equations and inequalities, the algorithm selects the cells from D which are sub-
sets of {(x, t,y)|y = γ(x, t)}. Denote the set of the selected cells by B. Observe
that for any fixed x′ ∈ G1 the set
⋃
B∈B B∩{(x, t,y)|x = x′} coincides with the
integral curve Γ̂x′ . Then the algorithm determines the order in which the cells
B ∈ B intersected with {(x, t,y)| x = x′} appear in the trajectory Γx′ .
More precisely, for each pair of distinct cellsB1, B2 ∈ B the algorithm decides,
using the oracle, whether
∃x∃t1∃t2∃y1∃y2 ((x, t1,y1) ∈ B1 ∧ (x, t2,y2) ∈ B2 ∧ (t1 < t2)).
For a given C ∈ E , after all pairs of cells are processed we get the ordered set of
cells B1, . . . , Bk in D such that for any 1 ≤ i ≤ k and any x′ ∈ C the sequence
of points and intervals
B1 ∩ {(x, t,y)| x = x′}, . . . , Bk ∩ {(x, t,y)| x = x′}
Safety Properties Verification for Pfaffian Dynamics 253
forms the integral curve Γ̂x′ . By the definition of cylindrical decomposition, for
any pair Bi, Pj either Bi ⊂ (C × (−T, T )× Pj) or Bi ∩ (C × (−T, T )× Pj) = ∅.
The algorithm uses the oracle to decide for every pair which of these two cases
takes place. As the result, the sequence B1, . . . , Bk becomes partitioned into
subsequences of the kind
(B1, . . . , Bk1), (Bk1+1, . . . , Bk2), . . . , (Bk`−1+1, . . . , Bk),
where for any i, 0 ≤ i ≤ `− 1, the cells Bki+1, . . . , Bki+1 lie in C× (−T, T )×Pji
for some ji, while Bki∩C×(−T, T )×Pji = ∅ and Bki+1+1∩C×(−T, T )×Pji = ∅.
Then the word ω := Pj0 · · ·Pj`−1 corresponds to the cell C. Considering all cells
in E the algorithm finds Ω.
Then the algorithm collects all cells from E such that their union is U . If all
of these cells corresponds to the word P1, then γ is safety under the control U .
This completes the description of the algorithm.
A straightforward analysis shows that the complexity of the algorithm does
not exceed (4), taking into account the bounds from Theorem 1.
References
1. S. Basu, R. Pollack and M.-F. Roy, Algorithms in Real Algebraic Geometry,
Springer, Berlin-Heidelberg, 2003.
2. L. Blum, , F. Cucker, M. Shub, and S. Smale, Complexity and Real Computation,
Springer, New York, 1997.
3. T. Brihaye, C. Michaux, C. Riviere, C. Troestler, On o-minimal hybrid systems, in:
Hybrid Systems: Computation and Control, R. Alur, G. J. Pappas, (Eds.), LNCS,
2993, Springer, Heidelberg, 2004, 219–233.
4. A. Gabrielov, N. Vorobjov, Complexity of computations with Pfaffian and Noethe-
rian functions, in: Normal Forms, Bifurcations and Finiteness Problems in Dif-
ferential Equations, Yu. Ilyashenko et al., (Eds.), NATO Science Series II, 137,
Kluwer, 2004, 211–250.
5. A. Gabrielov, N. Vorobjov, Complexity of cylindrical decompositions of sub-
Pfaffian sets, J. Pure and Appl. Algebra, 164, 1–2, 2001, 179–197.
6. A. Khovanskii, Fewnomials, Number 88 in Translations of Mathematical Mono-
graphs. American Mathematical Society, Providence, RI, 1991.
7. M. Korovina and N. Vorobjov, Pfaffian hybrid systems. In Springer Lecture Notes
in Comp. Sci., volume 3210 of Computer Science Logic’04, 2004, 430–441.
8. M. Korovina and N. Vorobjov, Upper and lower Bounds on Sizes of Finite Bisim-
ulations of Pfaffian Hybrid Systems. In Proceedings of CiE’06, invited talk, LNCS
3988, 2006, 235–241.
9. S. Pericleous, N. Vorobjov, New complexity bounds for cylindrical decompositions
of sub-Pfaffian sets, in: Discrete and Computational Geometry. Goodman-Pollack
Festschrift, B. Aronov et al. (Eds.), Springer, 2003, 673–694.
10. L. van den Dries, Tame Topology and O-minimal Structures, Number 248 in
London Mathematical Society Lecture Notes Series. Cambridge University Press,
Cambridge, 1998.
On Extending Wand’s Type Reconstruction
Algorithm to Handle Polymorphic Let?
Sunil Kothari and James L. Caldwell
Department of Computer Science
University of Wyoming
Laramie, WY 82071-3315, USA
{skothari,jlc}@cs.uwyo.edu
Abstract. We have extended Wand’s type reconstruction algorithm to
polymorphic let by extending the constraint language and by using a
multi-phase unification algorithm in the constraint solving phase. We
show the correctness of our approach by extending the Wand’s sound-
ness and completeness results. We have validated our approach against
other popular type reconstruction algorithms by implementing OCaml
prototypes and running them on non-trivial examples.
1 Introduction
The general type reconstruction problem can be formulated as:
Given a well-formed term M without any types, does there exist a
type τ and a type environment1 Γ such that a judgment Γ ` M : τ is
valid ?
Type reconstruction is a popular feature in modern functional programming
languages. Underlying any type reconstruction algorithm is a set of rules encod-
ing a type system. One of the most widely used type systems is the Hindley-
Milner (HM) type system, first mentioned in [Mil78] by Milner, but discov-
ered independently by Hindley [Hin69]. Various type reconstruction algorithms
[Mil78,DM82,LY98] have been proposed to implement the HM type system.
Many of these algorithms are characterized by intermittent constraint gener-
ation and constraint solving. But over the years, focus has shifted to algorithms
having a clear separation of constraint generation and constraint solving phases
[Hee05,PR05,Wan87]. This separation leads to better error messages [Hee05]
when the constraint set is unsatisfiable (since a larger set of constraints is avail-
able to reason about the error). Moreover, the separation provides a clean ab-
? This material is based upon work supported by the National Science Foundation
under Grant No. NSF CNS-0613919.
1 We assume the initial type environment is empty since we are dealing with closed
terms.
On Extending Wand’s Type Reconstruction Algorithm 255
straction of the various substitution-based algorithms2 since most well known
algorithms are specific instances of various constraint solving strategies.
The type inference involving polymorphic let construct is a non-trivial prob-
lem. In fact, in the worst case, it is a DEXPTIME3-complete and PSPACE-
hard problem [PJ89,Mai89] in the level of nested lets. Moreover, the litera-
ture on constraint-based type reconstruction is sparse and uneven. For example,
Pierce’s book [Pie02] has no references on how to handle ML-Let construct in
constraint-based algorithms, HM(X) [SOW97,PR05] requires specialized knowl-
edge, whereas Aiken and Wimmers [AW93] use subtyping constraints. Further-
more, none of the literature, in our view, describes it as a direct extension to
well known Wand’s algorithm [Wan87].
The Helium compiler [HLI03] is known for giving good quality error mes-
sages and a very simple constraint representation is used for handling the let
construct4. This paper describes an approach where Helium’s constraint repre-
sentation is used to handle let polymorphism. Our approach builds upon Wand’s
algorithm, and our proofs rely on the soundness and completeness of Wand’s
algorithm. We have validated our approach with some of the known type recon-
struction algorithms [Kot07]. In summary, our contributions are:
1. A new algorithm extending Wand’s algorithm [Wan87] to include polymor-
phic let.
2. New soundness and completeness proofs for Wand’s system and the extended
system using a novel desugaring of polymorphic lets.
The rest of this paper is organized as follows: Section 2 reviews the previous
methods for inferring the type of the let construct. Section 3 introduces the
concepts and terminologies needed for this paper. Section 4 gives an overview
of Wand’s algorithm and states soundness and completeness theorems. Section
5 describes the changes needed for the extension. Section 6 gives an overview of
the correctness proofs. Section 7 summarizes our current work.
2 Literature Review
We review some of the constraint-based algorithms in their handling of the
let construct. A detailed survey of substitution-based algorithms is available in
[Kot07]. Wand [Wan87] looked at the type inference problem as a type-erasure:
whether it is decidable that a term of the untyped lambda calculus is the image
under type-erasing of a term of the simply typed lambda calculus, and presented
a type reconstruction algorithm. This was the first successful attempt at sepa-
rating constraint generation from constraint solving phase. However, extending
the algorithm to handle the let construct remained a future work5. Heeren
2 Throughout this paper we term substitution-based algorithms as those algorithms
which intermix constraint generation and constraint solving, whereas algorithms
with a clear separation are termed constraint-based algorithms.
3 DTIME(2n
O(1)
)
4 Personal communication from Bastiaan Heeren.
5 Personal communication from Mitchell Wand.
256 Sunil Kothari and James L. Caldwell
[Hee05] suggested three constraint representations to handle the let construct;
each equally expressive but differing in constraint solving.
Approach 1. Qualification of type constraints: Type schemes contain a
constraint component as part of its type as shown by the following grammar:
σ ::= ∀−→α .σ | C ⇒ τ
For example, an expression λx.x can be assigned a type τ1 → τ2 under the
constraint τ1 ≡ τ2. So the type of the expression is ∀τ1, τ2.τ1 ≡ τ2 ⇒ τ1 → τ2.
In many ways, this approach is similar to HM(X)[SOW97], Pottier and Re´my’s
account of ML type inference [PR05], and Jones’s qualified types [Jon95].
Approach 2. Type scheme variables as placeholders: The type constraint
language is extended to take into account generalization and instantiation of
type schemes by means of type scheme variables. The constraint language is
given by the following grammar:
C ::= τ1 ≡ τ2 | σ := GEN(Γ, τ) | τ := INST (σ)
Our approach uses a slightly modified version of the above constraint represen-
tation. We know of no published account of the constraint solving phase for
this representation, although Heeren does mention in his thesis that a special
substitution that maps type scheme variables to type schemes is needed for this
representation. Our algorithm does not require any such substitution.
Approach 3. Using implicit instance constraints: This approach merges
the generalization and instantiation constraints in the above approach. The con-
straint language is given by the following grammar:
C ::= τ1 ≡ τ2 | τ1 ≤M τ2
This representation is described, in detail, in Heeren’s thesis [Hee05].
3 Preliminaries
In this paper, we assume familiarity with the notion of types, functional pro-
gramming and the lambda calculus. The terms considered here are pure untyped
lambda terms given by the following grammar:
Λ ::= x | MN | λx.M
We follow the usual conventions for lambda calculus: arrow types associate to
the right, function applications associate to the left and application binds more
tightly than abstraction. The types for untyped lambda terms is given by the
following grammar:
τ ::= α | τ1 → τ2
The type is either a type variable or a function type. We call any expression
formed by following the above grammar as a type expression. We follow the
convention that α, β denote type variables, whereas τ denotes a type expression.
A type environment, denoted by Γ , maps type variables to type expressions. The
set of free type variables of a type expression τ is denoted by FTV(τ) and those
of a type environment Γ is denoted by FTV(Γ ). A term and its type is related
by an assertion, denoted by Γ `M : τ , where M is a term, τ is a type and Γ is
a type environment. We denote type environment where x is not in the domain
of Γ by Γ\x. A derivation of an assertion Γ `M : τ is a finite tree of assertions,
On Extending Wand’s Type Reconstruction Algorithm 257
where the root is Γ ` M : τ . Every interior node in the tree is related to its
parent by an instance of one of the non-axiom type rules and every leaf node is
an instance of an axiom type rule. In this paper, we consider three different type
systems: the Hindley-Milner type system illustrated in Fig. 1, Wand’s system
illustrated in Fig. 2, and the extended Wand system illustrated later in Section
5. We denote a judgment in a particular system by a subscript. For example,
Γ `HM M : τ is a HM judgment.
Γ `HM x : τ where x : τ ∈ Γ (HM-Var)
Γ\x ∪ {x : τ1} `HM M : τ0
Γ `HM λx.M : τ1 → τ0 (HM-Abs)
Γ `HM M : τ1 → τ Γ `HM N : τ1
Γ `HM MN : τ (HM-App)
Fig. 1. Hindley-Milner type system
Γ, {α e= τ} `W x : α where x : τ ∈ Γ and α is fresh (W-Var)
(Γ\x) ∪ {x : α}, E `W M : β
Γ,E ∪ {τ e= α→ β} `W λx.M : τ where α, β are fresh (W-Abs)
Γ, E1 `W M : α→ τ Γ, E2 `W N : α
Γ, E1 ∪ E2 `W MN : τ where α is fresh (W-App)
Fig. 2. Wand’s type system
A substitution is a mapping from type variables to type expressions. Let ρ1, ρ2
be two substitutions then substitution composition ρ1◦ρ2 is defined extensionally
as ∀α.(ρ1 ◦ ρ2)αdef= ρ1(ρ2α), where α is a type variable. Substitution composition
is associative but non-commutative. A substitution ρ is idempotent if ρ ◦ ρ =
ρ. We treat all the substitutions as idempotent substitutions. A type τ ′ is a
substitution instance of a type τ if and only if τ ′ = ρτ , for some substitution ρ.
Substitution application to a type environment Γ , denoted by ρΓ , is defined as
{x : ρτ | x : τ ∈ Γ}.
In Wand’s system, constraints plays a central role. Specifically, Wand’s algo-
rithm generates equality constraints, referred to as e-constraints, and are denoted
by τ1
e= τ2, where τ1, τ2 are type expressions. An e-constraint τ1
e= τ2 is solvable
if there exists a substitution ρ such that ρτ1 = ρτ2. More formally, we denote
solvability of a constraint by |= (read solve). We write ρ |= τ1 e= τ2, if ρτ1 = ρτ2.
A type judgment in Wand’s system is given as Γ,E `W M : α, where E denotes
a constraint set. Sometimes we elide the constraint component of the judgment
to simplify the presentation. In that case, we denote a judgment by Γ `W M : τ ,
where E is implicit and so there is a substitution generated by solving E such
that ρα = τ .
258 Sunil Kothari and James L. Caldwell
4 Wand’s Algorithm
Next we briefly discuss Wand’s algorithm [Wan87] and state the soundness
and completeness theorems. Central to Wand’s algorithm is the notion of action
table, which takes as input a type system and a term and generates equational
constraints. For untyped lambda calculus, the type system is shown in Fig. 2.
Let G denote a set of assertions (also called goals) and E a set of equational
constraints. Then the algorithm sketch is given as:
Input. A term M0 of Λ.
Initialization. Set E = ∅ and G = {(Γ0,M0, α0)}, where α0 is a fresh type variable
and Γ0 is an empty environment.
Loop Step. If G = ∅ then return E else choose a sub-goal (Γ,M, τ) from G, delete
the sub-goal from G and add to E and G new verification conditions and subgoals
generated by the action table6.
Solve Constraints. Unify constraints in E.
We can now describe the soundness and completeness results as stated by Wand,
but before that we introduce some terminology. We denote a goal by a 3-tuple,
i.e. (Γ,M, τ), and we write ρ |= g i.e. ρ |= (Γ,M, τ) if and only if ρΓ `HM M :
ρτ . We write ρ |= G if and only if ∀g ∈ G. ρ |= g. Similarly, if E is a set of
e-constraints, we write ρ |= E if and only if ∀e ∈ E. ρ |= e. Finally, we say ρ
solves (E,G), where (E,G) result from applying Wand’s algorithm, if and only
if ρ |= E and ρ |= G.
(Soundness) ∀ρ. ρ |= (E,G)⇒ ρΓ0 `HM M0 : ρτ0
(Completeness) Γ `HM M0 : τ ⇒ (∃ρ. ρ |= (E,G) ∧ Γ = ρΓ0 ∧ τ = ρτ0)
We have reformulated Wand’s statement of completeness and soundness and
made them more abstract (by eliminating the goal set). The reformulated theo-
rems are used later in the proofs of soundness and completeness of the extended
proof system. Our statement of the soundness and completeness are given below:
Theorem 1 (Soundness). If there is a derivation of Γ0, E `W M0 : τ0 gener-
ating constraint set E then, for any ρ such that ρ |= E, ρΓ0 `HM M0 : ρτ0 is
derivable.
Theorem 2 (Completeness). If there is a derivation of Γ `HM M0 : τ , then
for any ρ, τ0, Γ0, such that dom(ρ) = (FTV (Γ0) ∪ {τ0}), ρΓ0 = Γ , and ρτ0 = τ
then there exists a derivation of Γ0, E `W M0 : τ0, and there exists a substitution
ρ′ such that ρ ⊆ ρ′ and ρ′ |= E.
Both these theorems are proved in [KC07].
5 Extended Wand’s Algorithm
In this section we describe the changes needed to incorporate the let con-
struct. First, we enrich our term and type language. We call the extended lan-
guage Core-ML [MH88], and it is defined as:
Core-ML ::= x | MN | λx.M | let x =M in N
6 See Section 5 for action table behavior for untyped lambda calculus.
On Extending Wand’s Type Reconstruction Algorithm 259
The let expression let x =M in N is not merely a syntactic sugar for (λx.N)M .
For instance, the term let i = λx.x in i i is typable but the desugared term
(λi.i i)(λx.x) is not typable. This is true for both Haskell and ML type recon-
struction algorithms. To handle polymorphic types introduced by let-expressions,
the type syntax is extended with type scheme variable and type scheme as shown
below:
σ ::= α∗ | ∀−→α .τ
A type scheme, denoted by ∀−→α .τ , is a type where zero or more type variables
are universally quantified. We denote a type scheme variable by annotating a
type variable with a “*”. Generalizing a type τ with respect to a type environ-
ment Γ entails quantifying over the free variables of τ that are not free in Γ .
Thus gen(Γ, τ)def=∀−→α .τ , where −→α = FTV (τ)−FTV (Γ ). On the other hand, in-
stantiation of a type scheme involves replacing the quantified variables by fresh
type variables and is given by inst(∀−→α .τ)def= τ [α1 := β1, . . . , αn := βn], where
β1, . . . , βn are fresh type variables.
Next, we extend the constraint language C to include two other kinds of
constraints as shown below:
C ::= τ
e
= τ | τ s=Γ α∗ | α∗ i= τ
(e-constraint) (s-constraint) (i-constraint)
A s-constraint τ s=Γ α∗ expresses the fact that α∗ denotes a type scheme obtained
by generalizing a type τ with respect to the environment Γ . An i-constraint
α∗ i= τ expresses the fact that τ is constrained to be an instantiated value of the
type scheme denoted by α∗. Wand’s type system is now extended with two rules
to account for i&s-constraints. The new type system is called extended Wand’s
system and a judgment in the extended system is denoted by the subscript W+.
(W-Var-i) where x : τ ∈ Γ
Γ, {α∗ i= τ} `W+ x : α∗
Γ,E1 `W+ M : α1 (Γ\x) ∪ {x : α2∗}, E2 `W+ N : τ
(W-Let) where α1, α∗2 are fresh
Γ,E1 ∪ E2 ∪ {α1 s=Γ α2∗} `W+ let x =M in N : τ
Apart from extending the type rules, we also had to extend the notion of satisfia-
bility and substitution application to a constraint. First, we describe some nota-
tions used in the description of satisfiability. We use the notation Eα∗ to denote a
set of i-constraints related7 to a s-constraint τ0
s=Γ α∗. From this point onwards,
we think of a s-constraint and related i-constraint as a pair (τ0
s=Γα∗, Eα∗); the
first component being the s-constraint and the second component being the list
of related i-constraint(s). We use the symbol ≤ to express the notion of an in-
stance. Specifically, τ ≤ σ expresses the fact that τ is an instance of σ in the
sense that τ is obtained by instantiating all the bound variables of σ. This no-
tion can then be used to express the satisfiability of i&s constraints. We say
7 A s-constraint is related to an i-constraint if they share the same type scheme vari-
able.
260 Sunil Kothari and James L. Caldwell
ρ satisfies (τ0
s=Γ α∗, Eα∗) if ∀(α∗ i= τ1) ∈ Eα∗ . ρ τ1 ≤ ρ(gen(Γ, τ0)). Substitu-
tion application to a pair of s-constraint and related i-constraints is defined as:
ρ(τ0
s=Γ α∗, Eα∗)
def=(ρτ0
s=ρΓ α∗, {ρτ i= α∗ | (τ i= α∗) ∈ Eα∗}).
Next, we sketch the constraint generation phase8 for Core-ML. The algorithm
sketch remains the same except that E now is a list9 of equational constraints
instead of a set. The behavior of action table for Core-ML is same as that for
untyped lambda calculus except for the variable (a slight modification) and the
let case as shown below:
Case (Γ, x, τ0). If x is bound to a type scheme variable α
∗ in Γ , i.e. x : α∗ ∈ Γ , then
add α∗ i= τ0 to E else add τ0
e
= τ1 (where x : τ1 ∈ Γ ) to E.
Case (Γ,MN, τ0). Let α be a fresh type variable. Generate subgoals (Γ , M, α→ τ0)
and (Γ,N, α), and add to G.
Case (Γ, λx.M, τ0). Let α and β be two fresh type variables. Generate equation
τ0
e
= α→ β and sub-goal ((Γ\x) ∪ {x : α},M, β), and add to E & G respectively.
Case (Γ,let x =M in N, τ0). Let α1, α
∗
2 be fresh type variables. Append
10
El@[α1
s
=Γ α
∗
2]@Er to list E, where El, Er are obtained by recursively calling the
extended algorithm on (Γ,M,α1) and ((Γ\x) ∪ {x : α∗2}, N, τ0) respectively.
The next few paragraphs highlight the constraint solving phase. This phase
consists of two distinct unification phases: Phase I and Phase II. We first give an
informal description of both the phases and follow it with a formal description. In
the first phase, e-constraints are unified. Note that if there are no i&s-constraints,
i.e. the term is a pure lambda term, then our Phase I mirrors the constraint
solving phase for Wand’s algorithm. In the second phase, a s-constraint and
related i-constraints are chosen and transformed to e-constraints and unified
using the Phase I unification. Let E = Ee@Ei&s be the constraint list obtained
from the constraint generation phase, where Ee denotes a list containing e-
constraints, Ei&s denotes a list containing i&s-constraints. The constraint solving
algorithm, SOLVE, integrates the two unification phases as follows:
SOLVE(E) =
let ρ1 = unify1Ee in
ρ1 ◦ (unify2 ρ1(Ei&s))
The first phase, unify1 is defined as:
unify1(E) =
unify1 ((α
e
= β) :: E) = if α = β then unify1 E
unify1 ((α
e
= τ) :: E) = if α occurs in τ then raise Failure
unify1 ((α
e
= τ) :: E) = (α 7→ τ) ◦ unify1 (E[α := τ ])
unify1 ((τ
e
= α) :: E) = unify1 ((α
e
= τ) :: E)
unify1 ((τ1 → τ2 e= τ3 → τ4) :: E) = unify1 ((τ1 e= τ3) :: ((τ2 e= τ4) :: E))
8 We denote this constraint generation phase by Wand+ in Fig. 3.
9 This is needed to preserve the order of s-constraints.
10 This will ensure that we solve the leftmost innermost let first.
On Extending Wand’s Type Reconstruction Algorithm 261
The second phase, unify2, is defined as:
unify2 (E
′@E) = let ρ1 = unify1E′′
in ρ1 ◦ unify2 (ρ1E)
where E′′ = {inst(gen(Γ, τ1)) e= τ2 | (α∗ i= τ2) ∈ Eα∗}
and E′ = [(τ0
s
=Γ α
∗),Eα∗ ]
unify2 [ ] = Id
The first phase of our constraint solving algorithm is very similar to the
algorithm proposed by Martelli and Montanari [MM82], which is known to be
exponential11 in the size of the input in the worst case [BS01]. Therefore, the
first phase of the unification is linear while our implementation, which does not
use the efficient DAG representation, is exponential in the size of the input in the
worst case. The second phase of the algorithm involves calling Phase I algorithm
as many times as the number of s-constraints, i.e., the number of let-bound
variables and there can be only O(n) of those. Therefore, in the worst case, the
total time required by the algorithm is O(2n), where n is the size of the term.
6 Extension Correctness
For the correctness result, we need a function to transform polymorphic lets
to pure lambda terms since Wand’s type system has no notion of let. We call
this function ptol. Note that ptol is a type and value preserving transformation
[KC07]. We introduce two additional notations before formally describing ptol.
First, we use N [x] to denote one hole (denoted by [ ]) in a context (denoted
by N) filled with the let-bound variable (x in this case). Second, we use the
notation |FV (N)|x to denote the count12 of free occurrence of x in N . Notice
that we transform only the body of the let since we assume the let-binding is
let-free. This assumption is strictly not necessary since there is a type and value
preserving transformation mentioned by Mairson[Mai89], which can make a let-
binding let-free. We use the assumption to simplify our proofs but its certainly
not a restriction on our algorithm. With the above notations, we describe ptol
below:
ptol(x) = x
ptol(λx.M) = λx. ptol(M)
ptol(MN) = (ptol(M) ptol(N))
ptol(let x =M in N [x]) = letN1 = ptol(N) in if |FV (N) |x ≤ 1
(λx.N1[x])M
ptol(let x =M in N [x]) = letN1 = ptol(N) in if |FV (N) |x > 1
ptol(let x1 =M in let x =M in N1[x1])
where x1 is a fresh variable.
11 Linear if the input and output are represented as directed acyclic graph (DAG)
[PW76].
12 We use the conservative notion of occurrence of a let-bound variable rather than the
actual types to differentiate between a monomorphic and polymorphic let.
262 Sunil Kothari and James L. Caldwell
The correctness is given by the proof sketch in Fig. 3. The most complicated
proof was showing that the extended type system was sound and complete with
respect to the Hindley-Milner type system via Wand’s type system. The detailed
arguments involved in the correctness, the proofs of various theorems and lem-
mas, and a detailed discussion on the novel desugaring of polymorphic lets to
pure lambda terms can be found in [KC07].
-






BB
B
B
B
B
B
BBN
?
Extended Type System
Γ,E `W+ M : α is derivable ⇔
Γ,E `W ptol(M) : α is derivable ⇔
Γ `HM ptol(M) : τ is derivable
Main Result
Wand+(Γ,M,α) infers the
principal type for M under Γ
Constraint Generation
Wand+(Γ,M,α) =
−→
E ⇔
Γ,E `W+ M : α is derivable
Constraint Solving
Γ,E `W+ M : α is derivable ⇒
SOLVE(
−→
E )Γ `HM ptol(M) :SOLVE(−→E )α
is derivable
Fig. 3. Correctness proof overview
7 Conclusions and Future Work
The extension of Wand’s algorithm is a non-trivial extension and requires
careful handling of constraints. Our algorithm is a direct extension of Wand’s
algorithm and our soundness and completeness proofs rely on completeness and
soundness of Wand’s algorithm. The main idea behind our algorithm is to pre-
serve the order of generated s-constraints (as described in the action table for
Core-ML) and use a multi-phase unification, while preserving this order, in the
constraint solving phase. We have validated our approach by running the exam-
ples mentioned in this paper on Alg. W [Mil78,DM82], Alg. M [LY98] and Alg.
J [Mil78]. An implementation of our algorithm and other popular type recon-
struction algorithms in OCaml is available online at http://www.cs.uwyo.edu/
∼skothari. We are working on a formalization of the proofs in CoQ.
Acknowledgments
Thanks to Bastiaan Heeren for a detailed response to author’s query regard-
ing the constraint representations mentioned in his thesis. We also want to thank
anonymous referees for their detailed comments and suggestions (on an earlier
draft of this paper), which greatly improved the presentation of this paper.
On Extending Wand’s Type Reconstruction Algorithm 263
References
[AW93] A. Aiken and E. L. Wimmers. Type inclusion constraints and type inference.
In Functional Programming Languages and Computer Architecture, pages 31–
41, 1993.
[BS01] F. Baader and W. Snyder. Unification theory. In A. Robinson and
A. Voronkov, editors, Handbook of Automated Reasoning, volume I, chap-
ter 8, pages 445–532. Elsevier Science, 2001.
[DM82] L. Damas and R. Milner. Principal type-schemes for functional programs. In
POPL ’82, pages 207–212, New York, 1982. ACM Press.
[Hee05] B. Heeren. Top Quality Type Error Messages. PhD thesis, Universitiet
Utrecht, 2005.
[Hin69] J. R. Hindley. The principal type-scheme of an object in combinatory logic.
Trans. American Math. Soc, 146:29–60, 1969.
[HLI03] B. Heeren, D. Leijen, and A. IJzendoorn. Helium, for learning haskell. In
Haskell ’03: Proceedings of the 2003 ACM SIGPLAN workshop on Haskell,
pages 62–71, New York, NY, USA, 2003. ACM Press.
[Jon95] M. P. Jones. Qualified types: theory and practice. Cambridge University Press,
New York, NY, USA, 1995.
[KC07] S. Kothari and J. L. Caldwell. Wand’s Algorithm Extended for the Polymor-
phic ML-Let. Technical report, University of Wyoming, 2007.
[Kot07] S. Kothari. Type Reconstruction Algorithms: A Survey. Technical report,
University of Wyoming, 2007.
[LY98] O. Lee and K. Yi. Proofs about a folklore let-polymorphic type infer-
ence algorithm. ACM Transactions on Programming Languages and Systems
(TOPLAS), 20(4):707–723, 1998.
[Mai89] H. G. Mairson. Deciding ML typability is complete for deterministic expo-
nential time. In Proc. of the 16th ACM Sym. Principles of Programming
Languages, pages 382–401, 1989.
[MH88] J. C. Mitchell and R. Harper. The essence of ML. In POPL ’88: Proceedings of
the 15th ACM SIGPLAN-SIGACT symposium on Principles of programming
languages, pages 28–46, 1988.
[Mil78] R. Milner. A theory of type polymorphism in programming. Journal of
computer and system sciences, pages 348–375, 1978.
[MM82] A. Martelli and U. Montanari. An efficient unification algorithm. ACM Trans.
Program. Lang. Syst., 4(2):258–282, 1982.
[Pie02] B. C. Pierce. Types and Programming Languages. MIT Press, 2002.
[PJ89] P.C.Kanellakis and J.C.Mitchell. Polymorphic unification and ML typing.
In 6th ACM SIGPLAN-SIGACT symposium on Principles of programming
languages, pages 105–115. ACM Press, 1989.
[PR05] F. Pottier and D. Re´my. The essence of ML type inference. In Benjamin C.
Pierce, editor, Advanced Topics in Types and Programming Languages, chap-
ter 10, pages 389–489. MIT Press, 2005.
[PW76] M. S. Paterson and M. N. Wegman. Linear unification. In STOC ’76: Pro-
ceedings of the eighth annual ACM symposium on Theory of computing, pages
181–186, New York, NY, USA, 1976. ACM.
[SOW97] M. Sulzmann, M. Odersky, and M. Wehr. Type inference with constrained
types. In Fourth International Workshop on Foundations of Object-Oriented
Programming (FOOL 4), 1997.
[Wan87] M. Wand. A simple algorithm and proof for type inference. Fundamenta
Informaticae, 10:115–122, 1987.
Simulations Between Tilings
Gre´gory Lafitte1 and Michael Weiss2,?
1 Laboratoire d’ Informatique Fondamentale de Marseille (LIF), CNRS –
Aix-Marseille Universite´, 39, rue Joliot-Curie, F-13453 Marseille Cedex 13, France
2 Centre Universitaire d’ Informatique, Universite´ de Gene`ve, Battelle baˆtiment A
7 route de Drize, CH-1227 Carouge, Switzerland
Abstract. Wang tiles are unit size squares with colored edges. To know
whether a given finite set of Wang tiles can tile the plane while respecting
colors on edges is undecidable. Robinson’s tiling is a self-similar tiling
where the computation of a Turing machine can be carried out. We
introduce finer notions of simulation between tilings, giving rise to a finer
notion of universality. We show how to use this construction to be able
to simulate a tile set by another tile set through Turing machines. We
study the computability of some problems related to simulation. Finally,
we show how to build a tile set that can simulate all the periodic tile
sets.
1 Introduction
Wang was the first to introduce in [13] the study of tilings with colored tiles. A
tile is a unit size square with colored edges. Two tiles can be assembled if their
common edge has the same color. To tile consists in assembling tiles from a tile
set (a finite set of tiles) on the grid Z2.
Wang conjectured that if a tile set tiles the plane, then it tiles it in a periodic
way. In [3], Berger was the first to construct an aperiodic tile set, i.e., a tile set
which tiles the plane only in an aperiodic way. This proof showed that Wang’s
conjecture is false and that the domino problem is undecidable. This domino
problem asks if one can decide whether given a tile set, there exists a tiling of
the plane generated by this tile set. Simplified constructions of aperiodic tile sets
can be found in [12] and later [1].
The main argument of this proof was to simulate the behavior of a given
Turing machine with a tile set, in the sense that the Turing machine M stops on
an instance w if and only if the tile set τ〈M,w〉 does not tile the plane. Therefore,
one of the most important fact concerning tilings is that tilings can constitute a
Turing equivalent computation model. This computation model is particularly
relevant as a model of computation on the plane.
Tilings have been studied for different purposes: some researchers have used
tilings as a tool for studying mathematical logical problems [1], others have
studied the different kinds of tilings that one tile set can produce [5,6,12,10], or
defined tools to quantify the regular structure of a tiling [7,2].
? This author has been supported by the FNS grant 200020-113257.
Simulations Between Tilings 265
The most used construction to simulate Turing machines with tile sets is
Robinson’s tiling. In [12], Robinson has built a tile set that generates only self-
similar aperiodic tilings. The construction lays on a hierarchy of squares of ever
increasing sizes. In each of this square, some zones can be used to simulate the
behavior of a Turing machine. We show how to use this construction to be able
to simulate a tile set by another tile set through Turing machines. In [10], notions
of simulation and reduction between tilings and tile sets have lead to notions of
universality for tilings and completeness for tile sets. From these definitions of
simulation, we define finer notions of simulation between tile sets (totally and
exactly). These new notions of simulation give rise to a finer notion of universal-
ity. We use our construction to study the computability of problems related to
simulation. Then we use this construction to build tile sets that generate tilings
with specific properties, e.g., universality for a certain class.
The main result of the paper is to use Robinson’s construction to build a
recursive aperiodic tile set that totally simulates each and every periodic tile
set and only them, i.e., a tile set that is totally universal for periodic tile sets.
This is an interesting tile set since the problem to know whether a tile set is
periodic is undecidable. We then generalize this construction and give sufficient
conditions for tile sets classes to be totally simulated by one tile set.
In section 2, we recall the basic notions of tilings and simulation between tile
sets and give two definitions of simulation, the total and the exact simulations,
and a definition of universality, which are finer than the ones introduced in [10].
In section 3, we show how we can simulate a Turing machine which stops on
every input independently of the time and space needed for the computation.
We use our construction to simulate a tile set with another tile set through the
computation of a Turing machine in Robinson’s tiling . In section 4, we study
the computability of problems related to simulation and show that the problem
to know whether a recursive tiling P reduces to another recursive tiling Q is
Σ2-complete. In the last section, we build a tile set that totally simulates each
and every periodic tile set and only them. We then generalize this result to give
sufficient conditions for a tile set class to be totally simulated by one tile set.
2 Notions of simulation
We start by recalling the basic notions of tilings. A tile is an oriented unit size
square with colored edges from C, where C is a finite set of colors. A tile set
is a finite set of tiles. To tile consists in placing the tiles of a given tile set on
the grid Z2 such that two adjacent tiles share the same color on their common
edge. Since a tile set can be described with a finite set of integers, then we can
enumerate the tile sets, and τi designates the ith tile set.
Let τ be a tile set. A tiling P generated by τ is called a τ -tiling. It is associated
to a tiling function fP where fP (x, y) gives the tile at position (x, y) in P . When
we say that we superimpose the tiles of a tile set τ on the tiles of a tile set τ ′,
we mean that for any tile t ∈ τ and any tile t′ ∈ τ ′, we build a tile u = t × t′
where the colors of the sides of u are the cartesian product of the colors of the
266 Gre´gory Lafitte and Michael Weiss
sides of t and t′. Then two tiles u1 = t1 × t′1 and u2 = t2 × t′2 match if and only
if t1 and t2 match and t′1 and t
′
2 match.
Different notions of reduction have been introduced in [10]. We recall some
of the notions relative to these reductions and we refer the reader to this paper
for detailed explanations and properties.
A pattern is a finite tiling. If it is generated by τ , we call it a τ -pattern. A
finite set of rectangular τ -patterns of same size is a τ -pattern set. By analogy
with tilings, to tile with a pattern set consists in placing the patterns on a regular
subgrid of Z2 in such a way that the connection between two patterns respects
the local constraint of color matching. We call a tiling P generated by a pattern
set M , a M -tiling. If M is a set of τ -patterns, then for any M -tiling P , there
exists a τ -tiling Q that is a representation of P at the unit tile level.
From this remark we obtain notions of simulation. We say that our pattern
tiling P simulates a tiling P ′ if there exists a mapping R from the patterns of P
to the tiles of P ′ such that if we replace the patterns of P by their corresponding
tiles given by R, then we obtain P ′. In such a case, we write P ′ PR P and say
that P ′ reduces to P .
In [10], the following definitions were introduced: a tiling P is strongly uni-
versal if for any tile set τ , there exists a τ -tiling Q such that Q P P and a tile
set τ is complete if for any tile set τ ′ and any τ ′-tiling Q there exists a τ -tiling
P such that Q P P .
The following definitions present finer notions of simulation:
Definition 1. Let τ and τ ′ be two tile sets. We say that τ totally simulates τ ′
if there exist a, b ∈ Z and a reduction R from the a× b patterns of τ to the tiles
of τ ′ such that the two following conditions are respected:
1. for any τ ′-tiling Q, there exists a τ -tiling P such that Q PR P ,
2. for any τ -tiling P , there exists a τ ′-tiling Q such that Q PR P .
We denote it by τ ′ Pt τ (or τ ′ PRt τ to specify the reduction R).
If τ ′ Pt τ , then there exists a reduction R such that any τ -tiling can be cut
in rectangle patterns of size a × b such that if one replaces these patterns by
their corresponding tiles given by R then one obtains a τ ′-tiling. And the set of
all τ ′-tilings that reduce to a τ -tiling is exactly the set of all τ ′-tilings. The total
simulation is thus more specific than the simulation introduced in [10]. In this
way, τ can be seen as a tile set which computes in a same way than τ ′.
To be able to study these notions of simulation, we now describe some specific
aspects of the classical Robinson construction that we will use later on.
3 Some specific aspects of Robinson’s construction
We study here the implication of the complexity of Turing machines on their
simulation by Robinson’s construction.
Simulations Between Tilings 267
Since Berger’s proof of the undecidability of the domino problem, we know
how to simulate a Turing machine with a tiling. For a detailed explanation of
the construction, we refer the reader to [1].
We now show that the simulation in Robinson’s tiling of the computation of
a given Turing machine, which stops on all input, does not depend on the time
and space needed for the computation.
Notice that, a problem could occur if, given a Turing machine M with L(M)
recursive, we wanted to simulate in the nth square of Robinson’s tiling (the
square of size 22n − 1) the computation of M on the nth word. Since in the
nth square of Robinson’s tiling , we have a square of size 2n + 1 dedicated to
the computation, if M uses more than 2n + 1 times or spaces to compute the
result, then the space×time diagram of M on this nth input will not fit in the
nth square.
Without loss of generality, we can suppose that M stops after (2n+1)×s(n)
spaces and k × (2n + 1) × s(n) times. If we simulate this Turing machine in
Robinson’s tiling stretched vertically by a factor k, i.e., where we have replaced
the tiles of Robinson’s tile set by rectangular patterns of size 1 × k, then the
space×time diagram of the computation ofM on the nth word will enter exactly
in the square of size ((2n + 1)× s(n))× (k × (2n + 1)× s(n)). We have to force
the computation to be carried out in this rectangle.
The first line of a rectangle of computation in Robinson’s tiling is filled in
a non-deterministic way with a number of 1’s followed by a 2 and then by ’s
(representing the empty symbol). The goal is to check whether 1n is the greatest
input on which computation can be carried out in this rectangle. To do this, we
simulate in parallel the computation of M on the input 1n and the computation
of a modification of M , say M ′, which works as M except that a 2 is seen as a 1.
Therefore, if M computes on 1n, then M ′ simulates M computing on 1n+1. So,
1n is the greatest input whose computation can be simulated in this rectangle if
and only if the computation on 1n enters in the rectangle and if the computation
on 1n+1 does not.
To be sure that we are in this case, if the computation of M on 1n stops,
then a special color is sent by the final state to the north side of the rectangle in
which the computation is made, which forces the whole border of this rectangle
to be marked with this special color. Then we just have to add the condition
that the computation of M ′ on 1n2 matches with the special colored border if
and only if it does not reach a final state.
Therefore, a rectangle can be filled if and only if the input wrote on the first
line is the greatest input whose computation can be simulated in this rectangle.
At the end of our tiling process, our tile set simulates the computations of M
on every input.
Thus, the simulation of a Turing machine in a tiling does not depend on the
space and time needed to stop.
268 Gre´gory Lafitte and Michael Weiss
4 Computability of simulation
In this section, we study the computability of problems related to simulation.
We define the following problems:
Problem TSR (Tile sets reduction)
– Input: Tile sets τ, τ ′.
– Question: Does a τ -tiling P and a τ ′-tiling Q exist such that P P Q?
Problem C-ness (Completeness)
– Input: Tile set τ .
– Question: Is τ complete?
Problem TR (Tilings reduction)
– Input: Two recursive tilings P,Q.
– Question: Does P P Q?
We first show that the halting problem reduces to problem TR, using the
above construction. The construction of this proof can be used to prove the same
result for the other two problems. At the end of this section, we will show the
exact location of problem TR in the arithmetical hierarchy.
Theorem 1. Problem TR is undecidable.
Proof. We show that the halting problem reduces to problem TR.
Let P be a deterministic tiling, i.e., a tiling generated by a tile set that tiles
the plane in an unique way, such that P does not reduce to Robinson’s tiling.
Let M be a one-tape Turing machine and w an input word. We build a recursive
tiling Q such that P P Q⇔M(w) ↓. We consider the one-tape Turing machine
M ′ which takes as input the empty word  and has the following behavior: it
writes w on the tape and simulates M .
Let τM ′ be the tile set which simulates M ′. We superimpose τM ′ on Robin-
son’s tiling in such a way that the computation of M ′ is simulated in the free
zones of the squares of size 22n − 1 in Robinson’s tiling with the special con-
dition that if the computation reaches a final state before reaching the square
perimeter, then a special color is sent to the north side of the rectangle in which
the computation is made, which forces the whole border of the rectangle to be
marked with this special color.
Thus, a square is marked with the special color if and only if the square
is big enough to allow the computation to reach a final state. Of course, if M
stops on w using time t and space s, then the square of size 22m − 1, where
m  max(log2(t), log2(s)), is big enough to allow the computation to reach a
final state.
Therefore, if M(w) ↓, then there exists an integer n0 such that any square
of size 22n − 1 can be filled with the computation of M ′ for all n ≥ n0 and the
squares of size 22l − 1 are squares without computation for any l < n0.
Simulations Between Tilings 269
In [10], we have shown how to build a Turing machine M which generates
space×time diagrams which are isomorphic to a tile set τ . We refer the reader
to this paper for detailed explanations. Now, we consider the Turing machine
MP which simulates the tiles of the tiling P and the tile set τP which simulates
MP . We superimpose τP on τM ′ with the condition that the computation of the
tiles of P can begin if and only if the square in which the computation is made
is marked with the special color. Then a square of size 22l − 1 can be a pattern
that simulates a tile of P if and only if:
– the square is big enough to allow the simulation of M ′ to reach a final state,
– and the square is big enough to compute a tile of P .
If these two conditions are respected, then the squares of size 22l − 1 are
the core of the patterns that simulate the tiles of P . We denote by Q the tiling
generated by the tile set we have build until now, with the special condition
that any square big enough to allow the simulation of M ′ does carry out the
simulation. Since P is a deterministic tiling, then the tile can be assembled in an
unique way and thus, if only one square can be filled with a pattern representing
a tile of P , then Q simulates the tiling P . To be sure that P simulates Q, we
impose that the pattern centered around (0, 0) in Q simulates the tiles of P at
position (0, 0).
We have to check that Q is a recursive tiling, i.e., there exists a Turing
machine which given as input two integers a, b, outputs the tile of P at position
(a, b). This machine has the following behavior:
– Find the smallest square containing position (a, b) in Robinson’s tiling.
– If this position is out of a square of size 22n−1, output the tile of Robinson’s
tiling at position (a, b).
– Otherwise, compute in the square of Robinson’s tiling the tile set which
simulates M ′ superimposed on the tile set which computes the tiles of P .
– If the computation reaches a final state and gives a pattern simulating a tile
of P before reaching the square perimeter, then fill any square of size 22n−1
with the computation of the patterns simulating the tiles of P , beginning by
the one centered around the origin until we arrive to the square containing
the position (a, b).
– Output the tile at position (a, b).
– If the computation does not halt in the square, output the tile of Robinson’s
tiling at position (a, b). uunionsq
By using the same kind of construction, we show that the halting problem
reduces to problems TSR and C-ness:
Corollary 1. Problem C-ness and TSR are undecidable.
We can precisely locate problem TR in the arithmetical hierarchy. The
next result shows that in fact, problem 3 is equivalent to problem Fin =
{ i | The Turing machine with code i accepts a finite language L(Mi) } which
is Σ2-complete.
270 Gre´gory Lafitte and Michael Weiss
Theorem 2. Problem TR is Σ2-complete.
Proof. [Problem TR∈ Σ2]: Let P and Q be two recursive tilings. Then P P Q
if there exist two integers a, b such that for any integer n, the n×n pattern of P
centered around the origin reduces to the na× nb pattern of Q centered around
the origin.
Therefore, problem TR can be defined as a ∃∀ arithmetical property.
[Fin ≤ Problem TR]: Let i be the code of a Turing machine. We transform
it in a pair of recursive functions (f, g) which represent two tilings Pf and Pg
such that Pf P Pg if and only if L(Mi) is finite.
Let Pf be a deterministic tiling. We explain the behavior of the tiling function
g. We use a tile set, say τ , composed of a unicolor blue tile and four tiles with
three sides blue and one red. In [10] it has been shown that this tile set can
simulate any tile set with infinitely many different reductions. At step one, we
begin to simulate Pf(i) with τ by computing the tiles closest to (0, 0) and in
parallel of this computation, we enumerate the words accepted by Mi. If no
word is accepted, then g keeps tiling the plane with patterns that simulate Pf .
If a word is accepted, we pass to the next step.
At step n, we have already tiled a rectangular pattern A that simulates a
rectangular pattern B of Pf . We do not want anymore this pattern A to be
a simulation of the pattern B of Pf . We thus consider a larger enough square
around A that we tile with the unicolor tile to guarantee that A does not simulate
B anymore. Let A′ be this square. We start the simulation of Pf , using the
pattern A′ already tiled. In parallel, we still enumerate the words accepted by
Mi. If no word is accepted, then g goes on simulating Pf . If a new word is
accepted by Mi, we go on to the next step.
Therefore, g produces a tiling that simulates Pf if and only Mi accepts a
finite set of words. If L(Mi) is infinite, the simulation never finishes and Pg does
not simulate Pf(i). uunionsq
In the following theorem, we give an upper bound on the computability of
the C-ness and TSR problems.
Theorem 3. Problems C-ness and TSR are respectively Π3 and Σ2.
Proof. i) In [10], it has been shown that a tile set is complete if it can generate
a weakly dense universal tiling, i.e., a tiling P such that for any tile set τ ,
there exist two integers a, b and a reduction function R from the a × b
patterns of P to the tiles of τ , such that the tiling P ′, obtained by replacing
the a×b patterns of P by their corresponding tiles of τ given by R, contains
any τ -pattern.
Thus, we have the following description of the problemC-ness: τ is complete
if and only if for any tile set τ ′, there exist two integers a, b such that for
any integer n and for any τ ′-pattern m′ of size n, there exists an na × nb
τ -pattern to which m′ reduces.
Therefore, problem C-ness can be defined as a ∀∃∀ arithmetical property.
Simulations Between Tilings 271
ii) This problem can be defined as follows: there exists a τ -tiling P which
reduces to a τ ′-tiling Q if and only if there exist two integers a, b such that
for any n, there exists an n × n τ -pattern which reduces to an na × nb
τ ′-pattern.
Therefore, problem TSR can be defined as a ∃∀ arithmetical property. uunionsq
In the next section, we show how this construction of simulating a tile set
by another tile set through Turing machines can be a powerful tool to build tile
sets that generate tilings with specific properties.
5 Total simulation of all periodic tile sets
In this section, we build a recursive aperiodic tile set that totally simulates each
and every periodic tile set and only them. This tile set is thus totally universal
for periodic tile sets. This is an interesting tile set since the problem to know
whether a tile set is periodic is undecidable. We then generalize this construction
and give sufficient conditions for tile sets classes to be totally simulated by one
tile set.
Theorem 4. The set of periodic tile sets can be totally simulated by one tile
set, i.e., there exists a tile set τ such that for any periodic tile set τ ′, τ ′ Pt τ .
Proof. First of all, let f be the projection of a recursive one-one function from
N to N2 such that f(n) ≤ n. Thus, the set {f(1), f(2), . . .} contains infinitely
many times any integer. We can make the assumption that f can be computed
by a Turing machine Mf in time and space t with unary input.
We now build a one-tape Turing machine M with the following behavior:
it takes as input a word written in unary (1n), computes 1f(n) by simulating
Mf , and then simulates the tile set τf(n) to enumerate the rectangular patterns
generated by this tile set. If a periodic pattern is found, then the computation
stops and never halts otherwise.
We now simulate this Turing machine in Robinson’s tiling stretched vertically
by a factor 2. We have seen that the simulation of the computation of a Turing
machine does not depend on the time and space needed to stop. In the square
of size 2n − 1 we simulate Mf on the greatest input 1m whose computation fits
in this square. It thus outputs 1f(m). Since Robinson’s tiling is stretched with a
factor 2, then we still have remaining spaces where we can compute M .
We simulate M in this tiling with the special condition that if a final state is
reached, then a special color is sent to the north side of the rectangle in which
the computation is made, which forces the whole border of the rectangle to be
marked with this special color. If no final state is reached, then the computation
keeps going on until it matches with the north side of the rectangle and the
special color cannot be added to the sides of this rectangle.
This special color triggers the beginning of the computation of a new Turing
machine, say N , in this rectangle. The behavior of N is the following: an input
is an integer x - the index of a tile set - and an integer y - the index of a color.
272 Gre´gory Lafitte and Michael Weiss
We check whether y is a valid color of the tile set τx. If yes, we choose in a
non-deterministic way a tile of τx with south color y, and we simulate this tile,
i.e., we build a space×time diagram which is a pattern representing this tile.
We simulate N in our rectangle with the following conditions:
– The computation of N can begin if and only if the rectangle is marked with
the special color.
– The first line of the rectangle marked with the special color is filled in a
non-deterministic way with the letters of the alphabet. The computation of
N will say whether the input was valid or not, and if it is not, then the
computation stops and the tiling cannot be completed to constitute a full
tiling of the plane.
– Since the choice of x and y are made in a non-deterministic way, we have to
check that x is the code of the tile set that Mf has computed before (even
if it was not actually computed before since in our tiling the computations
are made simultaneously). To do this, the bits of x are sent to the north and
have to match with the value of 1f(n) to let the computation go on.
Therefore, at the end, the computation of N is carried out in the rectangle
if and only if M has stopped, i.e., if the tile set τf(n) is periodic, and if and only
if N has simulated a tile of τf(n).
The final stage consists in sending the colors of the simulation of the tiles of
τf(n) outside the rectangle to be sure that two Robinson rectangles simulating
the tiles of τf(n) match if and only if the tiles they represent match.
Since the index of any tile set is enumerated infinitely many times by f , then
if a tile set τ is periodic, then there exist infinitely many squares big enough to
carry out the computation of M to find a periodic τ -pattern and to stop, and
thus, to trigger the beginning of the simulation of the tiles of τ . Therefore, the
periodic tile sets are totally simulated by our tile set. uunionsq
In this proof, we have used the fact that the property is recursively enumer-
able, and that no tile set, that does not tile the plane, has the property, here
being periodic. If a tile set that does not tile the plane has the property, then
when we simulate the tiles of this tile set, we arrive at a step where no more tiles
can be simulated and the tiling process goes unfinished. We say that a property
is decent if the class of tile sets satisfying it is recursively enumerable and if this
property can not be verified by any tile set that does not tile the plane. This is
equivalent to having a recursively enumerable property such that if a tile set can
generate a pattern having this property, then it tiles the plane. Thus, we have
the following generalization:
Theorem 5. Let P be a decent property, then the set LP = { i | τi satisfies P }
can be totally simulated by one tile set.
This is equivalent to say that there exists a tile set which is totally universal
for LP . We finish this section by giving some classes of tile sets that can be
totally simulated by one tile set:
Simulations Between Tilings 273
Corollary 2. The class of tile sets that generate a period using all of its tiles
can be totally simulated by one tile set.
The next corollary shows that the class of tile sets that exactly reduces to a
tile set that tiles the plane can be totally simulated by one tile set:
Corollary 3. Let τ be a tile set that tiles the plane. The following set can be
totally simulated by one tile set:
L = { i | τi P≡ τ }
Proof. L is recursively enumerable, since it is enough to enumerate the patterns
generated by τi and to find whether they are isomorphic to the tiles of τ . Since τ
tiles the plane, no tile set that does not tile the plane satisfies the property. uunionsq
References
1. Allauzen (C.) and Durand (B.), The Classical Decision Problem, appendix
A: “Tiling problems”, p. 407–420. Springer, 1996.
2. Ballier (A.), Durand (B.) and Jeandel (E.), Structural Aspects of Tilings,
in Proceedings of the Symposium on Theoretical Aspects of Computer Science,
Dagstuhl Seminar Proceedings no 08001, p. 61–72, 2008.
3. Berger (R.), « The undecidability of the domino problem », Memoirs of the
American Mathematical Society, vol. 66, 1966, p. 1–72.
4. Cervelle (J.) and Durand (B.), « Tilings: recursivity and regularity », The-
oretical Computer Science, vol. 310, no 1-3, 2004, p. 469–477.
5. Culik II (K.) and Kari (J.), « On aperiodic sets of Wang tiles », in Founda-
tions of Computer Science: Potential - Theory - Cognition, p. 153–162, 1997.
6. Durand (B.), Levin (L. A.) and Shen (A.), « Complex tilings », in Proceed-
ings of the Symposium on Theory of Computing, p. 732–739, 2001.
7. Durand (B.), « Tilings and quasiperiodicity », Theoretical Computer Science,
vol. 221, no 1-2, 1999, p. 61–75.
8. Durand (B.), « De la logique aux pavages », Theoretical Computer Science,
vol. 281, no 1-2, 2002, p. 311–324.
9. Hanf (W. P.), « Non-recursive tilings of the plane. I », Journal of Symbolic
Logic, vol. 39, no 2, 1974, p. 283–285.
10. Lafitte (G.) and Weiss (M.), « Universal Tilings », in Proceedings of the
Symposium on Theoretical Aspects of Computer Science, Lecture Notes in
Computer Science no 4393, p. 367–380, 2007.
11. Myers (D.), « Non-recursive tilings of the plane. II », Journal of Symbolic
Logic, vol. 39, no 2, 1974, p. 286–294.
12. Robinson (R.), « Undecidability and nonperiodicity for tilings of the plane »,
Inventiones Mathematicae, vol. 12, 1971, p. 177–209.
13. Wang (H.), « Proving theorems by pattern recognition II », Bell System
Technical Journal, vol. 40, 1961, p. 1–41.
14. Wang (H.), « Dominoes and the ∀∃∀-case of the decision problem », in Pro-
ceedings of the Symposium on Mathematical Theory of automata, p. 23–55,
1962.
Discrete Non Determinism and Nash Equilibria
for Strategy-Based Games
Ste´phane Le Roux⋆
E´cole normale supe´rieure de Lyon, Universite´ de Lyon, LIP, CNRS, INRIA, UCBL
http://perso.ens-lyon.fr/stephane.le.roux/
Abstract. Not all strategic games have Nash equilibria, so Nash defined
probabilistic equilibria, which exist in all strategic games. However, the
probability approach fails for the slightly more general abstract strategic
games that are defined in this paper. Instead, this paper uses discrete
non determinism, which yields relevant notions of equilibrium with guar-
anteed existence. These equilibria are payoff-wise efficient and easy to
find. Moreover, the discrete approach still works for much more general
games supporting both sequential and simultaneous decision-making.
Key words: Abstract strategic games, games in graphs, discrete non
determinism, Nash equilibrium, fixed-point, constructive existence, effi-
cient algorithm.
1 Introduction
Not all finite strategic games have (pure) Nash equilibria. To cope with this,
Nash [8] defined probabilistic equilibria and proved their existence, i.e. he derived
from a finite game a continuous game that has a (pure) Nash equilibrium.
This paper summarises [5], but without the proofs: it generalises strategic
games by replacing real-valued payoff functions with abstract outcomes; it trans-
lates the notion of Nash equilibrium from the real-valued to the abstract setting,
by replacing the usual order over the reals with one binary relation per agent in
order to account for his preferences over outcomes, as in [9]. However like in the
real-valued case, some abstract strategic games have no Nash equilibrium.
Unlike for real-valued games, using probabilities to weaken the notion of
strategy is irrelevant in this abstract setting, as detailed in [5] or [6]. So, this
paper says that an agent does not choose (resp. may choose) a strategy instead
of saying that he chooses the strategy with probability 0 (resp. p > 0). The set of
strategies that may be chosen by an agent is named a discrete non deterministic
(nd for short) strategy, similarly to [4]. Unlike Nash’s probability approach, this
weakening keeps things discrete and finite, i.e. easy to handle.
Five relevant extensions of Nash equilibrium, named nd Nash equilibrium, are
defined as follows on the nd strategy profiles: First, define nd games and their nd
equilibria, and five embeddings of abstract strategic games into nd games. Then,
⋆ Now postdoc at INRIA-Microsoft Research. I thank Pierre Lescanne for his com-
ments on the draft of this paper, and a reviewer for his suggestions.
Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 275
define the nd Nash equilibria as the preimages of the nd equilibria by the five
embeddings respectively. In the five cases, nd Nash equilibrium existence follows
nd equilibrium existence, which follows a preliminary pre-fixed point result.
The constructive proofs of existence of this paper give (exact) algorithms
computing nd Nash equilibria. Time complexity happens to be polynomial for
the five embeddings (yet might be exponential for other embeddings). In the
real-valued setting, playing according to the computed nd Nash equilibrium may
give better average payoff than playing randomly. Together with a robustness
argument, it shows that these nd Nash equilibria can serve as recommendations,
whereas an example shows that probabilistic Nash equilibria cannot.
This paper also defines multi strategic games, i.e. graphs whose nodes are
abstract strategic games. A play in such a game is a sequence of local plays of
strategic games each of whose outcome points to the next node. Multi strategic
games can thus model within a single game both sequential and simultaneous
decision-making mechanisms. Embedding them into nd games also provides them
with a notion of non deterministic Nash equilibrium, with an existence result.
Section 2 defines abstract strategic games and their Nash equilibria; section 3
states a pre-fixed point result; section 4 defines nd games and their nd equilibria,
and states existence results; section 5 embeds abstract strategic games into nd
games in five ways, thus providing the formers with five notions of nd Nash
equilibrium, with existence results. An example and efficiency results follow;
informal section 6 defines multi strategic games and embeds them into nd games,
thus providing them with a notion of nd Nash equilibrium.
Conventions: the notation P
∆
= Q defines P as coinciding with Q.
Let E =
∏
i∈I Ei be a cartesian product. For e in E, let ei be the Ei-component
of e. Let E−i denote
∏
j∈I−{i}Ej . For e in E, let e−i be (. . . , ei−1, ei+1, . . . ) the
projection of e on E−i. For x in Ei and X in E−i, define X;x in E as (X;x)i
∆
= x
and for all j 6= i, (X;x)j
∆
= Xj .
2 Abstract Strategic Games
This section defines abstract strategic games and their Nash equilibria.
Abstract strategic games are strategic games whose real-valued payoff func-
tions are replaced with abstract outcomes. In addition for each agent, an arbi-
trary binary relation accounts explicitly for the agent’s preference over outcomes.
These relations replace the (implicit) usual total order over the real numbers.
Definition 1 (Abstract strategic games) Abstract strategic games are 4-
tuples 〈A, S, P, (⊳a)a∈A〉 where:
– A is a non-empty set of agents,
– S =
⊗
a∈A Sa is the Cartesian product of non-empty sets of strategies,
– P : S → Oc is a function mapping strategy profiles to given outcomes,
– ⊳a is a relation over outcomes, and o⊳a o
′ says that agent a prefers o′ to o.
For s and s′ in S, s⊳a s
′ stands for P (s)⊳a P (s
′), by abuse of notation.
276 Ste´phane Le Roux
Below, the left-hand strategic game is real-valued, as the right-hand one is
abstract. Both games involve agent v (with strategies v1 and v2) and agent h.
h1 h2
v1 v 7→ 0, h 7→ 2 v 7→ 1, h 7→ 1
v2 v 7→ 0, h 7→ 1 v 7→ 2, h 7→ 0
h1 h2
v1 oc1 oc2
v2 oc3 oc4
The following notion of happiness (first item of definition 2) generalises the
traditional one, i.e. they coincide on real-valued strategic games. Happiness is
also characterised using the notion of best response (second item). As usual,
Nash equilibrium is defined through happiness for all agents (third item).
Definition 2 (Happiness, Nash equilibrium) Let g = 〈A, S, P, (⊳a)a∈A〉.
Happyg(a, s)
∆
= ∀s′ ∈ S, ¬(s′−a = s−a ∧ s⊳a s
′)
Happyg(a, s) ⇔ sa ∈ BRa(s−a)
∆
= {x ∈ Sa | ∀y ∈ Sa, ¬(s−a;x⊳a s−a; y)}
Eqg(s)
∆
= ∀a ∈ A, Happyg(a, s)
For instance in the abstract strategic game before definition 2, the profile
(v1, h1) is a Nash equilibrium iff ¬(oc1⊳v oc3), ¬(oc1⊳h oc2), ¬(oc1⊳v oc1),
and ¬(oc1⊳h oc1). Also, note that there is a natural embedding of real-valued
strategic games into abstract ones that preserves and reflects Nash equilibria.
3 A Preliminary Pre-Fixed Point Result
This section proves a pre-fixed-point result, i.e. the existence of a y such that
y  F (y) for all F and  that comply with given constraints.
First, meet semi-lattices are defined below like in the literature.
Definition 3 (Meet semi-lattice) A meet semi-lattice is a partially ordered
set (S,) each of whose 2-element subset has a greatest  lower bound.
In the rest of this section, (S,) is a generic meet semi-lattice with least
element m; inf is the infimum function related to ; F is a generic function
from S to S. As defined below, a meeting point is an x in the lattice such that
every sequence decreasing from x is not too much ”scattered” by F .
Definition 4 (Meeting point) Let x be in S − {m}. If for all x1 . . . xn such
that m 6= x1  · · ·  xn  x, we have inf(F (x1), . . . , F (xn), x) 6= m, then x is
said to be a F -meeting point, and one writes MF (x).
The F -meeting point predicate is preserved by the greatest lower bound
operator used with the meeting point and its image by F , as stated below.
Lemma 5 MF (x) ⇒ MF ◦ inf(x, F (x)).
If there is a F -meeting point but no infinite strictly decreasing sequence,
iteration of lemma 5 yields a non-trivial pre fixed point of F , as stated below.
Lemma 6 If  is well-founded and if there exists a F -meeting point, then there
exists a F pre fixed point different from m, i.e. y  F (y) for some y 6= m.
Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 277
4 Non Deterministic Games
This section defines non deterministic games, nd games for short, and their nd
equilibria. Three results of existence of nd equilibrium follow. In this paper, these
definitions and results are merely general intermediate tools.
Informally, nd games generalise real-valued or abstract strategic games in
three ways: First, a nd game involves a set I of ”strategic games”. Second, for
each i in I, each agent a selects one or more ”pure strategies” in Sia. This discrete
non determinism amounts to forgetting the probabilities of a probabilistic strat-
egy while remembering whether or not they equal zero. Third, there is no payoff
function or abstract outcome, but each agent is given a best-response function
telling what his best nd strategy against his opponents’ nd strategies is.
Definition 7 (Nd games) A nd game is a pair 〈(Sia)
i∈I
a∈A, (BRa)a∈A〉 where:
– I is a non-empty set of indices and A is a non-empty set of agents.
– For all a in A, BRa is a function from Σ−a to Σa,
where Σ =
⊗
a∈AΣa and Σa =
⊗
i∈I P(S
i
a)−{∅} and Σ−a =
⊗
a′∈A−{a}Σa′ .
Elements of Σa (resp. Σ) are called nd strategies for a (resp. nd strategy profiles).
In the rest of this section, g = 〈(Sia)
i∈I
a∈A, (BRa)a∈A〉 is a generic nd game, σ
is a generic nd profile in the corresponding Σ, and a is a generic agent.
In definition 8, agent a is happy with σ if his own nd strategy σa is in-
cluded in his nd best response against his opponents’ combined nd strategies,
i.e. BRa(σ−a). So, informally, an agent is happy if any ”pure strategy” that he
selects is a ”pure best response” against others’ nd strategies. This inclusion gen-
eralises the membership that is used in happiness characterisation of definition 2.
Like for Nash equilibria, nd equilibria amount to happiness for all agents.
Definition 8 (Happiness and non deterministic equilibrium)
Happyg(σ, a)
∆
= σa ⊆ BRa(σ−a) ⇔ ∀i ∈ I, σa,i ⊆ (BRa(σ−a))i ∈ P(S
i
a)
Eqg(σ)
∆
= ∀a ∈ A, Happyg(σ, a)
Eqg(σ) ⇔ σ ⊆ BR(σ) where BR(σ)
∆
=
⊗
a∈ABRa(σ−a)
Below, a meet lattice is identified within the nd game, and invoking lemma 6
yields a first equilibrium existence result.
Lemma 9 The poset (Σ ∪ {∅},⊆) is a meet semi-lattice with least element ∅,
and if there exists a BR-meeting point in Σ, there exists a nd equilibrium for g.
The next equilibrium existence result invokes lemma 9 and the mentionned
σ being a BR-meeting point. Lemma 10 is invoked in the rest of the paper.
Lemma 10 Assume that for all agents a, for all γ1 . . . γn in Σ−a, if γn ⊆ · · · ⊆
γ1 ⊆ σ−a then ∩1≤k≤nBRa(γk)∩σa 6= ∅. Then, the game g has a nd equilibrium.
Also, equilibria are preserved when ”increasing” the best response functions.
Lemma 11 Let g′ = 〈(Sia)
i∈I
a∈A, (BR
′
a)a∈A〉 be another nd game such that for
all a in A, for all γ in Σ−a, BRa(γ) ⊆ BR
′
a(γ). Then Eqg(σ) ⇒ Eqg′(σ).
278 Ste´phane Le Roux
5 Non Deterministic Nash Equilibria
Like real-valued strategic games, not all abstract ones have Nash equilibria;
unlike for real-valued strategic games, using probabilities to weaken the notion
of strategy is irrelevant for abstract ones, as detailed in [5]. This section invokes
the concept of discrete non determinism of section 4 instead. Informally, a nd
strategy for an agent of an abstract strategic game is a non-empty subset of
the set of his (pure) strategies. Then, if a strategy belongs to the nd strategy,
the agent may eventually choose it; if it does not, he will not. Conceptually, nd
strategies lie between strategies and probabilistic strategies, while being simpler
than the latters. Indeed, a probabilistic strategy induces a nd strategy, but it
further specifies that the agent may choose strategies with given probabilities.
By embeddings into nd games, this section provides abstract strategic games
with five notions of non deterministic Nash equilibrium. Existence results follow.
this section also discusses the complexity of finding such equilibria, gives an
example, and shows efficiency of these equilibria.
Definitions and existence: The following definition extends orders over func-
tions’ codomains to orders over functions, in a Pareto style.
Definition 12 Let f and f ′ be functions of type A→ B, and let A′ be a subset
of A. Let ≺ be an irreflexive relation over B, and let  be its reflexive closure.
f A
′
f ′
∆
= ∀x ∈ A′, f(x)  f ′(x)
f ≺A
′
f ′
∆
= f A
′
f ′ ∧ ∃x ∈ A′, f(x) ≺ f ′(x)
For instance if both functions are represented by vectors of naturals with
the usual total order, (1, 1) < (1, 2), (0, 2) < (1, 2) and (0, 2) < (1, 3), but
(2, 0) 6< (1, 3) . The extension preserves strict partial ordering, as stated below.
Lemma 13 If ≺ is a strict partial order over B, then the derived ≺A over
functions of type A→ B is also a strict partial order.
The following lemma is proved by induction on the mentionned n.
Lemma 14 Let E be a finite set of functions of type A → B. Let ≺ be
irreflexive and transitive over B, and let  be its reflexive closure. For any
∅ 6= A0 ⊆ · · · ⊆ An ⊆ A there is an f that is maximal in E wrt all the ≺
Ai .
In the rest of this subsection g = 〈A, S, P, (⊳a)a∈A〉 is a generic abstract
strategic game, a is a generic agent, and γ is a generic element in the correspond-
ing Σ−a. Also, the ⊳a are strict partial orders, i.e. irreflexive and transitive.
Five embeddings of g into nd games are defined below. The five images of
g have nd equilibria by lemmas 10, 11 and 14. This provides abstract strategic
games with five definitions of non-deterministic Nash equilibrium and guarantees
of existence. In the notation s⊳γa s
′ below, the strategies s and s′ are seen as
functions from Sa to the outcomes and γ corresponds to A
′ in definition 12.
Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 279
Lemma 15 For i between 0 and 4, 〈(Sa)a∈A, (BR
i
a)a∈A〉 is a nd game (derived
from g), and it has a nd equilibrium that is named nd Nash equilibrium for g.
BR0a(γ)
∆
= {s ∈ Sa | ∀s
′ ∈ Sa, ¬(s⊳
S−a
a s′) ∧
∀s′ ∈ Sa, ¬(s⊳
γ
a s
′) ∧
∃c ∈ γ,∀s′ ∈ Sa, ¬(s⊳
{c}
a s′) }
BR1a(γ)
∆
= {s ∈ Sa | ∀s
′ ∈ Sa, ¬(s⊳
S−a
a s′) }
BR2a(γ)
∆
= {s ∈ Sa | ∀s
′ ∈ Sa, ¬(s⊳
γ
a s
′) }
BR3a(γ)
∆
= {s ∈ Sa | ∃c ∈ γ,∀s
′ ∈ Sa, ¬(s⊳
{c}
a s′) }
BR4a(γ)
∆
= {s ∈ Sa | ∀s
′ ∈ Sa, ∃c ∈ γ, ¬(s⊳
{c}
a s′) }
The BRia for i = 1, 2, 3 correspond tho the three conjuncts of BR
0
a. (Note
that BR1a(γ) does not depend on γ.) Also, BR
4
a is even more generous than
BR2a. Depending on the application, a user of this theory may choose either of
these definitions or design his own definition of nd Nash equilibrium. Note that
BR2 and BR4 somewhat relate to the notions of dominated strategy, studied
in [2] and [7], and rationalizability, studied in [1] and [10]. These notions are
more recently discussed in [3], for instance.
The rest of the subsection discusses a few properties of these equilibria. First,
the equilibria related to BR1 form a simple structure, as stated below.
Lemma 16 The nd equilibria related to BR1 are the elements of Σ that are
included in
⊗
a∈A{s ∈ Sa | ∀s
′ ∈ Sa, ¬(s⊳
S−a
a s′)}.
The following lemma states that the nd Nash equilibria related to BR3 or
BR4 define a Cartesian union lattice.
Lemma 17 If equilibrium is defined through either BR3 or BR4, then
Eqg(σ) ∧ Eqg(σ
′) ⇒ Eqg(σ ∪
× σ′),
where σ ∪× σ′ is the smallest element of Σ containing both σ and σ′.
Algorithmic time complexity: The constructive proof of lemma 15 yields
algorithms finding a nd Nash equilibrium for each BRi. Computations relate to
iteration of lemma 5, i.e. successive calls to BRi starting with γ = S, the full
nd strategy profile. For the five BRi, overall complexity in calls to preferences
is polynomial in the number of profiles |S|: indeed, a call to BRi dismisses at
least one strategy of one agent (unless the nd profile is a nd Nash equilibrium),
so there are at most |S| such calls; each of them calls the BRia which decides
whether or not each strategy of agent a is a best response (if agent a has only
one strategy, no need to call BRia); there are at most |S| such decisions per call
to BRi. For BR0, this decision requires at most 3×|S| calls to a ⊳a. So, finding
a BR0-equilibrium (similarly BRi-equilibrium) is at most cubic in |S|.
280 Ste´phane Le Roux
Example: The strategic game below uses naturals: the profile (v1, h2) induces
payoff 3 for agent v and 2 for h and preferences are the usual order over naturals.
Let us apply the BR0-equilibrium algorithm to the game, starting with the full
nd profile ({v1, v2, v3, v4, v5}, {h1, h2, h3, h4, h5}), as meant by the underlining.
h1 h2 h3 h4 h5
v1 0,0 3,2 2,2 2,1 3,0
v2 3,3 0,2 0,0 2,1 3,2
v3 2,2 1,0 0,1 3,1 0,2
v4 1,0 1,2 1,2 1,2 1,0
v5 2,0 1,0 0,0 0,1 0,0
h1 h2 h3 h4 h5
v1 0,0 3,2 2,2 2,1 3,0
v2 3,3 0,2 0,0 2,1 3,2
v3 2,2 1,0 0,1 3,1 0,2
v4 1,0 1,2 1,2 1,2 1,0
v5 2,0 1,0 0,0 0,1 0,0
h1 h2 h3 h4 h5
v1 0,0 3,2 2,2 2,1 3,0
v2 3,3 0,2 0,0 2,1 3,2
v3 2,2 1,0 0,1 3,1 0,2
v4 1,0 1,2 1,2 1,2 1,0
v5 2,0 1,0 0,0 0,1 0,0
A priori, v (resp. h) may play either of the vi (resp. hi). In this context, h5
(resp. row v5) is smaller than h1 (resp. v3) according to agent h (resp. v). So h5
(resp. v5) is not a best response of h (resp. v). Also, v4 is not a best response of
v because for each hi, row v4 is smaller than some other row. So, the combined
best responses against the full nd profile are v1 to v3 and h1 to h4, as shown in
the second picture above. Now, a priori v may play either v1 or v2 or v3. So h4
is not a best response of h since for each row v1 to v3, h4 is smaller than some
other column. So the nd profile ”shrinks” to the third one above.
h1 h2 h3 h4 h5
v1 0,0 3,2 2,2 2,1 3,0
v2 3,3 0,2 0,0 2,1 3,2
v3 2,2 1,0 0,1 3,1 0,2
v4 1,0 1,2 1,2 1,2 1,0
v5 2,0 1,0 0,0 0,1 0,0
h1 h2 h3 h4 h5
v1 0,0 3,2 2,2 2,1 3,0
v2 3,3 0,2 0,0 2,1 3,2
v3 2,2 1,0 0,1 3,1 0,2
v4 1,0 1,2 1,2 1,2 1,0
v5 2,0 1,0 0,0 0,1 0,0
Similarly, for h1, h2 and h3, row v3 is smaller than v1 or v2, hence the left-
hand nd profile above. Since h3 is smaller than h2 now, this yield the right-hand
irreducible nd profile above, which is a nd Nash (BR0-)equilibrium for the game.
Recommendation: Consider the following class of games G, where each ∗ can
be either 1 or −1, and where the preferences are along the usual order −1 < 1.
h1 h2
v1 ∗, ∗ ∗, ∗
v2 ∗, ∗ ∗, ∗
For each agent, the arithmetic mean of its payoff over its four payoffs for all
games in G is 0, by a ”symmetry” argument. However for each agent, the arith-
metic mean of its payoff over its payoff that are inside the nd Nash equilibrium
for all games in the class is 3/8, by a few combinatorial arguments.
Lemma 18 For a game g in G, let eq(g) be the nd Nash BR2-equilibrium built
by the proof of lemma 15.
1
|G|
×
∑
g∈G
1
|eq(g)|
×
∑
s∈eq(g)
P (s, v) =
3
8
Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 281
Probabilistic Nash equilibria cannot serve as recommendations to agents on
how to play because even for strategic games over rational numbers there is no
algorithm that finds such an equilibrium and that is robust to strategy renaming.
Below, let v (resp. h) model a real-life situation with the game to the left (resp.
right). Each game has only two probabilistic Nash equilibria, namely ((v1 7→
1
2 , v2 7→
1
2 ), (h1 7→
1
2 , h2 7→
1
2 )) and ((v3 7→
1
2 , v2 7→
1
2 ), (h3 7→
1
2 , h2 7→
1
2 )).
However, there is non way for agents to collectively choose one of them in a
non-cooperative setting.
h1 h2 h3
v1 3, 2 2, 3 0, 0
v2 2, 3 3, 2 2, 3
v3 0, 0 2, 3 3, 2
h3 h2 h1
v3 3, 2 2, 3 0, 0
v2 2, 3 3, 2 2, 3
v1 0, 0 2, 3 3, 2
This paper’s equilibrium computation is robust. Since it is more efficient than
random play, it can serve as a recommendation.
6 Multi Strategic Games
This section introduces multi strategic games and related notions informally.
Such a game is a graph whose nodes are ”abstract strategic games”. For each
node, each agent has to choose a node strategy once for all. These combine into
a node profile that yields an abstract outcome and points to another node .
The 2-agent game below has three nodes (big rectangles). Consider a play
starting at (or reaching) the central node. For instance, if v and h both choose
their first strategy at this node, the outcome is oc1 and the play reaches the
central node again; if h chooses his second strategy instead, the outcome is oc2
and the play reaches the right node, hence the small rectangle and the arrow.
oc9 oc10
oc1 oc2
oc3 oc4
oc5 oc6
oc7 oc8
A strategy for an agent is the combination of his node strategies, and a profile
is the combination of agents’ strategies. Nd node strategies allow an agent to
choose several node strategies at a same node. A nd profile is depicted below. At
the central node, underlining says h selects his first strategy and v selects both.
oc9 oc10
oc1 oc2
oc3 oc4
oc5 oc6
oc7 oc8
From each node, a nd profile induces a set of infinite sequences of outcomes.
Above, the nd profiles induces the sequence ocω9 at the left-hand node, and the
sequences ock1oc3oc
ω
9 , for k in N, at the central node.
As described in [5], embedding such games into nd games also provides a non-
trivial notion of nd Nash equilibrium with guaranteed existence. The constructive
282 Ste´phane Le Roux
proof yields a search algorithm exemplified below. The game involves agents v
and h. Agent v chooses the rows and gets the first figures of the payoff functions.
At first, all agents consider all of their options, as suggested by the under-
lining. At the bottom node, only agent h has a decision to make. If he chooses
right, he gets 0ω (and v gets 3ω, but h does not take it into account). If h chooses
left, he gets an infinite sequences of positive numbers whatever v’s strategy may
be, which is better than 0 at any stage of the sequence. So h dismisses for good
his right strategy at the bottom node, as depicted on the right-hand side below.
2, 2 2, 1
0, 4 1, 4
2, 2 2, 4
4, 2 3, 3
0, 1 3, 0
2, 2 2, 1
0, 4 1, 4
2, 2 2, 4
4, 2 3, 3
0, 1 3, 0
Now agent v considers the top-left node. If he chooses his bottom strategy,
induced sequences use only 0 and 1. If he choose his top strategy, induced se-
quences use numbers that are greater than 1, which is better. So v dismisses for
good his top strategy at this node, as depicted on the left-hand side below.
Then agent h considers the top-right node. If he chooses his left (resp. right)
strategy, induced sequences use only 1 and 2 (resp. 3 and 4). So agent h dismisses
his left strategy at the top-right node, as depicted on the right-hand side below.
2, 2 2, 1
0, 4 1, 4
2, 2 2, 4
4, 2 3, 3
0, 1 3, 0
2, 2 2, 1
0, 4 1, 4
2, 2 2, 4
4, 2 3, 3
0, 1 3, 0
Eventually, agent v dismisses his top strategy at the top-right node, which
yields the nd Nash equilibrium below. So, for each agent, fore each node, the
agent cannot get a better set of sequences by changing his strategy.
2, 2 2, 1
0, 4 1, 4
2, 2 2, 4
4, 2 3, 3
0, 1 3, 0
The universal existence of such equilibria actually follows an embedding of
multi strategic games into nd games. However, other more subtle notions of
equilibrium with guaranteed existence may be defined.
Discrete Non Determinism and Nash Equilibria for Strategy-Based Games 283
7 Conclusion
Using probability to weaken the notion of strategy and guarantee existence of
weak Nash equilibria works for real-valued strategic games but not for the slightly
more general abstract strategic games. Using discrete non determinism works in
both settings and enables (at least) five different relevant notions of discrete non
deterministic equilibrium. Some of these notions are actually related to the con-
cept of dominated strategy. However, this concept had not led to discrete Nash
equilibria in the past literature, perhaps because probabilities seemed more accu-
rate than discrete non determinism or because no alternative to Nash’s approach
seemed to be required. Since elimination of dominated strategies gives informa-
tion about the probabilistic Nash equilibria of a strategic game, studying discrete
Nash equilibria may actually give information about probabilistic Nash equilib-
ria. In addition, the discrete approach is better in at least four respects: First, it
still works in an even more general setting, namely multi strategic games, which
combine sequential and simultaneous decision-making. Second, the constructive
proof of existence builds one specific equilibrium that can serve as a recom-
mendation since it seems payoff-wise efficient. However, more work is needed
about payoff-wise efficiency. Third, the proof/algorithm has a (low) polynomial
time complexity for the five notions, which makes equilibria easy to find among
exponentially many candidates. Fourth, some other notions of discrete equilib-
rium might yield NP-complete problems, which suggests that finding discrete
non deterministic equilibria also lies on the boundary of P.
References
1. B Douglas Bernheim. Rationalizable strategic behavior. Econometrica, 52:1007–
1028, 1984.
2. D Gale. A theory of n-person games games with perfect information. Proceedings
of the National Academy of Sciences of the United States of America, 39:496–501,
1953.
3. J Hillas and E Kohlberg. Foundations of strategic equilibrium. In R.J. Aumann
and S. Hart, editors, Handbook of Game Theory with Economic Applications. 2002.
4. S Le Roux. Non-determinism and Nash equilibria for sequential game over partial
order. In Proceedings Computational Logic and Applications, CLA ’05. DMTCS
Proceedings, 2006. http://www.dmtcs.org/pdfpapers/dmAF0106.pdf.
5. S Le Roux. Discrete non neterminism and nash equilibria for strategy-based games.
Research report http://hal.inria.fr/inria-00195397/fr, INRIA, 2007.
6. S. Le Roux. Generalisation and formalisation in game theory. PhD thesis, Ecole
Normale Supe´rieure de Lyon, 2008.
7. RD Luce and H Raiffa. Games and Decisions. John Wiley and Sons, New York,
1957.
8. J Nash. Equilibrium points in n-person games. Proceedings of the National
Academy of Sciences, 36:48–49, 1950.
9. MJ Osborne and A Rubinstein. A Course in Game Theory. The MIT Press, 1994.
10. DG Pearce. Rationalizable strategic behavior and the problem of perfection.
Econometrica, 52:1029–1050, 1984.
Combining Concept Maps and Petri Nets to Generate 
Intelligent Tutoring Systems 
Maikel León, Isis Bonet, and Zenaida García 
Department of Computer Science 
Central University of Las Villas 
Highway to Camajuaní km 5.5 
Santa Clara, Cuba 
Phone: (53) (42) 281515 
Fax : (53) (42) 281608 
{mle, isisb, zgarcia}@uclv.edu.cu 
 
Abstract. The use of pedagogical methods with the technologies of the information and 
communications produce a new quality that favors the task of generating, transmitting and 
sharing knowledge. Such is the case of the pedagogical effect that produces the use of the 
Concept Maps, which constitute a tool for the management of knowledge, an aid to person-
alize the learning process, to exchange knowledge, and to learn how to learn. In this paper 
the authors present a new approach to elaborate Intelligent Tutoring Systems, where the 
techniques of Concept Maps and Artificial Intelligence are combined, using Petri Nets as 
theoretical frame, for the student model. The pedagogical model that controls the interaction 
between the apprentice and the generated Intelligent Tutoring Systems is implemented by 
Petri Nets. The Petri Nets transitions are controlled by conditions that refer to the apprentice 
model. The firing of these transitions produces actions that update this apprentice model. 
These conditions are automatically included into the pedagogical model and the teacher has 
only to specify the contents of the domain model. 
 
Key words: Intelligent Tutoring Systems, Petri Nets, Student Model, Artificial Intelligence, 
Concept Maps. 
1 Introduction 
To construct and to share knowledge, to learn significantly, and to learn how to learn, 
are ideas on which researchers have been pondering for a long time as well as the use 
of tools that would allow taking these aspirations into practice. To achieve this, dif-
ferent techniques and strategies have been used. Concept Maps (CM) provide a 
schematic summary of what is learned and they order it in a hierarchic range. The 
knowledge is organized and represented in all the levels of abstraction, the general 
knowledge goes to the upper part and the most specific one goes to the lower [1]. 
Over the last few years the CM have increasingly got a great popularity and its inte-
gration with the technologies of the information and the communications have be-
come a very important element in the plans for the improvement of the teaching-
learning process. The main application of the CM has had effect in the teaching-
learning process, which is its basic intention, they are based on an instrument that 
combines the scientific rigidity with simplicity and flexibility, producing a general 
 Combining Concept Maps and Petri Nets 285 
approval in the audience of students and professionals; this represents an important 
nexus between pedagogy and technology in a huge field of applications. Also, it con-
stitutes an important aid for those who generate, transmit, store, and divulge informa-
tion and knowledge and it comprises an important tool to obtain a highest practical 
value in the systems of the teaching-learning process [2].  
Thus, Petri Nets (PN) are a graphical and mathematical modeling tool. It consists 
of places, transitions, and arcs that connect them. Input arcs connect places with tran-
sitions, while output arcs start at a transition and end at a place. There are other types 
of arcs, e.g. inhibitor arcs. Places can contain tokens; the current state of the modeled 
system (the marking) is given by the number (and type if the tokens are distinguish-
able) of tokens in each place. Transitions are active components. They model activi-
ties which can occur (the transition fires), thus changing the state of the system (the 
marking of the PN). Transitions are only allowed to fire if they are enabled, which 
means that all the preconditions for the activity must be fulfilled (there are enough 
tokens available in the input places). When the transition fires, it removes tokens 
from its input places and adds some at all of its output places. The number of tokens 
removed/added depends on the cardinality of each arc. The interactive firing of transi-
tions in subsequent markings is called token game [3]. PN are a promising tool for 
describing and studying systems that are characterized as being concurrent, asynchro-
nous, distributed, parallel, nondeterministic, and/or stochastic. As a graphical tool, 
PN can be used as a visual-communication aid similar to flow charts, block diagrams, 
and networks. In addition, tokens are used in these nets to simulate the dynamic and 
concurrent activities of systems. As a mathematical tool, it is possible to set up state 
equations, algebraic equations, and other mathematical models governing the behav-
ior of systems [4].  
To study performance and dependability issues of systems it is necessary to in-
clude a timing concept into the model. There are several possibilities to do this for a 
PN; however, the most common way is to associate a firing delay with each transi-
tion. This delay specifies the time that the transition has to be enabled, before it can 
actually fire. If the delay is a random distribution function, the resulting net class is 
called stochastic PN. Different types of transitions can be distinguished depending on 
their associated delay, for instance immediate transitions (no delay), exponential 
transitions (delay is an exponential distribution), and deterministic transitions (delay 
is fixed). The concept of PN has its origin in Carl Adam Petri's dissertation Kommu-
nikation mit Automaten, submitted in 1962 to the faculty of Mathematics and Physics 
at the Technische Universität Darmstadt, Germany. 
Till now the Intelligent Tutoring Systems (ITS) have demonstrated its effective-
ness in diverse domains. However, its construction implies a complex and intense 
work of engineering of knowledge, which prevents an effective use of it. In order to 
eliminate these deficiencies there appears the target to construct the author's hardware 
which facilitates the construction of these systems to all users (not necessarily expert) 
in the computer field, in particular to those instructors who master a certain matter of 
education [5]. The field of the ITS is characterized by the application of Artificial 
Intelligence techniques, to the development of the teaching-learning process assisted 
by computers. If the key feature of an ITS is the aptitude to adapt itself to the student, 
the key component of the system is the Student Model, where the information associ-
ated to the student is stored. This information must be inferred by the system depend-
286 Maikel León et al. 
ing on the information available: previous data, response to questions, etc. This proc-
ess of inference is identified as diagnosis, and is undoubtedly the most complicated 
process inside an ITS, who uses the Artificial Intelligence techniques to represent 
knowledge, to shape the human reasoning, to emphasize the learning by means of the 
action, to combine experiences of resolution and discovery, to be able to solve prob-
lems by their own, to formulate diagnoses and to provide explanations. So, they count 
on a bank of instruction strategies which helps to decide what and how to inform to 
the student to get an effective direction.  
The aim of this paper is to present a new approach to elaborate ITS using a CM, 
questionnaires able to catch the cognitive and affective state of the student (Student 
Model) and by using PN to allow the student to sail in an oriented way according to 
their knowledge and not in a free way as the CM are conceived. And it is the capacity 
to adapt dynamically to the development of the student’s learning what makes this 
system intelligent. The pedagogical model is controlled by PN that access and up-
dates the apprentice model, while presenting the contents of the domain model. The 
PN are automatically generated from specifications defined by the teacher using a 
graphical interface and compiled into a neural net that actually implement the ITS. An 
important point in the proposed approach is that the teacher does not need to specify 
the interaction with the apprentice model; this interaction is automatically included in 
the PN using the prerequisite and difficulty orders previously defined by the teacher. 
2 Motivations for Concept Maps 
Concept Maps provide a framework for capturing experts' internal knowledge and 
making it explicit in a visual, graphical form that can be easily examined and shared. 
CM constitute a tool of great profit for teachers, investigators of educational topics, 
psychologists, sociologists and students in general, as well as for other areas espe-
cially when it is necessary to manage with large volumes of information. They have 
become a very important element in the plans for the improvement of the ITS and 
they have also extended its use to other areas of the human activity in which both 
management and the use of knowledge take up a preponderant place. If to define a 
CM is relatively simple, it is simpler to understand the meaning of the definition. The 
Concept Maps were defined by Novak, his creator, as a skill that represents a strategy 
of learning, a method to get the gist of a topic and a schematic resource to represent a 
set of conceptual meanings included in a set of propositions [2].  
It is necessary to point out that there is not only one model of CM, several may ex-
ist. The important point is the relations that are established between the concepts 
through the linking words to form propositions that configure a real value on the 
studied object. For such a reason, in a concept there may appear diversity of real 
values. In fact, it turns very difficult to find two exactly equal CM, due to the individ-
ual character of knowledge. The CM can be described under diverse perspectives: 
abstract, visualization, and conversation. Since significant learning is reached more 
easily when the new concepts or conceptual meanings are included under wider con-
cepts, the most used CM are the hierarchic ones, the most general and inclusive con-
 Combining Concept Maps and Petri Nets 287 
cepts are placed in the upper part of the map, and the progressively more specific and 
less inclusive concepts, in the lower part. The subordinated relations among concepts 
may change in different fragments of learning, so in a CM, any concept may rise up 
to the top position, and keep a significant propositional relation with other concepts 
of the map. The use of CM designed by the professor increases both learning and 
retention of scientific information. The students produce maps as learning tools. Con-
sidering that the CM constitute an explicit and clear representation of the concepts 
and propositions, they allow both teachers and students to exchange points of view on 
the validity of a specific propositional link and to recognize the missing connections 
in the concepts that suggest the need of a new learning. For this reason, this skill has 
complemented so favorably with the practice of distance learning which presupposes 
that students and teachers are not physically in the same place at the same time.  
Concept Maps have particular characteristics that make them amenable to smart 
tools [2]. These include: 
1. Concept Maps have structure: By definition, more general concepts are presented 
at the top with more specific concepts at the bottom. Other structural information, 
e.g. the number of ingoing and outgoing links of a concept, may provide addi-
tional information regarding a concept’s role in the map.  
2. Concept Maps are based on propositions: every two concepts with their linking 
phrase forms a “unit of meaning”. This propositional structure distinguishes Con-
cept Maps from other tools such as Mind Mapping and The Brain, and provides 
semantics to the relationships between concepts. 
3. Concept Maps have a context: A Concept Maps is a representation of a person’ 
understanding of a particular domain of knowledge. As such, all concepts and 
linking phrases are to be interpreted within that context. 
4. Concepts and linking phrases are as short as possible, possibly single words. 
5. Every two concepts joined by a linking phrase form a standalone proposition. 
That is, the proposition can be read independently of the map and still “make 
sense”. 
6. The structure is hierarchical and the root node of the map is a good representative 
of the topic of the map. 
 
The students who analyze CM will have a wider basic knowledge; therefore, they 
will be more prepared to solve problems in comparison to those students who learn 
by memorizing. The CM have turned into a useful instrument for teacher training and 
for the student’s understanding of diverse subjects. CM constitute an instrument that 
merges the scientific rigidity with the simplicity and flexibility. It represents an aid 
for those who generate, transmit, store and spread information and knowledge. They 
also constitute an important tool to achieve a practical value, especially in the Artifi-
cial Intelligence systems. 
288 Maikel León et al. 
3 Proposed Model 
The ITS that are created correspond with a CM, with the particular feature that in 
some of its nodes there appears a questionnaire, capable to get the cognitive and af-
fective state of the student and able to guide his navigation, creating this way an “In-
telligent” CM. At the first level of the authoring interface, the teacher must specify 
the set of pedagogical units that define each curriculum and, at the second level, the 
set of problems that define each pedagogical unit. The elements of these sets present a 
partial order and associated prerequisites [6]. To specify these partial orders and pre-
requisites the teacher disposes of a graphical interface that allows the construction of 
graphs. In these graphs, an edge from node n1 to node n2 means that n2 has n1 as a 
prerequisite. Each node n may have the following input edges: 
• None: node n has no prerequisite and can be executed anytime. This is the 
case only for initial nodes. 
• One: node n has only one node as prerequisite and this one must be executed 
before node n is available for execution. 
• Two or more necessary edges: node n has several prerequisite nodes and all 
of them must be executed, before node n is available for execution. 
• Two or more alternative edges: node n has several prerequisite nodes but 
only one of them must be executed before node n is available for execution. 
 
Necessary and alternative edges may occur simultaneously in the same node. 
Nodes and the different types of edges may be combined in a complex graph, accord-
ing to the intended course sequences, but they must satisfy one restriction: each graph 
must have only one initial node and only one final node. This restriction seems rea-
sonable, because pedagogical units and problems should have one begin and one end 
point and it also assures that the graphs of the two levels can be combined into a one 
level PN. Figure 1 shows a prerequisite graph. Note the AND constraint (with nodes 
n5 and n6 before and node n6 after) associated with necessary edges and the OR con-
straint (with nodes n6 and n7 before and node n8 after) associated with alternative 
edges. 
 
 
Fig. 1. Prerequisites Graph 
 Combining Concept Maps and Petri Nets 289 
The first step in the generation of the final code of an ITS using the proposed au-
thoring tool is to compile each prerequisite graph into a PN that represents the peda-
gogical model. The pedagogical model defines the control strategy of the interaction 
between the apprentice and the TA associated with each sub-domain and is specified 
by the teacher in the form of curriculum and pedagogical unit graphs. To represent 
the pedagogical model, we use an Object Petri Net (OPN) [4]. An OPN is defined by 
a control structure (places, transitions and arcs connecting places to/from transitions), 
by the data structure of its tokens and by its semantics, that defines how the token 
player should execute the OPN, given an initial marking. The OPN places can repre-
sent either pedagogical unit, if the graph corresponds to the first level of the interface, 
or problems, if it corresponds to the second level.  
The net structure is directly obtained from the graphs specified by the teacher and 
the token data structure is defined by the adopted apprentice model. As the tokens 
have the class Apprentice the marking of the OPN is associated with the current ac-
tivity of the apprentice and also with the possible future activities. A transition fires 
when the apprentice has finished the interactions associated with its input nodes. The 
token is then moved to the output places of the transition, meaning that the apprentice 
is ready to begin the activities associated with them. A prerequisite graph (curriculum 
or pedagogical unit) is automatically translated into an OPN structure. Each configu-
ration of nodes and edges in a prerequisite graph is associated with an OPN fragment 
that implements it and these fragments are combined into a single Petri net that im-
plements the graph. 
For instance, a simple sequence of nodes, see figure 2(a), is implemented by one 
transition (t1 2), one input place (n1) and one output place (n2). One node with two 
output edges, see figure 2(b), is implemented by a fork transition (tf1) with one input 
place (n1) and two output places (n2 and n3). After the activity associated with the 
input place is finished, the apprentice can do either the activity associated with one 
output place or the activity associated with the other. A node with two input neces-
sary edges, see figure 2(c), is implemented by a join transition (tj3) with two input 
places (b1 3 and b2 3) and one output place (n3). The places b1 3 and b2 3 are buffer 
places and do not correspond to pedagogical units or problems. They are necessary 
because after the activity associated with either one of the places n1 or n2 is finished, 
that information must be stored in the corresponding buffer and only when the activ-
ity associated with the other place is also finished (and also stored in the correspond-
ing buffer) the activity associated with the output place (n3) can be performed. For 
this, we need two extra transitions (tb1 3 and tb2 3) connecting, respectively, the 
input places n1 and n2 to the buffers b1 3 and b2 3. Finally, see figure 2(d), a node 
with two alternative input edges is represented by two transitions (t1 3 and t2 3), with 
input places n1 and n2, respectively, and both with output place n3. Transitions t1 
and t2 are used to remove the apprentice token from places n1 and n2, respectively; in 
the case the activities associated with n3 have already been performed, avoiding in 
this way the repetition of n3. Figure 3 shows the OPN obtained from the prerequisite 
graph of figure 1. 
290 Maikel León et al. 
 
Fig. 2. Graph Topologies and Petri Nets 
  
Fig. 3. Structure of an OPN 
The semantic of an OPN is the particular way the token player executes this net 
given an initial marking. An ordinary token player (used for PN with binary tokens) 
verifies, from an initial marking, whether there are enabled transitions. If there is just 
one, the token player fires it. If there is more than one, it chooses one of them accord-
ing to a priority order, or arbitrarily if no priority order is specified. In an OPN token 
player, because the token is associated with a data structure, the transitions can have 
pre-conditions that refer to the token attributes and/or to external variables [4]. The 
apprentice token attributes that are relevant to the definition of algorithm are name, 
doing and tobedone and the relevant external variables are next, that contains the next 
activity to be performed, and answer, that receives the interaction support unit re-
turned values. We say that the apprentice, identified by name, is doing a problem if 
the apprentice is interacting with one of its associated interaction support units. When 
the apprentice finishes the interaction with one support unit, the unit returns one of 
the following navigation commands: halt, continue, repeat, that is stored in the exter-
nal variable answer.  
The command halt means that the apprentice wants to interrupt the present session 
with the ITS, the command repeat means that the next activity should be another 
interaction support unit of the same problem and the command continue means that 
the current problem is finished and a next one must be chosen, either by the coordina-
tor or by the apprentice. The apprentice is asked to choose the next problem, when 
there is more than one problem in tobedone. Because the apprentice must choose only 
 Combining Concept Maps and Petri Nets 291 
one problem to do next, only one transition can fire at a given moment [7]. The coor-
dinator initializes the associated PN with a marking where the apprentice token is 
stored in the place associated with the first problem of the first pedagogical unit and 
calls the OPN token player. 
4 Petri Nets and Neural Networks. A possible solution 
PN are powerful and versatile tools for modeling, simulating, analyzing and de-
signing of complex workflow systems. This topic mainly discusses a hybrid approach 
using neural network and PN in the formal model of an ITS. Neural networks are 
used in different fields; classification is one of problems where they are commonly 
used [8]. PN are alternative tools for the study of non-deterministic, concurrent, paral-
lel, asynchronous, distributed or stochastic systems [9]. They can model systems in an 
easy and natural way. Furthermore, the PN approach can be easily combined with 
other techniques and theories such as object-oriented programming, fuzzy theory, 
neural networks, etc [10]. These modified PN are widely used in computing, manu-
facturing, robotic, knowledge based systems, process control, as well as in other 
kinds of engineering applications. Since PN offer advantages to model systems and 
can interact with other techniques easily, it would be advantageous to model neural 
networks starting from PN models, which allow not only the design adjustment but 
also the initialization of the neural network weights [11]. Following the algorithm 
proposed by Xiaoou Li and Wen Yu in [4], we can model a neural network starting 
from a PN with the application of weighty production rules in the algorithm. See the 
following example (figure 3). 
The following weighting production rules are obtained starting from the example 
of Figure 3 and following the algorithm described in [4]: 
1. If P1 and P4 then P5 (w1, w4) 
2. If P1 and P4 then P6 (w1, w4) 
3. If P1 and P4 then P12 (w1, w4) 
4. If P5 or P6 then P9 (μ4, μ5-9) 
5. If P6 and P1 then P15 (w6, w1) 
6. If P9 then P10 (μ6) 
 
The learning algorithm of the neural networks obtained is the same as the back-
propagation of multilayer neural networks. The main idea is that all layer weights can 
be updated through the backpropagation algorithm if certainty factors of all sink 
places are given. A complex neural network can be divided into several sub-networks 
starting from the modular design of an original PN. With this system a teacher can 
edit an ITS starting from a CM that will compiled into a PN, the neural network that 
control the system interaction is constructed starting from the PN. The initial model-
ling PN will allow us to design a feedforward neural network with the backpropaga-
tion learning algorithm. It is even possible to create several neural networks starting 
292 Maikel León et al. 
from only one PN, taking into account its possible modules [12]. To split neural net-
works in modules makes their training and performance easier. 
 
μ5-15
μ6
μ5-9
w1-5 
μ3-12 
μ3-5 
w4
w1-3 
w6
μ3-6 
μ4
P10 
P6
P15
P5
P9
T5
T4
T6
P12 
P4 
P1 
T3 
 
Fig. 3. Example of Petri Net 
 
μ6
w6
μ5-9
μ4
α(P10) 
w4 
w4 
w4 
w1-3
w1-3 
w1-3 w1-3 
α(P1) 
α(P4) 
α(P5)
α(P15)
α(P9)
α(P6)
α(P12)
G1 
G2 G4 G6
G5
G3 
 
 
Fig. 4. A neural network model obtained from the Petri Net of Figure 3 
5 Conclusions 
With this work we propose a new Student Model that could be considered in the 
construction of ITS, where are combined the facilities of the Concept Maps for the 
organization of the knowledge and the potentiality of the Petri Nets that allows to the 
student to sail in an oriented way according to their knowledge and not in a free way 
as the CM are conceived. The proposed idea has been applied successful in the elabo-
ration of ITS by users (not necessarily expert in the Computer Science field). 
The suggested hybrid system combines PN and neural networks using the advan-
tages of PN in order to overcome the neural network deficiencies concerning their 
original design and definition of their initial weighs. 
 Combining Concept Maps and Petri Nets 293 
References 
1. Cañas Alberto J., Marco Carvalho. (2004). Concept Maps and AI: an 
Unlikely Marriage? SBIE 2004: Simpósio Brasileiro de Informática na Edu-
cação Manaus, Brasil. 
2. Martínez, N., León M. (2006). Mapas Conceptuales y Redes Bayesianas en 
los Sistemas de Enseñanza/Aprendizaje Inteligentes. Congreso Internacional 
de Informática Educativa. Costa Rica. 
3. Xiao-Qiang Liu, Min Wu, Jia-Xun Chen. (2002). Knowledge aggregation 
and navigation highlevel petri nets-based in e-learning. In Proceedings In-
ternational Conferenceon Machine Learning and Cybernetics, volume 1, 
pages 420– 425. 
4. Li X., Yu W. (2000). Dynamic Knowledge Inference and Learning under 
Adaptive Fuzzy Petri Net Framework, IEEE Transactions on Systems, Man, 
and Cybernetics – Part C: Applications and reviews, Vol. 30, No. 4. 
5. Esposito F., Ferilli S., Basile A., Di Mauro N. (2006). Inference of abduction 
theories for handling incompleteness in first-order learning. Knowledge and 
Information Systems (KAIS) Journal, 1-27, ISSN: 0219-1377. 
6. Brusilovsky P. (2000). Adaptative hypermedia: From intelligent tutoring 
systems to web-based education. Lecture Notes in Computer Science. 5th In-
ternational Conference on ITS. Montreal, Canada. 
7. Murray T. (1999). Authoring intelligent tutoring systems: An analysis of the 
state of the art. In Journal of Artificial Intelligence and Education, volume 
10. 
8. Hilera, J. (1995). Redes neuronales artificiales: Fundamentos, modelos y 
aplicaciones. Ed. Addison-Wesley Iberoamericana, 1995. 
9. Yu X., Chen G., Cheng S. (1995). Dynamic Learning Rate Optimization of 
the Backpropagation Algorithm, IEEE Transaction on Neural Networks, 
Vol. 6, No. 3, pp. 669-677. 
10. Bello, R. (2002). Aplicaciones de la Inteligencia Artificial. Ediciones de la 
Noche, Guadalajara, Jalisco, México. ISBN: 970-27-0177-5. 
11. Liu W., Hong J. (2000). Re-investigating Dempster's Idea on Evidence 
Combination. Knowledge and Information Systems: An International Jour-
nal, Springer-Verlag, Vol.2, No.2, pp.223-241, ISSN 0219-1377. 
12. Cooley J. (1999). Data Preparation for mining World Wide Web browsing 
patterns. Journal of Knowledge and Information Systems. 
Query-Optimal Oracle Turing Machines for
Type-2 Computations
Chung-Chih Li
School of Information Technology
Illinois State University
Normal, IL 61790, USA
Abstract. With the notion of optimal programs defined in terms of
Blum’s complexity measure, the well known speed-up theorem states that
there exist computable functions that do not have optimal programs. For
type-2 computation, we argue that the same speed-up theorem can be
obtained as long as the complexity measure satisfies Blum’s complexity
measure under the Oracle Turing Machine model. In the present paper,
we consider the oracle query as a dynamic complexity measure. Since
the query complexity is not a Blum’s complexity measure, we have to
characterize the notions of optimal programs in a very different way. We
give three notions of query-optimal programs in different strength and
argue that the stronger the notion of query-optimal programs we define,
the more difficult to have query-optimal programs. In other words, with
a stronger definition of query-optimal programs, one can prove an easier
version of the speed-up theorem.
Key words: Type-2 Complexity Theory, Type-2 Computation, Query
Optimal Programs, Speed-up Theorems
1 Introduction
Searching for the best algorithm to solve a concerned computable problem is
a primary task for computer scientists. However, the speed-up theorem tells
us that, theoretically, we cannot succeed on all problems. In fact, the speed-
up phenomena have been extensively studied by mathematicians for more than
a half century, which in logical form was first remarked by Go¨del in terms of
the length of theorem proving [5, 6, 13]. Blum [1, 2] re-discovered the speed-up
phenomenon in the context of computational complexity.
Blum’s speed-up theorem is a result of his axiomatization of complexity mea-
sures. Fix a programming system for Turing machines and let ϕi denote the
function computed by the ith Turing machine and Φi denote the cost function
associated to it. Blum considers that dynamic complexity measures should meet
the following two requirements: for any i, x,m ∈ N, (i) ϕi is convergent on x
(denoted by ϕi(x) ↓) if and only if Φi is convergent on x (denoted by Φi(x) ↓)
and (ii) Φi(x) = m is effectively decidable. The two axioms have successfully
lifted the study of complexity theory to an abstract level with rich results that
Query-Optimal Oracle Turing Machines for Type-2 Computations 295
are independent from any specific machine models. At type-1, the meaning of
“more efficient programs” is intuitive: For a computable function f , we say that
program j is more efficient than program i if ϕi = ϕj = f and for all but finitely
many x we have Φj(x) < Φi(x). Precisely, Blum [1, 2] formulates the speed-up
theorem as follows: For any recursive function r, there exists a recursive function
f such that
(∀ i : ϕi = f) (∃j : ϕj = f) (
∞
∀ x)
[
r(Φj(x)) ≤ Φi(x)
]
. (1)
There are many sources for detailed proofs and discussions regarding this curious
speed-up phenomenon, e.g., [1–4, 6, 11–14]. We shall skip all irrelevant details due
to the space constraints.
When we try to lift the complexity theory to type-2 computation, we want to
know whether or not the same speed-up phenomenon can be described in terms of
some complexity measures that are sensible in the context of type-2 computation.
Clearly, time and space remain meaningful in type-2 computation. Since time and
space are Blum’s complexity measures, we have not faced too many difficulties
in translating the original proofs of the speed-up theorem and its variants into
type-2 (see [9]). For query complexity, on the other hand, the two Blum’s axioms
fail. As a result, we can’t reuse the same concepts and techniques but develop
our new ones in order to investigate this specular speed-up phenomenon.
2 Type-2 Computation Using Oracle Turing Machines
In this section we will provide necessary notations for type-2 computation; de-
tailed discussion about the type-2 complexity theory can be found in [7–10]. We
consider natural numbers as type-0 objects and functions over natural numbers
as type-1 objects. For type-2 objects, they are functionals that take as inputs
and produce as outputs type-0 or type-1 objects. We consider objects of lower
type as special cases of higher type, i.e., type-0 ⊂ type-1 ⊂ type-2. Without loss
of generality we restrict our type-2 functionals to the standard type T ×N⇀ N,
where T is the set of total functions and ⇀ means possibly partial.
We use the Oracle Turing Machine (OTM) as our standard computing for-
malism for type-2 computation. An OTM is a Turing machine equipped with a
function oracle. We say that a type-2 functional F : T ×N⇀ N is computable
if there is an OTM that computes F as follows. Before the OTM begins to run,
the type-1 argument should be presented to the OTM as an oracle. Every OTM
has two extra tapes called query-tape and answer-tape for oracle queries and
oracle answers, respectively. During the course of the computation, the OTM
may enter a special state called query-state where the oracle will read the query
left on the query-tape and return its answer to the answer-tape. We can fix
a programming system 〈ϕ̂i〉i∈N for OTM’s and let M̂e denote the OTM with
index e that computes ϕ̂e.
Let F denote the set of finite functions over natural numbers, i.e., σ ∈ F iff
the domain of σ is a subset ofN and its cardinality is inN. Given F : T ×N→ N,
296 Chung-Chih Li
F (f, x) ↓= y denotes that F is defined at (f, x) and its value is y. Since if F is
computable, F must be continuous (i.e., compact and monotone), it follows that
if F (f, x) ↓= y, there must be σ ∈ F with σ ⊂ f such that for every extension
η of σ (i.e., σ ⊆ η), we have F (f, x) ↓= F (σ, x) ↓= F (η, x) ↓= y. We say that
(σ, x) and (η, x) are locking fragments of F on (f, x). For any σ ∈ F , let ((σ))
denote the set of total extensions of σ, i.e., ((σ)) = {f ∈ T ∣∣ σ ⊂ f}. Also, if
(σ, x) ∈ F ×N, let ((σ, x)) = {(f, x) ∣∣ f ∈ ((σ))}. If σ1 and σ2 are consistent, we
have ((σ1))∩((σ2)) = ((σ1∪σ2)); otherwise, ((σ1))∩((σ2)) = ∅. The union operation
((σ1)) ∪ ((σ2)) is conventional.
3 Query-optimal Programs
Only in type-2 computation can we use the number of queries made through-
out the course of the computation as a dynamic complexity measure. We are
interested in whether or not there is a query-optimal program for any given
type-2 computable functional; if there is not, can we have a speed-up theorem
in terms of query-complexity. It turns out that the query-complexity fails to
satisfy Blum’s two axioms. Consequently, the arguments in the original proof
of the speed-up theorem are no longer valid. We thus need a new approach to
understand this curious phenomenon. In this section we define a few alternative
notions of query-optimal programs. These notions must be sensible and intuitive.
3.1 Unnecessary Queries
We shall argue that some intuitive notions of query-optimal programs are not
flexible enough to derive interesting results. We say that an oracle query is
necessary if its answer returned from the oracle will affect the result of the
computation. Intuitively, a query-optimal program should be a program that
does not make any unnecessary queries. Unfortunately, this notion of query-
optimal programs is too strong. It is easy to argue that there are functionals
that always make unnecessary oracle queries. Consider the following functional
F : T ×N→ N defined by,
F (f, x) =
{
f(0) + 1 if ϕx(x) ↓ in f(0) steps;
0 otherwise. (2)
Clearly, F is computable and total. Fix any a such that ϕa is diverged on a
(denoted by ϕa(a) ↑). Then, on input (f, a), the value of f(0) only affects the
speed of computing F (f, a). Thus, F (f, a) = 0 for any f ∈ T , and hence (∅, a) is
the minimal locking fragment of F on (f, a). That means any queries made during
the computation of F on (f, a) are unnecessary. By contradiction, if there were
an OTM that would not make any unnecessary queries for F , one could modify
such OTM to solve the halting problem, which is impossible. Therefore, the
type-2 functional defined in (2) can’t be computed by an OTM without making
unnecessary queries. From this example, it is clear that to require query-optimal
Query-Optimal Oracle Turing Machines for Type-2 Computations 297
programs to be ones that do not make unnecessary queries is too restricted. We
will loose the requirements by allowing a small amount of unnecessary queries.
By “a small amount”, we means “a compact set” in some topology determined
by the concerned computation. We formalize our idea in the next subsection.
3.2 Unnecessary Queries in Compact Sets
We use Q(s, e, f, x) to denote the collection of queries made during the run time
of OTM M̂e on (f, x) up to s steps. Formally,
Definition 1. Let e be a ϕ̂-program. Define Q : (N × N × T × N) → F by:
Q(s, e, f, x) = τ , where τ ∈ F with τ ⊂ f and dom(τ) is the collection of queries
made during the course of the computation of OTM M̂e on (f, x) in s steps. 
Clearly, Q(s, e, f, x) is monotone in s, i.e., s ∈ N, Q(s, e, f, x) ⊆ Q(s+1, e, f, x).
Definition 2. Let e, x ∈ N and f ∈ T . We say that Q(·, e, f, x) is defined in
the limit if and only if there exist s ∈ N and τ ∈ F such that, for every s′ ≥ s,
Q(s, e, f, x) = Q(s′, e, f, x) = τ.
In the case above, we write lim
s→∞Q(s, e, f, x) ↓ = τ. 
We wish to treat the set of queries an OTM made as a dynamic complexity
measure, but it fails to satisfy Blum’s two axioms for complexity measure. That
is, lim
s→∞Q(s, e, f, x) ↓ does not mean that ϕ̂e(f, x) ↓, and hence Blum’s first
axiom fails. Moreover, for some σ ∈ F with σ ⊂ f , lim
s→∞Q(s, e, f, x) = σ is not
necessarily decidable, and hence Blum’s second axiom fails too.
Let F : T ×N→ N be computable. Since we mean to characterize some ϕ̂-
programs e1, e2, . . . , en that compute the same functional F in terms of queries,
the topology T(· · · ) defined in [7–10] cannot serve as the relative topology in
which we need a workable notion of compactness. This is because ϕ̂e1 = ϕ̂e2 =
· · · ϕ̂en = F and thus T(ϕ̂e1 , ϕ̂e2 , . . . , ϕ̂en) = T(F ), and hence the complexity
of unnecessary queries cannot be described by the compact sets in T(F ). The
product topology, T × N (Baire and the Discrete topologies), is fine enough to
describe the needed compact sets but it suffers the same problem as discussed
in [7–10]. We thus introduce another family of topologies as follows.
Definition 3. Given e1, e2, . . . , en ∈ N, let B(e1, e2, . . . , en) ⊆ T ×N be defined
as follows: (σ, a) ∈ B(e1, e2, . . . , en) if and only if there exists some f ∈ T such
that, for each i ∈ {1, . . . , n}, we have
lim
s→∞Q(s, ei, f, a) ↓ = σi,
and σ =
⋂
i∈{1...n}
σi. Let Q(e1, . . . , en) be the topology on T ×N defined by the
basic open sets in B, where
B = {((σ, x)) ∣∣ (σ, x) ∈ B} .

298 Chung-Chih Li
Since a ϕ̂-program emay make unnecessary queries outside the domains of its
minimal locking fragments, it follows that the topology Q(e) may be finer than
the topology T(ϕ̂e) which is defined by the minimal locking fragments of ϕ̂e.
That is, T(ϕ̂e) ⊆ Q(e). In other words, if e is a ϕ̂-program for F : T ×N→ N,
then T(F ) ⊆ Q(e). On the other hand, if Q(e) ⊂ T(F ), then we can conclude
that e cannot be a ϕ̂-program for F . In general, we have
T(ϕ̂i, ϕ̂j) ⊆ Q(i, j).
For convenience, we define the following notations. Let card(S) be the cardinality
of set S. Suppose i and j are two ϕ̂-programs that compute the same functional,
i.e., ϕ̂i = ϕ̂j . Define
Q[i<j] =
{
(f, x)
∣∣ card( lim
s→∞Q(s, i, f, x)) < card( lims→∞Q(s, j, f, x))
}
,
Q[i=j] =
{
(f, x)
∣∣ card( lim
s→∞Q(s, i, f, x)) = card( lims→∞Q(s, j, f, x))
}
,
Q[i>j] =
{
(f, x)
∣∣ card( lim
s→∞Q(s, i, f, x)) > card( lims→∞Q(s, j, f, x))
}
.
The following definition makes use of the compactness in the topology we
just mentioned to define the meaning of “better” ϕ̂-programs in terms of query-
complexity.
Definition 4 (Query Speed-up). Let ϕ̂i = ϕ̂j.
1. We say that j is a weakly query sped-up version of i if and only if
Q[i<j] is compact and Q[i>j] is noncompact in Q(i, j).
If j is a weakly query sped-up version of i, we write ϕ̂i ≺∗Q ϕ̂j.
2. We say that j is a strongly query sped-up version of i if and only if
Q[i<j] ∪Q[i=j] is compact in Q(i, j).
If j is a strongly query sped-up version of i, we write ϕ̂i ≺≺∗Q ϕ̂j. 
Suppose that we are to write a new ϕ̂-program j for ϕ̂i. Intuitively, if the new
ϕ̂-program j can improve the query-complexity on a “large” (noncompact) set
of inputs and leaves a “small” (compact) set of inputs on which j queries more
than i does, we say that j is a weakly query sped-up version of i. Note that, we do
not care how many inputs on which the query-complexity remains unchanged.
Since T(ϕ̂j) ⊆ Q(i, j), it follows that, if Q[i<j] is compact in Q(i, j), so is it in
T(ϕ̂j), and hence we can patch ϕ̂-program j on Q[i<j] so that the patched ϕ̂-
program will never query more than i does. Whereas, if ϕ̂-program j is a strongly
query sped-up version of i, then the set on which j does not improve its query-
complexity must be “small” (compact). Note that, if Q[i<j] ∪Q[i=j] is compact
in Q(i, j), then Q[i>j] must be noncompact in Q(i, j). Clearly, for ϕ̂i = ϕ̂j , we
have the following implication:
ϕ̂i ≺≺∗Q ϕ̂j =⇒ ϕ̂i ≺∗Q ϕ̂j .
Query-Optimal Oracle Turing Machines for Type-2 Computations 299
Definition 5 (Query-optimal ϕ̂-programs). Let F : T ×N→ N be a com-
putable functional. Let e be a ϕ̂-program for F .
1. We say that e is an absolute query-optimal ϕ̂-program for F if and only if,
on every (f, x) ∈ T ×N, the OTM M̂e does not make unnecessary queries
during the course of computing ϕ̂e(f, x).
2. We say that e is a strong query-optimal ϕ̂-program for F if and only if
there is no ϕ̂-program i for F such that, ϕ̂e ≺∗Q ϕ̂i.
3. We say that e is a weak query-optimal ϕ̂-program for F if and only if there
is no ϕ̂-program i for F such that, ϕ̂e ≺≺∗Q ϕ̂i. 
Definition 5 gives us three versions of query-optimal ϕ̂-programs, where ver-
sion 1 is the strongest notion and version 3 is the weakest one. Suppose that e is
a ϕ̂-program for F : T ×N→ N. The following implications are straightforward:
1. If e is an absolute query-optimal ϕ̂-program for F , then it is also a strong
query-optimal ϕ̂-program for F .
2. If e is an strong query-optimal ϕ̂-program for F , then it is also a weak
query-optimal ϕ̂-program for F .
Moreover, if T(ϕ̂e) = Q(e), then e is an absolute query-optimal ϕ̂-program for
ϕ̂e. We shall argue that the stronger the notion of query-optimum, the easier
we can obtain a speed-up theorem. In other words, it is more difficult to obtain
a query-optimal OTM when the notion of query-optimum becomes stronger.
However, we do not know if the speed-up phenomenon still exists under our
weakest notion of query-optimum; neither do we know how far can one weaken
the notion of query-optimal programs to do away the speed-up phenomenon
while keeping the notion nontrivial. We give partial results in the next section.
4 Query-optimal Programs
With a fixed complexity measure that satisfies Blum’s axioms, the original speed-
up theorem states that there is a total computable function without optimal
program for it. Although query-complexity does not satisfy the Blum’s axioms,
each of the three versions of query-optimal programs gives rise to the following
questions:
1. Does every computable functional F : T × N → N always have a query-
optimal ϕ̂-program for it?
2. If the answer to the first question is negative, then can we construct the
query-speed-able functional?
3. If a given F : T × N → N does have a query-optimal ϕ̂-programs for it,
then can we uniformly construct a query-optimal program for F from an
arbitrary ϕ̂-program for F?
4. Suppose there is no query-optimal ϕ̂-program for ϕ̂e0 : T ×N → N. Can
we effectively construct an infinite sequence of query-sped-up version of ϕ̂-
programs from e0? That is, can we have e0, e1, e2, . . . such that, for each
i ∈ N, ei+1 is a query-sped-up version of ei?
300 Chung-Chih Li
We have a positive answer to the second question with respect to the abso-
lute and strong versions of query-optimal programs. Thus, a negative answer to
the first question follows immediately. Compared to the original nonconstruc-
tive speed-up theorem, the speedable functional for the second question is very
simple. However, we do not know if there is a computable F : T × N → N
that can forever strongly speed-up, i.e., we do not know if there is a computable
F : T ×N → N without a weak query-optimal ϕ̂-program. The answer to the
third question is negative given by Theorem 4. The fourth question is still open.
That is, given ϕ̂e0 : T ×N→ N that is known to be speedable, we do not know
if there is an effective way to construct a sequence of ϕ̂-programs such that,
ϕ̂e0 ≺∗Q ϕ̂e1 ≺∗Q ϕ̂e2 ≺∗Q · · · · · ·
or similarly,
ϕ̂e0 ≺≺∗Q ϕ̂e1 ≺≺∗Q ϕ̂e2 ≺≺∗Q · · · · · · .
Let ϕe(x) ↓s denote the case that the Turing Machine Me, on x, halts in s
steps. We now define a useful functional K : T ×N→ N in the following.
Definition 6. Define K : T ×N→ N by
K(f, x) =
{
ϕx(x) + 1 if ϕx(x) ↓s, where s =
∑f(0)
i=0 f(i);
0 otherwise.

Let H denote the set {x|ϕx(x) ↓}. For any (f, x) ∈ T ×N, if x 6∈ H, then
any ϕ̂-program that computes K, on input (f, x), may ask arbitrarily many
unnecessary queries, where the number of queries depends on the value of f(0).
It is easy to prove that there is no ϕ̂-program for K with all unnecessary queries
removed (Theorem 1). Even if we lower the standard to strong query-optimal
programs, we still cannot have a query-optimal ϕ̂-program for K (Theorem 2).
Theorem 3 states that K does have a weak query-optimal program for it.
Theorem 1. K has no absolute query-optimal ϕ̂-program.
Proof: We show that from an absolute query-optimal ϕ̂-program for K, if any,
one can construct a solution to the halting problem.
Suppose, by contradiction, ϕ̂e is an absolute query-optimal ϕ̂-program for
K. Clearly, if x 6∈ H, then for any f ∈ T , K(f, x) = 0. On the other hand, if
x ∈ H, there must be two distinct f, g ∈ T such that ϕ̂e(f, x) 6= ϕ̂e(g, x). Thus,
some queries to the oracles f and g must be made. Therefore, given any x ∈ N,
we can run ϕ̂e(∅∼0, x) first, where ∅∼0 is the zero everywhere function. Then, if
no queries were made during the course of computation, we know that x 6∈ H.
Hence, N−H is recursively enumerable, a contradiction. 
Theorem 2. K has no strong query-optimal ϕ̂-program.
Query-Optimal Oracle Turing Machines for Type-2 Computations 301
Proof: Suppose, by contradiction, ϕ̂e is a strong query-optimal ϕ̂-program for
K. Let i 6∈ H such that, for any f ∈ T , the OTM, M̂e, on (f, i), will make
oracle queries at points 0, 1, 2, . . . , f(0). Such i must exist, otherwise the halting
problem can be solved by a similar argument given for the proof of Theorem 1.
We construct a weakly sped-up version of e as shown in Figure 1.
Program
input f : T , x : N;
if x = i and f(0) is even
then output 0;
else output bϕe(f, x); /* by running bϕ-program e on (f, x) */
End program
Fig. 1. A Weakly Sped-up Version of bϕe
Let the index of the ϕ̂-program in Figure 1 be p. Clearly, if x 6= i or if f(0)
is odd, then the two ϕ̂-programs, e and p, will perform the same computation,
and hence the two make the same oracle queries. The two computations differ
on the following set: {
(f, i)
∣∣ f(0) is even} . (3)
Clearly, the set above is noncompact in Q(e, p). According to the program p and
the assumption on i, the following set is the same as (3),{
(f, i)
∣∣ card( lim
s→∞Q(s, e, f, i)) > card( lims→∞Q(s, p, f, i))
}
.
Thus, Q[e>p] is noncompact in Q(e, p). Moreover, Q[e<p] = ∅, and hence Q[e<p]
is compact in Q(e, p). It follows that p is a weakly sped-up version of e, i.e.,
ϕ̂e ≺∗Q ϕ̂p. Therefore, e cannot be a strong query-optimal ϕ̂-program for K. 
Theorem 3. There is a weak query-optimal ϕ̂-program for K.
Sketch of Proof: It is impossible to strongly speed-up any ϕ̂ forever. Otherwise,
we can reach two ϕ̂ programs e and p such that, ϕ̂e = ϕ̂p = K and ϕ̂e ≺≺∗Q ϕ̂p.
Then, we can compare the queries made during the course of the computations
between ϕ̂e and ϕ̂p to solve the halting problem. Therefore, a weak query-optimal
ϕ̂-program for K must exist. We skip details of the proof. 
However, we do not know if there is a computable type-2 functional such
that, there is no weak query-optimal ϕ̂-program for it. In other words, we do
not if there is a speed-up phenomenon associated with the notion of weak query-
optimum.
Suppose we know that a query-optimal program for ϕ̂e does exist. In general,
there is no effective way to construct a query-optimal version of e in any strength,
i.e., absolute, strong, and weak query-optimum. We state this in the following
theorem, which is the strongest version in the sense that the weakest version of
query-optimal programs is considered.
302 Chung-Chih Li
Theorem 4. There is no recursive r : N → N such that, for every ϕ̂-program
e, if ϕ̂e is total and there is a weak query-optimal program for it, r(e) is a weak
query-optimal ϕ̂-program for ϕ̂e.
Proof: By contradiction, suppose such recursive r : N→ N exists. Consider the
following program shown in Figure 2. Suppose that, by the recursion theorem,
the index of the ϕ̂-program is e.
Program e
input f : T , x : N;
Q←− Q(f(0), r(e), f, x);
if bϕr(e)(f, x) converges in f(0) steps
then output f(max(dom(Q)) + 1);
else output f(1);
End program e
Fig. 2. A bϕ-program with index e for Theorem 4
According to the construction, it is clear that e is a total ϕ̂-program. By
assumption, r(e) is a weak query-optimal version of e. On any (f, x) ∈ T ×N,
the OTM M̂r(e) will never halt in f(0) steps, otherwise the OTM M̂r(e) does
not compute ϕ̂e. This is because if the OTM M̂r(e) on (f, x) halts in f(0) steps,
Q must contain the complete collection of queries made by the OTM M̂r(e) on
(f, x). But ϕ̂e(f, x) is the value of f at a point not in Q.
Thus, we conclude that for every (f, x) ∈ T ×N, ϕ̂e(f, x) = f(1). But if that
is the case, f(0) will not be queried by the OTM M̂r(e), namely, the value of f(0)
does not affect the computation of M̂r(e) at all. Thus, if f(0) is sufficiently large,
ϕ̂r(e)(f, x) will converge in f(0) steps, and hence ϕ̂e(f, x) = f(max(dom(Q))+1).
Thus, the OTM M̂r(e) does not compute ϕ̂e.
Therefore, r(e) cannot be a weak query-optimal version of ϕ̂-program e. 
5 Conclusion
The oracle query is a unique dynamic complexity measure in type-2 computa-
tion. Although Blum’s complexity measure has created a rich chapter in ma-
chine independent complexity theory, it is not appropriate to impose the same
requirement to query complexity. We therefore provide new notions in order to
describe the concept of query-optimal programs in type-2 computation. We ob-
tain a speed-up theorem under a reasonable notion of query-optimal programs,
which means that there exists a computable type-2 functional that does not have
a query optimal OTM for it. Our version of speed-up theorem is stronger than
the classical one in a sense that the speedable functional, K, does not depend
on the speedup factor, i.e., the computable function r in equation (1).
Query-Optimal Oracle Turing Machines for Type-2 Computations 303
Clearly, there are many open questions that deserve further invitation. For
example, can we uniformly construct a query-sped-up program for our functional
K? Do we have a speed-up theorem under our weaker notions of query-optimal
programs? Some classical questions can also be asked. For examples, do we have
a union theorem? gap theorem? compression theorem? Or do we have a complex-
ity hierarchy characterized by the query-complexity? Since Blum’s two axioms
cannot be applied to the query-complexity, the approach and results must be
very different from the classical ones. It seems to us that the framework proposed
in this paper may be a feasible direction for computer science theorists to study
the query complexity closely.
References
1. Manuel Blum. A machine-independent theory of the complexity of recursive func-
tions. Journal of the ACM, 14(2):322–336, 1967.
2. Manuel Blum. On effective procedures for speeding up algorithms. Journal of the
ACM, 18(2):290–305, 1971.
3. Critian Calude. Theories of Computational Complexity. Number 35 in Annals of
Discrete Mathematics. North-Holland, Elsevier Science Publisher, B.V., 1988.
4. Nigel Cutland. Computability: An introduction to recursive function theory. Cam-
bridge, New York, 1980.
5. Martin Davis, editor. The Undecidable. Raven Press, New York, 1965.
6. Kurt Go¨del. U¨ber die la¨nge der beweise. Ergebnisse eines Math. Kolloquiums,
7:23–24, 1936. Translation in [5], pages 82-83, “On the length of proofs.”.
7. Chung-Chih Li. Asymptotic behaviors of type-2 algorithms and induced Baire
topologies. In Proceedings of the Third International Conference on Theoretical
Computer Science, pages 471–484, Toulouse, France, August 2004.
8. Chung-Chih Li. Clocking type-2 computation in the unit cost model. In Arnold
Beckmann, Ulrich Berger, Benedikt Lo¨we, and John V. Tucker, editors, Proceed-
ings of Computability in Europe, CiE 2006: Logical Approaches to Computational
Barriers, CSR 7-2006, pages 182–192, Swansea, UK, June/July 2006.
9. Chung-Chih Li. Speed-up theorems in type-2 computation. In S. Barry Cooper,
Benedikt Lo¨we, and Andrea Sorbi, editors, Proceedings of Computability in Europe,
CiE 2007: Computation and Logic in the Real World, pages 478–487, Siena, Italy,
June 2007. Springer, LNCS 4497.
10. Chung-Chih Li and James S. Royer. On type-2 complexity classes: Preliminary
report. In Proceedings of the Third International Workshop on Implicit Computa-
tional Complexity, pages 123–138, Aarhus, Denmark, May 2001.
11. A. R. Meyer and P. C. Fischer. Computational speed-up by effective operators.
The Journal of Symbolic Logic, 37:55–68, 1972.
12. Joel I. Seiferas. Machine-independent complexity theory. In J. van Leeuwen, edi-
tor, Handbook of Theoretical Computer Science, volume A, pages 163–186. North-
Holland, Elsevier Science Publisher, B.V., 1990. MIT press for paperback edition.
13. P. Van Emde Boas. Ten years of speed-up. Proceedings of the Fourth Symposium
Mathematical Foundations of Computer Science 1975, pages 13–29, 1975. Lecture
Notes in Computer Science.
14. Klaus Wagner and Gerd Wechsung. Computational Complexity. Mathematics and
its applications. D. Reidel Publishing Company, Dordrecht, 1985.
From Hilbert’s Program
to a
Logic Tool Box
J. A. Makowsky
Department of Computer Science
Technion – Israel Institute of Technology
Haifa, Israel
janos@cs.technion.ac.il
www.cs.technion.ac.il/∼janos
For Witek Marek, first mentor,
then colleague and true friend,
at the occasion of his 65th birthday.
Abstract. In this paper I discuss what, according to my long expe-
rience, every computer scientists should know from logic. We concen-
trate on issues of modeling, interpretability and levels of abstraction.
We discuss how the minimal toolbox of logic tools should look like for
a computer scientist who is involved in designing and analyzing reliable
systems. We shall conclude that many classical topics dear to logicians
are less important than usually presented, and that less known ideas
from logic may be more useful for the working computer scientist.
The following text is not a scientific paper. It is really a prose version of
a set of slides in which I present my ideas on the subject. I have presented a
first version of these slides at the LPAR’07 conference in Yerevan, Armenia, in
October 2007. I do hope that I will finally turn these thoughts into a proper
scholarly paper. In these sketchy notes I mostly give references to monographs,
and not the original papers.
The Students I Have in Mind
I want to examine what we should teach from logic to our non-specialized under-
graduate students. I mean, what does every graduate of Computer Science have
to learn in/from logic? The current syllabus is often justified more by the tra-
ditional narrative than by the practitioner’s needs. The practitioner’s needs are
determined by what he needs to understand his own activity in dealing with his
computing environment. As a computer/computing engineer he should be aware
of the inherent difference between consumer products and life-critical hardware
and software. The occasional failure of consumer goods is beneficial to the func-
tioning of the Fordistic consumer society in as it maintains the consumption cy-
cles needed for its functioning. The failure of life-critical products is disastrous
From Hilbert’s Program to a Logic Tool Box 305
for all the parties involved. Life-critical products have to be properly specified,
verified, tested and certified before they can be released. The practitioner there-
fore needs a basic understanding of what it means to properly specify,test, verify
and possibly certify. a product.
Practically speaking,
– he should understand the meaning and implications of modeling his environ-
ment as precise mathematical objects and relations;
– he should understand and be able to distinguish intended properties of this
modeling and side-effects;
– he should be able to discern different level of abstraction;
– he should master the (non-formalized) language of sets and second order
logic which enables him to speak about the modeled objects;
– he should understand what it means to prove properties of modeled objects
and relations;
– but he should also understand the inherent limitations of what can be achieved,
and of his own activity.
1 Sets and the Logical Foundations of Mathematics
Whether we like or not depends on our philosophical position, if we at all have
one, but it is a fact supported by a large social consensus, that the language of
sets is the most used and most accepted way of modeling mathematical objects.
A very convincing discussion, why sets are used that way, is given in [BG08].
We are used to model automata of all sorts including Turing machines as tuples
of sets, functions and relations. We do the same when we discuss behavior of
hardware and software, when we prove properties of modeled artefacts, and
when we show that certain combination of properties of such artefacts cannot
be achieved.
The emergence of the language of sets goes back to the work of G. Cantor and
G. Frege, who both felt the need to put mathematics on new rigorous founda-
tions upon which the growing edifice of real and complex analysis could be built.
Cantor initiated the use of sets for modeling natural, real and complex numbers
and their functions, and Frege wanted to derive the rules of set formation from
logic. Frege’s program intended to derive the foundations of mathematics from
logical principles. It derived set theory as the universal data structure for mod-
eling mathematical objects from logic. The history of logic in the years between
1850 and 1950 is the history of successes and failures of Frege’s program. This
history forms the traditional narrative along which we are used to teach logic. I
will argue that this narrative is misleading as far as the working mathematician
or computer scientist or engineer is concerned.
Let us look at this traditional narrative the way I see it. We start by para-
phrasing the history of Logicism from Frege to Go¨del, and further to the re-
evaluation of Frege’s program.
306 J. A. Makowsky
Act I: Cantors Paradise
– First G. Cantor (1874 - 1884) created the Paradise of Sets.
– Then G. Frege (1879) created the modern Logical Formalisms, including the
correct binding rules for quantification, and
– set out to lay the Foundations of Mathematics with his Die Grundgesetze
der Arithmetik, Volume1 (1893), see [Bur05]
– The book was not well received. Only G. Peano, author of The principles
of arithmetic, presented by a new method (1889), [Ken73] wrote a positive
review of it.
Act II: Paradise lost
– On 16 June 1902, Bertrand Russell pointed out, with great modesty, that
the Russell paradox gives a contradiction in Frege’s system of axioms.
– . . . and with Russel’s paradox started the crisis of the Foundations of Math-
ematics,
– G. Cantor had sensed this, when he noticed trouble with the ”set of all sets”
and his notion of cardinality.
Let V be the set of all sets. Then its power set P (V ) is a subset of V . But
Cantor proved that the cardinality of the power set P (A) of a set A is always
strictly bigger than the cardinality of A. On the other hand the cardinality
of a subset of A is at most the cardinality of the set A, a contradiction.
Act III: Hilbert’s Program
D. Hilbert around 1920 designs a program to provide a secure foundations for
all mathematics. In particular this should include1:
– Formalization of all mathematics: all mathematical statements should be
written in a precise formal language, and manipulated according to well
defined rules.
– Completeness: a proof that all true mathematical statements can be proved
in the formalism.
– Consistency: a proof that no contradiction can be obtained in the formalism
of mathematics. This consistency proof should preferably use only ”finitistic”
reasoning about finite mathematical objects.
– Conservation: a proof that any result about ”real objects” obtained using
reasoning about ”ideal objects” (such as uncountable sets) can be proved
without using ideal objects.
– Decidability: there should be an algorithm for deciding the truth or falsity
of any mathematical statement.
1 This subsection is a quote from Wikipedia. Its author is unknown.
From Hilbert’s Program to a Logic Tool Box 307
Hilbert’s Logic Lectures
In 1928, D. Hilbert and W. Ackermann publish Grundzu¨ge der theoretischen
Logik, [HA28,HA49,HA50]. Here are the points of interest to us:
– The Logic in question is Second Order Logic.
– What we call First Order Logic, is called there the restricted calculus.
– They prove soundness of the calculus, and ask the question of completeness.
The book is soon translated into English, French and Russian and remains the
most widely used reference for more than thirty years. K. Go¨del, as a graduate
student, reads the book in 1928. The original book contains several technical
mistakes which are fixed in subsequent editions. The first English edition [HA50]
gives credit to A. Church and W. Quine for pointing out some mistakes in the
second German edition.
The book states as the main problem of Logic its axiomatization and proofs
of the
– Independence of the axioms
– Consistency of the axioms
– Completeness of the axioms
– The decision problem of the conse-
quence relation
Act IV: Rise and Fall of Hilbert’s Program
Initial successes:
– Leopold Lo¨wenheim (1915), Thoralf Skolem (1920), Mojz˙esz Pressburger
(1929), Alfred Tarski (1930), Frank Plumpton Ramsey (1930), La´szlo´
Kalma´r (1939) and many others prove partial decidability results for
fragments of Logic, and for Arithmetic, Algebra, Geometry.
– In 1929 Kurt Go¨del proves the completeness of the Hilbert-Ackermann
axiomatization of the the restricted (first order) calculus.
Final blows:
– 1931 K. Go¨del proves that every recursive theory which contains arith-
metic is incomplete.
– 1931 K. Go¨del proves that every recursive consistent theory which con-
tains arithmetic cannot prove its own consistency.
– 1936 Alonzo Church and Alain Turing show that already for the re-
stricted calculus with free relation variables the set of tautologies is not
computable (but is semi-computable). Hence, they gave a negative solu-
tion to the Decision Problem.
The most comprehensive account of solvable and unsolvable cases of the
Decision Problem can be found in [BGG97].
308 J. A. Makowsky
Act V: Clarifications and Repairs
Out of the ashes rise the four classical sub-disciplines of mathematical logic:
Set Theory arises from work by E. Zermelo, D. Mirimanoff, J. von Neumann,
A. Fra¨nkel, K. Go¨del and P. Bernays. Alternative approaches were developed
by, among others, W. Quine, W. Ackermann, and J.L. Kelley and A.P. Morse,
and more recently, by P. Aczel.
Set theory, in contrast to using sets in mathematical practice, is mostly
concerned with settling questions around the axiom of choice and cardinal
arithmetic, or in formulating alternatives, such as the axiom of determinacy,
and in clarifying their impact on questions in topology and analysis. Today,
set theory is a highly specialized branch of technical mathematics with little
impact on computer science.
Proof Theory arises from work by W. Ackermann, G. Gentzen, J. Herbrand,
D. Hilbert and P. Bernays.
It has developed into a full-fledged theory of proofs, comprising the analy-
sis of (transfinite) consistency proofs in terms of ordinals and fast-growing
functions, program extraction from proofs, and resource analysis related to
provability. It also plays an important role in all aspects of automated rea-
soning, an important branch of Artificial Intelligence.
Recursion Theory arises from work by E. Post, J. Herbrand, K. Go¨del, A.
Church, A. Turing, H. Curry. Recursion theory developed at one side into
degree theory, classifying non-computable functions according to their dif-
ferent levels of complexity, at the other side it developed into Computability
Theory and has become one of the pillars of Computer Science education in
its own right.
Model theory arises from work by T. Skolem, A. Tarski, A. Robinson, R.
Fra¨ısse´ and A. Mal’cev.
Two main directions evolve, classification theory, and a more algebraic and
geometric theory, linking model theory with algebraic geometry and number
theory. Although finite model theory has its early origin, it was through
Automata Theory, Database Theory and Complexity Theory that it evolved
into its own discipline with a legitimate place in advanced Computer Science
education.
. . . and for long this remained the classical divide of Mathematical Logic.
J. Shoenfield’s monograph [Sho67] is possibly the only monograph covering
all aspects of Mathematical Logic up to the boundaries of research of his time.
Since then the four classical disciplines pursue their own paths, and among the
younger generations of researchers it cannot be taken for granted that they have
studied the four disciplines in depth.
Act VI: 100 years later - Fixing Frege
If only G. Frege had not been so scared by B. Russel’s letter. C. Wright, P.
Geach and H. Hodes suggested, and G. Boolos proved (1987) that a modified
From Hilbert’s Program to a Logic Tool Box 309
Frege program actually is feasible, [Boo98a,Boo98c,Boo98b]. They noted that
the famous contradiction stemmed from the axiom which states, roughly, that
the extension of any concept is a set. However, this axiom is only used to derive
an abstraction principle, called Hume’s principle, which states, again roughly,
that two extensions have the same cardinality if and only there is a bijection.
So we have for the modified Frege system:
Frege: The Peano Postulates can be deduced in dyadic second order logic from
Hume’s principle and suitable definitions of the natural numbers (Frege’s
Arithmetic).
Boolos: Frege’s arithmetic is interpretable in second order Peano Arithmetic.
Some (the Neo-Logicists) argue that this justifies a revival of Logicism. But
it also creates new problems. A thorough discussion of the pros and cons of
Neo-Logicism can be found in J. Burgess [Bur05]. A thorough discussion of ab-
straction principles similar to Hume’s Principle can be found in K. Fine [Fin02].
That much for the ”big crisis”.
At least the set theory needed for the foundations of Computer Science can
be derived from logical principles.
2 The Foundations of Mathematics and Computer
Science
What Frege and Russel and Whitehead had in mind, viz. to build the foundations
of mathematics from scratch, was done in a more intelligible (but still not too
user friendly) way by E. Landau in his Foundations of Analysis first published
in German in 1930, and in English in 1951, with many reprints, the latest in
1999 with a German-English vocabulary by the American Mathematical Society,
[Lan99]. In this book he explicitely constructs the real and complex numbers from
the standard model of Peano arithmetic. Landau’s style is very dry and concise,
and the text was written for mature mathematicians. A more pedagogical version
of the same constructions can be found in S. Feferman’s [Fef64], which I also
would love to see reprinted. One can view such a foundation of Analysis as a
pragmatic version of Frege’s program. Roughly, one proceeds as follows:
– One starts with a cumulative hierarchy of sets, based on the empty set alone
(or with urelements) and natural set construction principles which allow to
construct also infinite sets.
– Then one defines inductively the natural numbers with a successor function,
and the sets of finite words over a (not necessarily) finite alphabet (a set),
with an append operation for each element of the alphabet.
310 J. A. Makowsky
– One then proceeds with defining the number systems N, Z, Q and their
arithmetic operations inductively and using quotient structures.
– Then one construct the reals R, using Dedekind cuts.
– In similar ways one constructs other structures, say, groups, fields, topolog-
ical spaces, Banach spaces, Lie algebras, which are specified axiomatically.
– Existence of axiomatically defined objects had to be established by an explicit
sequence of set construction steps within the cumulative hierarchy.
– Clearly, one can apply the same methods to model objects of the computing
world, such as automata, formal languages, programs, data structures, etc.
One should adapt Landau’s way for modeling the basic data structures of Com-
puter Science. I have attempted to do this in the course Sets and Logic for
Computer science, which we teach in the third semester in our Computer Sci-
ence undergraduate program.
Set Theory in Computer Science
Besides using set theory for modeling purposes, the computer scientist uses only
few ingredients of set theory:
1. The Cantor-Bernstein Theorem to prove equicardinality,
2. The fact that a countable union of countable sets is countable,
3. The fact that the cardinality of the power set of a set A is always bigger
than that of A,
4. The relationship between the termination of processes and well-orderings.
5. The Recursion Theorem.
6. Some Fixed-Point Theorems.
The first three of these go ultimately back to Cantor’s original work and are very
basic. The Recursion Theorem and Fixed Point Theorems should be taught in
a more advanced course.
Recursion Theory vs Computation Theory
Recursion Theory got its name for a good reason: The computable functions over
the natural numbers were defined recursively, and early Recursion Theory con-
sisted in studying the strength of various proposed recursion schemes. Recursion
schemes can be replaced by register machines. Computability Theory studies
usually the computable relations and functions over sets of words. The three
approaches are inter-translatable, but they are not the same. It is a pity that
teaching all these complementary notions of computability is not always part of
the Computer Science curriculum. Be that as it may, Computation Theory has
emancipated itself from Logic and in Computer Science the two are often taught
independently. A good exception is the monograph [Pap94].
From Hilbert’s Program to a Logic Tool Box 311
Proof Theory
Proof theory evolved around the question what type of consistency proofs are
at all possible. As a spin-off the field of deduction-based automated reasoning
and automated theorem proving came into being. Proof theory is also used in
the foundations of programming languages, where it generated a rich literature
of deep insights into the nature of programming. Some basic principles of auto-
mated reasoning do belong into a beginner’s course on Artificial Intelligence, and
some basic facts about functional programming do belong into a basic course on
Programming Languages. However, only little of this rich material is suitable
for the undergraduate student I have in mind. A comprehensive survey of Proof
Theory is [Bus98].
Model Theory
The main tools of classical model theory almost all derive from the compactness
theorem and variations of the model existence theorem. Using these tools one
proves preservation theorems, the omitting types theorem and develops a general
understanding of the possible structure of models of first order theories. I have
described how to use these tools in the Computer Science context, in database
theory, the foundations of Logic Programming, and the specification of data
types, [Mak84,Mak92]. It turned out, however, that the Compactness Theorem
is mostly suitable when dealing with infinite structures, and the most prominent
application of the Compactness Theorem in Computing is Herbrand’s Theorem
with its ramifications in Automated Reasoning and Logic Programming. The
most important tools from Model Theory in algorithmic applications are the
Ehrenfeucht-Fra¨ısse´ games and the Feferman-Vaught Theorem and its variations.
The former is omnipresent in Finite Model Theory, cf. [EF95,Lib04] and the a
survey of the uses of the latter can be found in [Mak04]. For other failures of
classical theorems of First Order Logic when restricted to the finite case, cf.
[Gur88].
One should add here that the combination of the Compactness Theorem with
the Ehrenfeucht-Fra¨ısse´ games leads to Lindstro¨m’s characterization of First Or-
der Logic. The attempts to develop an Abstract Model Theory are documented
in the monumental [BF85]. This line of reasoning had a considerable impact
on Finite Model Theory and Descriptive Complexity in providing techniques for
defining logics which capture complexity classes. For the advanced student appli-
cations of Finite Model Theory to Computer Science are surveyed in [GKL+07].
The classical textbooks in Logic
The available undergraduate texts of Logic for Computer Science follow too often
the narrative of the em Rise and Fall of Hilbert’s Program. They emphasize the
classical Hilbertian topics.
– Logic is needed to resolve the paradoxes of set theory.
312 J. A. Makowsky
– First Order Logic is THE LOGIC due to its completeness theorem.
– The main theorems of logic are the
Completeness Theorem and the Compactness Theorem
– The tautologies of First Order Logic are not recursive.
– Arithmetic Truth is not recursive enumerable.
– One cannot prove CONSISTENCY within rich enough systems.
This is NOT what a Practitioner
of Computing Sciences NEEDS !
Other texts are often written with a very special agenda reflecting the au-
thor’s research interest or his particular tastes. Finally, there are texts which are
really written with the undergraduate Computer Science student in mind. But
then they are often either written specifically with programmers in mind, and
do not deal with the data modeling issues2. Logic is first of all language in which
we express ourselves before we prove statements. We first have to formulate a
specification, a database query, an intermediate assertion, a loop invariant, be-
fore we prove them to hold, to be valid, or for two of them to be equivalent or
not. Logic deals with definability issues as much as with provability issues,
something which in the Hilbertian tradition is easily forgotten. An introductory
text in Logic for Computer Science should choose its topics in way, that the
student meets the topics taught again in later courses. When we teach Linear
Algebra in the first year of a mathematics curriculum, most of the topics reap-
pear in vector analysis, differential equations, physic, statistics etc. We have to
build our syllabus of logic keeping this in mind.
3 So what Does a Practitioner of Computing Sciences
Need?
We distinguish between knowledge of theoretical orientation and practical knowl-
edge, which consists of tools and skills. In our case this means:
Theoretical orientation:
– awareness that our domain of discourse is an
idealized world of artefacts which models fairly accurately the artefacts
which allow us to run and interact with computing machinery.
– awareness of the different levels of abstractions.
– awareness that in this world of artefacts there are
a priori limitations. Not everything is realizable, computable, etc.
2 The recent book by R. Bornat[Bor05] is a lovely introduction to Logic for program-
mers.
From Hilbert’s Program to a Logic Tool Box 313
Practical knowledge:
– tools which allow us to model new artefacts, whenever they arise;
– tools which allow us to prove properties of the modeled artefacts.
The student needs a carefully adapted blend of the practical Frege program,
with the knowledge of its limitations. He needs both proficiency and performance
in his practical knowledge.
4 Lessons from 150 years of History
I have spent so much space reviewing what I consider noteworthy in the evolution
of the Logicists program because I do want to draw some lessons from it which
are not foundational but practical. I would like the B.Sc. graduates of Computer
Science to be familiar with the following:
Lesson 1. Modeling the world
Our scientific language: Natural Language enhanced by precise use of boolean
operations, quantification and the use of naive language of sets.
Our universal data structure: A cumulative world of sets.
Modeling the world: We model all artefacts of our computing world by con-
structed objects in the world of sets.
Modeling involves side effects:
Modeled artefact have properties not intended.
Digression: the ordered pair: Ordered pairs could be introduced via an ab-
straction principle:
(x, y) = (x′, y′) if and only if x = x′ and y = y′.
But usually the ordered pair is modeled directly:
N. Wiener: (x, y)W := {{{x}, ∅}, {{y}}},
K. Kuratowski: (x, y)K = {{x}, {x, y}},
Simplified : (x, y)S = {x, {x, y}}.
Now one has to verify that (x, y) = (x′, y′) if and only if x = x′ and y = y′.
All the proposed versions do satisfy this, but the proofs differ. The simpli-
fied version requires the axiom of foundation. Kuratowski’s version is the
accepted definition today. But all definitions have side effects, e.g., x is an
element of {x, {x, y}} but not of {{x}, {x, y}}. Proving properties of objects
which depend on the use of ordered pairs should not use these side effects,
but only the defining property.
The distinction between specified properties
and side effects should be taught early on!
Fixing levels of abstraction: Introducing structures, and fixing which sets
are not further to be analyzed.
A graph is a pair < V,E >.
A finite automaton is a tuple < S,Σ,R, I, T >.
314 J. A. Makowsky
Like in the foundations of Analysis, as practiced by R. Dedekind, E. Landau and
N. Bourbaki, we need the precise language mix of normalized natural language
augmented by the language of sets to model the idealized artefacts of computer
science. To model the artefacts we also need basic tools.
Artefacts:
strings, concatenation, natural numbers,
graphs, relational structures stacks, arrays;
circuits, Turing machines, register machines;
specification and programming languages,
Tools: Inductive definitions, proofs by induction;
enumerations,
proving countability and uncountability;
well-orderings (for termination)
Is this not ”too denotational” ? ....
... our friends may ask.
Yes, this approach does map everything into sets. But “truth” does not nec-
essarily presuppose a world of sets. Truth in the sense of Frege’s world is defined
by the laws (introduction and elimination rules) of logic and of the Fregean
constructs. It does leave your foundational options open ...
Lesson 2. Modeling Computability and its limitations (when
modeled)
We have already said that computability is usually taught in a separate course,
be it together with formal languages or with an introduction to basic complex-
ity theory. Nevertheless, our student should understand that computability is
modeled over different domains, computing operations, resource restrictions.
Natural numbers and recursion:
The original definition of the set of recursive functions.
Natural numbers and register machines:
Close to early programming languages.
Turing machines and words: Close to assembly languages.
Other models: Logic programs, Lambda calculus, cellular automata, quantum
computing
Showing their equivalence involves modeling also
– translation between the domains;
– translations between programs (interpreters and compilers).
Here I want to stress The different basic structures involved, and their bi-
interpretability. In terms of knowledge of orientation and practical knowledge
we have:
From Hilbert’s Program to a Logic Tool Box 315
Orientation:
Not everything is computable.
Not everything is feasibly computable.
Tools:
Using the non-solvability of the Halting Problem to prove non-computability.
Using different types of reducibilities (and simulations).
I have observed that even my colleagues are sometimes imprecise: The Church
Turing Hypothesis is often carelessly invoked. There is also a trend to say com-
putable when actually one means feasibly computable, where feasibly computable
may mean computable in deterministic polynomial time, sometimes computable
in polynomial time with randomized algorithms.
It is also important to distinguish between complexity classes defined as equiv-
alence classes of problems under certain reductions, and sometimes defined as
classes of decision (counting, approximation) problems solvable in a specific com-
putational model.
Using polynomial time Turing reductions, the class [SAT ]T of problems re-
ducible to SAT is of the first type, NP is of the second type, and NP = [SAT ]T
is a theorem. In the case of counting problems [#SAT ]T is of the first type, #P
is of the second type, and #P = [#SAT ]T is not true. Using reductions in
First Order Logic we still have [SAT ]FOL = NP , but not every problem X,
which is NP -complete with respect to polynomial time Turing reductions satis-
fies [X]FOL = [SAT ]FOL.
It is important to insist that slogans
are replaced by precise definitions.
Lesson 3. Modeling Syntax and Semantics
We look at Propositional, First Order, Second Order Logic, or any other logic
of assertions. Again we model them in our framework of sets.
Syntax:
The syntax is an inductively defined set of words, the well formed
expressions.
Semantics: Structures are interpretations of the basic non-logical symbols.
Assignments are interpretations of the variables. The meaning function as-
sociates with structures, assignments, and formulas a truth value.
What is the meaning of an assertion ?
316 J. A. Makowsky
Without free variables: The meaning of an assertion is a truth value.
But this is misleading!
With free variables: The meaning of an assertion is the set of interpretations
of its free variables. In the case of first order variables only it is a relation.
As in Classical Geometry one speaks of the geometrical lieu of all points
satisfying an equation, we can speak of the logical lieu defined by a formula.
In modern data base parlance this is called a query.
We define usually logical validity via truth values. It would be preferable to define
validity and logical consequence directly for formulas with free variables.
Our student is more likely to meet in the sequel of courses formulas with free
variables that just formulas without.
Do we need the Completeness Theorem?
For the practical knowledge we need:
– The semantic notion of logical consequence.
– Enough basic logical equivalences to to prove the Prenex Normal Form The-
orem (PNF).
– Introduction and elimination rules for quantifiers (via constants).
– A game theoretic interpretation of formulas in PNF.
For the knowledge of orientation we might state (but not prove) the Com-
pleteness Theorem for our redundant set of manipulation rules.
Here are the arguments for and against proving the Completeness Theorem
in the first course of Logic.
The classical argument pro:
– Completeness and its corollary, Compactness is at the heart of logic.
My arguments against:
– None of these are part of the practical knowledge we aim at.
– The proof of the Completeness Theorem is a waste of time at the expense of
teaching more the important skills of understanding the manipulation and
meaning of formulas.
– First Order Logic is not privileged in our context. We deal very often with
finite structures, where the Completeness Theorem is not true.
– Second Order Logic anyhow is the natural logic we work in, and not taking
that seriously confuses the student.
From Hilbert’s Program to a Logic Tool Box 317
We should instead concentrate on
understanding quantification
As tools we need to
– Read, write and understand the meaning of First Order and Second Order
formulas.
– Understand the relationship between projection of relations and first order
quantification.
– Understand that Relational Calculus and First Order Logic are really the
same (i.e., bi-interpretable).
– Introduce immediately after the proof of the Prenex Normal Form Theorem
the Ehrenfeucht-Fra¨ısse´ Game, and proceed to show the easy direction of
the Ehrenfeucht-Fra¨ısse´ Theorem, i.e., if a formula (say in Prenex Normal
Form) of quantifier rank k is true in one but not in another structure, then
we can derive from the formula a winning strategy for player I (the spoiler)
for the game with k moves.
– Play with the game interpretation of quantifiers to analyze the amount of
quantification needed to express, say ”there exists at least n elements x such
that φ(x)”.
– One can even point out that (easy direction) of Ehrenfeucht-Fra¨ısse´ Theorem
holds also for Second Order Logic.
Lesson 4. Limitations of formalisms: Definability
Before we find time to prove the Completeness Theorem, I would like the students
to understand the difference between First Order (FO) and Second Order (SO)
Logic.
– Look at the statement
”There are an equal number of x
with P (x) and with Q(x)”
where P,Q are unary predicate symbols.
This is expressible in SO but not in FO, and we can even show the proof
having the Ehrenfeucht-Fra¨ısse´ Games available.
– We can even be daring, and show that connectedness on finite graphs is
SO-definable, but not FO-definable.
– In the natural numbers N,multiplication is SO-definable, but not FO-definable,
using addition only. However, multiplications is FO-definable using addition
and squaring.
The negative result we cannot prove in an undergraduate course, as we need
the decidability of FO Pressburger Arithmetic. But we can explain it.
318 J. A. Makowsky
Lesson 5. Interpretability and Reducibility
Again before we use our time prove the Completeness Theorem I would like
the students to understand what it means that a structure is FO-interpretable
in another structure. Let look at the case of the natural numbers N and the
integers Z with their arithmetic operations.
The integers Z with their arithmetic are FO-interpretable inside the nat-
ural numbers N with their arithmetic.
To get the interpretation we define a new structure from N, called a transduction
T (N), and which will be isomorphic to Z, as follows.
– The new universe consists of equivalence classes of pairs of natural numbers
such that (x, y) ∼ (x′, y′) iff x+ y′ = x′ + y.
– The new equality is this equivalence.
– The new addition is the old addition on representatives.
– Same for multiplication.
T is a semantic map. Its syntactic counterpart is the interpretation S : Formulas→
Formulas, defined as follows:
For any SO-formula φ we let S(φ) be the result of substituting the new
definitions of addition and multiplication and equality for the corresponding
symbols. In the exact definition one has to be careful with the renaming of free
variables.
S and T are intimately related:
Z = T (N) |= φ iff N |= S(φ)
which is the Fundamental Property of Transductions and Interpretations.
In the same way we can see that
– The Cartesian product is interpretable in the disjoint union.
– Many graph transformations are given as transductions.
– All implementations of one data structure in another are of this form.
– Transductions and interpretations are everywhere
5 The Fundamental Properties of SO and FO
In teaching our students to think and speak Second Order Logic, we should teach
– that isomorphic structures satisfy the same SO sentences;
– the Fundamental Property of Transductions and Interpretations;
– the Prenex Normal Form Theorem and its visualization as a two person
game.
and we should practice thinking in SO as the natural language of specifying
properties of modeled artefacts.
From Hilbert’s Program to a Logic Tool Box 319
The Fundamental Properties of FO
Besides the properties of SO we have
The Ehrenfeucht-Fra¨ısse´ Theorem:
Two structures can be distinguished by a sentences of quantifier depth k
iff Player I (the spoiler) can force a win in the EF-game of length k.
or, equivalently
Two structures cannot be distinguished by a sentences of quantifier depth
k iff Player II (the duplicator) can force a win in the EF-game of length
k.
We say that two structures are k-isomorphic if Player II can force a win in the
EF-game of length k.
Furthermore:
k-isomorphism is preserved under the formation of disjoint unions of
structures.
Modified versions also hold for Monadic Second Order Logic, but not for SO.
Combining EF-Games and Interpretations
Combining games and interpretations gives a very powerful tool to compute
the meaning function of a FO formula in a complex structure by reducing this
computation to simpler structures.
If G is obtained from graphs H1,H2 by applying disjoint unions, Cartesian
products, and first order definable transductions T1, T2, say
G = T1(H1 × T2(H2))
then the truth of the formulas of quantifier rank k in G is uniquely and effectively
determined by the the truth of the formulas of quantifier rank k which hold in
H1 and H2.
This is the Feferman-Vaught Theorem. It allows us to compute the meaning
function for FO-formulas (or MSO-formulas) of composite structures by reducing
its computation to the meaning functions of the formulas on the components. I
have surveyed how to use the Feferman-Vaught Theorem in Computer Science
in my paper in [Mak04].
6 My Logic Tool Box
So we finally come to the description of the Logic Tool Box I would like to
give to our students. Tools to do what, you will ask. Tools to think rigorously in
order to approach the disciplines of programming and information processing,
tools to model accurately new artefacts, as they occur, tools to grasp the scope
320 J. A. Makowsky
of abstraction and modularity. Our students are not logicians. Logic per se is not
their main interest. Logic is for Computer Science what Hygienics is to Medicine.
They should learn rigorous informal reasoning before they learn to model
this kind of reasoning as formalized proof sequences. Needless to say that each
tool comes with a required skill how to use it.
My Logic Tool Box contains:
Modeling tools:
– Basic set construction principles;
– Inductive definitions;
– Proofs by induction;
– Basic cardinality arguments.
Logic tools:
– Propositional Logic and its axiomatization.
– Second Order Logic as the main formalism to express properties of the
modeled artefacts.
– The semantic notion of logical consequence and validity.
– Validity over finite structures.
– Quantifier manipulation rules.
– Skolem functions.
– The Fundamental Property of Transductions and Interpretations.
– First Order Logic as an amenable fragment of Second Order Logic.
– The Ehrenfeucht-Fra¨ısse´ Theorem and its refinements.
– The Feferman-Vaught Theorem and its variations.
We said before that the Completeness Theorem for First Order Logic holds only,
if we define validity over all First Order Structures. For Second Order Logic one
would have to explain the difference between Henkin’s notion of validity and
standard second order quantification. Just stating the Completeness Theorem
for First Order Logic misleads the student, and explaining its true subtleties
may be beyond the undergraduate level.
Where these tools work
I have chosen the Logic Tool Box with a view on the courses our student has to
take during his undergraduate studies. Ideally, the course I have in mind, Sets
and Logic for Computer Science, should be taught in the second or third
semester. The student should have studied alreadyDiscrete Mathematics and
Algorithms and Data Structures, so the teacher can rely on the intuition of
the students, and the examples developed in these courses. The course should
play the same role as the course N
¯
umber Systems used to play when it was still
customary to teach it, cf. [Fef64],
The modeling skills taught in our course should help him in the following
(usually compulsory) courses:
– Automata and Formal Languages
– Introduction to Computability
From Hilbert’s Program to a Logic Tool Box 321
– Database Systems
– Graph Algorithms
– Principles of Programming Languages
– Computer Architecture
– Introduction to Artificial Intelligence
The more specialized topics of Logic should be taught in advanced courses: A
course Advanced Topics in Logic could have three parts, covering the Com-
pleteness and Incompleteness Theorem, Ordinals and Termination, and Tempo-
ral and Modal Logic. Other topics belong there where they are rally used, in
the courses on Verification, Automated Theorem Proving, Principles of
Logic Programming, Database Theory, Functional Programming and
so forth.
7 What was omitted?
I have not included in my discussion what is called in the standard classification
Non-classical Logics. These logics can also be modeled using set-theoretic tools,
and indeed they are. When they find applications to Computer Science, as Tem-
poral Logic [MP95] or the Logic of Knowledge, [FHMV95], they also find there
way into more advanced courses.
I have not included in my discussion two classical concerns of the debate
around the Foundations of Mathematics and Computer Science: Epistemology
and degrees of constructivism. A delightful and insightful presentation and dis-
cussion of these matters from a contemporary point of view can be found in
[Sha00]. Although I tend to be a Platonist, viewing mathematical concepts as
real, I am aware of the difficulties inherent in this position, cf. [Mad90,Mad98].
I am also aware of the social and cultural mechanisms at work which strongly
influence how science evolves, cf. [Wil81]. However, I strongly object to the argu-
ments which take the social and cultural mechanisms at work as a justification
for the erroneous claim that scientific truth is purely social and cultural.
From a more pragmatic point of view I tend to be a Formalist, viewing
the observable part of the mathematical and logical enterprise as happening on
(virtual) paper written with (virtual) pencils. Concerning the degrees of con-
structivism I subscribe to, I only want to remark that often non-constructive is
confused with lack of detail. The axiom of choice is an example of lack of detail.
I assume that the choice function exist and I want to proceed from there, filling
in the details (implementation) later, or leaving them to others. Software engi-
neering always proceeds like this and is not considered non-constructive even by
the most extreme constructivists. Using a cardinality argument to prove the ex-
istence of, say, transcendental numbers, expander graphs or other combinatorial
objects, is considered non-constructive, but can be explained in the same way.
Our students, however, should rather follow the advise of the Rabbinic Sages,
who admonish us not to study Kabbala (Jewish Mysticism) before the mature
age of forty years and before serious exposure to the more down-to-earth matters
of Talmud and Torah. Our students should view the Philosophy of Mathematics
322 J. A. Makowsky
and of Computer Science as something to be left for later. Children do not
question linguistic principles before they learn their first language. Scientists
should not question Science before they master the craft.
Acknowledgments
I would like to thank N. Francez, D. Giorgetta, S. Halevy and D. Hay for stimulating
discussions and suggestions about how to teach Logic to Computer Science students.
I would like to thank the Trade Union of University Professors (Irgun HaSegel) of
Israel for giving me time to prepare this paper. At the moment of completion we were
in the forth week of our teaching strike.
References
[BF85] J. Barwise and S. Feferman, editors. Model-Theoretic Logics. Perspectives
in Mathematical Logic. Springer Verlag, 1985.
[BG08] A. Blass and Y. Gurevich. Why sets? In A. Avron, N. Dershowitz, and
A. Rabinowich, editors, Pillars of Computer Science: Essays Dedicated to
Boris (Boaz) Trakhtenbrot on the Occasion of His 85th Birthday, volume
4800 of Lecture Notes in Computer Science, page In press. Springer, 2008.
[BGG97] E. Bo¨rger, E. Gra¨del, and Y. Gurevich. The Classical Decision Problem.
Springer-Verlag, 1997.
[Boo98a] G. Boolos. The consistency of Frege’s “foundations of arithmetic”. In Logic,
Logic, Logic, pages 182–201. Harvard University Press, 1998.
[Boo98b] G. Boolos. Logic, Logic, Logic. Harvard University Press, 1998.
[Boo98c] G. Boolos. On the proof of Frege’s theorem. In Logic, Logic, Logic, pages
275–90. Harvard University Press, 1998.
[Bor05] R. Bornat. Proof and Disproof in Formal Logic. Number 2 in Oxford Texts
in Logic. Oxford University Press, 2005.
[Bur05] J.P. Burgess. Fixing Frege. Princeton University Press, 2005.
[Bus98] S. Buss, editor. Handbook of Proof Theory, volume 137 of Studies in Logic
and the Foundations of Mathematics. Elsevier Science Publishers, 1998.
[EF95] H.D. Ebbinghaus and J. Flum. Finite Model Theory. Perspectives in Math-
ematical Logic. Springer, 1995.
[Fef64] S. Feferman. The number systems: foundations of algebra and analysis.
Addison-Wesley, 1964.
[FHMV95] R. Fagin, J. Halpern, Y. Moses, and M. Vardi. Reasoning About Knowledge.
MIT Press, 1995.
[Fin02] K. Fine. The Limits of Abstraction. Oxford University Press, 2002.
[GKL+07] E. Gra¨del, P. Kolaitis, L. Libkin, M. Marx, J. Spencer, M. Vardi, Y Venema,
and S. Weinstein. Finite Model Theory and its Applications. Springer, 2007.
[Gur88] Y. Gurevich. Logic and the challenge of computer science. In E. Bo¨rger,
editor, Trends in Theoretical Computer Science, Principles of Computer
Science Series, chapter 1. Computer Science Press, 1988.
[HA28] D. Hilbert and W. Ackermann. Grundzuge der theoretischen Logik.
Springer, 1928.
[HA49] D. Hilbert and W. Ackermann. Grundzuge der theoretischen Logik, 3rd
edition. Springer, 1949.
From Hilbert’s Program to a Logic Tool Box 323
[HA50] D. Hilbert and W. Ackermann. Principles of Mathematical Logic. Chelsea
Publishing Company, 1950.
[Ken73] H.C. Kennedy. What Russel learned from Peano. Notre Dame Journal of
Formal Logic, 14.3:367–372, 1973.
[Lan99] E. Landau. Die Grundlagen der Analysis. American Mathematical Society,
1999.
[Lib04] L. Libkin. Elements of Finite Model Theory. Springer, 2004.
[Mad90] P. Maddy. Realisms in Mathematics. Oxford University Press, 1990.
[Mad98] P. Maddy. Naturalisms in Mathematics. Oxford University Press, 1998.
[Mak84] J.A. Makowsky. Model theoretic issues in theoretical computer science, part
I: Relational databases and abstract data types. In G. Lolli and al., editors,
Logic Colloquium ’82, Studies in Logic, pages 303–343. North Holland, 1984.
[Mak92] J.A. Makowsky. Model theory and computer science: An appetizer. In
S. Abramsky, D. Gabbay, and T. Maibaum, editors, Handbook of Logic in
Computer Science, volume 1, chapter I.6. Oxford University Press, 1992.
[Mak04] J.A. Makowsky. Algorithmic uses of the Feferman-Vaught theorem. Annals
of Pure and Applied Logic, 126:1–3, 2004.
[MP95] Z. Manna and A. Pnueli. Temporal verification of reactive systems. Springer,
1995.
[Pap94] C. Papadimitriou. Computational Complexity. Addison Wesley, 1994.
[Sha00] S. Shapiro. Thinking about Mathematics. Oxford University Press, 2000.
[Sho67] J. Shoenfield. Mathematical Logic. Addison-Wesley Series in Logic.
Addison-Wesley, 1967.
[Wil81] R.L. Wilder. Mathematics as a Cultural System. Pergamon Press, 1981.
From Program Verification to Certified Binaries?
The Quest for the Holy Grail of Software Engineering
Angelos Manousaridis, Michalis A. Papakyriakou, and Nikolaos S. Papaspyrou
National Technical University of Athens
School of Electrical and Computer Engineering
Software Engineering Laboratory
Polytechnioupoli, 15780 Zografou, Athens, Greece
{amanous, mpapakyr, nickie}@softlab.ntua.gr
Abstract. The long tradition of formal program verification and the
more recent frameworks for proof-carrying code share a common goal:
the construction of certified software. In this paper, mainly through a
simple motivating example, we describe our vision of a complete hybrid
system that combines the two approaches. We discuss the feasibility of
such an ambitious project and report on progress made so far.
Key words: Formal methods, type systems and type theory, certified
code, proof-preserving compilation.
1 Introduction
Program verification aims at formally proving the correctness of a computer
program, with respect to a certain formal specification or property. As a research
field of computer science, program verification is well into the fourth decade
of its existence. However, it can hardly be argued that it is often adopted in
practice by software engineers, except for verifying mission-critical systems. For
the vast majority of software systems, quality assurance amounts to dynamic
testing, which unfortunately can produce no guarantees. In this respect, software
engineering, defined as “the application of a systematic, disciplined, quantifiable
approach to the development, operation, and maintenance of software” [1], is
still very far from reaching the maturity of other branches of engineering.
Several formal logics, their majority greatly influenced by Hoare Logic [2],
have been proposed in combination with programming languages as the vehicles
for program verification [3]. Most of the proposed approaches advocate a clear
separation between the language in which specifications are given (e.g. first-
order predicate logic), the programming language, and the methodology and the
tools—if any—that support the construction of proofs.
? This work has been funded by the research programme ΠENE∆ (grant number
03E∆ 330), cofinanced by public expenditure (75% by the European Social Fund and
25% by the Greek Ministry of Education, General Secretariat of Research and Tech-
nology) and by the private sector, under measure 8.3 of the Operational Programme
“Competitiveness” in the European Union’s 3rd Community Support Framework.
From Program Verification to Certified Binaries 325
Proof-carrying code [4, 5] and foundational proof-carrying code [6] are general
frameworks, expressing a relatively modern philosophy towards the verification
of low-level (e.g. machine language) programs. A certified binary is a value (a
function, a data structure, or a combination of both) together with a proof that
the value satisfies a given specification. Certified binaries are essential in mod-
ern distributed computer systems, where executable code is transferred among
computing devices that do not necessarily trust one another. The recipient of a
certified binary does not need to trust the producer: the proof can be mechan-
ically checked and, once found valid, it is known beyond doubt that the binary
conforms to its specification. Existing compilers that produce certified binaries
have mostly focused on simple memory and control-flow safety properties. Al-
though the two frameworks are general enough to express arbitrary program
properties, in the general case not much is known on how to construct certified
binaries or how to automatically generate them from high-level source programs.
More recently, type-theoretic frameworks for constructing, composing and
reasoning about certified software have been proposed [7, 8], based on the “formu-
lae as types” principle [9]. The type-theoretic approach provides an embedding
of logic in the type system of the programming language: program properties
are encoded in types and proof checking is reduced to type checking. In analogy
to a type-preserving compiler, which uses a typed intermediate [10] and a typed
assembly language [11] and propagates type information from the source pro-
gram down to the lower-level equivalent programs, a type-theoretic framework
for certified binaries can support proof-preserving compilation. Provided that a
common logic (type language) is used for expressing properties and proofs, from
the source language to the target language, certified binaries can be generated
by compiling previously verified source programs. This is important, because
high-level programs are easier to reason about than low-level programs.
Still, in a type-theoretic framework such as that proposed by Shao et al.
[7], constructing a proof of correctness even for a small program, written in an
appropriate high-level source language, is far from simple. As the logic is part
of the programming language (more accurately, part of the type language), the
proof must be embedded in the code and has to be constructed at the same time
with it. Although this has long been proposed as the “right way” to produce
software [12, 13], it is not popular with programmers, who generally prefer to
write down their algorithm first and then (if ever) prove its correctness. Further-
more, if something in the specification changes, the code has to change as well
and, sometimes, the modifications can be substantial in size even if the code’s
operational behaviour does not change.
Due to the complexity of the type languages used in the type theoretic frame-
works that support proof-preserving compilation, type inference (or proof infer-
ence) is in general undecidable. The type system of the source language cannot
do miracles. It can therefore be argued that, although the embedding of logic in
the programming language is appropriate for the lower-level languages used by
the compiler, it is not appropriate for the source language, in which the task of
constructing the proof is—more or less—the programmers’ responsibility.
326 Angelos Manousaridis et al.
Fig. 1. Overview of a hybrid system for generating certified binaries.
In this paper, we register our dream of a hybrid system (half based on tra-
ditional program verification and half type-theoretic). Although similar dreams
must be common among computer scientists who advocate program verification
and proof-carrying code, it seems that they are still rather far from becoming
reality. We outline our experience—limited, so far—in building such a system. If
it turns out that, with the assistance of appropriate program verification tools,
programmers are able to prove the correctness of their programs (we want to be
optimistic and believe this hypothesis to be true), such a system can be thought
of as the Holy Grail of software engineering.
2 A Hybrid System for Generating Certified Binaries
A hybrid system for generating certified binaries from annotated source programs
can be structured in two layers, as depicted in Fig. 1. First, a “programmer-
friendly” program verification layer assists programmers in constructing valid
proofs for their source programs according to the specifications that they have
set. In this layer, specifications and proofs are separate from the actual code and
an ordinary, general-purpose programming language can be used.
The program verification layer can follow the methodology suggested in the
work of Filliaˆtre et al. related to the Why software verification platform [14, 15].
The source code, written in any from a variety of languages, must be annotated
From Program Verification to Certified Binaries 327
with specifications (preconditions, postconditions, invariants, etc.) in some ap-
propriate logic. It is then given to a tool serving two purposes: (i) to compile
the source code into a lower-level intermediate language λ−H ; and (ii) to generate
proof obligations that must be proved, in order to verify the correctness of the
source code w.r.t. its specifications.
The language λ−H can be thought of as a typed intermediate language, such as
λH in the paper by Shao et al. [7], with some proofs missing. The missing proofs
are exactly those corresponding to the generated proof obligations. A variety of
tools (from automatic theorem provers to human-driven proof assistants) can be
used to discharge the proof obligations and generate the missing proofs. Subse-
quently, the intermediate program in λ−H can be automatically “linked” with the
constructed proofs, resulting in a λH program. An additional type/proof check-
ing step can be performed, to ensure the correctness of the “linked” program.
The second layer of the hybrid system consists of a type/proof preserving
compiler, which transforms the program in λH and produces a certified binary.
This compiler performs type/proof preserving program transformations that pro-
duce progressively lower-level code. Shao et al. have shown how to perform type
preserving CPS transformation and closure conversion on λH (and call the in-
termediate languages λK and λC respectively). One or more type-preserving
“code generation” steps are required to obtain a certified binary in the form of
a (machine dependent or independent) typed assembly language.
It should be noted that the trusted computing base, i.e. the piece of soft-
ware that the recipient of a certified binary must blindly trust (not shown in
Fig. 1), consists only of a type/proof checker for the typed assembly language
and a (type/proof erasing) translator to native assembly language. Both pieces
of software are of moderate size and relatively easy to build.
3 A Motivating Example
In this section we present a small case study: the construction of a certified binary
from a C function annotated with its specification. The example is intentionally
chosen to be very simple, so that self-contained equivalent programs in λH and
λK can fit in this paper.
Consider a function root that calculates the integer square root of an integer
number n, i.e. the greatest integer r with the property r2 ≤ n. A na¨ıve C program
that implements this function is the following:
int root (int n) {
int y = 0;
while ((y+1)*(y+1) <= n) y++;
return y;
}
Following the notation used by the Why verification platform and the verifi-
cation tool Caduceus for C programs [14, 15], the same program annotated with
the function’s pre- and postcondition and the loop invariant is given in Fig. 2.
328 Angelos Manousaridis et al.
//@ predicate leRoot(int r, int x) { r >= 0 && r*r <= x }
//@ predicate isRoot(int r, int x) { leRoot(r, x) && (r+1)*(r+1) > x }
/*@ requires n >= 0
@ ensures isRoot(\result, n)
@*/
int root (int n) {
int y = 0;
//@ invariant leRoot(y, n)
while ((y+1)*(y+1) <= n) y++;
return y;
}
Fig. 2. The example program, annotated with its specification.
root . ∀n :Z. ∀n∗ : (n ≥ 0). sintn ∃x :Z. ∃x∗ : isRootxn. sintx
= poly n :Z. poly n∗ : (n ≥ 0). lambda n :sintn.
(fix loop :∀y :Z. ∀y∗ : leRoot y n. sint y  ∃x :Z. ∃x∗ : isRootxn. sintx.
poly y :Z. poly y∗ : leRoot y n. lambda y :sint y.
if [♣,♣] ((y+ cint [1])2 > n,
p∗1 . pack (y, pack (♣, y) as ∃y∗ : isRoot y n. sint y) as
∃x :Z. ∃x∗ : isRootxn. sintx,
p2
∗. loop [y + 1] [♣] (y+ cint [1])))
[0] [♣] cint [0]
Fig. 3. The λ−H term with the missing proofs that correspond to proof obligations (♣).
The program in Fig. 2 is subsequently compiled to the λ−H program of Fig. 3.
Readers not familiar with the syntax of λH will probably find it hard to decipher
the code. However, two things are obvious. First, the specifications in Fig. 2
have been translated to the types that are embedded in the term of Fig. 3. For
instance, the type of root itself contains a direct translation of the function’s
pre- and postconditions. Second, there are five parts of this code, marked with
the symbol ♣, that are missing. The first of these five is the predicate associated
with the condition of the if expression. The remaining four are proofs that have
to be constructed externally. Four proof obligations are therefore produced by
the verification condition generator.
The proof obligations must now be discharged, either by an automatic theo-
rem prover or by a proof assistant. Suppose that the second alternative is used
and the human prover provides the code given in Fig. 4 for the Coq1 proof as-
sistant [16]. The missing parts of Fig. 3 can then be filled in, resulting in the λH
program of Fig. 5.
1 Coq uses the Calculus of Inductive Constructions (CIC) as its type language and,
for this reason, Coq proofs can be directly embedded in λH , which is also based on
CIC. Other theorem provers or proof assistants can be used instead, but the resulting
proofs would then have to be translated to CIC. It is worth mentioning that all proof
obligations were easily proved automatically (by auto) in Isabelle/HOL.
From Program Verification to Certified Binaries 329
Definition leRoot (r : Z) (x : Z) := (r >= 0 /\ r*r <= x)%Z.
Definition isRoot (r : Z) (x : Z) := leRoot r x /\ ((r+1)*(r+1) > x)%Z.
Definition decidable (P : Prop) (b : bool) := if b then P else ~P.
Lemma geDecidablePrf: forall n m : Z, decidable (n >= m)%Z (Zge_bool n m).
intros; unfold decidable, Zge, Zge_bool;
case (Zcompare n m); [ discriminate | auto | discriminate ].
Lemma gtDecidablePrf: forall n m : Z, decidable (n > m)%Z (Zgt_bool n m).
intros; unfold decidable, Zgt, Zgt_bool;
case (Zcompare n m); [ discriminate | discriminate | auto ].
Lemma Z_ge_refl: forall n : Z, (n >= n)%Z.
auto with zarith.
Lemma Zplus_ge_compat:
forall n m p q : Z, (n >= m -> p >= q -> n + p >= m + q)%Z.
intros n m p q; intros H1 H2; apply Zle_ge; apply Zplus_le_compat;
apply Zge_le; assumption.
Fig. 4. Coq code, useful in discharging the proof obligations.
root . ∀n :Z. ∀n∗ : (n ≥ 0). sintn ∃x :Z. ∃x∗ : isRootxn. sintx
= poly n :Z. poly n∗ : (n ≥ 0). lambda n :sintn.
(fix loop :∀y :Z. ∀y∗ : leRoot y n. sint y  ∃x :Z. ∃x∗ : isRootxn. sintx.
poly y :Z. poly y∗ : leRoot y n. lambda y :sint y.
if [decidable ((y + 1)2 > n), gtDecidablePrf (y + 1)2 n] (
(y+ cint [1])2 > n,
p∗1 . pack (y, pack (conj y
∗ p1
∗, y) as ∃y∗ : isRoot y n. sint y) as
∃x :Z. ∃x∗ : isRootxn. sintx,
p2
∗. loop [y + 1] [conj (Zplus ge compat y 0 1 0
(proj1 y∗)
(geDecidablePrf 1 0))
(Znot gt le (y+1)2 n p2
∗)] (y+ cint [1])))
[0] [conj (Z ge refl 0) (Zge len 0n∗)] cint [0]
Fig. 5. The λH term with the “linked” proofs.
330 Angelos Manousaridis et al.
Proof-preserving compilation phases can now be applied to the λH program.
However, after CPS transformation, the size and complexity of the resulting
program are too much for the human reader. The λK program obtained by
the CPS transformation of the λH program of Fig. 5 and after some simple
optimizations (such as constant propagation and beta contraction) is given in
Fig. 6 at the end of this paper. To increase readability, the types of continuation
parameters have been omitted from the λK program. In subsequent phases,
still lower-level programs are obtained. The corresponding λC program, after a
na¨ıve closure conversion, is a few hundred lines long when expressed in the same
textual format. To obtain an efficient implementation, it is essential to invent
and implement proof-preserving compiler optimizations and to find a compact
representation for proofs.
4 Conclusion
The realization of a complete hybrid system for constructing certified binaries
requires the implementation of the system’s two main software layers. Both for
program verification and for type-based proof-preserving compilation, there is
still a long way to go. However, in order to exploit the feasibility of such a system,
we have used existing techniques and tools. To verify high-level source programs
and produce proof obligations, a platform such as Why/Caduceus can be used.
The integration of such a platform with a compiler from the source language to
λ−H and the implementation of a proof checker and linker are still future work.
So far, we have partially implemented a proof-preserving compiler in OCaml,
manipulating programs in the set of languages described by Shao et al. [7]. As
an implementation of the type language (CIC) we have used the Coq proof
assistant, whose source code is freely available. In this way, we can build on Coq’s
rich set of proof libraries. Our system is incompetent with long and complex
source programs. There is much to be done before such an approach to software
verification can be applied to real software.
References
1. IEEE: Standard Glossary of Software Engineering Terminology. IEEE Standard
610.12-1990.
2. Hoare, C.A.R.: An axiomatic basis for computer programming. Communications
of the ACM 12(10) (1969) 576–585
3. Cousot, P.: Methods and logics for proving programs. In van Leeuwen, J., ed.:
Formal Models and Semantics. Volume B of Handbook of Theoretical Computer
Science. Elsevier Science Publishers B.V., Amsterdam, The Netherlands (1990)
843–993
4. Necula, G.: Proof-carrying code. In: Proceedings of the 24th ACM Symposium on
the Principles of Programming Languages. (1997) 106–119
5. Necula, G.: Compiling with Proofs. PhD thesis, Carnegie Mellon University (1998)
6. Appel, A.W.: Foundational proof-carrying code. In: Proceedings 16th IEEE Sym-
posium on Logic in Computer Science. (2001) 247–258
From Program Verification to Certified Binaries 331
(lambda k. k
(poly n :Z. lambda k. k
(poly n∗ : (n ≥ 0). lambda k. k
(lambda xarg :sintn× Kc(∃x :Z. ∃x∗ : isRootxn. sintx).
let n = sel [N lt prop 0 2] (xarg, cnat [0]) in
let k0 = sel [N lt prop 1 2] (xarg, cnat [1]) in
(fix loop [y :Z] (k :Kc(∀x∗ : leRoot y n. sint y  ∃x :Z. ∃x∗ : isRootxn. sintx)). k
(poly y∗ : leRoot y n. lambda k. k
(lambda xarg :sint y × Kc(∃x :Z. ∃x∗ : isRootxn. sintx).
let y = sel [N lt prop 0 2] (xarg, cnat [0]) in
let k1 = sel [N lt prop 1 2] (xarg, cnat [1]) in
let z1 = y+ cint [1] in
let z2 = z1 ∗ z1 in
let z3 = z2 > n in
if [decidable ((y + 1)2 > n), gtDecidablePrf (y + 1)2 n] (z3,
p∗1 . k1 (pack (y, pack (conj y
∗ p∗1 , y) as K(∃y∗ : isRoot y n. sint y)) as
K(∃x :Z. ∃x∗ : isRootxn. sintx)),
p∗2 . loop [y + 1] (lambda k. k [conj (Zplus ge compat y 0 1 0 (proj1 y
∗)
(geDecidablePrf 1 0))
(Znot gt le (y + 1)2 n p∗2 )]
(lambda k. let z1 = y+ cint [1] in k 〈z1, k1 〉)))))) [0]
(lambda k. k [conj (Z ge refl 0) (Zge len 0n∗)] (lambda k. k 〈cint [0], k0 〉))))))
Fig. 6. The λK term, after the proof-preserving CPS transformation and some opti-
mizations.
7. Shao, Z., Trifonov, V., Saha, B., Papaspyrou, N.: A type system for certified
binaries. ACM Transactions on Programming Languages and Systems 27(1) (2005)
1–45
8. Crary, K., Vanderwaart, J.C.: An expressive, scalable type theory for certified
code. In: Proceedings of the 7th ACM International Conference on Functional
Programming. (2002) 191–205
9. Howard, W.A.: The formulae-as-types notion of constructions. In Seldin, J.P.,
Hindley, J.R., eds.: To H. B. Curry: Essays on Computation Logic, Lambda Cal-
culus and Formalism. Academic Press, Boston, MA (1980) 479–490
10. Harper, R., Morrisett, G.: Compiling polymorphism using intensional type analy-
sis. In: Proc. 22nd ACM Symp. on Principles of Prog. Lang. (1995) 130–141
11. Morrisett, G., Walker, D., Crary, K., Glew, N.: From System F to typed assembly
language. In: Proc. 25th ACM Symp. on Principles of Prog. Lang. (1998) 85–97
12. Dijkstra, E.W.: A Discipline of Programming. Prentice Hall (1976)
13. Gries, D.: The Science of Programming. Springer-Verlag (1981)
14. Filliaˆtre, J.C.: Why: A multi-language multi-prover verification tool. Research
Report 1366, LRI, Universite´ Paris Sud (March 2003)
15. Filliaˆtre, J.C., Marche´, C.: The Why/Krakatoa/Caduceus platform for deductive
program verification. In: Computer Aided Verification. Volume 4590 of LNCS.
Springer (2007) 173–177
16. The Coq Proof Assistant Reference Manual, URL: http://coq.inria.fr/
Limiting Recursion, FM–representability, and
Hypercomputations
Marcin Mostowski
Department of Logic
Institute of Philosophy, Warsaw University
m.mostowski@uw.edu.pl
Abstract. We consider various methodologically well motivated notions
which appear to be essentially different, but surprisingly capture the
same class of relations. These are: the notion of FM–representability
(representability in Finite Models), statistical representability, limiting
recursion or algorithmic learning with empty data, and decidability by
accelerating Turing machines by computations of length ω. Finally, we
give a new result characterizing FM–representability in poor finite arith-
metics, weaker than divisibility arithmetic, but stronger than coprimality
arithmetic.
1 Introduction
Two classical bounds determined by computations were given in the thirties by
recursive — relations which can be decided by an algorithmic process, and recur-
sively enumerable — relations which can be positively decided by finding proofs.
Thirty years later — as a result of our computer experience — another important
bound was postulated: practical computability. Edmond’s thesis (called also: the
feasibility thesis) — which identifies practically computable notions with those
PTIME–computable — is still one of the crucial methodological motivations of
our research in computational complexity.
However, in about the same time we were led to recognize another impor-
tant bound falling far outside recursivity. It was motivated by our attempts of
computerising some of our intellectual activities, namely learning from finite
samples. In his classical paper [3] Mark Gold formulated fundamental ideas of
algorithmic learning theory. However in his earlier paper [2] he gave an idea
of limiting recursion. In the same issue of the Journal of Symbolic Logic [21]
Hilary Putnam independently postulated a similar explication for the notion of
learnability. As a matter of fact identifying learnability with limiting recursion
was Putnam’s idea. Nevertheless this notion seems to be a good explication
for the idea of algorithmic learning with no input data. Surprisingly this no-
tion coincides with some other methodologically motivated notions. These are
FM–representability (arithmetical expressibility in potentially infinite domains),
statistical representability, and decidability by hypercomputations of length ω.
Aristotle (in his Physics [1]) about 360 BC noticed the difference between
actual and potential infinities. Potentially infinite are collections which are finite
Limiting Recursion, FM–representability, and Hypercomputations 333
but unbounded, in the sense that they can be always enlarged. Actually infinite
are collections containing infinitely many members. More than 2000 years later
in his talk Hilbert ([6]) recalled this idea stressing its importance in foundations
of mathematics.1
Currently the idea is reflected — among others — in difference in ways of
thinking in classical and finite model theory. One of the crucial problems in the
latter is the question which notions can be expressed when we restrict possible
interpretations to finite models. We try — under such restriction — to express
various properties of computations, formulae, and some other combinatiorial
objects. Then we need a general method of determining the class of the notions
meaningful in finite models.
As an answer to this problem the author (in [14]) considered the notion of
FM–representability. A relation on natural numbers is FM–representable by a
given formula if each finite part of its characteristic function is corectly described
by this formula in all sufficiently large finite models. In Aristotelian spirite it can
be treated as an answer to the question: what can be meaningfully described in
a finite but potentially infinite world?
* * *
In this paper we discuss the notion of FM–representability and other equiv-
alent notions. Our research follow the ideas of the paper [14] and research done
in collaboration with Konrad Zdanowski ([19], [18], and [27]).
The paper is partially self contained. Only the last part about FM–represent-
ability in poor arithmetics is dependent on other papers ([17] and [18]). Giving
all the details would force us to repeat all the technical machinery which was
very carefully elaborated there.
2 Finite arithmetics
We say that A is an arithmetical model if it is of the form A = (N, R1, . . . , Rs),
where N is the set of natural numbers and R1, . . . , Rs are relations on N.
We consider finite initial fragments of arithmetical models. Namely, for every
n ∈ N − {0}, by An we denote the model An = ({0, . . . , n − 1}, Rn1 , . . . , Rns ),
1 Hilbert claimed there that lack of actual infinity is a characteristic feature of fini-
tistic mathematics in opposition to ideal mathematics which should be grounded
by representing it in the form of an axiomatic theory. His explanation of an idea of
finitistic mathematics — from the point of our current knowledge — allows various
nonequivalent explications. Later it was interpreted according to Go¨del’s results by
identifying finitistic with expressible in primitively recursive arithmetic (see [24]).
Nevertheless it would be interesting to investigate Hilbert’s idea just by taking lack
of actual infinity as the defining feature of finitistic mathematics considered as the
part of mathematics which is well defined and free of inconsistent intuitions. In
this way finitistic mathematics would be identified with mathematics restricted to
FM–representable notions.
334 Marcin Mostowski
where Rni is the restriction of Ri to the set {0, . . . , n− 1}. The family of models
{An : n = 1, 2, 3, . . .} is denoted by FM(A).
The most common arithmetical models are defined as algebraic structures,
e.g. with addition and multilication. However — as a rule — initial segments
of natural numbers are not closed under arithmetical operations.2 Therefore we
treat e.g. addition and multiplication as ternary relations.
Let ϕ(x1, . . . , xp) be an arithmetical formula and b1, . . . , bp ∈ N. We say that
ϕ is satisfied by b1, . . . , bp in all sufficiently large finite models of FM (A), what
is denoted by FM(A) |=sl ϕ[b1, . . . , bp], if there is k ∈ N such that for all n ≥
k An |= ϕ[b1, . . . , bp]. When no ambiguity arises we will write |=sl ϕ[b1, . . . , bp]
instead of FM(N ) |=sl ϕ(b1, . . . , bp), where N = (N,+,×).
Traditionally an arithmetical model A is called an arithmetical domain —
with possible qualifications, e.g. addition domain, multiplication domain, divis-
ibility domain, or coprimality domain, with obvious meaning pointing at the
relations considered. The corresponding class of finite models FM (A) is called
FM–domain or a finite arithmetic possibly with appropriate qualifications.
The research area devoted to study logical properties of FM–domains is also
called finite arithmetic. A report of our state of knowledge in this area can be
found in [11].
3 Representing concepts in finite models
One of the main questions related to finite arithmetics is the problem of FM–
representability in a given FM–domain.
Let ϕ(x1, . . . , xn) be a formula and S ⊆ Nn. We say that ϕ(x1, . . . , xn) FM–
represents S in FM(A) if for all a1, . . . , an ∈ N the following two conditions are
satisfied:
1. if S(a1, . . . , an) then FM(A) |=sl ϕ(a1, . . . , an),
2. if ¬S(a1, . . . , an) then FM(A) |=sl ¬ϕ(a1, . . . , an),
The idea of this definition is that a formula ϕ FM–represents a relation be-
tween natural numbers if for any given finite fragment of this relation ϕ correctly
describes this fragment in all sufficiently large finite initial segments of A (for
both positive and negative cases). We say that a relation is FM–representable
in a given FM–domain if it is FM–representable in this FM–domain by some
formula.
This notion and its basic properties was presented in the paper [14]. Origi-
nally it was motivated by studying truth definitions in finite models (FM truth
definitions).3 Tarski’s undefinability of truth theorem (see [25]) essentially re-
2 In earlier papers (see e.g. [14]) we assumed that operations take as their value the
greatest element MAX of the initial segment when they are not defined. However
this approach does not make things clearer.
3 See [14] and the later papers [15, 7]. Leszek Ko lodziejczyk successfully applied the
method for computational complexity questions in [8, 9]. See also recent discussion
of the idea in [11].
Limiting Recursion, FM–representability, and Hypercomputations 335
quires expressibility of some syntactical relations in the model considered. Ap-
plication of Tarski’s idea in finite models also requires expressibility of some
relations in finite models. FM–representability just gives a proper notion of ex-
pressibility for finite models.
Let us observe that except for the trivial case initial segments of natural
numbers are not closed on pairing function. Therefore we cannot restrict our
attention to sets — unary relations. However some arguments can be given
only for the unary case when no essential difference will follow from considering
arbitrary arities.
3.1 FM representability theorem
In this section we consider the problem of characterizing of FM–representable
relations. We begin with recalling classical characterizations of ∆02 arithmetical
relations (for the proof see e.g. [22] or [23]).
Theorem 1 (The characterisation of ∆02 arithmetical relations). Let R
be a relation on natural numbers. Then the following are equivalent:
1. R is recursive with recursively enumerable oracle; ( – in terms of oracle
machines)
2. R is of degree ≤ 0′; ( – in terms of Turing degrees)
3. R is recursive in the limit (see [2]), in the other words there is a recursive
sequence of recursive relations S0, S1, S2, . . . such that R = limn→∞ Sn; ( –
in terms of limits)
4. R is ∆02 in arithmetical hierarchy. ( – in terms of arithmetical definability)
The equivalence with condition 3 is also known as the limit lemma.
R = lim
n→∞Sn
means that the limit of the characteristic functions Sn exists, for all arguments,
and it is 1 exactly when R holds for these arguments. In other words it is equiv-
alent to the conjunction of the following two conditions:
∀a1, . . . , ak(R(a1, . . . , ak) ≡ ∃m∀n > m Sn(a1, . . . , ak))
and
∀a1, . . . , ak(¬R(a1, . . . , ak) ≡ ∃m∀n > m ¬Sn(a1, . . . , ak)).
Before stating the FM–representability theorem we consider its easier version
— namely FM–representability of recursively enumerable relations.
Proposition 1. Each recursively enumerable relation is FM–representable.
Proof. Let us consider Kleene T predicate such that T (e, n, c) if and only if e
is a number of a Turing machine and c is a code of a finished computation of
this machine with the input n. It is known that a proper arithmetical formula
336 Marcin Mostowski
can be chosen in such a way that all quantifiers are bounded by “< c”. If R is
recursively enumerable then R = We, for some e, where We is the set of inputs
for which e halts. Then R is FM–represented by the formula ∃c T (e, n, c).4
Now we are ready to characterize just FM-representable relations.
Theorem 2 (The FM–representability theorem, M. Mostowski, [14]).
Let R be a relation on natural numbers. Then all the conditions of theorem
1 are equivalent to the following: R is FM-representable (in FM(N,×,+)).
Proof. Let us observe that both cases postive and negative for FM–representability
are defined by Σ02 formulae. It follows that all FM–representable relations are
∆02–definable.
Following [14] we will show that all relations recognized by oracle Turing
machines with recursively enumerable oracles are FM–representable. Let M1 be
an oracle deterministic Turing machine recognising n–ary relation5 R and using
a recursively enumerable oracle S. Moreover, S is given by a deterministic Turing
machine M2 such that M2 halts on a given input n exactly when n ∈ S. Our
FM–representing formula ϕ(x1, . . . , xn) for R can be formulated as follows:
there is an accepting computation of M1 on input x1, . . . , xn such that each
oracle question y is answered positively exactly when ψ(y),
where ψ(y) is an arithmetical formula FM–representing the set S, it exists by
proposition 1.
For each given input a1, . . . , an we can find a model of size sufficient to contain
the unique M1–computation c and all required witnesses for oracle questions put
by c for accepting them correctly as members of S by the formula ψ(y).
All the claims up to now have assumed that we consider only representabil-
ity by first order arithmetical formulae and FM–domain N . In [14] we have
considered much stronger logic — namely finite order logic. However it was also
observed there that taking any logic stronger than first order logic we do not
obtain more FM–representable relations, provided the logic has decidable “truth
in a finite model” relation. A logic L has decidable “truth in a finite model” re-
lation if the relation “M |= ϕ” is recursive for arguments: a finite model M and
L–formula ϕ.
Theorem 3. Let A be a model on natural numbers having all relations recur-
sive and L be a logic with decidable “truth in a finite model” relation. Then
the relations FM–representable in FM(A) by L–formulae are ∆02 arithmetically
definable.
4 Let us observe that by the Matiyasevich theorem recursively enumerable sets are
exactly sets definable by purely existential arithmetical formulae. This gives slightly
easier argument.
5 In finite models we cannot restrict to sets of natural numbers because finite models
are not closed on pairing function.
Limiting Recursion, FM–representability, and Hypercomputations 337
3.2 Statistical representability
The notion of statistical representability was proposed by Konrad Zdanowski
(see [19] and [27]) with the intention of comparing it with FM-representability.
By µn(ϕ(a1, . . . , ak)) we mean the density of the models satisfying
ϕ(a1, . . . , ak) between models of size not greater than n, that is
µn(ϕ(a1, . . . , ak)) =
1
n
card{s : 1 ≤ s ≤ n and Ns |= ϕ(a1, . . . , ak)}.
A relation R ⊆ Nk is statistically representable if there is arithmetical formula
ϕ(x1, . . . , xk) such that for each a1, . . . , ak ∈ N:
– the limit µ(ϕ(a1, . . . , ak)) = limn→∞ µn(ϕ(a1, . . . , ak)) exists;
– if R(a1, . . . , ak) then µ(ϕ(a1, . . . , ak)) > 12 ;
– if ¬R(a1, . . . , ak) then µ(ϕ(a1, . . . , ak)) < 12 .
Theorem 4. The statistical representability theorem, K. Zdanowski,
[19] and [27]
Let R be a relation on natural numbers. Then all the conditions of theorem
1 and theorem 2 are equivalent to the following: R is statistically representable.
Proof. By the definition each FM–representing formula statistically represents
the same relation. On the other hand if a formula ϕ(x1, . . . , xk) statistically
represents a relation R then a formula ψ(x1, . . . , xk) saying that “majority of
x–s satisfy 0 < x∧ϕ≤x(x1, . . . , xk)” FM–represents the same relation, where the
formula ϕ≤x(x1, . . . , xk) is obtained by bounding all quantifiers in ϕ(x1, . . . , xk)
by the condition ≤ x. Obviously the logic with majority quantifier has decidable
“truth in a finite model” relation. Then by theorem 3 the relation R is FM–
representable.
3.3 Learnable relations
In his currently classical paper Gold [3] considers learning algorithms supply-
ing a natural framework for algorithmic modelling the phenomenon of learning
grammar of a language given by finite samples. The algorithm in his sense has
to identify on the basis of finite samples a grammar in such a way that the iden-
tification has to stabilise on a correct grammar after finite number of guesses.
In a similar way we can think of learning mathematical notions. However in this
case no empirical data are required. What we need is more and more work. So
at each stage t we have some answer but after finitely many stages the answer
is stabilising.
We say that A is a mathematical learning algorithm for a relation R ⊆ Nk if
– A works with inputs a1, . . . , ak, t ∈ N,
– R(a1, . . . , ak) if and only if there is s such that for all t > s A for the input
a1, . . . , ak, t answers “YES”,
338 Marcin Mostowski
– ¬R(a1, . . . , ak) if and only if there is s such that for all t > s A for the input
a1, . . . , ak, t answers “NO”.
We say that a relation R is mathematically learnable if there is a mathemat-
ical learning algorithm for R.
Theorem 5 (Mathematical learnability theorem).
Let R be a relation on natural numbers. Then all the conditions of theorem 1
and theorems 2, 4 are equivalent to the following: R is mathematically learnable.
Proof. Let us observe that a mathematical learning algorithm A for R gives a
recursive sequence of relations S0, S1, S2, . . . such that
R = lim
n→∞Sn,
where Sn is the relation computed when we fix t = n.
3.4 Relations decidable by Zeno machines
Now we will consider a characterisation of ∆02 arithmetical relations in terms
of hypercomputations. Currently hypercomputations have vast literature (see
e.g. [5], [4], [20]). In spite of our idea of potential infinity hypercomputations
essentially use actual infinity.6 We restrict our interest here to so called Zeno
machines which are the simplest (hyper)computing devices.
A Zeno machine — called also accelerating Turing machine — is defined in
the same way as the simplest Turing machine. It has an infinite tape consisting
of ω cells. The first cell (0 cell) is used for giving outputs: 0 or 1. It is accelerating
because it carries out each next step of computation two times quicker than the
previous one. So its computation consists of ω steps. If the first step is done in
1 second then all the computation will be finished after 2 seconds. After that
we look at the first cell, if it contains 1 then the answer is “YES”, and if it
contains 0 then the answer is “NO”. When the content of the first cell stabilises
after finitely many moves then the situation is clear — it contains the stabilised
character. Otherwise, when this value is changed infinitely many times, Potgieter
[20]7 says that it does not halt, but Hamkins [4] says that it takes the supremum
value = 1. These two approaches are not equivalent, and the Hamkins’ approach
seems to be slightly artificial. Then we assume that if the machine infinitely
many times changes its output then the output is undefined.
We say that a relation is Zeno decidable if its characteristic function can
be computed by a Zeno machine. The following characterises Zeno decidable
relations.
6 For discussion of these two views in the context of computational power see [16].
7 The requirement for the halting condition in [20] that also the head stabilises is an
obvious mistake, because in this case Zeno machines would be equivalent to Turing
machines.
Limiting Recursion, FM–representability, and Hypercomputations 339
Theorem 6 (Zeno decidability theorem).
Let R be a relation on natural numbers. Then all the conditions of theorem
1 and theorems 2, 4, 5 are equivalent to the following: R is Zeno decidable.
Proof. It is easy to check by writing down the halting condition for Zeno ma-
chines that each Zeno decidable relation is ∆02 arithmetically definable.
Let us consider a relation R decidable by a Zeno machine Z. Firstly, let us
observe that we can assume that in each computation of Z there are infinitely
many steps in which the machine writes something in the first cell. This can be
achieved by adding after each instruction a journey to the first cell, writing the
same value as that read, and then going back. So we consider an algorithm A
taking as inputs all inputs of Z and additionally t. The algorithm simulates the
behaviour of Z and counts how many times writing on the first cell was done, if
it was done t times then it halts and answers “YES” if in the first cell is 1 and
“NO” otherwise. So the defined algorithm is — by the assumptions on Z — a
mathematical learning algorithm for R.
The idea of Zeno machine can be easily generalised for computations of length
defined by any countable ordinal. It is observed in [5] that arithmetical truth can
be computed by accelerating Turing machines in ω2 steps. An easy generalisation
of the above theorem gives another proof for this fact.
4 FM–representability in poor arithmetics
In this section we consider FM–representability in some FM–domains with poor
arithmetical notions. The weakest known such notion, which is sufficient for
FM–representability of all ∆02 relations, is the relation of divisibility.
Theorem 7 (FM–representability for divisibility FM–domain, [17]). Let
R be a relation on natural numbers. Then all the conditions of theorem 1 and
theorems 2, 4, 5, 6 are equivalent to the following: R is FM–representable in the
divisibility FM–domain.
Proof. We give here only a sketchy argument, for details we refer to [17]. The
result is based on the observation that the product of any two coprime numbers
can be defined in terms of divisibility. Therefore we can define the standard
ordering on initial segments of finite models by saying that the first number can
be multiplied by something by which the second number cannot be multiplied.
In this way in each finite divisibility model we can reconstruct sufficiently large
model of divisibility and ordering. However, it is known (see [13]) that addition
and multiplication are definable in finite models in terms of divisibility and
ordering.
Now let us consider arithmetics which are too poor for obtaining full FM–
representability theorem. We start with the observation by Micha l Krynicki and
Konrad Zdanowski.
340 Marcin Mostowski
Theorem 8 (FM–representability in arithmetic of addition, [12]). Rela-
tions which are FM–representable in FM((N,+)) are just the relations definable
in (N,+).
Proof. It follows from more general fact (see [27]) that if A is an arithmetical
domain with standard ordering then the FM–representable relations in the cor-
responding FM–domain are exactly the ∆02–definable relations in A. However
in arithmetic of addition we have elimination of quantifiers then each definable
relation is ∆02–definable.
Now we are going to characterise FM–representability in coprimality FM–
domain. Firstly we need a few auxiliary notions.
Let ∼ be an equivalence relation on N and let R ⊆ Nk. We say that ∼ is a
congruence relation for R if for all a1, . . . , ak, b1, . . . , bk ∈ N such that ai ∼ bi
for, i = 1, . . . , k,
(a1, . . . , ak) ∈ R ⇐⇒ (b1, . . . , bk) ∈ R.
For a, b ∈ N, we define a ≈ b if a and b have the same prime divisors, that
is ∀x (x⊥a ≡ x⊥b), where x⊥y means that x and y have no common prime
divisors. A relation R ⊆ Nk is coprimality invariant if ≈ is a congruence for R.
We know that the structure (N,⊥) has many automorphisms and only copri-
mality invariant relations are preserved by these automorphisms. In particular
any two powers of the same prime can be interchanged. Moreover, this property
determines definability also on initial segments. Therefore only coprimality in-
variant relations can be FM–represented. It foolows, by theorem 3, that only ∆02
arithmetically definable coprimality invariant relations can be FM–represented
in FM–domain of coprimeness.
Theorem 9 (FM–representability in arithmetic of coprimeness, [18]).
R is FM–representable in FM((N,⊥)) if and only if R is FM–representable in
FM(N ) and R is coprimality invariant.
Proof. We skip details from [18], giving only general idea and those technicalities
which are needed for the next argument. By a similar trick as in the proof of
theorem 7 we can define the standard ordering between primes on sufficiently
large initial segments. Then using properties of the distribution of primes we
encode pairs of primes and compare lengths of sequences of primes. This allows
to interpret relations R+ and R× in coprimality finite models. These relations
are coprimality invariant versions of R′+ and R
′
× defined as follows:
– R′+(pi, pj , pk) if and only if i+ j = k,
– R′×(pi, pj , pk) if and only if i× j = k,
where pi is the i–th prime. So we define R+(x, y, z) as ∃x′ ≈ x∃y′ ≈ y∃z′ ≈
z R′+(x
′, y′, z′) and similarly R×.
In this way we can FM–represent all FM–representable relations on indices
of primes. We have to transfer them into other relations. For this let us observe
Limiting Recursion, FM–representability, and Hypercomputations 341
that our method of defining ordering works also for products of finite sets of
primes (of course up to equivalence ≈). By ind(x) we mean the index of x in
this ordering. Then we define the relation W such that
(x, y) ∈W if and only if y ≈ pind(x).
In [18] it is shown that the relation W is FM–representable in coprimality domain
and this essentially finishes the proof.
Korec observed in [10] that between coprimality and divisibility we have
infinitely many relations ordered according to their definability power.
Let n ∈ N − {0} or n = ∞. We define the following relations on natural
numbers:
– x|ny if and only if for each prime q and k ≤ n, if qk|n then qk|y.
– x ≈n y if and only if x|ny and y|nx.
The relation |∞ is just divisibility | and ≈∞ is the identity. The relation |1 is
mutualy definable with coprimality.
The following theorem generalizes the above results on FM–representability
in FM((N, |)) and in FM((N,⊥)).
Theorem 10. For any n > 0 or n = ∞, a relation R ⊆ Nr is FM–representable
in FM((N, |n)) if and only if R is ∆02–definable and the relation ≈n is a congru-
ence for R.
Proof. We will show only how to modify the proof of theorem 9 for obtaining
this generalisation. We fix n 6= 0,∞. Of course the relation ⊥ is definable by |n,
then we can assume that all required relations are definable provided they use
only coprimality. Thus the ordering we define in a similar way, x ≺n y means
that there is z coprime with both x and y such that there is w divisible by z
and each power pi of prime p 6= z (i ≤ n) divides w exactly when pi divides x,
but no such w exists for y and z. The main difference is that now the ordering
is defined up to ≈n instead of ≈.
The only relation which should be defined essentially in a different way is
W . This is so because now ind(x) is the index of x in a more subtle ordering.
However also in this case the argument from [18] can be repeated in extenso.
5 Summarizing FM representability theorem
In this final section we collect together all the results about the notions equivalent
to FM–representability in one theorem.
Theorem 11 (The FM–representability theorem and equivalent no-
tions). Let R be a relation on natural numbers. Then the following are equiva-
lent:
1. R is recursive with recursively enumerable oracle; ( – in terms of oracle
machines)
342 Marcin Mostowski
2. R is of degree ≤ 0′; ( – in terms of Turing degrees)
3. R is ∆02 in arithmetical hierarchy; ( – in terms of arithmetical definability)
4. R is recursive in the limit; ( – in terms of limits)
5. R is FM-representable (in FM(N,×,+)); ( – just FM–representablity)
6. R is statistically representable; ( – in terms of density)
7. R is mathematically learnable; ( – in terms of algorithmic learning)
8. R is Zeno decidable; ( – in terms of hypercomputations)
9. R is FM–representable in the divisibility FM–domain.
References
1. Aristotle. Physics. about 360 BC.
2. E. M. Gold. Limiting recursion. The Journal of Symbolic Logic, 30:28–48, 1965.
3. E. M. Gold. Language identification in the limit. Information and Control, 10:447–
474, 1967.
4. J. D. Hamkins. Infinitary computability with infinite time Turing machines. In
B. Cooper, B. Loewe, and L. Torenvliet, editors, Proceedings of the conference
Computability in Europe, volume 3526 of Lecture Notes in Computer Science, pages
180–187. Springer, 2005.
5. J. D. Hamkins and A. Lewis. Infinite time Turing machines. The Journal of
Symbolic Logic, 65:567–604, 2000.
6. D. Hilbert. U¨ber das Unendliche. Mathematische Annalen, 95:161–190, 1926.
7. L. Ko lodziejczyk. A finite model-theoretical proof of a property of bounded query
classes within ph. The Journal of Symbolic Logic, 69:1105–1116, 2004.
8. L. Ko lodziejczyk. Truth definitions in finite models. The Journal of Symbolic
Logic, 69:183–200, 2004.
9. L. A. Ko lodziejczyk. Truth definitions and higher order logics in finite models.
PhD thesis, Warsaw University, 2005.
10. I. Korec. A list of arithmetical structures complete with respect to first–order
definability. Theoretical Computer Science, 257:115–151, 2001.
11. M. Krynicki, M. Mostowski, and K. Zdanowski. Finite arithmetics. Fundamenta
Informaticae, 81(1–3):183–202, 2007.
12. M. Krynicki and K. Zdanowski. Theories of arithmetics in finite models. Journal
of Symbolic Logic, 70(1):1–28, 2005.
13. T. Lee. Arithmetical definability over finite structures. Mathematical Logic Quar-
terly, 49:385–393, 2003.
14. M. Mostowski. On representing concepts in finite models. Mathematical Logic
Quarterly, 47:513–523, 2001.
15. M. Mostowski. On representing semantics in finite models. In A. Rojszczak†,
J. Cachro, and G. Kurczewski, editors, Philosophical Dimensions of Logic and
Science, pages 15–28. Kluwer Academic Publishers, 2003.
16. M. Mostowski. Potential infinity and the Church Thesis. Fundamenta Informaticae,
81(1–3):241–248, 2007.
17. M. Mostowski and A. Wasilewska. Arithmetic of divisibility in finite models. Math-
ematical Logic Quarterly, 50(2):169–174, 2004.
18. M. Mostowski and K. Zdanowski. Coprimality in finite models. In Luke Ong,
editor, Computer Science Logic: 19th International Workshop, CSL 2005, volume
3634 of Lecture Notes in Computer Science, pages 263–275. Springer, 2005.
Limiting Recursion, FM–representability, and Hypercomputations 343
19. M. Mostowski and K. Zdanowski. FM–representability and beyond. In B. Cooper,
B. Loewe, and L. Torenvliet, editors, Proceedings of the conference Computability
in Europe, volume 3526 of Lecture Notes in Computer Science, pages 358–367.
Springer, 2005.
20. P. H. Potgieter. Zeno machines and hypercomputation. Theoretical Computer
Science, 358:23–33, 2006.
21. Hilary Putnam. Trial and error predicates and the solution to a problem of
mostowski. Journal of Symbolic Logic, 30(1):49–57, 1965.
22. J. R. Shoenfield. Recursion Theory. Lectures Notes in Logic. Springer–Verlag,
1993.
23. R. Soare. Recursively enumerable sets and degrees. Springer-Verlag, 1987.
24. W. W. Tait. Finitism. Journal of Philosophy, 78:524–546, 1981.
25. A. Tarski. Poje¸cie prawdy w je¸zykach nauk dedukcyjnych. Nak ladem Towarzystwa
Naukowego Warszawskiego, 1933. English version in [26].
26. A. Tarski. The concept of truth in formalized languages. In J. H. Woodger, editor,
Logic, semantics, metamathematics, pages 152 – 278. Oxford at The Clarendon
Press, 1956. translated from German by J. H. Woodger.
27. K. Zdanowski. Arithmetics in finite but potentially infinite worlds. PhD thesis,
Warsaw University, 2005.
Using Tables to Construct Non-Redundant
Proofs
Vivek Nigam
INRIA & LIX/E´cole Polytechnique, Palaiseau, France
nigam@lix.inria.fr
⋆
Abstract. Proofs containing more than one subproof for a common
subgoal are less preferred in frameworks such as Proof Carrying Code,
where proofs are stored and communicated, than proofs that don’t con-
tain such redundancies. In this paper, we show how (cut-free) proofs can
be transformed into proofs containing cuts and where no atom is proved
twice, called non-redundant proofs. Two main questions arise when try-
ing to construct these non-redundant proofs: First, which cut-formulas
should be used; Second, where to perform cut rules. Some advances in
proof theory, namely, our better understanding of focused proofs, allows
us to propose the following answers: We use only atomic subgoals of the
original proof; and we place cut rules only at the end of the asynchronous
phases. The backbone of a non-redundant proof is a tree, called tree of
multicut derivations (tmcd), where a node is a derivation containing
only multicut rules, and an edge represents the provability dependency
between a subgoal introduced by a node’s multicut rule and another (tree
of) multicut derivation. We show how to obtain a tmcd from an existing
proof.
1 Introduction
Frameworks such as Proof Carrying Code [9], where mobile codes are sent with
proofs assuring that these codes satisfy certain properties, provide “real world”
concerns, not only for provability, but also for the shape and format of proofs. In
these frameworks, since proofs need to be stored and communicated, proofs have
to attend certain engineering aspects; for instance, the size of proofs is relevant;
more precisely, smaller proofs are preferred.
A redundant proof is a proof that contains more than one non-trivial subproof
of the same atom. For example, consider the proofs that the 12th Fibonacci
number is 144 (fib 12 144) and obtained from the following logic specification
{fib 1 1,fib 2 1,∀XY Z.[fib X Y ∧ fib (X + 1) Z ⊃ fib (X + 2) (Y + Z)]}. Two
types of proofs can be distinguished: one where a forward chaining behavior is
⋆ I thank Dale Miller, Miki Hermann, David Baelde, and anonymous reviewers for their
helpful comments and discussions. This work has been supported in part by INRIA
through the “Equipes Associe´es” Slimmer and by the Information Society Technolo-
gies programme of the European Commission, Future and Emerging Technologies
under the IST-2005-015905 MOBIUS project.
Using Tables to Construct Non-Redundant Proofs 345
adopted, and another where a backward chaining behavior is adopted. In the
frameworks previously mentioned, the former linear size non-redundant proof is
preferred to the latter exponential size redundant proof.
We propose a procedure to construct a non-redundant sequent calculus proof
from a redundant sequent calculus proof. This is done by collecting (or tabling)
from an existing proof a set of atomic lemmas, called table. These lemmas are
then used as cut formulas to construct a non-redundant proof containing cuts.
The use of cuts in non-redundant proofs is not surprising; it is well known
that, when compared with cut proofs, cut-free proofs are potentially bigger (also
known as the cut-elimination blow-up).
There are two main problems to be addressed: (1) which lemmas should be
used; and (2) when to use these lemmas, that is, while constructing the non-
redundant proof, when should a cut be used. Our answers for these question lie
in the structure of focused proofs.
By distinguishing rules that are invertible, called asynchronous rules, from
rules that are not invertible, called synchronous rules, focused proofs are orga-
nized in two alternating phases: asynchronous phases, where asynchronous rules
are eagerly applied; and synchronous phases, where a formula is picked, or fo-
cused on, and synchronous rules are applied hereditarely to its subformulas (for
more about focused proofs, we invite the reader to [1]). Some recent advances on
our understanding about focused proofs in classical and intuitionistic logics [5]
and about the effect of atomic polarities1 in the shape of proofs [2, 7], provides us
with the machinery necessary to propose the following answers to the previous
questions:
1) We collect (or table), in the existing proof, all the atomic subgoals. This
restriction allows us to construct non-redundant proofs with a polarized cut-
rule [7], where an atomic cut-formula has negative polarity in one branch of the
proof tree and positive polarity on the other branch of the proof tree. As we
investigate elsewhere [7], by using this polarized cut-rule, it is possible to mix
a forward chaining behavior with a backward chaining behavior, what enforces
some subgoals not to be re-proven;
2) The idea is to use the lemmas as close as possible to the root of the tree,
so that if a lemma is to be proved again later in the tree, then it would already
be available in the set of hypothesis of the sequent and allow to immediately
complete the proof with an initial rule. However, it may happen that a lemma
can’t be proved right from the bottom of the tree and should be introduced into
the proof only when there is an increment in the set of hypothesis of a sequent
(by for example, a right implication rule). We show that focusing provides the
discipline necessary to identify the places in a proof tree where new lemmas
should be introduced, namely, at the end of the asynchronous phases.
This paper is structured as follows: we introduce in Section 2 some key con-
cepts related to focusing and introduce the intuitionistic system LJFt and the
use of tables to specify multicut derivations (mcd). In Section 3, we specify
1 In a focused system, atoms are assigned either positive or negative polarity. This
assignment is necessary to organize focused proofs.
346 Vivek Nigam
how to extract tree of multicut derivations (tmcd), that is the backbone to
construct non-redundant proofs, from different types of proofs, namely Horn
Theory proofs, Uniform proofs, and LJFt proofs. In Section 4, we show and
discuss some experimental results, and finally in Section 5, we finish with some
concluding remarks.
2 Preliminaries
2.1 LJFt
In focused proof systems, formulas are classified as positive and negative. The
formulas true, ⊥, or the formulas with main connective ∧, ∨, or ∃ are positive,
while the remaining formulas are negative. This classification is natural since for
negative formulas, their right introduction rules are invertible, while this is not
necessarily the case for positive formulas. Focused proof systems capitalize on
this classification by organizing focused proofs in two phases: the asynchronous
phases, where only invertible rules are applied, and synchronous phase, where
non-invertible rules are applied hereditarely to a formula and its subformulas.
Notice that backtracking is only necessary in the synchronous phase. In many
focused systems, such as LJF, proposed by Liang and Miller in [5], this classifi-
cation is extended to atoms by assigning arbitrarily their polarities.
The Figure 1 depicts the inference rules in LJF, where four different types
of sequents can be identified: (1) The sequent [Γ ]−A→ is a right-focusing se-
quent (the focus is A); (2) The sequent [Γ ]
A
−→ [R]: is a left-focusing sequent
(with focus on A); (3) The sequent [Γ ], Θ −→ R is an unfocused sequent. Here,
Γ contains negative formulas and positive atoms, and R is either in brackets,
written as [R], or without brackets; (4) The sequent [Γ ] −→ [R] is an instance
of the previous sequent where Θ is empty.
Asynchronous phases use the third type of sequent above (the unfocused
sequents): in that case, Θ contains positive or negative formulas. If Θ contains
positive formulas, then an introduction rule (either ∧l,∃l or falsel) is used to
decompose it; if it is negative, then the formula is moved to the Γ context (by
using the []l rule). The end of the asynchronous phase is represented by the fourth
type of sequent. Such a sequent is then established by using one of the decide
rules,Dr orDl. The application of one of these decide rules then selects a formula
for focusing and switches proof search to the synchronous phase or focused phase.
This focused phase then proceeds by applying sequences of inference rules on
focused formulas: in general, backtracking may be necessary in this phase of
search. Moreover, according to which phase rules can be performed, we classify
the rules ∧l,∃l, falsel,⊃r,∀r, []l, []r,∧
−
r as asynchronous rules, and the remaining
rules as synchronous rules.
As pointed out elsewhere [5, 2, 7], the atomic polarities play an important role
in the shape of the proofs, without affecting in no way provability. For instance, if
all atoms have positive polarity, only proofs with a forward chaining behavior are
possible, and on the other hand, if all atoms have negative polarity, only proofs
with a backward chaining behavior are possible, for example uniform proofs [6].
Using Tables to Construct Non-Redundant Proofs 347
[N,Γ ]
N
−→ [R]
[N,Γ ] −→ [R]
Dl
[Γ ]−P→
[Γ ] −→ [P ]
Dr
[Γ ], P −→ [R]
[Γ ]
P
−→ [R]
Rl
[Γ ] −→ N
[Γ ]−N→
Rr
[Γ ]
An−→ [An]
Il
[Γ,Ap]−Ap→
Ir
[Γ,Na], Θ −→ R
[Γ ], Θ,Na −→ R
[]l
[Γ ], Θ −→ [Pa]
[Γ ], Θ −→ Pa
[]r
[Γ ], Θ,⊥ −→ R
falsel
[Γ ], Θ −→ R
[Γ ], Θ, true −→ R
truel
[Γ ]−true→
truer
[Γ ], Θ,A,B −→ R
[Γ ], Θ,A ∧ B −→ R
∧l
[Γ ]−A→ [Γ ]−B→
[Γ ]−A∧B→
∧r
[Γ ]−A→ [Γ ]
B
−→ [R]
[Γ ]
A⊃B
−→ [R]
⊃l
[Γ ]
Ai−→ [R]
[Γ ]
A1∧
−A2−→ [R]
∧−l
[Γ ], Θ −→ A [Γ ], Θ −→ B
[Γ ], Θ −→ A ∧− B
∧−r
[Γ ], Θ,A −→ B
[Γ ], Θ −→ A ⊃ B
⊃r
[Γ ], Θ,A −→ R
[Γ ], Θ, ∃yA −→ R
∃l
[Γ ]−A[t/x]→
[Γ ]−∃xA→
∃r
[Γ ]
A[t/x]
−→ [R]
[Γ ]
∀xA
−→ [R]
∀l
[Γ ], Θ −→ A
[Γ ], Θ −→ ∀yA
∀r
Fig. 1. LJF: Here, Γ is a set of formulas, Θ is a list of formulas, An denotes a negative
atom, Ap a positive atom, and P a positive formula, N a negative formula, Na a
negative formula or an atom, and Pa a positive formula or an atom. All other formulas
are arbitrary and y is not free in Γ,Θ or R.
LJF t capitalizes on the observation that atomic polarities can be arbitrarily
assigned, and extends LJF in two ways. (1) Extends the LJF sequents with a
polarity context, P, which specifies all the positive atoms in a sequent. An atom
in a sequent, P; [Γ ] −→ [R], is positive if and only if A ∈ P; (2) extends LJF
with the following polarized multicut rule:
P; [Γ ] −→ [A1] · · · P; [Γ ] −→ [An] P ∪∆; [Γ ∪∆] −→ [R]
P; [Γ ] −→ [R]
mc.
Where ∆ = {A1, . . . , An} is a set of atoms. The multicut rule is the only rule
that can change the polarity context in a proof.
Remarks: (1) The results in this paper could be easily applied to (focused)
classic logics, such as LKF [5]. (2) Here we only consider focused proofs; however,
it seems possible to apply the results obtained here to a more general setting
where non-focused proofs are considered, by using methods such as in [8] to
convert non-focused proofs to focused proofs, but this is left out of the scope of
this paper. (3) We use interchangeably the same names for the rules in LJF and
LJF
t, but always being clear from the context to which system we refer to.
In the next subsection, we use this polarized multicut rule to construct mul-
ticut derivations that are the basic element used to construct non-redundant
proofs.
2.2 Tables as Multicut Derivations
We consider a table as a partially ordered finite set of atoms.
348 Vivek Nigam
Definition 1. A table is a tuple T = 〈A,≺〉, where A is some finite set of
atoms, and ≺ is a partial order relation over the elements of A.
In a table, each atom represents, intuitively, a provable sub-goal necessary
in the proof of a sequent (say Γ −→ G), and the order relation the provability
dependency between the atoms, that is, if A ≺ B then A is a subgoal used to
prove the goal B.
The next definition specifies a derivation composed only of multicut rules,
represented by a table.
Definition 2. Let T = 〈A,≺〉 be a table. The multicut derivation for T and the
sequent S = Γ −→ G, written as mcd(T ,S), is defined inductively as follows:
if A is empty, then mcd(T ,S) is the derivation containing just the sequent S.
Otherwise, if {A1, . . . , An} is the collection of ≺-minimal elements in A and if
Π is the multicut derivation for the smaller table 〈A \ {A1, . . . , An},≺〉 and the
sequent Γ,A1, . . . , An −→ G, then mcd(T ,S) is the derivation
Γ −→ A1 · · · Γ −→ An
Π
Γ,A1, . . . , An −→ G
Γ −→ G
mc
Multicut derivations are always open derivations (that is, they contain leaves
that are not proved). A proof of a multicut derivation is any (closed) proof that
extends this open derivation.
Elsewhere [7], we investigated the use of tables for obtaining non-redundant
proofs in the Horn fragment. We used the following observation:
Proposition 1. [7] Let Γ be a set of Horn clauses, A ∈ P ∩ Γ , and Ξ be an
arbitrary LJFt proof tree for P; [Γ ]−G→. Then every occurrence of a sequent
with right-hand side the atom A is the conclusion of an Ir rule.
If in a proof of Ξ, there are several non-trivial proofs for the subgoal A,
that is, proofs that contain more than an inference rule, one could table A and
construct the corresponding multicut derivation for this table and construct a
non-redundant proof. For example, when comparing the two derivations below,
the left derivation could have several non-trivial subproofs for A, while the right
derivation must have only one non trivial proof for A: the proof of the cut’s left
branch.
Γ −→ A Γ −→ G
Γ −→ A ∧G =⇒
P; [Γ ] −→ [A] P ∪ {A}; [Γ,A] −→ [A ∧G]
P; [Γ ] −→ [A ∧G]
mc.
In the next sections, we use mcds to construct non-redundant proofs. To
represent amcd when we specify the algorithms to extract non-redundant proofs,
we use the following data type mcd ::= sequent * atom list2.
2 We use a list of atoms representing the topological sort of a table’s partial ordering.
Using Tables to Construct Non-Redundant Proofs 349
M0
· · ·
.
.
.
.
.
.
Asynchronous + Synchronous phases
M0
M1 Mi
Fig. 2. The left figure illustrates of how the function buildTmcd extracts a tmcd from
a proof tree. The right figure depicts the general architecture of a completed tmcd
proof. The triangles representmcds and the dashed lines represents end of asynchronous
phases.
3 Tree of Multicut Derivations - tmcd
The backbone of a non-redundant proof is a tree, called tree of multicut deriva-
tions (tmcd), where a node is a derivation containing only multicut rules, and
an edge represents the provability dependency between a subgoal introduced
by a node’s multicut rule and another (tree of) multicut derivation. The ar-
chitecture of a tmcd is depicted in Figure 2. The idea is that each multicut
derivation is only used when the context is augmented with new atoms or new
positive formulas. Since contexts can only be augmented by the asynchronous
rules ∀r,∃l, and ⊃r
3 and asynchronous rules are invertible, focusing provides a
natural way to identify where to place a multicut derivation, namely, whenever
an asynchronous phase ends and the context is augmented with a new positive
formula or a new atom. While focused cut-free proofs are structured in two alter-
nating phases, asynchronous phases and synchronous phases, focused cut proofs
are structured with one more phase, called cut phase, appearing always between
an asynchronous phase and the following synchronous phase.
In the following subsections, we specify algorithms to extract tmcds from
proofs in Horn Theory, from Uniform Proofs [6], and from LJF proofs. We rep-
resent tmcds by the following data type: tmcd ::= atom * mcd * tmcd list.
3.1 Horn Theory
A characteristic of uniform Horn Theory proofs, such as the proofs generated
by Prolog, is that contexts never changes. Therefore, all proved atoms in such a
proof are provable from the initial context. This property enables us to construct
a tmcd with only one node, obtained from the function buildTable shown in
Figure 3. This function uses the function atomPOT that performs a postorder
traversal (i.e., process a nodes premises before processing the node), and uses
3 The first two rules augment the context with a new eigenvariable, and the last rule
augments the context with some formula.
350 Vivek Nigam
buildTable : tree → mcd
input: Ξ
(rootOf(Ξ), eliRed(atomPOT Ξ))
where:
atomPOT : tree → atom list
input: Node(seq, b1, . . . , bn)
if seq = [Γ ] −→ [A]
then atomPOT b1 :: · · · ::
atomPOT bn :: A
else atomPOT b1 :: · · · ::
atomPOT bn
getSCT : context → tree → tree
input: Γ ,Node(seq,branches)
if Γ = contextOf(seq)
then
Node(seq, map (getSCT Γ ) branches)
else Nil
getChT : context → form → tree list → (form * tree) list
input: Γ ,F,Node(seq,branches)::list
if seq = [Γ ] −→ [A]
then (getChT Γ A branches) :: (getChT Γ F list)
else if seq = [Γ ′] −→ [ ] and Γ 6= Γ ′
then (F, Node(seq,branches)) :: (getChT Γ F list)
else (getChT Γ F branches) :: (getChT Γ F list)
buildTmcd : tree → tmcd
input: Ξ
eliRed(tmcdAux Empty Ξ)
where:
tmcdAux : (form * tree) → tmcd
input: (A, Ξ)
(A, buildTable(getSCT (contextOf(Ξ)) Ξ),
map tmcdAux (getChT (contextOf(Ξ)) A (Ξ :: [])))
Fig. 3. Functions used to extract a tmcd from a goal directed proof. Here the functions:
contextOf returns the bracket context of a sequent; map applies a function to all the
elements of a list.
the function eliRed that retains only the first occurrence of any repeated atomic
formula in a list of formulas.
The correctness of this algorithm can be shown by a simple induction and
can be found elsewhere [7]. It is easy to show that buildTable extracts a mcd
from a Horn Theory proof in time O(n), where n is the size of the input proof.
3.2 Uniform Proofs
We now specify how to extract a tree of multicut derivations from a finite (goal-
directed) proof tree, that is, LJF proofs where all atoms are assigned negative
polarity. In this more general class of proofs, it can happen that, after an asyn-
chronous phase, the context is augmented with a new positive formula or with a
new atom. Hence, differently from the Horn Theory case, not all atomic subgoals
are provable from the initial context, and therefore, a tmcd with more than one
node will be the backbone of the non-redundant proof of an uniform proof.
The function buildTmcd, shown in Figure 3, extracts a tmcd from an uniform
proof. We use the illustration in Figure 2 to explain how this function works.
The three different kinds of nodes (filled squares, ellipses, and blank squares)
represent sequents with different bracket contexts. First, the subtree with the
filled squares is extracted, by using the function getSCT, shown in Figure 3,
and, from this subtree, the multicut derivation M0 is constructed by using the
function buildTable. Second, by using the function getChT, also shown in Figure
3, the remaining children trees are extracted together with an atom, appearing
in M0, representing the provability dependency of two nodes of the extracted
tmcd. Third, buildTmcd is recursively applied to each child tree. Fourth, as
done with in the Horn Theory case, we eliminate redundancies with the function
eliRed as follows: let Ma be a multicut derivation containing the atom A, and
let Md be a descendent multicut derivation of Ma. If Md has the base sequent
[Γ ] −→ [A] then we obtain a new tmcd, by removing the tmcd’s subtree with
Using Tables to Construct Non-Redundant Proofs 351
rootMd; or if A is inMd, then we obtain a new tmcd by removing A fromMd
and its possible descendent subtrees from the tree of multicut derivations. There
is a final step that is not shown here concerning the sequent’s polarity context,
P and context Γ , which need to be augmented with the previous application
of multicut rules. This can be done in a straightforward way, by traversing the
obtained tmcd.
We prove the following correctness result by induction on the height of the
input tree Ξ.
Proposition 2. Let Ξ be a uniform LJF proof and let τ=buildTmcd(Ξ) be the
tree of multicut derivations obtained from Ξ. Then τ can be completed to a proof
by adding derivations containing only one Dl rule.
As argued before, the synchronous rules are not invertible and hence, have
an inherent don’t know non-determinism. In particular, the Dl rule has higher
degree of don’t know non-determinism, since it is usually the case that the con-
text of sequents contain several formulas and therefore, when applying the Dl
rule, an interpreter might need to backtrack more often to this rule and choose
another formula to focus on. Hence, the proposition above states that not only
tmcds are correct, but also that one can complete a tmcd in an optimal way,
since an interpreter only needs to search for derivations containing only one Dl
rule.
3.3 LJF proof
In the previous subsection, we considered only uniform LJF proofs, that is, proofs
where all atoms have negative polarity. We now extend the results obtained
before to all LJF proofs where there can also be atoms with positive polarity.
Now, there are two main differences with respect to uniform LJF proofs: (1)
Initial Right Rule - Initial right rules can end the proof; (2) Reaction Left
with Atomic Formula - By allowing atoms with positive polarity, proofs can
perform forward chaining steps. For instance, consider the following derivation
where the atom A has positive polarity:
[Γ ]−G→
[Γ,A] −→ [G′]
[Γ ]
A
−→ [G′]
Rr, []r
[Γ ]
G⊃A
−→ [G′]
⊃l
This type of derivation is not possible to
occur in a uniform proof, since there, the only
rule that introduces atoms focused in the left
is the initial left rule.
We change the functions atomPOT, getSCT, and getChT, to accommodate
these differences. For the first difference, namely that initial right rules can
finish the proof, it suffices to table the positive atom used to finish the proof,
and therefore, in the extracted tmcd, this atom will have its polarity changed to
positive4. For the second difference, namely that atomic reaction left rules can
happen, we table the forward chained atom performing, in the extracted tmcd,
the following transformation:
4 Remember that at the base of any tmcd, all atoms are assigned negative polarity.
352 Vivek Nigam
[Γ ]−G→
Ξ
[Γ,A] −→ [G′]
[Γ ]
A
−→ [G′]
Rr, []l
[Γ ]
G⊃A
−→ [G′]
⊃l
[Γ ] −→ [G′]
Dl
=⇒
P; [Γ ]−G→ P; [Γ ]
A
−→ [A]
Il
P; [Γ ]
G⊃A
−→ [A]
⊃l
P; [Γ ] −→ [A]
Dl
P′; [Γ,A] −→ [G′]
P; [Γ ] −→ [G′]
mc
However, a new question arises: where in the tmcd should we perform a
cut with A as cut formula. The answer is to insert this cut before inserting the
cuts with the atomic subgoals appearing in Ξ because to prove these subgoals
it might be necessary to use A.
Accordingly, we add new cases, shown in Figure 4, to the functions atomPOT,
getSCT, and getChT. To atomPOT, the first new case inserts a positive atom
focused on the left to the beginning of the list of atomic subgoals; the second
new case inserts to the list of atomic subgoals any positive atom that is used
to finish a proof with an initial right rule. The function getSCT, that is used to
extract subtrees used to construct tmcd’s nodes, is modified so that extracted
subtrees include atomic reaction left rules. Since the subtree extracted by getSCT
is modified, getChT is modified to extract correctly the children subtrees.
function: atomPOT = ...
else if seq = [Γ ]
A
−→ [G] and A /∈ Γ
then A :: atomPOT b1 :: · · · ::
atomPOT bn
else if seq = [Γ ]−A→
then A ...
function: getSCT = ...
else if seq = [Γ ]
A
−→ [G]
then Node(seq, map (getSCT (Γ ∪ {A})) branches) ...
function: getChT = ...
else if seq = [Γ ]
A
−→ [G]
then (getChT (Γ ∪ {A}) F branches) ::
(getChT (Γ ∪ {A}) F list) ...
Fig. 4. New cases added to the functions shown in Figure 2 to handle all LJF proofs.
We prove the following correctness result by induction on the height of the
input tree Ξ.
Proposition 3. Let Ξ be an LJF proof and let τ =buildTmcd(Ξ) be the tree
of multicut derivations obtained from Ξ. Then τ can be completed to a proof by
adding derivations containing only one Dl rule.
The complexity of buildTmcd lies in the comparison of two context (e.g. con-
texof(seq) = Γ ). Considering an upperbound, s, on the length of formulas, we can
show that buildTmcd extracts a tmcd from a LJFt proof in time O(ns (log n)),
where n is the size of the input proof. This is done by assigning an ordering to
clauses, represented by strings; for instance by using the ASCII numbering. To
determine that two sequents have the same context, it suffices to sort the con-
texts of the sequents and then, check one by one the equivalence of the clauses
in the contexts.
4 Experiments
As the experimental results in the following table shows, by completing the tmcd
extracted from the original tree (Ξ), we obtain a considerably smaller proof.
Using Tables to Construct Non-Redundant Proofs 353
Also, the execution time of buildTmcd is much lesser than the time needed to
find the original proof.
Size - number of nodes Time - ms
Ξ proof from tmcd buildTmcd(Ξ) find(Ξ) complete tmcd
5th fib 15 15 2.8 45.0 50
8th fib 67 27 3.1 85.0 120
11th fib 287 39 4.8 330.0 170
In the case for the 8th Fibonacci, the time needed to complete a tmcd is more
than of finding the original proof. However, we observed in our experiments that
it takes a constant time of 20 ms to complete one of the open branches of a
tmcd and there are only a linear number of them, that is, one open branch
for each subgoal. Therefore, we expect that while the time to search for proofs
exponentially grows with the Fibonacci number, the time of completing the
extracted tmcd should increase linearly.
5 Conclusions
This paper presents an approach, from a proof theoretic point of view, for reduc-
ing size of proofs through redundancy elimination. By a careful study of focused
proofs, we propose a new structure, called tree of multicut derivations, to be
the backbone for the construction of non-redundant proofs. The theoretical and
experimental results in this papers suggests that the procedure proposed can
be used to obtain smaller proofs. Further research is needed to investigate the
plausibility of this approach in the Proof Carrying Code framework.
References
1. Jean-Marc Andreoli. Logic programming with focusing proofs in linear logic. J. of
Logic and Computation, 2(3):297–347, 1992.
2. Kaustuv Chaudhuri, Frank Pfenning, and Greg Price. A logical characterization of
forward and backward chaining in the inverse method. In Proceedings of IJCAR’06,
2006.
3. Pierre-Louis Curien and Hugo Herbelin. The duality of computation. In Proceed-
ings of ICFP ’00, 2000.
4. Radha Jagadeesan, Gopalan Nadathur, and Vijay Saraswat. Testing concurrent
systems: An interpretation of intuitionistic logic. In Proceedings of FSTTCS, 2005.
5. Chuck Liang and Dale Miller. Focusing and polarization in intuitionistic logic. In
Proceedings of CSL’07, 2007.
6. Dale Miller, Gopalan Nadathur, Frank Pfenning, and Andre Scedrov. Uniform
proofs as a foundation for logic programming. Annals of Pure and Applied Logic,
51:125–157, 1991.
7. Dale Miller and Vivek Nigam. Incorporating tables into proofs. In Proceedings of
CSL’07, 2007.
8. Dale Miller and Alexis Saurin. From proofs to focused proofs: a modular proof of
focalization in linear logic. In Proceedings of CSL’07, 2007.
9. George C. Necula. Proof-carrying code. In Proceedings of POPL’97, 1997.
Classifying the Phase Transition Threshold for
Unordered Regressive Ramsey Numbers
Florian Pelupessy and Andreas Weiermann
Vakgroep Zuivere Wiskunde en Computeralgebra
Krijgslaan 281 Gebouw S22, 9000 Ghent, Belgium
{weierman, pelupessy}@cage.ugent.be
Abstract. Following ideas of Richer (2000) we introduce the notion of
unordered regressive Ramsey numbers or unordered Kanamori-McAloon
numbers. We show that these are of Ackermannian growth rate. For
a given number-theoretic function f we consider unordered f -regressive
Ramsey numbers and classify exactly the threshold for f which gives rise
to the Ackermannian growth rate of the induced unordered f -regressive
Ramsey numbers. This threshold coincides with the corresponding thresh-
old for the standard regressive Ramsey numbers. Our proof is based on
an extension of an argument from a corresponding proof in a paper by
Kojman,Lee,Omri and Weiermann 2007.
Key words: regressive Ramsey numbers, Ackermann function, un-
ordered canonical Ramsey theorem, Kanamori McAloon theorem
1 Introduction
There exist three basic and well known infinitary Ramsey principles which give
rise to independence results for PA: Ramsey’s theorem, the canonical Ramsey
theorem (Erdo¨s Rado) and the regressive Ramsey theorem (Kanamori McAloon)
[8,14,13,12]. The latter two principles make essential use of a pre-existing order
on the natural numbers in order to speak about min-colourings, max-colourings
and min-homogeneous sets, etc. The three basic infinitary Ramsey-principles
give rise to finitary Ramsey principles which can be formulated in the language
of arithmetic: the finite Ramsey principle with a suitable largeness condition
(i.e. the Paris-Harrington principle [14]), the Kanamori McAloon principle [8]
and the finite canonical Ramsey principle with a suitable largeness condition.
It turns out that all these finite versions make essential use of the standard <-
relation and one might wonder if it is possible to find strong principles which do
not depend so intrinsically on the less than relation.
An interesting approach to this question can be obtained from a recent paper
by Richer [17] about unordered canonical Ramsey numbers and their asymptotic
classification. It is quite natural to extend Richer’s approach to the context of
strong Ramsey principles and in this paper we do this for the Kanamori McAloon
Ramsey theorem.
We consider the phase transition for unordered regressive Ramsey numbers.
For simplicity we limit ourselves to only colourings of pairs (graphs) and expect
Unordered Regressive Ramsey Numbers 355
that the result extends to higher dimensions (hyper graphs) using appropriate
bounds from the fast growing hierarchy. We also expect that our results gen-
eralize to the unordered canonical Ramsey theorem with a suitable largeness
condition. This and other related questions will be investigated jointly in a big-
ger research project with A. Bovykin, L. Carlucci, G. Lee et al.
This contributes to a general research program of the second author about
phase transitions in logic and combinatorics (see, for example, [19,20,21,22,23]
for more information).
We identify the natural numbers with their corresponding sets of predecessors
and use C(u, v) instead of C({u, v}) for a colouring C : [R]2 → N. Here [R]2
denotes the set of pairs of unequal elements from R. No ordering of u, v is
implied here and when known we will give the relative order of the two elements.
Denote the collection of subsets of size m of a given set R with Pm(R). Given
a number-theoretic function f let us call a colouring C of pairs f -regressive
if C(u, v) ≤ f(u) for all u, v with u < v. Call an ordered set (H,≺) min≺-
homogeneous for C if C(x, y) = C(x, z) for all x, y, z in H with x ≺ y, x ≺ z.
Then given m there exists a least number R := uKMf (m) such that for all
f -regressive colourings C : [R]2 → N there exists an H ∈ Pm(R) and a linear
ordering ≺ on H such that H is min≺-homogeneous for C.
The class of primitive recursive functions is the smallest class of functions
Nd → N which contains the constant functions, projections and successor func-
tion and is closed under composition and recursion. We call a function Acker-
mannian if it eventually dominates every primitive recursive function. Define
the Ackermann function A as follows:
A0(i) := i+ 1
An+1(i) := (An)(i)(i)
A(i) := Ai(i)
The Ackermann function is Ackermannian. It is easy to see that for constant
functions f the function uKMf is primitive recursive and so in between the
constant function and the identity function there will be phase transition from
being primitive recursive to Ackermannian of uKMf . Roughly speaking, for
f(i) = k
√
i, the function uKMf is Ackermannian whereas for f(i) = log(i) the
function uKMf is still elementary recursive. In a final step we let k in
k
√
i
depend on i. We show that function uKMf is still primitive recursive for any d
if f(i) ≤ A−1d (i)√i but becomes Ackermannian if f(i) ≥ A−1(i)√i.
2 Upper Bounds
We use results from the Kanamori-McAloon principle to derive upper bounds
on the unordered case.
Theorem 1 (uKM). For every m there exists an R such that for every f-
regressive colouring C : [R]2 → N there exists an H ∈ Pm(R) with linear order
≺⊆ H2 which is min≺-homogeneous for C.
356 Florian Pelupessy and Andreas Weiermann
Proof. Let KMf (m) be the minimal R¯ such that for every f-regressive colouring
C : [R¯]2 → N there exists an H ∈ Pm(R) which is min<-homogeneous for C.
(see [8] for proof of existence of such R¯ )
For given f take R = KMf (m) and an f -regressive colouring C. Then there
exists H ∈ Pm(R) which is min-homogeneous for C, hence taking for ≺ the
ordering < suffices to make H min≺-homogeneous for C.
Notation: uKMf (m) := the smallest such R. The proof of this theorem also
delivers upper bounds on uKMf .
Theorem 2. uKMf is primitive recursive if f is:
1. a constant function,
2. i 7→ log i,
3. i 7→ A−1d (i)√i.
Proof. Because uKMf (m) ≤ KMf (m) and the primitive recursive functions are
closed under bounded search it suffices that KMf is primitive recursive. For
proof of that for all three cases see [2].
3 Lower Bounds
For the lower bound it is not possible to easily transfer earlier results. We modify
the proofs for KMf from [2] and [10] to suit the problem that allowing differing
orderings on H gives. As a preliminary we begin with some results about prim-
itive recursive functions.
For s > 0, define the following sequence:
As0(i) := i+ 1
Asn+1(i) := (A
s
n)
(b s√ic)(i)
As(i) := Asi (i)
Note that for s = 1 this is the Ackermann function.
For R2c(i) we take the minimal R such that for every colouring C : [R]
2 7→ c
there exists Y ∈ Pi(R) such that Y is C-homogeneous. (C is constant on [Y ]2).
Lemma 1. 1. The Ackermann function is Ackermannian.
2. If the composition of two non-decreasing functions functions is Ackerman-
nian and one of those is primitive recursive, then the other is Ackermannian.
3. (i, c) 7→ R2c(i) is primitive recursive.
Proof. A proof of the first two statements can be found in [1] and [11], for a
proof of the latter one see [7].
We now give a lower bound for uKMf which should ensure that it is Acker-
mannian. This proof rests on two ideas, namely the use of the particular colour-
ings similar to proofs of the ordered KMf and increasing the ’space’ in the
D-homogeneous set to solve the problem caused by allowing any linear order to
determine min≺-homogeneity. Fix s ∈ N.
Unordered Regressive Ramsey Numbers 357
Lemma 2 (lower bound for roots). Let f : i 7→ s√i, then:
uKMf (R2c(m+ 4)) ≥ Asc+1(m)
for all c,m ∈ N.
Proof. Take k = R2c(m + 4) and R = uKMf (k). Define a colouring C on R as
follows for x < y:
C(x, y) =
{
0 if Asc+1(x) ≤ y
l else
where l is such that for the smallest p for which Asp+1(x) > y we have A
s
p
(l)(x) ≤
y < Asp
(l+1)(x).
Taking p for x, y as above, define colouring D of R for x < y:
D(x, y) =
{
0 if Asc+1(x) ≤ y
p else
Note that C is f -regressive (because Asp
(bf(x)c)(x) = Asp+1(x) ). Let H ∈ Pk(R)
with order ≺ be min≺-homogeneous for C, then by definition of k there exists
Y ∈ Pm+4(H) which is D-homogeneous. Enumerate such a Y with a strictly
<-increasing sequence Y = {y1, · · · , ym, x, y, z, z′}. Then we have the following
cases for the relative ≺-ordering of x, y, z, z′:
1. x ≺ y, x ≺ z
Claim: Asc+1(x) ≤ y.
Assume for a contradiction that Asc+1(x) > y, then by definition of C we get
C(x, y) = l 6= 0. Hence (by min≺-homogeneity of H) C(x, y) = C(x, z) = l.
By definition of D and D-homogeneity of Y we also get D(y, z) = D(x, y) =
p 6= 0.
So the definition of C gives us:
Asp
(l)(x) ≤ y < Asp(l+1)(x)
and
Asp
(l)(x) ≤ z < Asp(l+1)(x),
that of D delivers:
Asp(y) ≤ z.
Combining these inequalities, taking note that Asp is increasing, we get the
contradiction:
z < Asp
(l+1)(x) = Asp(A
s
p
(l)(x)) ≤ Asp(y) ≤ z
2. z ≺ x, z ≺ y
Claim: Asc+1(x) ≤ z.
Assume Asc+1(x) > z, then by definition and min≺-homogeneity of C we
358 Florian Pelupessy and Andreas Weiermann
have C(x, z) = C(y, z) = l 6= 0, by definition and homogeneity of D we get:
D(x, y) = D(x, z) = p. This gives us inequalities:
Asp
(l)(x) ≤ z < Asp(l+1)(x),
Asp
(l)(y) ≤ z < Asp(l+1)(y)
and
Asp(x) ≤ y.
Combining these we get:
z < Asp
(l+1)(x) = Asp
(l)(Ap(x)) ≤ Asp(l)(y) ≤ z
3. y ≺ x, y ≺ z, we distinguish two possibilities:
(a) y ≺ z′
Claim: Asc+1(y) ≤ z.
Assume Asc+1(y) > z, then C(y, z) = C(y, z
′) = l 6= 0 and D(z, z′) =
D(y, z′) = p. So we have inequalities:
Asp
(l)(y) ≤ z < Asp(l+1)(y),
Asp
(l)(y) ≤ z′ < Asp(l+1)(y)
and
Asp(z) ≤ z′.
Combining these:
z′ < Asp(A
s
p
(l)(y)) ≤ Asp(z) ≤ z′
(b) z′ ≺ y
Claim: Asc+1(x) ≤ z′.
Assume Asc+1(x) > z
′, then C(x, z′) = C(y, z′) = l 6= 0 and D(x, y) =
D(x, z′) = p. So we have:
Asp
(l)(x) ≤ z′ < Asp(l+1)(x),
Asp
(l)(y) ≤ z′ < Asp(l+1)(y)
and
Asp(x) ≤ y.
Combining these:
z′ < Asp
(l)(Asp(x)) ≤ Asp(l)(y) ≤ z′
Examining the cases above allows us to conclude Asc+1(x) ≤ z′. But then:
Asc+1(m) ≤ Asc+1(ym) ≤ Asc+1(x) ≤ z′ ∈ Y ⊆ H ⊆ R. So we finally have:
Asc+1(m) ≤ R
Unordered Regressive Ramsey Numbers 359
For this lower bound to result in Ackermannian functions uKMf the As have
to be Ackermannian as well:
Lemma 3. An(i) ≤ Asn+2s2+1(i) for any i ≥ 4s.
Proof. See [2], corollary 4.3.
Theorem 3. uKMf is Ackermannian for f = fs : i 7→ s
√
i and for f : i 7→
A−1(i)√i.
Proof. For the first assertion combine lemmas 1, 2 and 3.
For the second we claim that
N(i) := uKMf (R2i+2i2+1(4
i + 3)) > A(i)
for all i. Assume for contradiction that N(i) ≤ A(i) for some i. Then for l ≤ N(i)
we have A−1(l) ≤ i, so i√l ≤ A−1(i)√l. Hence:
uKMf (R2i+2i2+1(4
i + 3)) ≥ uKMfi(R2i+2i2+1(4i + 3))
≥ Aii+2i2+1(4i)
> A(i)
Where the first inequality is a consequence if the definition of uKMf , the second
of lemma 2 and the third of lemma 3. The resulting inequality contradicts with
our assumption.
Now the claim with lemma 1 implies that uKMf is Ackermannian.
References
1. C. Calude.: Theories of computational complexity, Annals of Discrete Mathematics
volume 35, North-Holland, Amsterdam (1988)
2. L. Carlucci, G. Lee, A. Weiermann: A Sharp Phase Transition for Go¨del Incom-
pleteness and Finite Combinatorics.
http://www.lix.polytechnique.fr/∼leegy/Publi/kmjams.pdf
3. P. Cholak, C. Jockusch, T. Slaman On the strength of Ramsey’s theorem for pairs.
J. Symbolic Logic 66 (2001), no. 1, 1–55.
4. P. Erdo˝s, A. Hajnal, A. Ma´te´, and R. Rado. Combinatorial set theory: partition
relations for cardinals, volume 106 of Studies in Logic and the Foundations of
Mathematics. North-Holland, Amsterdam, 1984.
5. P. Erdo˝s and R. Rado. Combinatorial theorems on classifications of subsets of a
given set. Proc. London Math. Soc. (3), 2:417–439, 1952.
6. K. Go¨del. U¨ber formal unentscheidbare Sa¨tze der Principia Mathematica und
verwandter Systeme. Monatshefte f. Math. u. Phys., 38:173–198, 1931.
7. R. L. Graham, B. L. Rothschild, and J. H. Spencer. Ramsey theory. Second edition.
John Wiley & Sons Inc., New York, 1990.
8. A. Kanamori and K. McAloon. On Go¨del incompleteness and finite combinatorics.
Ann. Pure Appl. Logic, 33(1):23–41, 1987.
360 Florian Pelupessy and Andreas Weiermann
9. J. Ketonen and R. Solovay. Rapidly growing Ramsey functions. Ann. of Math.
(2), 113(2):267–314, 1981.
10. M. Kojman, S. Shelah: Regressive Ramsey Numbers Are Ackermannian, Journal
of Combinatorial Theory, Series A 86, 177-181 (1999)
11. M. Kojman, G. Lee, E. Omri and A. Weiermann. Sharp Thresholds for the Phase
Transition between Primitive Recursive and Ackermannian Ramsey Numbers. To
appear in Journal of Combinatorial Theory.
http://www.math.bgu.ac.il/∼kojman/Thresh.pdf
12. J. Mileti, Partition theorems and computability theory, dissertation at UIUC
(2004)
13. J. B. Paris. Some independence results for Peano arithmetic. J. Symbolic Logic,
43(4):725–731, 1978.
14. J. B. Paris and L. Harrington. A mathematical incompleteness in Peano arithmetic.
In J. Barwise, ed., Handbook of Mathematical Logic, volume 90 of Studies in Logic
and the Foundations of Mathematics, pages 1133–1142. North-Holland, 1977.
15. R. Pe´ter. Recursive functions. Third edition. Academic Press, New York, 1967.
16. F. P. Ramsey. On a problem of formal logic. Proc. London Math. Soc., 30:264–285,
1930.
17. D. Richer: Unordered Canonical Ramsey Numbers. Journal of Combinatorial The-
ory, Series B 80, 172-177 (2000)
18. S.G. Simpson: Subsystems of Second Order Arithmetic.
19. A. Weiermann: 2005: Analytic combinatorics, proof-theoretic ordinals, and phase
transitions for independence results. APAL 136, Issues 1-2 , 189-218.
20. A. Weiermann: Phasenu¨berga¨nge in Logik und Kombinatorik. MDMV 13 (3)
(2005), 152-156.
21. A. Weiermann: An extremely sharp phase transition threshold for the slow growing
hierarchy. MSCS 16 (5) (2006), 925-46.
22. A. Weiermann: Phase transition thresholds for some natural subclasses of the re-
cursive functions. Proceedings of CiE’06, LNCS 3988 (2006), 556-570.
23. A. Weiermann: 2007 Phase transition thresholds for some Friedman-style indepen-
dence results. MLQ. 53 (1), (2007) 4-18.
24. A. Weiermann. A classification of rapidly growing Ramsey functions. Proc. Amer.
Math. Soc., 132(2):553–561, 2004.
Almost Partial m-Reducibility
Katya Petrova and Boris Solon
Ivanovo State University of Chemistry and Technology
Abstract. New reducibility (the so-called apm-reducibility) of enumer-
ation type which is weaker than pm-reducibility is introduced in this
paper. Initial segments of the upper semilattice of apm-degrees are stud-
ied here.
The notations and terminology similar to those of the monograph [3] are
used. Let ω denote the set of positive integers; A,B, . . . ,X, Y (with or without
indices) are used to denote the subsets of ω and A = ω − A. We will use the
letters V andW as variables which range over a set of all c.e. sets. Given a partial
function α : ω → ω let domα, ranα and graphα = {〈x, α(x)〉 : x ∈ domα} be
the domain, the range and the graph of α respectively. We will write α(x) ↓ if
x ∈ domα and α(x) ↑ if x /∈ domα. Let α−1(X) = {x : x ∈ domα ∧ α(x) ∈ X}.
We will use the letters ϕ and ψ as variables which range over a set of all p.c.
functions.
Yu.L. Ershov introduced in [1] partial m-reducibility of sets:
A ≤pm B ⇐⇒ (∃ϕ)(∀x)[x ∈ A ⇐⇒ x ∈ domϕ ∧ ϕ(x) ∈ B] ⇐⇒
⇐⇒ (∃ϕ)[A = ϕ−1(B)].
In this case we will talk that A is pm-reducible to B via ϕ.
In this paper we introduce a reducibility which slightly weakens pm-
reducibility :
A ≤apm B ⇐⇒ (∃W )(∃A1)[A =W ∪A1 ∧A1 ≤pm B] ⇐⇒
⇐⇒ (∃W )(∃ϕ)[A =W ∪ ϕ−1(B)].
In this case we will talk that A is apm-reducible to B via (W,ϕ). These new
ideas of defining the reducibility were proposed in 70-s by S.D.Zakharov.
It is clear that if A ≤pm B via ϕ then A ≤apm B via (∅, ϕ). It is easy to
check that apm-reducibility is reflexive and transitive, hence it is a reducibility
in ordinary sense. Also it is clear that A ≤apm B ⇒ A ≤e B for all A and B,
i.e. apm-reducibility is a reducibility of enumeration type.
Let degapm(A) = {X : X ≤apm A ∧ A ≤apm X} be apm-degree of A and
Dapm be the p.o. set of all apm-degrees.
Theorem 1 Dapm is an upper semilattice with least element 0apm = {Wt :
t ∈ ω} in which the least upper bound of the apm-degrees a = degapm(A) and
b = degapm(B) is a ∪ b = degapm(A⊕B).
362 Katya Petrova and Boris Solon
Proof. Obviously that W ≤apm A via (W,φ) where φ is p.c. function with
domφ = ∅. Next let A ≤apm C via (V1, ψ1) and B ≤apm C via (V2, ψ2), we set
V = V1 ∪ V2 and ψ = ψ1 ⊕ ψ2. Then it is clear that A⊕B ≤apm C via (V, ψ).
The following theorem gives an example of two sets such that A 6≤pm B ∧
A ≤apm B, i.e. pm-reducibility is stronger than apm-reducibility.
Theorem 2 There are the sets A and B such that A 6≤pm B and A ≤apm B.
Proof. At first we construct c.e. sets W and V such that
(i) V is infinite;
(ii) ∀y[|ranϕy| =∞⇒ (∃x)[x ∈W ∩ domϕy ∧ ϕy(x) ∈ V ]].
Step 0. Set W = V = ∅.
Step z+1. Let z = 〈k, y〉. Compute k steps in enumerations for each of the
y + 1 c.e. sets domϕ0, . . . ,domϕy. For each i = 0, 1, . . . , y see whether
(∃x)[x ∈ domϕi ∧ ϕi(x) > 2i]. (1)
If (1) is true then let x∗ be the least number x which satisfies (1). In this case
we place x∗ in W and ϕi(x∗) in V .
The end of the construction.
As a result we have
|{0, 1, . . . , 2y} ∩ V | ≤ y
for every y ∈ ω. Hence the set V is infinitive. The construction is such that all
steps z + 1 are computable, hence sets W and V are c.e. So the conditions (i)
and (ii) are satisfied.
Now we construct a set B ⊂ V such that W ∪{〈x, x〉 : x ∈ B} is not c.e. Let
Z = {z : Wz ⊇ W}, Z = {z0, z1, . . .} and V = {c0, c1, . . .}. (Both enumerations
are not c.e.)
For every i = 0, 1, 2, . . . we see whether
〈ci, ci〉 ∈Wzi . (2)
If (2) is true then let j > i such that Wzj = Wzi then we place cj in B and we
do not place ci in B. If (2) is not true then we place ci in B. As a result we have
W ∪ {〈x, x〉 : x ∈ B} 6=Wz for all z ∈ ω, i.e. W ∪ {〈x, x〉 : x ∈ B} is not c.e. set.
Let ϕ(z) be a p.c. function such that ∀z[ϕ(z) ↓ ⇐⇒ ∃x[〈x, x〉 = z]] and
∀x[ϕ(〈x, x〉) = x]. As {〈x, x〉 : x ∈ B} ≤pm B via ϕ then W ∪ {〈x, x〉 : x ∈
B} ≤apm B via (W,ϕ).
Prove that W ∪ {〈x, x〉 : x ∈ B} 6≤pm B. We assume that
W ∪ {〈x, x〉 : x ∈ B} ≤pm B
then W ∪ {〈x, x〉 : x ∈ B} = ϕ−1y (B) for some y ∈ ω. If ranϕy is infinite then
∃x[x ∈W ∩ domϕy ∧ ϕy(x) ∈ V ].
In this case the construction of B guarantees that
∃x[x ∈W ∩ domϕy ∧ ϕy(x) /∈ B]
Almost Partial m-Reducibility 363
what contradicts the premise.
If ranϕy is finite then W ∪ {〈x, x〉 : x ∈ B} is a c.e. set that contradicts the
premise too.
Let A =W ∪ {〈x, x〉 : x ∈ B}, then it is clear that A 6≤pm B and A ≤apm B
and the theorem is proved completely.
Very simple properties of amp-reducibility are given in the following
Theorem 3 (i). If A differs from B on a finite set then A ≡apm B;
(ii). If V is c.e. set then A ∪ V ≤apm A for any A;
(iii). For any A and c.e. V if there is c.e. W such that A ⊆W and W ∩V = ∅
then A ≡apm A ∪ V .
A set A is called cohesive [2] if A is infinite and A ∩ V is finite or A ∩ V is
finite for any c.e. set V . In other words an infinite set A is cohesive iff it cannot
be divided into two infinite parts by a c.e. set.
Let (D,≤) be a partial ordering, a,b ∈ D and b < a. The element a is called
b-minimal if
∀x[x ∈ D ∧ x ≤ a⇒ [x ≤ b ∨ a ≤ x]].
Let 0 be the least element in D; 0-minimal element a is called minimal.
M.Rozinas proved that a pm-degree of cohesive set is a minimal element in
Dpm. The following theorem shows that this is true in Dapm.
Lemma 1 If A ≤apm B via (V, ψ) then A ≡apm (B ∪ V ) ∩ ranψ.
Proof. Let A ≤apm B via (V, ψ) then A = V ∪ψ−1(B). It is clear that ψ−1(B) =
ψ−1(B ∩ ranψ) thus A = V ∪ψ−1(B ∩ ranψ) = V ∪ψ−1((B ∪ V )∩ ranψ). Then
A ≤apm (B ∪ V ) ∩ ranψ via (V, ψ).
Conversely, let W = {〈y, x〉 : 〈x, y〉 ∈ graphψ}. It is well known that there is
p.c. ϕ such that domϕ = 〈W 〉1 = ranψ and graphϕ ⊆ W . Let V −1 = ϕ−1(V ),
now we will prove that
(V ∪B) ∩ ranψ = V −1 ∪ ϕ−1(A).
We have for any y ∈ ω
y ∈ (V ∪B) ∩ ranψ ⇐⇒ y ∈ V ∩ ranψ ∨ y ∈ B ∩ ranψ ⇒
⇒ ϕ(y) ↓ ∧ϕ(y) = x ∈ (V ∪A)⇒ y ∈ V −1 ∪ ϕ−1(A)
and
y ∈ V −1 ∪ ϕ−1(A)⇒ y ∈ V −1 ∨ y ∈ ϕ−1(A).
We consider each case separately:
y ∈ V −1 ⇐⇒ ϕ(y) ↓ ∧ϕ(y) = x ∈ V ⇒ y ∈ V ∩ ranψ ⇒ y ∈ (V ∪B) ∩ ranψ
and
y ∈ ϕ−1(A) ⇐⇒ ϕ(y) ↓ ∧ϕ(y) = x ∈ A⇒ y ∈ B∩ ranψ ⇒ y ∈ (V ∪B)∩ ranψ.
So (V ∪B)∩ ranψ = V −1 ∪ϕ−1(A), it implies (B ∪ V )∩ ranψ ≤apm A. Now
let’s move to
364 Katya Petrova and Boris Solon
Theorem 4 There are minimal elements in Dapm.
First we prove a lemma on cohesive sets.
Lemma 2 There is cohesive set B such that
∀n[|Wn| =∞ ⇐⇒ |Wn ∩B| =∞] (3)
Proof of the lemma. Let B−1 = ω, next by induction on n if |Bn−1∩Wn| =
∞ then set Bn = Bn−1∩Wn and if |Bn−1∩Wn| <∞ then set Bn = Bn−1∩Wn.
Let b0 ∈ B0, b1 ∈ B1, . . . , bn ∈ Bn, . . . such that b0 < b1 < . . . < bn < . . ., set
B = {bi : i ∈ ω}. Prove that B is a cohesive set and satisfies (3).
Assume that B is not a cohesive set then |B ∩Wm| =∞ and |B ∩Wm| =∞
for some m. According our construction Bm ⊂ Wm or Bm ⊂ Wm. Since all
but a finite number of members of B must lie in Bm, either |B ∩Wm| < ∞ or
|B ∩Wm| <∞ that contradicts the premise.
Now prove that (3) is true. Let |Wn| = ∞ and let m0 < m1 < . . . be an
infinite sequence such thatWn =Wmi for all i ∈ ω. It is clear that bmi ∈ B∩Wn
for all i ∈ ω, hence |B ∩Wn| =∞. The lemma is proved.
Proof of the theorem. Let B be a cohesive set satisfying (3) and b =
degapm(B). It is sufficient to show that
∀x[x ∈ Dapm ∧ x ≤ b⇒ [x = 0apm ∨ b ≤ x]].
Let a = degapm(A) and A ≤apm B via (V, ψ), i.e. A = V ∪ ψ−1(B). Lemma 1
implies A ≡apm (B ∪ V ) ∩ ranψ.
As B is cohesive then two alternative cases are possible: B ∩ ranψ is finite or
B ∩ ranψ is finite. Let B ∩ ranψ be finite then (B ∪ V ) ∩ ranψ is c.e., hence we
have a = 0apm.
Let B ∩ ranψ is finite. In this case as B is cohesive two alternative cases are
possible: V ∩B is finite or V ∩B is finite. Let V ∩B is finite then V is a finite
set by (3). In this case we have
A ≡apm (B ∪ V ) ∩ ranψ = (B ∩ ranψ) ∪ (V ∩ ranψ) ≡apm B ∩ ranψ ≡apm B
and so we have a = b.
If V ∩B is finite then B ∪ V differs from V on a finite set so in this case we
have a = 0apm. The theorem is proved completely.
Theorem 5 If B1, . . . , Bn, n ≥ 1 are cohesive sets satisfying (3) and their apm-
degrees are different in pairs then the initial segment [0apm,degapm(B1 ⊕ . . . ⊕
Bn)] ⊂ Dapm is isomorphic to Boolean algebra of subsets of a n-element set.
Proof. Let B = B1⊕ . . .⊕Bn and A ≤apm B, we show that in this case A is
c.e. or A ≡apm
⋃
i∈I B
∗
i where B
∗
i = {xn+ i : x ∈ Bi} for some I ⊆ {1, . . . , n},
I 6= ∅.
Let A ≤apm B via (W,ψ) and V = ranψ. Let
I = {i : 1 ≤ i ≤ n ∧ |V ∩B∗i | =∞}.
Almost Partial m-Reducibility 365
If I = ∅ then V ∩ B∗i is finite for all i = 1, . . . , n, thus V ∩ B is finite and then
A is a c.e. set.
Let I 6= ∅. Since Bi is a cohesive set satisfying (3) then B∗i is a cohesive set
satisfying (3) too, thus V ∩B∗i is finite for all i ∈ I. Then
|V ∩ (∪i∈IB∗i )| = | ∪i∈I B∗i − V | <∞.
In this case the set
⋃
i∈I B
∗
i differs from
⋃
i∈I(B
∗
i ∩ V ) on a finite set, thus⋃
i∈I
B∗i ≡apm
⋃
i∈I
(B∗i ∩ V ). (4)
Let i ∈ {1, . . . , n} − I then the set B∗i ∩ V is finite, thus the set B ∩ V =
∪ni=1(B∗i ∩ V ) differs from ∪i∈I(B∗i ∩ V ) on a finite set. So
B ∩ V ≡apm ∪i∈I(B∗i ∩ V ). (5)
By Lemma 1 A ≤apm B via (W,ψ) implies
A ≡apm (B ∪W ) ∩ ranψ = (B ∩ V ) ∪ (W ∩ V ).
Taking into consideration the fact that the set V ∩W may be finite or infinite we
conclude that A is c.e. or A ≡apm B∩V . In the last case we have A ≡apm ∪i∈IB∗i ,
as every B∗i satisfies (3).
Now we show that for any I1 ⊆ {1, . . . , n} and I2 ⊆ {1, . . . , n}
I1 ⊆ I2 ⇐⇒ ∪i∈I1B∗i ≤apm ∪i∈I2B∗i .
If I1 ⊆ I2 then ∪i∈I1B∗i ≤apm ∪i∈I2B∗i via (∅, α) where α(x) ↓ ⇐⇒ x ∈
∪i∈I1{nx+ i : x ∈ ω} and x ∈ domα⇒ α(x) = x.
Let ∪i∈I1B∗i ≤apm ∪i∈I2B∗i . We assume that I1 6⊆ I2 and j ∈ I1 − I2. It is
clear that {j} ⊆ I1 implies
B∗j ≤apm ∪i∈I1B∗i ≤apm ∪i∈I2B∗i .
Let B∗j ≤apm ∪i∈I2B∗i via (W ∗, ϕ) i.e. B∗j = W ∗ ∪ (∪i∈I2ϕ−1(B∗i )). Denote by
Vi = ϕ−1({nx+ i : x ∈ ω}) for all i ∈ {1, . . . , n}. As the B∗j is a cohesive set and
B∗j ⊆ W ∗ ∪ (∪i∈I2Vi) then there is l ∈ I2 (note that l 6= j) such that B∗j ∩ Vl
is an infinite set. Otherwise B∗j = W
∗ ∪D for some finite set D and then B∗j is
c.e. that contradicts the premise.
Let ϕ∗ be a restriction of ϕ to Vl. ObviouslyB∗j∩Vl ≤apm B∗l via (W ∗∩Vl, ϕ∗).
As B∗j ∩ Vl is a finite set then
B∗j ≡apm B∗j ∩ Vl ≤apm B∗l ⇒ Bj ≤apm Bl.
As the B∗l is a cohesive set satisfying (3)then by Theorem 4 the set B
∗
j is
c.e. or Bl ≤apm Bj . But the first is not possible as Bj is a cohesive set and
366 Katya Petrova and Boris Solon
the second is not possible as Bj and Bl should be in a distinct apm-degrees by
hypothesis of Theorem. So we get a contradiction that proves
∪i∈I1B∗i ≤apm ∪i∈I2B∗i ⇒ I1 ⊆ I2.
Let Ξ be the mapping from the set of all non-empty subsets of {1, . . . , n}
into Dapm, which is defined by
Ξ(I) = degapm(∪i∈IB∗i )
for all ∅ 6= I ⊆ {1, . . . , n} and Ξ(∅) = 0apm. We proved above that Ξ is an
order-preserving embedding Boolean algebra of subsets of {1, . . . , n} to the ini-
tial segment [0apm,degapm(B1 ⊕ . . . ⊕ Bn)] ⊂ Dapm. The theorem is proved
completely.
Corollary 1 For all n ≥ 1 Dapm contains a continuum of initial segments each
of which is isomorphic to Boolean algebra of subsets of a n-element set.
In conclusion we would like to announce another weak pm-reducibility:
A ≤wpm B ⇐⇒ (∃k)(∃ψ1, . . . , ψk)[A = ψ−11 (B) ∪ . . . ∪ ψ−1k (B)].
In this case we will talk that A is wpm-reducible to B via (ψ1, . . . , ψk).
It is clear that ≤wpm is reflexive and transitive, hence it is a reducibility in
ordinary sense. It is also clear that A ≤apm B ⇒ A ≤wpm B and A ≤wpm B ⇒
A ≤e B (moreover A ≤wpm B ⇒ A ≤s B) for all A and B, i.e. apm-reducibility
is a reducibility of enumeration type. We are able to construct two sets A and
B such that A 6≤apm B ∧A ≤wpm B.
We invite to study this wpm-reducibility as a reducibility of enumeration
type which has a good intuitive background.
References
1. Ershov Yu. L.: Numbering Theory. ”Nauka”. Moskou. 1977
2. Rogers H.,Jr.: Theory of Recursive Functions and Effective Computability.
McGraw-Hill. New York. 1967
3. Soare Robert I.: Recursively Enumerable Sets and Degrees. Springer-Verlag. Berlin,
Heidelberg, New York, London. 1987
Two-Dimensional Cellular Automata Transforms 
for a Novel Edge Detection 
Yongri Piao1, *Seok-Tae Kim1, Sung-Jin Cho2  
1Department of Telematics, Pukyong National University,  
599-1, Daeyeon 3-Dong, Nam-Gu, Busan, Republic of Korea (608-737) 
pyr-bww@hanmail.net,  setakim@pknu.ac.kr 
2 Division of Mathematical Sciences, Pukyong National University,  
599-1, Daeyeon 3-Dong, Nam-Gu, Busan, Republic of Korea (608-737) 
sjcho@pknu.ac.kr   
Abstract. In this paper, we propose a novel edge detection scheme using two-
dimensional cellular automata transforms (CAT). Cellular Automata (CA) is 
discrete dynamical system whose function is completely specified in terms of 
local relation. First, we get the gateway values such as wolfram Rule, number 
of cells in lattice, number of cells per neighborhood, initial configuration and 
boundary configuration. Second, we use the gateway values to generate a dual-
state, two-dimensional cellular automata and dual-coefficients basis function. 
Finally, we transform images into cellular automata domain according to the 
basis function. Then we use the basis function and cellular automata transform 
coefficients to extract the edge of the image.  The experimental results verify 
that the proposed scheme is a new attempt to process image with cellular auto-
mata model. 
1 Introduction 
Sharp variations of image intensity could be described by edges, which convey im-
portant information in an image. Edge detection is a pivotal technique in pattern rec-
ognition, image procession, and computer vision.  
One edge detection algorithm can not be applied to all of the images. Based on the 
complexity in edge detection, each algorithm has positive and negative performance. 
For instance, Smith et al. [1] proposed low-level feature extraction-SUSAN operator. 
It attempted to provide robust signature for every edge point. The method proposed 
by Smith et al, however, is strongly influenced by the presence of edges and corners. 
On the other hand, noise contamination is always a problem and edge detection in 
noisy environment can be treated as an optimal linear filter design problem [2-6]. The 
Canny [3] edge detector is more accurate, though more complex. It employs Gaussian 
first order differential equation which is able to reach the balance between robust 
noise and edge detection. Unfortunately, the Canny detector failed to verify whether 
discontinuity is contributed by noise or true edges. Unlike previous works, we present 
                                                          
* Corresponding Author. Tel.: +82-51-629-6234;  Fax: +82-51-629-6210 
368 Yongri Piao et al. 
a novel edge detection method in the CAT. The essence of CAT is that we can always 
find CA rules (and its associated neighborhood, initial/boundary configuration, lattice 
arrangement etc) which will result in basis function and transform coefficients. 
CA was originally introduced by Ulam [7] and Von Neumann [8]. Afterwards, the 
cellular automata theory was developed by Stephen Wolfram [9-10]. Now cellular 
automata have generated much interest because of their diverse functions and useful-
ness as a discrete model for many processes [11-13]. Along with the development of 
the digital image technology, some efforts are expended in using CA in image proc-
essing. C. L. Chang et al. [14] proposed an image edge detection using CA. They 
defined a new kind of neighborhood of the CA. Then a suitable local rule of the CA 
is designed. However, their method only uses the relationship of the neighborhood to 
detect edges in the spatial domain and is weak at noise.  
In this paper, we propose a novel edge detection method using two-dimensional 
cellular automata transforms. After we get the gateway values (such as Wolfram Rule, 
the number of cells in lattice, the number of cells per neighborhood, the initial con-
figuration, and the boundary configuration), we use them to generate a dual-state, 
two-dimensional cellular automata dual-coefficients basis function. Then the basis 
function and their associated transform coefficients transform an image into cellular 
automata domain components to extract the edge of the image. The experimental 
results verify that the proposed method is not only a new attempt to detect edge detec-
tion with the cellular automata model but also is proved to be very efficient. 
2 Cellular Automata Basics 
CA is a dynamical system in which space and time are discrete. The cells, which 
are arranged in the form of a regular lattice structure, have a finite number of states. 
These states are updated synchronously according to a specified local rule of interac-
tion. Using a specified rule, the values are updated synchronously in discrete time 
steps for all cells[15].  
In general, for a K-state, m-site neighborhood CA, there are 
mKK  rules. For ex-
ample, there are 28 rules for the two- state, 3-site neighborhood CA. The Wolfram 
Rule convention is to assign the integer R to rule generating the function F such that: 
 
                                                 
1
0
2
mK
n
n
n
R C
−
=
= ∑                                                       (1) 
 
where Cn is the Boolean value generated by the rule given the n-th configuration. 
Consider a 3-site neighborhood dual-state one-dimensional CA. The quantity ait 
represents the state of the i-th cell, at discrete time t, whose two neighbors are in the 
following states: ai-1t , ai+1t. In general, we seek a rule that will be used to synchro-
nously calculate the state ait+1 from the state of the cells in the neighborhood at the t-
th time level. The cellular automaton evolution can be expressed in the form: 
 Two-Dimensional Cellular Automata Transforms 369 
                                                1 1 1( , , )it i t it i ta F a a a+ − +=                                     (2) 
 
where F is a Boolean function defining the rule. 
The number of cellular automata rules can be astronomical even for a modest lat-
tice space, neighborhood size, and CA state. Therefore, in order to develop practical 
applications, a system must be developed for addressing a subset of this infinitely 
large universe of CA rules. Consider, for example, a K-state, m-site neighborhood 
CA with m=2r+1 points per neighborhood. We define the rule of evolution of a cellu-
lar automaton by using a vector of integers ( 0,1, 2, 3, 2 )mjW j = L  such that  
22 2
( )( 1 ) 2 1
0
m o d
mm
m
W
r t j j
j
a W W Kα−+ −=
⎛ ⎞= +⎜ ⎟⎝ ⎠∑                           
(3) 
 
where 0 jW K≤ < , r is a distance. jα are made up of the permutations of the states of 
the cells in the neighborhood. Hence, each set of jW  results in a given rule of evolu-
tion.  
3 Cellular Automata Transforms 
Given a process described by a function f, defined in physical space of lattice grid 
i, we seek basis function A and their associated transform coefficients c, defined in 
cellular automata space k [15]. We write 
 
i k ik
k
f c A=∑                                                     (4) 
Equation (4) represents a mapping of the process f (in the physical domain) in to c 
(in the cellular automata domain) using the building blocks A as transfer functions.  
In a 2-D square space consisting of N N×  cells, the transform base 
, ( , , , 0,1, , 1)ijklA A i j k l N= −= L . For the data sequence ( , 0,1, 2, , 1)ijf i j N= −L  
we can write: 
1 1
0 0
( , 0,1, , 1)
N N
ij kl ijkl
k l
f c A i j N
− −
= =
= = −∑∑ L                  (5) 
 
in which ckl are the transform coefficients.  
There are two approaches for generating two-dimensional CA transform bases: 
1. Using the evolving states derived from two-dimensional cellular spaces. 
Here, Aijkl is calculated from , ( , , 0,1, , 1)ijta a i j t N≡ = −L . 
2. Calculating the canonical products of one-dimensional bases so that 
Aijkl=AikAjl. 
370 Yongri Piao et al. 
4 Edge Detection Using CAT 
In this section, we introduce the edge detection scheme using two-dimensional cel-
lular automata transforms in details.  
 
Step 1: We get the gateway values (wolfram Rule, number of cells in lattice, number 
of cells per neighborhood, initial configuration and boundary configuration etc.).  
 
Step 2: We use the gateway values to generate a dual-state and two-dimensional cel-
lular automata dual-coefficients basis function.  
Consider, for example, a 2-state 8-node CA with 3points per neighborhood. In Ta-
ble 1, we show the gateway values for generating a dual-state, two-dimensional CA 
dual –coefficient basis function.  
 
Table 1. Gateway Values for 2-D dual-coefficient basis function 
Wolfram Rule Number 46 
Number of Cells per Neighborhood 3 
Number of Cells in Lattice 8 
Initial Configuration 00001001 
Boundary Configuration Cyclic 
Basis Function Type 2 
 
The cyclic boundary conditions imposed on the end sites are of the form equation (6):  
 
                                    1, 1, , 0k N k N k ka a a a− −= =                                    (6) 
 
Then a dual-coefficient cellular automata basis function is described by equation (7): 
 
2 1ik ik kiA a a= −                                        (7) 
 
where aik is the state of the CA at the node i at time t=k. The states are obtained from 
N (N=8) cells evolved from a specific initial configuration for N time steps. 
 
Step 3:  The CA basis function is derived from the 1-D types in form equation (8): 
 
ijkl ik jlA A A=                                                       (8) 
 
2-D basis functions are derived from the evolving one-dimensional CA as below: 
 
( ){ }mod ( 1)ijkl w ik ki jl lj w wA L a a a a L L= + − −                     (9) 
 
where 2wL ≥  is the number of states of the automaton. 
 Two-Dimensional Cellular Automata Transforms 371 
Step 4: The two-dimensional cellular automata basis function of the edge detection 
rule transforms images into cellular automata domain. Then we use the basis function 
and cellular automata transform coefficients to extract the edge of the image.   
 
In Figure 1 we show the steps for generating two-dimensional dual-coefficient ba-
sis function. This method produces many types of these basis functions which can 
detect edges. Of the basis functions generated, Figure 2 graphically shows the one 
using the gateway values from Table 1.  A00kl is the block at the extreme upper left 
corner. The top row represents 0 8j≤ < ; i=0. The left column is j=0; 0 8i≤ < . Aij00 
is the upper left corner of each block. The white rectangular dots represent 1 while 
the black dots are -1. 
 
 
Figure 1.  Process of  2D basis function 
 
 
Figure 2. 2D basis function of Table 1 gateway values 
5 Experimental Results 
The performance of the proposed scheme is tested on various types of images. 
Here, the results are represented by three grayscale 8-bit images of size 256×256 
(Figure 3 (a)-(c)). Table 2 shows the wolfram rule and its initial configuration of the 
dual-coefficient CAT filters in detecting the edge of tested images. The experimental 
results consist of two parts. Part one is the performance of the proposed method on 
the edge detection of real images, including noise and non-noise images. Part two is 
the performance of different edge detectors by using a synthetic image.  
Gateway 
Values 1D aik 1D Aik
Generate 
2D Aikjl 
372 Yongri Piao et al. 
5.1 Edge detection of clean and noisy images 
The Sobel, Laplacian of Gaussian (LoG) and Canny detectors are the most com-
mon edge operators. Figure 3 (d)-(f) shows the extracted edge maps using 2D CAT 
filter (Figure 2). The edge maps of the test images obtained by the three detectors are 
shown in Figure 3 (g)-(o). Figure 3 (g)-(i), (j)-(l) and (m)-(o) show the extracted edge 
maps using Sobel, LoG and Canny detector in MATLAB, respectively. From the 
experimental results, we can find that the proposed method has good performance in 
the delicate region where the gray level is finely changing.  
Figure 4 shows the proposed method compared with the Sobel, LoG and Canny 
detector under random noise. Figure 4 verifies that the Sobel, LoG and Canny edge 
detectors can detect the edges; though, noise pixels still exist in the extracted edge 
maps. The Sobel edge detector can remove the small speckles from the face and head. 
Using LoG and Canny edge detector to extract edge maps, it is difficult to find the 
true edges because of the noise.  Even though the edges extracted by proposed edge 
detector include partial noise pixels, we can still easily find the true edges by using 
proposed CAT edge detector. 
 
 
Table 2. Wolfram rule and initial configuration 
Filter Wolfram rule Initial configuration 
A 46 18 
B 155 137 
C 174 33 
D 209 28 
E 231 109 
F 245 232 
 
5.2 Edge detection of a synthetic image 
On the other hand, noise contamination is always a problem and edge detection in 
noisy environment can be treated as an optimal linear filter design problem. The per-
formance of the proposed method comparing with other methods (Sobel, LoG and 
Canny) is tested on the synthetic image (Figure 5 (a)). That image is an 8-bit gray-
scale image of size 256×256.  The result of synthetic image added Gaussian noise as 
shown in Figure 5(b). The FoM (Figure of Merit) [16] is used to evaluate the per-
formance of the edge detection methods. The FoM is defined as follows 
 
2
1
1 1
1
AI
iN i
F
I dβ== +∑ ,  max( , )N AI I I=                      (10) 
 
where I is original pixels of edge, IA is detected pixels of edge, β (β=1/9) is scaling 
parameter, di is distance of i-th detected pixel of edge between original pixel of edge.  
 Two-Dimensional Cellular Automata Transforms 373 
 
Figure 3. (a)-(c) Test images (d)-(f) CAT detector (g)-(i) Sobel detector (j)-(l) LoG detector 
(m)-(o) Canny detector. 
374 Yongri Piao et al. 
 
Figure 4. Extracted edge maps of Lena image with random noise (a) CAT detector (b) Sobel 
detector (c) LoG detector (d) Canny detector. 
 
 
 
Figure 5. (a) Synthetic image (b) Synthetic image with Gaussian noise 
 
 
Table 3. The proposed method compared with other methods 
Power of Gaussian Noise Detector 
0.05 0.10 0.15 0.20 0.25 
Sobel 0.8301 0.7059 0.3947 0.2706 0.2341 
LoG 0.1776 0.1774 0.1768 0.1763 0.1753 
Canny 0.3194 0.2945 0.2940 0.2778 0.2219 
CAT 0.5320 0.2953 0.2117 0.1726 0.1520 
 
 Two-Dimensional Cellular Automata Transforms 375 
Table 3 shows the proposed method compared with other methods. As shown in  
Table 3, the different gateway values generate two-dimensional CAT edge detection 
filters which are close to FoM results. Also,  the performance of the proposed method 
was better than LoG and Canny methods in the region where the power of Gaussian 
noise was less than 0.15. Moreover, we found many basis functions which could 
detect edges. Furthermore, we expect them to be applied in various types of image 
processing in the future. 
6 Conclusions 
This paper presents a new edge detection scheme using two-dimensional cellular 
automata transforms. By using the gateway values (such as Wolfram Rule, the num-
ber of cells in lattice, the number of cells per neighborhood, the initial configuration, 
and the boundary configuration), we generated a dual-state, two-dimensional, and 
dual-coefficients cellular automata basis function. Then we transformed images into 
cellular automata domain with the basis function. Finally, we used the basis function 
and cellular automata transform coefficients to extract the edge of the image. 
The experimental results verify that the proposed method is a new attempt to detect 
edge with cellular automata model as well as to maintain its high efficiency. 
 Although researchers continue to experiment on innovative edge detectors, they 
are still not able to define the ultimate suitable method for all conditions. However, 
with our proposed method, we can equally apply it to all images. We may collate 
many basis functions produced through the method and open possibilities for more 
developed edge detectors with novel features. Overall, our method will plays as a clue 
to solving such inconvenience. 
References 
1. S. Smith, M. Brady : SUSAN-A New Approach to Low Level Image Processing. Int. J. 
Comput. Vision 23 (1) (1997) 45-47 
2. V. Torre, T. Poggio: On Edge Detection, IEEE Trans. Pattern Anal. Mach. Intell. PAMI 2 
(1980) 147-163 
3. J. Canny: A Computational Approach to Edge detection. IEEE Trans. Pattern Anal. Mach. 
Intell. PAMI 8 (1986) 679-698 
4. T. D. Sanger : Optima Unsupervised Learning in A Single-Layer Feedforward Neural Net-
work. Neural Networks 2 (1989) 459-473 
5. B. S. Manjunath, R. Chellappa : A Unified Approach to Boundary Perception : Edges, Tex-
tures and Illusory Contours. IEEE Trans. Neural Network 4 (1993) 96-108 
6. S. Sarkar, K. L. Boyer : On Optimal infinite impulse response edge detection filters. IEEE 
Trans. Pattern Anal. Mach. Intell. PAMI 13 (1991) 1154-1171 
7. Ulam. S : Some Ides and Prospects in Biomathematics. Anm. Rev. Biophys. Bioengin, Vol. 
1 (1963)  277-291 
8. Von Neumann. J : Theory of Self-Reproducing Automata, University of Illinois Press, IL, 
(1966) 
9. Wolfram S : Statistical Mechanics of Cellular Automata. Rev. Mod, Phys. 55 (1983) 601 
376 Yongri Piao et al. 
10. Wolfram S : Computation Theory of Cellular Automata.  Commun. Math. Phys. 96 (1984) 
15-57 
11. S. K. Lee, S. T. Kim and S. J. Cho: A Potts Automata algorithm for Noise Removal and 
Edge detection. Journal of Korean institute of Communication Science, Vol. 28-3C (2003) 
327-335 
12. S.J. Cho, U.S. Choi, H.D. Kim, Y.H. Hwang, J.G. Kim and S.H. Heo: New Synthesis of 
One-Dimensional 90/150 Linear Hybrid Group Cellular Automata. IEEE Transactions on 
Computer-Aided Design of Integrated Circuits and Systems, Vol. 26(9) (2007) 1720-1724 
13. S.J. Cho, U.S. Choi,  H.D. Kim and Y.H. Hwang: Analysis of complemented CA derived 
from linear hybrid group CA. Computers and Mathematics with Applications, Vol. 53(1) 
(2007) 54-63 
14. C. L. Chang, Y. J. Zhang, Y. Y. Gdong : CA for Edge detection of Images. IEEE Interna-
tional Conference on Machine Learning and Cybernetics, Shanghai (2004) 3830-3834. 
15. Olu Lafe: Cellular Automata Transforms: Theory and Application in Multimedia Compres-
sion, Encryption, and Modeling. Kluwer Academic Publishers, Boston/ Dordrecht/London, 
(2000) 
16. I. E. Abdou, W. K. Pratt : Quantitative design and evaluation of enhancement/thresholding 
edge detectors. Proc. IEEE, 69 (1979) 753-763.  
 
Computable Counter-examples to the Brouwer
Fixed-point Theorem⋆
Petrus H. Potgieter
Department of Decision Sciences, University of South Africa (Pretoria)
PO Box 392, Unisarand, 0003, Republic of South Africa
php@member.ams.org, potgiph@unisa.ac.za, www.potgieter.org
Abstract. This paper is an overview of results that show the Brouwer
fixed-point theorem (BFPT) to be essentially non-constructive and non-
computable. The main results, the counter-examples of Orevkov and
Baigger, imply that there is no procedure for finding the fixed point in
general by giving an example of a computable function which does not fix
any computable point. Research in reverse mathematics has shown the
BFPT to be equivalent to the weak Ko¨nig lemma in RCA0 (the system
of recursive comprehension) and this result is illustrated by relating the
weak Ko¨nig lemma directly to the Baigger example.
Key words: Computable analysis, Brouwer fixed-point theorem, weak
Ko¨nig lemma
1 Introduction
We consider the Brouwer fixed-point theorem (BFPT) in the following form,
where the standard unit interval is denoted by I = [0, 1].
Theorem 1 (Brouwer). Any continuous function f : I2 → I2 has a fixed
point, i.e. there exists an x ∈ I2 such that f(x) = x.
A computable real number is a number for which a Turing machine exists that,
on input n, produces a rational approximation with error no more than 2−n. A
computable point is a point all the coordinates of which are computable reals.
The notation
N0 for the non-negative natural numbers;
Rc for the set of computable reals;
Ic for I ∩ Rc; and
δX for the boundary of a set X, being X ∩Xc
is also used. The two examples discussed use distinct definitions of a computable
function of real variables.
⋆ This work was supported in part by a grant under the South African-Hungarian
Science and Technology Agreement (National Research Foundation of South Africa
UID: 62110) and in part by a research grant from the College of Economic and
Management Sciences of the University of South Africa.
378 Petrus H. Potgieter
Russian school In the Russian school of Markov and others, a computable
function maps computable reals to computable reals by a single algorithm
for the function that translates an algorithm approximating the argument
to an algorithm approximating the value of the functions. It need not be
possible to extend a function that is computable in the Russian school to
a continuous function on all of the reals. These functions are often called
Markov-computable.
Polish school In the Polish school of Lacombe, Grzegorczyk, Pour-El and Ri-
chards, and others, a function is computable on a region if it maps every
every computable sequence of reals to a computable sequence of reals and it
has a computable uniform modulus of continuity on the region [1].
2 Orevkov’s example for the Russian school
One can construct a Markov-computable function f through a computable map-
ping of descriptions of computable points x ∈ I2c to descriptions of f(x) ∈ I
2
c ,
such that
f(x) 6= x ∀x ∈ I2c .
That is, no computable point is a fixed point for f . Unfortunately the f which
is constructed in this way, cannot be extended to a continuous function on I2.
This is the construction of [2], another instance of which can be found in [3].
· · ·
Fig. 1. Basic contraction in the Orevkov counter-example
Lemma 1. Suppose Ak is a sequence of rectangles in I
2 with computable ver-
tices, disjoint interiors, and such that
(i) ∅ 6= δAj \
⋃
i<j Ai for all j;
(ii) for each j there exists n > j such that δAj ⊂
(⋃
i≤nAi
)◦
; and
(iii) I2c ⊆
⋃
i≥1 Ai
then there exists a Markov computable g, mapping I2c to δI
2
c and fixing δI
2
c .
Computable Counter-examples to the Brouwer Fixed-point Theorem 379
The conditions ensure that
(i) rectangles Aj , when added, have some part of their boundary in I
2 \⋃
i<j Ai;
(ii) each Aj is eventually closed off by new rectangles on all sides;
(iii) all computable points lie in
⋃
i≥1 Ai.
The function f is obtained by composing g with a 90◦ rotation. It therefore
remains only to prove the lemma and the existence of a sequence of rectangles
which is as required. Suppose that g has been defined on
⋃
i<j Ai. For
∅ = δAj ∩
⋃
i<j Ai: let g on Aj consist of the simplest possible mapping to δI
2,
that fixes δI2;
∅ 6= δAj ∩
⋃
i<j Ai: we extend g to Aj by using (i)—if g has already been defined
on the crosshatched set in Figure 1 then the definition can be extended to
the solid gray set Aj by composing a contraction of the solid gray set in
Figure 1 to 
δAj ∩⋃
i<j
Ai

 ∪ (δAj ∩ δI2)
with the function g as it has already been defined on the crosshatched set.
This is always possible because, by construction of the Ai, our Aj will always
have at least two sides non-contiguous with
⋃
i<j Ai, at least one of which
will not coincide with δI2.
So far only condition (i) has been used. Conditions (ii) and (iii) are necessary
for showing that g is Markov-computable on I2. Let a description of any x ∈ I2c
be given. We can find a description of g(x) in the following way.
– Simultaneously, compute approximations of x using the given description
and construct g on
⋃
i≤nAi for n = 1, 2, . . ..
– Together, (ii) and (iii) imply that for some n we will be able to verify that
x ∈

⋃
i≤n
Ai


◦
where the interior is with respect to the subset topology on I2, of course.
– When such an n has been identified, we already know the definition of g for⋃
i≤nAi as well as the modulus of continuity of g on the same set. This is
now used to describe g(x).
It remains to be shown that a suitable sequence of rectangles (An)n≥1 exists.
This follows from the next fact, assumed without proof for now1.
Lemma 2 (see [4], for example). There exist computable sequences of ra-
tional numbers (an) and (bn) in the interval I = [0, 1] such that the intervals
Jn = [an, bn] have the following properties.
1 Later we shall deduce the fact from the existence of a Kleene tree.
380 Petrus H. Potgieter
(i) If n 6= m then |Jn ∩ Jm] ≤ 1.
(ii) If an 6= 0 then an ∈ {b0, b1, . . .} and if bn 6= 1 then bn ∈ {a0, a1, . . .}.
(iii) Ic (
⋃
n Jn, i.e. the Jn cover the computable reals in I = [0, 1].
Now, let (An)n≥1 be any computable enumeration of the Jk×Jℓ. This completes
the proof of the lemma, and the example.
3 Baigger’s example for the Polish school
Let a be any non-computable point in I2. Consider the function f which moves
each point half-way to a,
f(x) = x+
1
2
(a− x)
and has a single fixed point, namely a itself. The function f is continuous and
defined on all of I2 and has no computable fixed point. Nevertheless, this is not
really interesting since
– the fixed point a has no reasonable description—since it is itself not com-
putable; and therefore
– the function f has no reasonable description—it is not computable in any
sense.
One would like to see a function which is computable, defined (and therefore con-
tinuous) on all of I2 and yet avoids fixing any of the computable points I2c . The
following example, having appeared in [5] and in [3], modifies the construction of
Orevkov to produce a computable f defined on all of I2 having no computable
fixed point. One uses the intervals Jn = [an, bn] of Orevkov’s example and sets
Cn =
⋃
k,ℓ≤n
Jk × Jℓ
after which one defines f progressively, using the sets Cn. The points
tn = (vn, vn)
where
vn = min
x∈I
{x | (x, x) 6∈ Cn}
are used as “target point” at each stage of the construction, as in Figure 2. Note
that
v = lim
n→∞
vn
is not a computable number and (v, v) will be one of the fixed points of f .
Definition 1. For any W ⊆ I2 we define
Wε =
{
x ∈W
∣∣ d (x, δW \ δI2) ≥ ε}
and
Wε =
{
x ∈W
∣∣ d (x, δW \ δI2) = ε} .
Computable Counter-examples to the Brouwer Fixed-point Theorem 381
0 1
0
1
b
t2
C2
0 1
0
1
b
t5 C5
Fig. 2. The “target points” tn
One can define fn such that
1. fn moves every point in the interior of C
2−n
n but is the identity outside the
set, and is computable;
2. fn+1 agrees with fn on C
2−n· 32
n and therefore
3. f = limn→∞ fn is computable.
Every computable point eventually lies in some
C
2−n· 32
n ⊂
(
C2
−n
n
)◦
and is therefore moved by f . Clearly f(I2) ⊆ I2 and f will be as required. In
fact, f has no fixed point in ⋃
n
Cn =
⋃
k,ℓ≥1
Jk × Jℓ.
Also, f has no isolated fixed point—its fixed points all occur on horizontal and
vertical lines spanning the height and breadth of the unit square. Further details
of the construction appear in Appendix A. The construction cannot be applied in
the one-dimensional case because it is impossible to effect a change of direction
by continuous rotation.
4 BFPT and the Ko¨nig lemma
In reverse mathematics it is known that in RCA0, the system of recursive com-
prehension and Σ01 -induction, the weak Ko¨nig lemma, WKL0, is equivalent to
the Brouwer FPT [6].
Lemma 3 (WKL0, Ko˝nig). Every infinite binary tree has an infinite branch.
The Ko¨nig lemma does not have a direct computable counterpart.
382 Petrus H. Potgieter
Theorem 2 (Kleene [7]). There exists an infinite binary tree, all the com-
putable paths of which are finite.
The relation of the Kleene tree to the Baigger counterexample is reviewed in
this section. The discussion is informal and attempts only to give the essential
ideas that have been revealed by the approach of reverse mathematics. In RCA0,
the weak Ko¨nig lemma WKL0 has been shown to be equivalent to a number of
other results in elementary analysis, such as the fact that any continuous function
on a compact interval is also uniformly continuous [8]. WKL0 and RCA0 can,
furthermore, be used to prove Go¨del’s incompleteness theorem for a countable
language [9].
4.1 From Baigger f to Kleene tree
Let f be a computable function, as in the Baigger example, mapping I2 to
itself—with no computable fixed point. The following auxiliary result will be
used to construct the Kleene tree.
Lemma 4. Let a computable g : I2 → [0, 1] be given. Then there exists a Turing-
computable h : N90 → N
2
0 such that for any (n1, n2, . . . , n8, k) with
0 ≤
n1
n2
≤
n3
n4
≤ 1 and 0 ≤
n5
n6
≤
n7
n8
≤ 1
we have h : (n1, n2, . . . , n8, k) 7→ (m1,m2) with m1 ≤ m2 where
m1
m2
≤ min g
([
n1
n2
,
n3
n4
]
×
[
n5
n6
,
n7
n8
])
≤
m1
m2
+
1
k
.
Let g = ||f(x)−x|| and let h be as in the lemma. Note that g(x) = 0 if and only
if x is a fixed point of f . We shall use only the essential consequences that
– g(x) > 0 for all computable x; and
– there exists a (non-computable) x0 such that g(x0) = 0.
As usual, {0, 1}∗ denotes the set of finite binary sequences and ab is the con-
catenation of a and b.
Definition 2. A binary tree is a function t : {0, 1}∗ → {0, 1} such that
t(ab) = 0 for all b whenever t(a) = 0.
An infinite branch of a tree t is an infinite binary sequence, on all of which finite
initial segments t takes the values 1.
The tree is computable whenever the function t is Turing-computable and a
computable branch is a computable binary sequence which is an infinite branch.
Define the Kleene tree as follows. Let
t(i1 . . . in) =
n∏
m=1
s(i1 . . . im)
Computable Counter-examples to the Brouwer Fixed-point Theorem 383
where s is a function taking values in {0, 1}. This definition of t ensures that t is
in fact a tree and if s is computable, t will be a computable tree. The function
s will use h to estimate whether g gets close to zero on a specific square and if
g has been bounded away from zero on the square, that branch of the tree will
terminate.
Define s : {0, 1}∗ → {0, 1} for all sequences i1j1 . . . injn of even length by
s(i1j1 . . . injn) = χ{0}
(
m1
m2
)
where
(m1,m2) = h (i1 . . . in, 2
n, i1 . . . in + 1, 2
n, j1 . . . jn, 2
n, j1 . . . jn + 1, 2
n, n)
and binary strings have been interpreted as the natural numbers which they
represent. Let s take the value 1 on sequences of odd length.
The tree t defined in this way is obviously computable. It remains to show
that t is
– infinite; and
– has no infinite computable branch.
Let x0 be any point where g(x0) = 0. Then there exist infinite sequences (in)
and (jn) such that
x0 ∈
[
i1 . . . in
2n
,
j1 . . . jn
2n
]
×
[
i1 . . . in + 1
2n
,
j1 . . . jn + 1
2n
]
for all n
and therefore, for all n, s(i1j1 . . . injn) = 1 and so t(i1j1 . . . injn) = 1 which
proves the existence of an infinite branch, hence that the tree t is infinite.
Suppose that t had an infinite computable branch. The branch would corre-
spond to a decreasing chain of closed squares, the intersection of which would
be non-empty. Let x1 be a point in the intersection. Since, by construction of
the tree, g(x1) ≤
1
n
for all n, g(x1) = 0 and hence x1 would be a fixed point
of f . However, by the construction—the branch being computable—the point
x1 would also be computable, contradicting the fact that f has not computable
fixed point. Therefore the tree t has no infinite computable branch.
4.2 From Kleene tree to Baigger f
Suppose we are given a computable tree t with no infinite computable branch.
This tree can be used to construct a sequence of closed intervals with a com-
putable sequence of end-points, covering all the computable real numbers in
the unit interval and for which the corresponding open intervals are pair-wise
disjoint.
Using the computable function t, one can enumerate all of the maximal finite
branches of the tree. Say,
b(n) = b1(n) . . . bλ(n)(n)
384 Petrus H. Potgieter
and set
Jn,1 =
[
b1(n) . . . bλ(n)
2λ(n)
,
b1(n) . . . bλ(n) +
1
2
2λ(n)
]
Jn,m =
[
b1(n) . . . bλ(n) + 2
−m+1
2λ(n)
,
b1(n) . . . bλ(n) + 2
−m
2λ(n)
]
for m ≥ 2.
It remains to show that the union of the intervals Jn,m covers all the computable
points Ic but not all of the unit interval I. It is easy to see that
– for every computable x ∈ Ic there exists a computable binary sequence (xn)
such that
x1 . . . xn
2n
≤ x <
x1 . . . xn + 1
2n
for all n
and since t has no infinite computable branch t(x1 . . . xℓ) = 0 for some least
ℓ, in which case x ∈ ∪mJn,m where b(n) = x1 . . . xℓ;
– if (xn) is an infinite branch of t then, since it is not computable, for all w
we have x1x2 . . . 6= w1111 . . . and therefore
lim
n
x1 . . . xn + 1
2n
6∈
⋃
m
Jℓ,m
for every ℓ.
The Baigger example f can now be constructed using the intervals Jn,m and by
that construction one obtains a computable f with no computable fixed point,
as required.
5 Conclusion
The existence of the Kleene tree can quite easily be derived from the impos-
sibility of ensuring the existence of a computable fixed point for a computable
function (in both Russian and Polish senses), in two dimensions (or higher). The
ingenuous constructions of Orevkov and Baigger provide a way of defining a com-
putable function with no computable fixed point from the set of intervals derived
from the Kleene tree, in a constructive manner. This correspondence is, perhaps,
more attractive for the “working mathematician” than the elegant derivation of
the result in reverse mathematics. In one dimension, any computable f : I → I
does have a computable point x ∈ Ic such that f(x) = x, which can be seen by
fairly straight-forward reduction ad absurdum from the assumption that this is
not the case.
References
1. Pour-El, M.B., Richards, J.I.: Computability in analysis and physics. Perspectives
in Mathematical Logic. Springer-Verlag, Berlin (1989)
Computable Counter-examples to the Brouwer Fixed-point Theorem 385
2. Orevkov, V.P.: A constructive map of the square into itself, which moves every
constructive point. Dokl. Akad. Nauk SSSR 152 (1963) 55–58
3. Wong, K.C., Richter, M.K.: Non-computability of competitive equilibrium. Eco-
nomic Theory 14(1) (1999) 1–27
4. Miller, J.S.: Degrees of unsolvability of continuous functions. J. Symbolic Logic
69(2) (2004) 555–584
5. Baigger, G.: Die Nichtkonstruktivita¨t des Brouwerschen Fixpunktsatzes. Arch.
Math. Logik Grundlag. 25(3-4) (1985) 183–188
6. Shioji, N., Tanaka, K.: Fixed point theory in weak second-order arithmetic. Ann.
Pure Appl. Logic 47(2) (1990) 167–188
7. Kleene, S.C.: Recursive functions and intuitionistic mathematics, Providence, R. I.,
Amer. Math. Soc. (1952) 679–685
8. Simpson, S.G.: Which set existence axioms are needed to prove the cauchy/peano
theorem for ordinary differential equations? The Journal of Symbolic Logic 49
(1984) 783–802
9. Simpson, S.G.: Subsystems of second order arithmetic. Perspectives in Mathemat-
ical Logic. Springer-Verlag, Berlin (1999)
Appendix A: details of the construction in Section 3
The constructions should guarantee that at each stage, the function fn moves
every point of
Dn =
(
C2
−n
n \ C
2−n· 54
n
)◦
in the direction of tn by an amount proportional to its distance to C
2−n
n . The
construction of f1 with this property is trivial. We proceed to construct fn+1
from fn.
(i) Extend and modify fn to C
2−n
n+1 so that every point x of(
C2
−n
n+1 \ C
2−n· 54
n+1
)◦
is moved in the direction of tn by an amount proportional to d
(
x,C2
−n
n+1
)
.
(ii) Modify the resulting function so that each point in
C2
−n
n+1 \ C
2−n· 98
n+1
is mapped a non-negative amount proportional to its distance to C2
−(n+1)
n+1
in the direction of tn.
(iii) By rotation of the direction of the mapping, extend the function to
C2
−(n+1)
n+1
such that every point x of
Dn+1 =
(
C2
−(n+1)
n+1 \ C
2−(n+1)· 54
n+1
)◦
386 Petrus H. Potgieter
Cn
C
2
−n
n
C
2
−n
·
5
4
n
Dn
2−n
Fig. 3. Sets used in the construction
is mapped in the direction of tn+1 by an amount proportional to
d
(
x,C2
−(n+1)
n+1
)
.
The final step is the only one in which we use the fact that we are working in
two dimensions as this step requires the continuous (computable) rotation of a
vector in the direction of tn to a vector in the direction of tn+1.
A construction is given explicitly in [5] but it should be clear from the pre-
ceding that it can be done in many different ways. The important part of the
proof is that the construction is, at each stage, extended at the boundary to
“look right” from the outside. This ensures that, eventually every point is in fact
moved towards one of a sequence of points that converge to the non-computable
fixed point (v, v) on the diagonal. The Baigger construction is a somewhat deli-
cate construction of a function that is in fact computable but that—somehow—
mimics a simple mapping of every point in I2 in the direction of (v, v).
Polynomial Iterations over Finite Fields
Mihai Prunescu1;2
1 Brain Products, Freiburg, Germany
2 Institute of Mathematics “Simion Stoilow” of the Romanian Academy
Bucharest, Romania
mihai.prunescu@math.uni-freiburg.de
Abstract. Consider the following natural algorithm: given a finite field
F and a polynomial f ∈ F[x, y, z] one produces the double sequence
(ai,j) defined by a0,j = ai,0 = 1 und ai,j = f(ai,j−1, ai−1,j−1, ai−1,j). If
the polynomials f are linear, self-similarity arises. On the other hand,
the class of double sequences (ai,j) generated by symmetric polynomials
f(x, z) over arbitrary finite fields is Turing complete.
1 Introduction
The research reported here has a strong experimental back-ground. Let F be a
finite field and f ∈ F[x, y, z] some polynomial. Iterating f one gets a recurrent
double sequence (ai,j) defined by a0,j = 1, ai,0 = 1 and:
ai,j = f(ai,j−1, ai−1,j−1, ai−1,j).
If one fixes a correspondence between the elements of F and a set of colours,
one can draw an image corresponding to the initial matrix (ai,j)0≤i,j<n of the
recurrent double sequence. Such images will be sometimes called carpets. The
starting point of this research was a conjecture of Lakhtakia and Passoja [3]
telling that the polynomial f(x, y, z) = x + y + z generate self-similar carpets
over the prime fields Fp.
In [5] the author proved that all polynomials f(x, y, z) = x+my+ z produce
self-similar (fractal) images over arbitrary finite fields and classified the occurring
symmetries. The proofs uses elements of algebra and number theory, but also
applies modern algebraic algorithms as those by Wilf and Zeilberger. In [6] the
author proved that for arbitrary polynomials in only two variables f(x, z) the
recurrent double sequences interpret instances of the Halting Problems and have
undecidable properties, as for exemple the property of being ultimately zero.
The goal of this extended abstract is to present all definitions and results,
but only (at most) sketches of proofs. It should be seen as a complementary text
to [5] and [6] and has also the goal to emphasize their contrast.
2 The linear case
Definition 1. Let Fq be an arbitrary finite field and fix an elementm ∈ Fq. The
matrices occurring in this section are always indexed from 0 and have elements
388 Mihai Prunescu
in Fq, if not otherwise specified. Let the prime p be the characteristic of the
finite field, q = pk for some k. Let Md = (ai,j) be the pd×pd matrix constructed
following the recurence ai,0 = a0,j = 1 and ai,j = ai−1,j +m · ai−1,j−1 + ai,j−1.
The matrix M1 shall be denoted by F (p,m) and called fundamental block.
Definition 2. The black and white image Id is defined as follows: one tiles the
compact square [0, 1] × [0, 1] in pd × pd many equal squares Si,j , and excludes
the interior of Si,j if and only if ai,j = 0.
Definition 3. The self-similar set in question shall be I = lim Id. The name of
the variable d is chosen to mean the depth of the recursive approximation of I.
The limit operator can be understood in the sense of the Hausdorff metric for
compact subsets of R2.
2.1 The recurrent function
Definition 4. Let K be an arbitrary field and the element m ∈ K be fixed.
We consider the function f : N × N → K recursively defined by the conditions
f(n, 0) = f(0, k) = 1 and:
f(n, k) = f(n, k − 1) +m · f(n− 1, k − 1) + f(n− 1, k)
for n, k ≥ 1.
Lemma 1. The function f is symmetric and satisfies:
f(n, k) =
min(n,k)∑
a=0
ma
(
n
a
)(
n+ k − a
k − a
)
.
Proof. The symmetry follows from the symmetry of the recurrence formula and
of the initial conditions. To compute f , use the method of generating functions,
see [12]. Define the generating function An(x) =
∑
k≥0
f(n, k)xk. It follows:
An+1(x) = An(x) + xAn+1(x) +mxAn(x).
This recurrence have the solution:
An(x) =
( 1
1− x
)n+1
(1 +mx)n.
Using that (1 +mx)n =
∑
k≥0
(
n
k
)
mkxk and that
(
1
1−x
)n+1
=
∑
k≥0
(
n+k
k
)
xk, one
gets the Lemma.
Polynomial Iterations over Finite Fields 389
2.2 Tensor powers and the automorphism of Frobenius
In this section we prove some properties of the fundamental block F (p,m) ∈
Mp×p(Fp). Recall the notation F (p,m) = (ai,j) with i and j = 0, . . . , p− 1.
Lemma 2. The last column and the last row of F (p,m) are exactly:
1, −m, (−m)2, . . . , (−m)p−1.
This works also for m = 0.
Proof. Take k ≤ n = p− 1 and work over Fq. For a < k the term:
t(a, p− 1, k) = ma
(
p− 1
a
)(
p− 1 + k − a
k − a
)
= ma
(
p− 1
a
)
· p · · · · = 0,
so all these terms do not contribute in Fq. For the last term one has:
t(k, k, p− 1) = mk (p− 1) . . . (p− k)
k!
= mk(−1)k k!
k!
= (−m)k.
Definition 5. Let R be some commutative ring and A = (ai,j) ∈ Ms×t(R),
B ∈ Mu×v(R) two matrices. Then the tensor product A ⊗ B is a matrix
in Msu×tv(R) having the block-representation (ai,jB). If A1, A2, . . . , An are
arbitrary matrices, we denote the tensor term:
((. . . ((A1 ⊗A2)⊗A3) . . . )⊗An−1)⊗An.
by:
A1 ⊗A2 ⊗ · · · ⊗An−1 ⊗An.
For all n ≥ 1 we define the tensor power A⊗n of A inductively by: A⊗1 = A
and A⊗(n+1) = A⊗n ⊗A.
Remark 1. (Principle of Substitution) For some n ≥ 2 consider a matrix A ∈
Mn×n({0, 1}) containing at least one zero and at least two ones. Let Id be
the black and white image associated to A⊗d. Then Id is the d-th step in the
transfinite construction of a non-trivial self-similar set I = lim Id.
Definition 6. The automorphismus of Frobenius ϕ : Fq → Fq is defined by
ϕ(x) = xp. This automorphism generates the Galois group G(Fq/Fp).
Lemma 3. Let F = F (p,m) be a fundamental block for some m ∈ Fq. Consider
the matrix in construction:
αF βF
γF ·
with α, β, γ ∈ Fq. By application of the recurrent rule one gets:
αF βF
γF δF
with δ = ϕ(m)α+ β + γ.
390 Mihai Prunescu
Definition 7. For a matrix A = (ai,j) over Fq, let ϕ(A) be the matrix (ϕ(ai,j)).
Theorem 1. Recall that Md is the pd × pd matrix computed by the recurrent
rule over the finite field Fq and F = F (p,m) = M1 is the fundamental block.
Then for all d ≥ 1:
Md = ϕd−1(F )⊗ ϕd−2(F )⊗ · · · ⊗ ϕ(F )⊗ F.
Proof. The proof works by induction and is a immediate application of the
Lemma 3.
Corollary 1. For all finite fields Fq and all m ∈ Fq, if the fundamental block
F (p,m) contains at least a zero, the black and white image Id of Md is the d-th
step in the transfinite construction of a non-trivial self-similar set I.
Proof. Immediate application of the Principle of Substitution.
Lemma 4. If m ∈ Fp, the fundamental block F (p,m) contains zeros if and only
if m 6= −1. In this situation it contains in the row i = 1 exactly one zero:
a1,k = 0 ↔ Fp |= k = −(m+ 1)−1.
Note: in general there are many other zeros in the fundamental block.
Proof. The element a1,k = km + (k + 1) = k(m + 1) + 1 which is zero only for
k = −(m+1)−1, existing for all m 6= −1 in Fp. Every such k has a representative
between 1 and p − 1 inclusively. If m = −1 the matrix F (p,−1) contains only
the repeated element 1.
Now from Remark 1 and from the Lemma 1 the main result follows:
Theorem 2. For all primes p and all m ∈ Fp \ {−1} the black and white image
Id of Md is the d-th step in the transfinite construction of a non-trivial self-
similar set I. For m = −1 the set I is the full square. The Pascal Triangle
modulo p (got for m = 0) and the Passoja-Lakhtakia Carpets (got for odd q = p
and m = 1) are non-trivial self-similar sets.
The following example shows the step M2 for p = 3 and m = 1, a step in the
construction for the celebrated Sierpinski Carpet, used in [11]. The zeros are not
displayed.
1 1 1 1 1 1 1 1 1
1 −1 1 −1 1 −1
1 −1 1 1 −1 1 1 −1 1
1 1 1 −1 −1 −1
1 −1 −1 1
1 −1 1 −1 1 −1
1 1 1 −1 −1 −1 1 1 1
1 −1 −1 1 1 −1
1 −1 1 −1 1 −1 1 −1 1
Polynomial Iterations over Finite Fields 391
2.3 Multiplicative inverse means mirroring
For studying the groups of symmetries of the black and white image I is enough
to understand the symmetries for the fundamental block F (p,m). All groups of
symmetries we are looking for are subgroups of the dihedral group of symmetries
D8 of the square. We start with the least symmetric case, the case of Pascal’s
Triangle:
Lemma 5. If m = 0 the group of symmetries consists of two elements: the
identity and the reflection through the first diagonal.
Proof. In F (p, 0) for 0 ≤ i, j ≤ p− 1 :
ai,j = 0 ↔ p | f(i, j) =
(
i+ j
i
)
↔ i+ j ≥ p.
So exactly the elements strictly below the second diagonal are 0.
Definition 8. For a matrix A we define the mirrored image ΣA using the def-
inition of a matrix as a family of column-vectors. If A = (a1, . . . ,an) then
ΣA = (an, . . . ,a1).
Definition 9. For m 6= 0 we define the operator O acting over the fundamental
block F (p,m) in the following way:
For i = 0 to p− 1, one divides the row i by (−m)i.
The result is denoted by OF (p,m).
Lemma 6. For all finite fields Fq and for all m ∈ Fq \{0} the following identity
holds:
OF (p,m) = ΣF (p,m−1).
Proof. The Lemma follows from the following claims: (1) The first row and the
last column of OF (p,m) consist only of ones. (2) For every connected 2 × 2
sub-block of OF (p,m):
A B
C D
is true that C = m−1B+A+D. The first claim follows from Lemma 2 and from
the definition of the operator O: one divides exactly with the elements of the
last column. We prove the second claim. Let (a, b | c, d) be the corresponding
elements in F (p,m). They fulfill the equality:
d = ma+ b+ c.
Using the definition of OF (p,m), we see that:
A = µa, B = µb,
392 Mihai Prunescu
C = (−m)−1µc, D = (−m)−1µd,
where µ = (−m)i for some i. It follows that:
C = (−m)−1µc = (−m)−1µ(d−ma− b) =
= (−m)−1µd+ µa− (−m)−1µb = D +A+m−1B.
Lemma 7. The following statements follow directly from Lemma 6:
1. For all 0 ≤ i, j ≤ p− 1:
a′i,p−1−j = ai,j(−m)−i,
ai,j(−m)−i = ap−1−j,p−1−i(−m)j+1−p.
2. If m ∈ Fq \ {0} then:
δF (p,m) = δΣF (p,m−1) = ΣδF (p,m−1).
Moreover, the matrix δF (p,m) allows two diagonal symmetries; and so all
its tensor powers.
3. Given m ∈ Fq \ {0} fixed, some matrix Md contains zeros if and only if
M1 = F (p,m) contains zeros. If this takes places, then
deg(m/Fp) ≤ p− 12 .
The last condition occurring here is quite strong and implies that there cannot
be too much elementsm generating non-trivial self-similar sets in arbitrary finite
fields. Look at the case F192 = F361 seen as F19[x] where x2+1 = 0. Encode the
element ax+b in the natural number 19a+b. I do not mention both m and m−1
because they produce mirrored carpets. Also, if m has been already mentioned,
I don’t mention its Frobenius m19, because it produces the same carpet. So, up
to Frobenius and multiplicative inverse, one has non-trivial self-similar carpets
over F361 if and only if m is equal with one of the following 29 elements: 0, 1, 2,
3, 4, 6, 7, 8, 9, 14, 19, 21, 35, 47, 52, 53, 56, 63, 69, 76, 78, 88, 92, 102, 130, 136,
137, 148, 168. Values of m ∈ F361 which are not itself, inverses of, or Frobenius
of elements in this list generate however interesting coloured images. Over the
prime fields Fp the situation looks better:
2.4 Fp as a field of self-similar carpets
Theorem 3. Let p be a prime and m ∈ Fp. Exactly one of the following situa-
tions arrises:
1. m = 0. In this case I is a self-similar Pascal Triangle, I is only symmetric
through the first diagonal, and the group of symmetries of I is isomorphic
with S2.
Polynomial Iterations over Finite Fields 393
2. m = ±1. In this case I is a full square (for m = −1) or a nontrivial self-
similar set (for m = 1) and the group of symmetries of I is the full dyhedral
group D8 of the square.
3. p ≥ 5, m ∈ Fp \ {−1, 0, 1}. In this case I is a non-trivial self-similar set
and the group of symmetries of I is generated by the reflexions through the
diagonals of the square. This group is isomorphic with Klein’s group K4.
Proof. Let now m ∈ Fp \ {0}, let K be the group generated by the symmetries
through the both diagonals (isomorphic with Klein’s group K4) and let G be
the group of symmetries of I. From Lemma 7 it follows that K ≤ G ≤ D8. If
m = −1 then I is the full square and trivially G = D8. If m = 1 than it follows
from Lemma 7 that:
δF (p, 1) = ΣδF (p, 1),
because 1−1 = 1 so G is strictly bigger than K which already has 4 elements,
hence G = D8.
The converse is easy to prove using Lemma 4.
2.5 The special case m ∈ {−2,−2−1}: Diagonal Carpets
For m = −2 one has a1,1 = 0. Mirror-symmetric: for m = −2−1 one has a1,p−2 =
0. In fact, in these cases, all the elements of odd index on the corresponding
diagonal are zero!
Definition 10. Call first odd diagonal (respectively second odd diagonal)
the following set of indexes:
D+ = {(i, i) | 0 < i < p− 1 ∧ 2 6 | i}.
D− = {(i, j) | i+ j = p− 1 ∧ 2 6 | i}.
Theorem 4. Let p ≥ 5 be a prime. Following statements hold: D+ consists of
zeros of F (p,−2) and D− consists of zeros of F (p,−2−1). Moreover, the elements
of even index on the respective diagonals are 6= 0.
Proof. We prove that for m = −2 ∈ Z the recurrent function f : N × N → Z
defined in the second section has the property f(2s+1, 2s+1) = 0 for all k ∈ N.
This follows from the following identity:
n∑
a=0
(−2)a
(
n
a
)(
2n− a
n− a
)
=
{
(−1)s(2ss ), if n = 2s,
0, if n = 2s+ 1.
This identity can be proved with Zeilberger’s Algorithm, see [7] and [1]. In fact,
after running the software from [1], one gets the recurrent formula:
4(n+ 1)S(n) + (n+ 2)S(n+ 2) = 0,
where S(n) is the sum on the left side of the equality. Starting with S(0) = 1
and S(1) = 0 one gets the result. D− follows from the case m = −2 and the
dualism from Lemma 6. Note that the corresponding values of f(n, k) are no
more 0 in Z but become 0 if projected in Fp.
394 Mihai Prunescu
2.6 The special case m = 1: Cross-carpets
The only one fully symmetric non-trivial case (where p is odd and m = 1) is
worth for a closer look. This is exactly the case of the spectacular Passoja-
Lakhtakia Carpets, described in [3]. It is worth to notice that the infinite sym-
metric matrix (f(n, k)) form = 1 is known as the double sequence of the Delanoy
Numbers.
Definition 11. Let us call N = {(i, j) | ai,j = 0} the set of zeros of the funda-
mental cell F (p, 1). The set:
C = {(p− 1
2
, i) ; (i,
p− 1
2
)| 0 ≤ i ≤ p− 1 ∧ 2 6 | i}
shall be called the Cross, and S = N \C the set of sporadic zeros. We call the
elements of the cross regular zeros.
Corollary 2. If p is an odd prime, the fundamental block F (p, 1) has the fol-
lowing properties:
1. If 0 ≤ n, k ≤ p− 1 then ap−1,k = (−1)k and an,k = (−1)nan,p−1−k.
2. The Cross C consists of zeros of F (p, 1).
Proof. This follows from Lemma 2. combined with Lemma 6.
The primes 3, 5, 7, 11, 19 have only regular zeros in F (p, 1). 13 is the first
odd prime with sporadic zeros, followed by 17. By all other primes tryed out by
the author (from 23 to 599) there are lots of sporadic zeros in the fundamental
block F (p, 1).
As final remark: the general diagonally symmetric linear polynomial ax +
by + az produce over Fp carpets consisting of overlappings of several periodic
motives and one self-similar carpet of type x +my + z. However, the complete
characterization of their behavior is an open problem.
3 General polynomials
The situation for the general polynomials is no more algebraic or combinatoric
anymore. In fact according to the property of interpolation over finite fields,
all functions are polynomial; and commutative binary operations can always be
represented by symmetric polynomials in two variables.
Definition 12. Consider an arbitrary finite algebra A = (A, f, 0, 1) where f :
A × A → A is a binary operation and 0, 1 are two constants. We call 1 start
symbol. The recurrent double sequence associated to A is a function a : N×N→
A defined as follows:
a(i, j) =
{
1 if i = 0 ∨ j = 0,
f(a(i, j − 1), a(i− 1, j)) if i > 0 ∧ j > 0.
If f is commutative, the recurrent double sequence is symmetric: a(i, j) = a(j, i).
Polynomial Iterations over Finite Fields 395
Definition 13. The recurrent double sequence a(i, j) is said to be ultimately
zero if in A holds:
∃N ∈ N ∀ i, j ∈ N i > 0 ∧ j > 0 ∧ i+ j > N −→ a(i, j) = 0.
Theorem 5. It is undecidable if the recurrent double sequence defined by an
algebra A is ultimately zero. This question remains undecidable if is restricted to
the class of commutative finite algebras.
Definition 14. An instance of the Halting Problem is a pair (M,w) where
M = (Σ,Q, q0, qs, b¯, δ) is a Turing machine and w ∈ Σ∗ is an input for M . Here
the tape ofM is infinite in both directions, Σ is the alphabet ofM , Q isM ’s set
of states, q0 and qs are the start state and respectively the stop state, b¯ ∈ Σ is
the blank symbol, and δ : Σ×Q→ Σ×Q×{R,L, S} is the transition function.
Lemma 8. To every instance (M,w) of the Halting Problem one can algorith-
mically associate a finite algebra A = (A, f, 0, 1) such that A ∈ Z if and only if
for input w the machine M stops and after stopping the tape is cleared.
Lemma 9. To every instance (M,w) of the Halting Problem one can algorith-
mically associate a commutative finite algebra A = (A, f, 0, 1) such that A ∈ CZ
if and only if for input w: (the machine M stops with cleared tape without having
done any step in the negative side of the tape) or (the machine M makes at least
one step in the negative side of its tape and the first time when M makes such
a step the tape of M is cleared).
The Lemma 9 together with the Theorem of Rice implies in fact alone the
Theorem 5, but is more natural to start considering the easy Lemma 8. Both
Lemmas are proved by interpreting temporally succesive tape configurations of
the Turing machine M in diagonals of the recurrent double sequence, given by
(ai,j) with i + j = k and k constant. To prove the Lemma 8 one needs two
alternating types of diagonals. The diagonals of type 0 encode the tape config-
urations. The diagonals of type 1 are needed only to transmit the information
between two diagonals of type 0:
b
δ (δ, b)
a (a, δ) c
This idea of proof is also used in the more difficult situation of the Lemma
9. There we cannot use ordered pairs of symbols because of the commutativity
of the algebra to construct.
Definition 15. Let Γ 6= ∅ be a set and ≡ be the partition of Γ × Γ consisting
of the following sets: for all a ∈ Γ the singleton sets {(a, a)} and for all a, b ∈ Γ
with a 6= b the two-element sets {(a, b), (b, a)}. Then ≡ is an equivalence relation
over Γ . Consider the set of equivalence classes:
Γ · Γ = (Γ × Γ )/ ≡
396 Mihai Prunescu
which is the set of unordered pairs of elements of Γ . We denote the equivalence
class of (a, b) with [a, b] and call this the unordered pair of a and b.
For the proof of Lemma 9, the alphabet of Σ ∪Σ×Q is extended by disjoint
union with two copies of itself and all letters c are encoded in special words
cc′c′′. For the simulation of the Halting Problem in recurrent double sequences,
one needs eight types of successive diagonals. Only the diagonals of type zero
simulates the Turing tape. The alphabets used for the diagonals of type i > 0
is always Γi = Γi−1 · Γi−1 with Γ0 = Σ ∪ Σ × Q together with its two disjoint
copies. The only thing to prove is that one can symmetrically define the function
f over diagonals of type seven such that one gets the successor Turing machine
configuration on the next diagonal, which is indeed of type zero.
References
1. Wolfram Ko¨pf: Hypergeometric Summation. An Algorithmic Approach to
Summation and Special Function Identities. Vieweg, Braunschweig/Wiesbaden,
1998. http://www.mathematik.uni-kassel.de/ koepf/hyper.html
2. Benoit B. Mandelbrot: The fractal geometry of nature. W. H. Freeman and
Company, San Francisco, 1977, 1982.
3. Dann E. Passoja, Akhlesh Lakhtakia: Carpets and rugs: an exercise in num-
bers. Leonardo, 25, 1, 1992, 69 - 71.
4. Dann E. Passoja, Akhlesh Lakhtakia: Variations on a Persian theme. Jour-
nal of Recreational Mathematics, 24, 1, 1 - 5, 1992.
5. Mihai Prunescu: Self-similar carpets over finite fields. Submitted.
6. Mihai Prunescu: An undecidable property of recurrent double sequences. To
appear in The Notre Dame Journal of Formal Logic, 2008.
7. Marko Petkovsek, Herbert Wilf and Doron Zeilberger: A = B. A K
Peters. Ltd, 1997. http://www.cis.upenn.edu/ wilf/AeqB.html.
8. Gordon H. Rice: Classes of recursively enumerable sets and their decision prob-
lems. Transactions of the American Mathematical Society, 74, 358 - 366, 1953.
9. N. J. Rose: The Pascal triangle and Sierpinski’s tree. Mathematical Calendar,
Releigh, N. C, Rome Press, 1981.
10. Marjorie Senechal: Quasicrystals and Geometry. Cambridge University Press,
1995.
11. Waclaw Sierpinski: Sur une courbe cantorienne qui contient une image biuni-
voque et continue de toute courbe donne. C. R. Acad. Sci, Paris, Sr. 162, 629,
1916.
12. Herbert S. Wilf: Generatingfunctionology. Academic Press, 1990, 1994.
13. Stephen J. Willson: Cellular automata can generate fractals. Discrete Applied
Mathematics, 8, 1984, 91 - 99.
Simulations of Quantum Turing Machines by
Quantum Multi-Counter Machines?
Daowen Qiu
Department of Computer Science, Zhongshan University, Guangzhou 510275, China
{E-mail:issqdw@mail.sysu.edu.cn(D.Qiu)}
Abstract. We establish a kind of quantum multi-stack machines and
quantum multi-counter machines, and use them to simulate quantum
Turing machines. The major technical contributions are stated as fol-
lows: (i) We define quantum multi-stack machines (abbr. QMSMs) by
generalizing a kind of quantum pushdown automata (abbr. QPDAs), and
the well-formedness (abbr. W-F) conditions for characterizing the uni-
tary evolution of the QMSMs are presented. (ii) By means of QMSMs
we define quantum multi-counter machines (abbr. QMCMs) whose state
transition functions are different from the quantum counter automata
(abbr. QCAs) in the literature. (iii) To simulate quantum Turing ma-
chines (abbr. QTMs), we show that any given QMCM allowed to count
with ±n for n > 1 can be simulated by another QMCM that counts
with 0,±1 only. (iv) We demonstrate the efficient simulations of QTMs
in terms of QMSMs, and show that QTMs can be simulated by QMCMs
as well.
Key words: Quantum Computation; Quantum Turing Machines; Quan-
tum Multi-Counter Machines; Quantum Multi-Stack Machines.
1 Introduction
1.1 Motivation and purpose
Quantum computing is an intriguing and promising research field, which touches
on quantum physics, computer science, and mathematics [8]. To a certain extent,
this intensive attention given by the research community originated from Shor’s
findings of quantum algorithms for factoring large integers in polynomial time
[17] and Grover’s algorithm of searching in a database of size n with only O(
√
n)
accesses [7] which could also be sped up on a quantum computer.
Let us briefly recall the work of pioneers in this area. In 1980, Benioff [1] first
considered that the computing devices in terms of the principles of quantum
mechanics could be at least as powerful as classical computers. Then Feynman
? This work was supported by the National Natural Science Foundation under Grant
90303024 and Grant 60573006, the Research Foundation for the Doctorial Program
of Higher School of Ministry of Education under Grant 20050558015, and Program
for New Century Excellent Talents in University (NCET) of China.
398 Daowen Qiu
[4] pointed out that there appears to be no efficient way of simulating a quan-
tum mechanical system on a classical computer, and suggested that a computer
based on quantum physical principles might be able to carry out the simulation
efficiently. In 1985 Deutsch [3] re-examined the Church-Turing Principle and
defined quantum Turing machines (abbr. QTMs).
Quantum computation from the complexity theoretical viewpoint was stud-
ied systematically by Bernstein and Vazirani [2] and they described an efficient
universal QTM that can simulate a large class of QTMs. Notably, in 1993 Yao
[19] demonstrated the equivalence between QTMs and quantum circuits. More
exactly, Yao [19] showed that for any given QTM, there exists a quantum Boolean
circuit (n, t)-simulating this QTM with polynomial time slowdown, where n de-
notes the length of input strings, and t is the number of move steps before the
machine stops.
In the theory of classical computation [10], both 2-stack machines, as a gener-
alization of pushdown automata, and 2-counter machines can efficiently simulate
Turing machines [12, 5, 10]. However, as far as the author is aware, the simula-
tions of QTMs in terms of QMSMs and QMCMs still have not been considered.
Since Turing machines, circuits, multi-stack machines, and multi-counter ma-
chines are equivalent in classical computation, we naturally hope to clarify their
computing power in quantum computers. Therefore, our focuses in this article
are to introduce QMSMs and QMCMs that are somewhat different from the
quantum counter automata (abbr. QCAs) in the literature [11, 18], and particu-
larly, to simulate QTMs by virtue of these two quantum computing devices.
Indeed, in quantum computing devices, the unitarity of evolution operators
is generally characterized by the W-F conditions of the local transition function
of the quantum models under consideration. Bernstein and Vazirani [2] gave the
W-F conditions for the QTMs whose read/write heads are not allowed to be
stationary in each move. In QTMs whose read/write heads are allowed to be
stationary (called generalized QTMs, as in [2]), the first sufficient conditions for
preserving the unitarity of time evolution were given by Hirvensalo [9], and then
Ozawa and Nishimura [14] further presented the W-F conditions for the general
QTMs. For the details, see ([8], p. 173). Also, Yamakami [20] gave the simple W-F
conditions for multiple-tape stationary-head-move QTMs. Golovkins [6] defined
a kind of QPDAs and gave the corresponding W-F conditions; Yamasaki et. al.
[18] defined quantum 2-counter automata and presented the corresponding W-F
conditions, as well.
We see that those aforementioned W-F conditions given by these authors
for corresponding quantum computing devices are quite complicated. Therefore,
based on the QPDAs proposed in [16] where QPDAs in [16] and [13] are shown
to be equivalent, we would like to define QMSMs that generalize the QPDAs in
[16], and further define QMCMs. Also, we will give the W-F conditions for these
defined devices. Notably, these W-F conditions are more succinct than those
mentioned above. In particular, motivated by Yao’s work [19] concerning the
(n, t)-simulations of QTMs by quantum circuits, we will use QMSMs to (n, t)-
simulate QTMs, where n denotes that the length of input strings are not beyond
Simulations of Quantum Turing Machines 399
n, and t represents that the number of move steps of QTMs (time complexity)
is not bigger that t for those input strings.
1.2 Main results
According to the above analysis, we state the main contributions in this article.
In Section 2, we define QMSMs by generalizing QPDAs in [16] from one-stack to
muti-stack and present the corresponding W-F conditions (Theorem 1) for the
quantum devices.
In Section 3, by means of QMSMs we define QMCMs that are somewhat
different from the QCAs by Kravtsev [11] and Yamasaki et al. [18]; also, the
W-F conditions (Theorem 2) are given for the defined QMCMs. It is worth
indicating that the state transition functions in QCAs defined by Kravtsev [11]
and Yamasaki et al. [18] have local property, since they are defined on Q ×
{0, 1}×(Σ∪{#, $})×Q×{0, 1}, but their W-F conditions are quite complicated,
while in QMCMs defined in this article, the state transition functions are on
Q × Nk × (Σ ∪ {#, $}) × Q × Nk, and consequently, the corresponding W-F
conditions are more succinct (see Theorem 2). To simulate QTMs, we deal with
a number of properties regarding simulations between QMCMs with different
counters and different counts (Lemmas 1 and 2). We show that QMCMs allowed
to count with 0,±1,±2, . . . ,±n can be simulated by QMCMs that are able to
count with 0,±1 only but need more counters.
In particular, in Section 4, we present the simulations of QTMs in terms of
QMCMs with polynomial time slowdown. More specifically, we prove that for
any QTM M1, there exists QMCM M2 (n, t)-simulating M1, where n denotes
the length of input strings not bigger than n, and t represents that the number
of move steps of QTMs is not bigger than t for those input strings. Also, we show
that QMCMs can be simulated by QMSMs with the same time complexity, and
by this result it then follows the efficient simulations of QTMs by QMSMs.
Due to the limit space, all proofs in the article are omitted and are referred
to [15].
2 Quantum multi-stack machines
Here we will define quantum k-stack machines by generalizing the QPDAs in
[16] from one stack to k stacks.
Definition 1.A quasi-quantum two-stack machine is defined asM = (Q,Σ, Γ,
δ, Z0, q0, qa, qr) where Q is the set of states, Σ is the input alphabet, Γ is the
stack alphabet, Z0 ∈ Γ denotes the bottom symbol that is not allowed to be
popped, q0 ∈ Q is the initial state, and qa, qr ∈ Q are respectively the accepting
and rejecting states, and transition function δ is defined as follows:
δ : Q× Γ ∗ × Γ ∗ × (Σ ∪ {#, $})×Q× Γ ∗ × Γ ∗ → C
where Γ ∗ denotes the set of all strings over Γ , and δ(q, γ1, γ2, σ, q
′
, γ
′
1, γ
′
2) 6= 0 if
and only if (i) γ1 = γ
′
1 or Xγ1 = γ
′
1 or γ1 = Xγ
′
1 for some X ∈ Γ\{Z0}; and (ii)
400 Daowen Qiu
γ2 = γ
′
2 or Y γ2 = γ
′
2 or γ2 = Y γ
′
2 for some Y ∈ Γ\{Z0}. In addition, if γ1 = Z0,
then γ
′
1 = Z0 or γ
′
1 = ZZ0 for some Z ∈ Γ with Z 6= Z0; similar restriction is
imposed on γ2.
A configuration of the machine is described by |q〉|γ1〉|γ2〉, where q is the
current control state, γ1 and γ2 represent the current strings of stack symbols in
two stacks, respectively, and the leftmost symbol of γi represents the top stack
symbol of stack i for i = 1, 2. Therefore, the rightmost symbol of γi is Z0. We
denote by CM the set of all configurations of M , that is,
CM = {|q〉|γ1Z0〉|γ2Z0〉 : q ∈ Q, γi ∈ Γ ∗ \ Z0, i = 1, 2}.
LetHX represent the Hilbert space whose orthonormal basis is the setX, that is,
HX = l2(X). Therefore, HQ⊗HΓ∗ ⊗HΓ∗ is a Hilbert space whose orthonormal
basis is CM , that is, l2(CM ) = HQ ⊗HΓ∗ ⊗HΓ∗ . In addition, we assume that
there are endmarkers # and $ representing the leftmost and rightmost symbols
for any input string x ∈ Σ∗, respectively. Therefore, any input string x ∈ Σ∗ is
put on the input tape in the form of #x$, and the read head of M begins with
# and ends after reading $.
For any σ ∈ Σ ∪ {#, $} we defined the time evolution operators Uσ and U ′σ
from HQ ⊗HΓ∗ ⊗HΓ∗ to HQ ⊗HΓ∗ ⊗HΓ∗ as follows:
Uσ(|q〉|γ1〉|γ2〉) =
∑
q′ ,γ′1,γ
′
2
δ(q, γ1, γ2, σ, q
′
, γ
′
1, γ
′
2)|q
′〉|γ′1〉|γ
′
2〉, (1)
U
′
σ(|q〉|γ1〉|γ2〉) =
∑
q′ ,γ′1,γ
′
2
δ∗(q
′
, γ
′
1, γ
′
2, σ, q, γ1, γ2)|q
′〉|γ′1〉|γ
′
2〉, (2)
where δ∗ denotes the conjugate complex number δ. By linearity Uσ and U
′
σ can
be extended to HQ ⊗HΓ∗ ⊗HΓ∗ .
Remark 1. U
′
σ is the adjoint operator of Uσ. Indeed, for any (qi, γi1, γi2) ∈
Q× Γ ∗ × Γ ∗, by means of Eqs. (1,2) we have
〈Uσ|q1〉|γ11〉|γ12〉, Uσ|q2〉|γ21〉|γ22〉〉
=
∑
q,γ1,γ2
δ(q1, γ11, γ12, σ, q, γ1, γ2)× δ∗(q2, γ21, γ22, σ, q, γ1, γ2)
=
〈
|q1〉|γ11〉|γ12〉, U ′σUσ|q2〉|γ21〉|γ22〉
〉
. (3)
Definition 2. Let M be a quasi-quantum two-stack machine with input
alphabet Σ. If Uσ is unitary for any σ ∈ Σ∪{#, $}, then M is called a quantum
two-stack machine.
Now we give the well-formedness conditions for justifying the unitarity of Uσ
for any σ ∈ Σ ∪ {#, $}.
Theorem 1. Let M be a quasi-quantum two-stack machine with input al-
phabet Σ. Then for any σ ∈ Σ ∪ {#, $}, linear operator Uσ is unitary if and
only if δ satisfies the following well-formedness conditions:
Simulations of Quantum Turing Machines 401
(I) For any σ ∈ Σ ∪ {#, $},∑
q′ ,γ′1,γ
′
2
δ(q1, γ11, γ12, σ, q
′
, γ
′
1, γ
′
2)× δ∗(q2, γ21, γ22, σ, q
′
, γ
′
1, γ
′
2)
=
{
1, if (q1, γ11, γ12) = (q2, γ21, γ22),
0, otherwise. (4)
(II) For any σ ∈ Σ ∪ {#, $},∑
q′ ,γ′1,γ
′
2
δ(q
′
, γ
′
1, γ
′
2, σ, q1, γ11, γ12)× δ∗(q
′
, γ
′
1, γ
′
2, σ, q2, γ21, γ22)
=
{
1, if (q1, γ11, γ12) = (q2, γ21, γ22),
0, otherwise. (5)
3 Quantum multi-counter machines
As stated above, QCAs were first considered by Kravtsev [11], and further devel-
oped by Yamasaki et al. [18]. In this section, we introduce a different definition
of quantum k-counter machines.
Definition 3. A quasi-quantum k-counter machine is defined asM = (Q,Σ,
δ, q0, qa, qr) where Q is a set of states with initial state q0 ∈ Q and states qa, qr ∈
Q representing accepting and rejecting states, respectively, Σ is an input alpha-
bet, and transition function δ is a mapping from Q×Nk×(Σ∪{#, $})×Q×Nk
to C, where N denotes the set of all nonnegative integer and #, $ represent two
endmarkers that begins with # and ends with $, and δ satisfies that
δ(q, n1, n2, . . . , nk, σ, q
′
, n
′
1, n
′
2, . . . , n
′
k) 6= 0 (6)
only if |ni − n′i| ≤ 1 for i = 1, 2, . . . , k. Furthermore, let |q〉|n1〉|n2〉 . . . |nk〉
represent a configuration of M , where q ∈ Q, ni ∈ N for i = 1, 2, . . ., and let the
set CM = {|q〉|n1〉|n2〉 . . . |nk〉 : q ∈ Q,ni ∈ N, i = 1, 2, . . . , k} be an orthonormal
basis for the space HCM = l2(CM ). For any σ ∈ Σ, linear operator Vσ on HCM
is defined as follows:
Vσ|q〉|n1〉|n2〉 . . . |nk〉
=
∑
q′ ,n′1,n
′
2,...,n
′
k
δ(q, n1, n2, . . . , nk, σ, q
′
, n
′
1, n
′
2, . . . , n
′
k)|q
′〉|n′1〉|n
′
2〉 . . . |n
′
k〉 (7)
and Vσ is extended to HCM by linearity.
Definition 4.We say that the quasi-quantum counter machine M = (Q,Σ,
δ, q0, qa, qr) defined above is a quantum k-counter machine, if Vσ is unitary for
any σ ∈ (Σ ∪ {#, $}). Also, we define linear operator V ′σ on HCM as follows:
V
′
σ|q〉|n1〉|n2〉 . . . |nk〉
=
∑
q′ ,n′1,n
′
2,...,n
′
k
δ∗(q
′
, n
′
1, n
′
2, . . . , n
′
k, σ, q, n1, n2, . . . , nk)|q
′〉|n′1〉|n
′
2〉 . . . |n
′
k〉.(8)
402 Daowen Qiu
Remark 2. Clearly V
′
σ is an adjoint operator of Vσ, which can be checked
in terms of the process of Remark 1, and the details are therefore omitted here.
Now we give the W-F conditions for characterizing the unitarity of Vσ. With-
out loss of generality, we deal with the case of k = 2.
Theorem 2. Let M be a quasi-quantum two-counter machine with input
alphabet Σ. Then for any σ ∈ Σ ∪{#, $}, Vσ defined as Eq. (7) is unitary if and
only if δ satisfies the following W-F conditions:
(I) For any σ ∈ Σ ∪ {#, $},∑
p,n
′
1,n
′
2
δ(q1, n11, n12, σ, p, n
′
1, n
′
2)× δ∗(q2, n21, n22, σ, p, n
′
1, n
′
2)
=
{
1, if (q1, n11, n12) = (q2, n21, n22),
0, otherwise. (9)
(II) For any σ ∈ Σ ∪ {#, $},∑
p,n
′
1,n
′
2
δ(p, n
′
1, n
′
2, σ, q1, n11, n12)× δ∗(p, n
′
1, n
′
2, σ, q2, n21, n22)
=
{
1, if (q1, n11, n12) = (q2, n21, n22),
0, otherwise. (10)
In order to simulate QTMs by QMSMs, we need some related lemmas and
definitions.
Definition 5. A quasi-quantum k-counter machine M = (Q,Σ, δ, q0, qa, qr)
is called to count with ±r for r ≥ 1, if its k’s counters are allowed to change
with numbers 0,±1, or ±r at each step. In this case, if |ni − n′i| ≤ 1 or |ni −
n
′
i| = r for i = 1, 2, . . . , k, then δ(q, n1, n2, . . . , nk, σ, q
′
, n
′
1, n
′
2, . . . , n
′
k) 6= 0 may
hold; otherwise it is 0. We say that the quasi-quantum k-counter machine M is
quantum if for any σ ∈ Σ ∪ {#, $}, Vσ is a unitary operator on l2(CM ), where
CM = {|q〉|n1〉|n2〉 . . . |nk〉 : q ∈ Q,ni ∈ N, i = 1, 2, . . . , k}.
It is ready to obtain that Theorem 2 also holds for quantum k-counter ma-
chines with count ±r for r ≥ 1.
Theorem 3. Let M = (Q,Σ, δ, q0, qa, qr) be a quasi-quantum k-counter
machine that is allowed to count with a certain ±r for r ≥ 1. Then for any
σ ∈ Σ ∪ {#, $}, Vσ defined as Eq. (7) is unitary if and only if δ satisfies Eqs.
(9,10).
Definition 6. Let M1 and M2 be quantum k1-counter machine M1 and
quantum k2-counter machine M2, respectively, and, M1 and M2 have the same
input alphabet Σ. For any σ ∈ Σ ∪ {#, $}, V (1)σ and V (2)σ defined as Eq. (7)
represent the evolution operators in M1 and M2, respectively. We say that M1
can simulate M2, if for any string σ1σ2 . . . σn ∈ Σ∗,∑
i1,i2,...,ik1≥0
∣∣∣〈ik1 | . . . 〈i1|〈q(1)a |V (1)$ V (1)σn V (1)σn−1 . . . V (1)σ1 V (1)# |q(1)0 〉|0〉 . . . |0〉∣∣∣2
Simulations of Quantum Turing Machines 403
=
∑
j1,j2,...,jk1≥0
∣∣∣〈jk1 | . . . 〈j1|〈q(2)a |V (2)$ V (2)σn V (2)σn−1 . . . V (2)σ1 V (2)# |q(2)0 〉|0〉 . . . |0〉∣∣∣2(11)
where q(i)0 and q
(i)
a denote the initial and accepting states of Mi, respectively,
i = 1, 2.
For convenience, for any quantum k-counter machineM = (Q,Σ, δ, q0, qa, qr),
we define the accepting probability PMaccept(σ1σ2 . . . σn) for inputting σ1σ2 . . . σn
as:
PMaccept(σ1σ2 . . . σn)
=
∑
i1,i2,...,ik≥0
∣∣∣〈ik| . . . 〈i1|〈qa|VM$ VMσn VMσn−1 . . . VMσ1 VM# |q0〉|0〉 . . . |0〉∣∣∣2 , (12)
where VMσ is unitary operator on l2(CM ) for any σ ∈ Σ ∪ {#, $}.
Lemma 1. For any quantum k-counter machine M1 that is allowed to count
with ±r for r ≥ 1, there exists quantum 2k-counter machine M2 simulating M1
with the same time complexity, where M2 is allowed to count with 0,±1, and
±(r − 1).
Lemma 2. For any quantum k-counter machine M1 that is allowed to count
with 0,±1,±2, . . . ,±r, then there exists a quantum kr-counter machine M2
simulating M1 with the same time complexity, where M2 is allowed to count
with 0,±1 only.
4 Simulations of quantum Turing machines
To simulate QTMs in terms of QMCMs, we give the definition of QTMs in terms
of Bernstein and Vazirani [2], in which the read-write head will move either to the
right or to the left at each step. Indeed, generalized QTMs can also be simulated
by QMCMs, but the discussion regarding unitarity is much more complicated.
For the sake of simplicity, we here consider the former QTMs.
Definition 7. A QTM is defined by M = (Σ,Q, δ,B, q0, qa, qr), where Σ
is a finite input alphabet, B is an identified blank symbol, Q is a finite set of
states with an identified initial state q0 and final state qa, qr 6= q0, where qa
and qr represent accepting and rejecting states, respectively, and the quantum
transition function δ is defined as
δ : Q×Σ ×Σ ×Q× {L,R} → C.
The QTM has a two-way infinite tape of cells indexed by Z and a single read-
write tape head that moves along the tape. A configuration of this machine
is described by the form |q〉|τ〉|i〉, where q denotes the current state, τ ∈ ΣZ
describes the tape symbols, and i ∈ Z represents the current position of tape
head. Naturally, a configuration containing initial or final state is called an initial
or final configuration. Let CM denote the set of all configurations in M , and
therefore HCM = l2(CM ), that is a Hilbert space whose orthonormal basis can
404 Daowen Qiu
be equivalently viewed as CM . Then the evolution operator UM on l2(CM ) can
be defined in terms of δ: for any configuration |c〉 ∈ CM ,
UM |c〉 =
∑
|c′ 〉∈CM
a(c, c
′
)|c′〉, (13)
where a(c, c
′
) is the amplitude of configuration |c〉 evolving into |c′〉 in terms of
the transition function δ. UM is a unitary operator on l2(CM ).
As in [2], we define that QTM halts with running time T on input x if after
the T ’s step moves beginning with its initial configuration, the superposition
contains only final configurations, and at any time less than T the superposition
contains no final configuration. Therefore, we assume that the QTM satisfies
this requirement.
Definition 8. For nonnegative integer n, T , let M1 = (Q1, Σ1, δ1, B1, q10,
q1a, q1r) be a quantum Turing machine with initial state q10, and let M2 be a
quantum k-counter machine with initial state q20 and the same input alphabet
Σ1 as M1. We say that M2 (n, T )-simulates quantum Turing machine M1 with
polynomial time O(n, T ) slowdown, if there exist some tape symbols added in
M2, say B2, B3, . . . , Bm such that for any input x = σ1σ2 . . . σl ∈ Σ∗2 (l ≤ n), if
the computation ofM1 ends with t steps (t ≤ T ), then there is nonnegative inte-
gers kl1 , kl2 , . . . , klm1 and ks1 , ks2 , . . . , ksm2 that are related to l and t, satisfying∑m1
i=1 kli +
∑m2
i=1 ksi ≤ O(n, T ), and
PM1a (x) = P
M2
a (x), (14)
where
PM1a (x) =
∑
−T≤i≤T,τ∈Σ[−T,T ]Z
∣∣〈i|〈τ |〈q1a|U tM1 |q10〉|τ0〉|0〉∣∣2 (15)
where τ0 is defined as: τ0(j) =
{
σj+1, if j ∈ [0, l − 1]Z,
B1, j ∈ [−T,−1]Z ∪ [l, T ]Z, and
PM2a (x) =
∑
n1,n2,...,nk≥0
∣∣∣〈nk| . . . 〈n1|〈q2a|V$V ksm2Bsm2 . . . V ks1Bs1
VσlVσl−1 . . . Vσ1V
klm1
Blm1
. . . V
kl1
Bl1
V#|q0〉|0〉 . . . |0〉
∣∣∣2 . (16)
One of the main result is as follows:
Theorem 4. For any QTM M1 = (Q1, Σ1, δ1, B1, q10, q1a, q1r) with initial
state q10 and accepting and rejecting states q1a, q1r, and for any nonnegative
integer n, t with n ≤ t+ 1, there exists a quantum (2t+2)-counter machine M2
that (n, t)-simulates M1 with most slowdown O(n+ t).
QTMs can be also (n, t)-simulated by quantum multi-stack machine, since
quantum k-counter machine can be simulated by quantum multi-stack machine
in terms of the following Theorem 5.
Definition 9.We say that quantum k-stack machineM2 = (Q2, Σ2, Γ2, δ2, Z0,
q20, q2a, q2r) simulates quantum k-counter machineM1 = (Q1, Σ1, δ1, q10, q1a, q1r)
Simulations of Quantum Turing Machines 405
that has the same input alphabet Σ1 = Σ2 with the same time complexity in
the sense of any efficient overhead, if for any input string x = σ1σ2 . . . σn ∈ Σ∗1 ,
we have
PM1accept(x) = P
M2
accept(x) (17)
where
PM1accept(x)
=
∑
γ1,γ2,...,γk
|〈γk|〈γk−1| . . . 〈γ1|〈q1a| U$Uσn . . . Uσ1U#|q10〉|0〉 . . . |0〉|2 . (18)
Also, the (n, t)-simulations of QTMs in terms of quantum k-stack machine
can be similarly defined as Definition 8, and we leave out the details here.
Theorem 5. For any given quantum k-counter machineM1 = (Q1, Σ1, δ1, q10,
q1a, q1r), there exists quantum k-stack machine M2 that simulates M1 with the
same time complexity.
Corollary 1. For any n, t ∈ N, and any QTM M1, there exists QMSM M2
that simulates M1 with slowdown O(n+ t).
5 Concluding remarks
The unitary evolution of quantum physics requires that quantum computation
should be necessarily time reversible (unitary). This makes some simulations
between quantum computing devices quite complicated. Indeed, the unitarity
is reflected by the W-F conditions. The W-F conditions for these QMSMs and
QMCMs defined in this paper are more succinct than the W-F conditions for
QCAs introduced by Yamasaki et al. [18], but we note that the transition func-
tions in our quantum devices employ the whole property of the symbols in the
stacks or counters at each move. An issue worthy of further consideration is to
give also succinct W-F conditions but yet more local transition functions for
characterizing the unitarity of these QMSMs and QMCMs defined in this pa-
per. Moreover, the relationships between QMCMs in the paper and QCAs by
Yamasaki et al. [18] still need to be further clarified. Finally, how to improve the
(n, t)-simulations of QTMs by QMCMs and QMSMs towards more general sim-
ulations and how to decrease the number of counters of QMCMs for simulating
QTMs are also worth studying.
The unitarity of the above models of computation leads to the complicated
W-F conditions [14, 18], while, in this article, the W-F conditions are simpler
but, the transition functions lose local property. Therefore, defining the above
models by means of measurement-based quantum computation [21] may be one
of the feasible ways to solve this problem, since it is still physically allowed.
Acknowledgment
The author would like to thank the four anonymous reviewers for invaluable
comments and suggestions.
406 Daowen Qiu
References
1. P. Benioff, The computer as a physical system: a microscopic quantum mechanical
Hamiltonian model of computers as represented by Turing machines, J. Statist.
Phys. 22 (1980) 563-591.
2. E. Bernstein and U. Vazirani, Quantum complexity theory, SIAM J. Comput. 26
(1997) 1411-1473.
3. D. Deutsh, Quantum theory, the Church-Turing principle and the universal quantum
computer, Proc. Roy. Soc. London A 400 (1985) 97-117.
4. R.P. Feynman, Simulating physics with computers, Internat. J. Theoret. Phys. 21
(1982) 467-488.
5. P.C. Fischer, Turing machine with restricted memory access, Information and Con-
trol 9 (4) (1966) 364-379.
6. M. Golovkins, Quantum Pushdown Automata, in: Proc. 27th Conf. on Current
Trends in Theory and Practice of Informatics, Milovy, Lecture Notes in Computer
Science, Vol. 1963, Spring-Verlag, Berlin, 2000, pp. 336-346.
7. L. Grover, A fast quantum mechanical algorithms for datdbase search, in: Proc.
28th Annual ACM Symp. Theory of Computing, Philadelphia, Pennsylvania, 1996,
pp. 212-219.
8. J. Gruska, Quantum Computing, McGraw-Hill, London, 1999.
9. M. Hirvensalo, On quantum computation. PhD thesis, Turku Center for Computer
Science, 1997.
10. J.E. Hopcroft and J.D. Ullman, Introduction to Automata Theory, Languages, and
Computation, Addision-Wesley, New York, 1979.
11. M. Kravtsev, Quantum finite one-counter automata, in: SOFSEM’99, Lecture
Notes in Computer Science, Vol.1725, Springer-Verlag, Berlin, 1999, pp.431-440.
12. M.L. Minsky, Recursive unsolvability of Post’s problem of ‘tag’ and other topics
in the theory of Turing machines, Annals of Mathematics 74 (3) (1961) 437-455.
13. C. Moore and J.P. Crutchfield, Quantum automata and quantum grammars, The-
oret. Comput. Sci. 237 (2000) 275-306. Also quant-ph/9707031, 1997.
14. M. Ozawa, H. Nishimura, Local transition functions of quantum Turing machines,
quant-ph/9811069, 1998.
15. D.W. Qiu, Simulations of quantum Turing machines by quantum multi-stack ma-
chines, quant-ph/0501176, 2005.
16. D.W. Qiu and M.S. Ying, Characterization of quantum automata, Theoret. Com-
put. Sci. 312 (2004) 479-489.
17. P.W. Shor, Algorithm for quantum computation: discrete logarithms and factoring,
in: Proc. 37th IEEE Annu. Symp. on Foundations of Computer science, 1994, pp.
124-134.
18. T. Yamasaki, H. Kobayashi, H. Imai, Quantum versus deterministic counter au-
tomata, Theoret. Comput. Sci. 334 (2005) 275-297.
19. A.C. Yao, Quantum circuit complexity, in: Proc. 34th IEEE Symp. on Foundations
of Computer science, 1993, pp. 352-361.
20. T. Yamakami, A Foundation of Programming a Multi-Tape Quantum Turing Ma-
chine, in: MFCS’99, Lecture Notes in Computer Science, Vol.1672, Springer-Verlag,
Berlin, 1999, pp. 430-441.
21. R. Jozsa, An introduction to measurement based quantum computation, quant-
ph/0508124, 2005.
Optimal Proof Systems and Complete Languages
(Extended Abstract)
Zenon Sadowski
Institute of Mathematics, University of Bia lystok
15-267 Bia lystok, ul. Akademicka 2, Poland
sadowski@math.uwb.edu.pl
Abstract. We investigate the connection between optimal propositional
proof systems and complete languages for promise classes. We prove that
an optimal propositional proof system exists if and only if there exists
a propositional proof system in which every promise class with the test
set in co-NP is representable. Additionally, we prove that there exists a
complete language for UP if and only if there exists a propositional proof
system such that UP is representable in it. UP is the standard promise
class with the test set in co-NP.
Key words: Optimal proof systems, promise classes, complete lan-
guages
1 Introduction
Although there are many different formal systems for proving propositional tau-
tologies in logic textbooks, they all fall under the concept of an abstract propo-
sitional proof system (a proof system for TAUT ) introduced by S. Cook and
R. Reckhow [5]. In order to compare the relative strength of different proof
systems for TAUT we use the notion of simulation [10] and the notion of p-
simulation [5]. A proof system for TAUT is optimal (p-optimal) if and only if it
simulates (p-simulates) any other proof system for TAUT . The still unresolved
problem of the existence of an optimal (p-optimal) proof system for TAUT was
posed by J. Kraj´ıcˇek and P. Pudla´k [10] in 1989.
The notion of p-simulation between proof systems for TAUT is similar to the
notion of reducibility between languages. Analogously, the notion of a p-optimal
proof system for TAUT should correspond to the notion of a complete language.
Informally, a class of languages is a promise class if the languages in this
class are accepted by nondeterministic polynomial-time clocked Turing machines
which obey special conditions (promises). Common promise classes are UP,
NP ∩ co-NP, and BPP. It is still open whether there exist complete lan-
guages for these classes. The reason lies in the undecidability of the problem
of whether a given nondeterministic polynomial-time Turing machine indeed
obeys the promise of any of these classes. Moreover, there exist relativizations
for which these classes do not have complete languages (see [8]).
408 Zenon Sadowski
Recently, O. Beyersdorff [3] introduced the notion of a disjoint NP-pair rep-
resentable in a given propositional proof system f . The disjointness of such a
pair is expressible by a sequence of propositional tautologies with short f -proofs.
He also considered the complexity class of all disjoint NP-pairs representable
in a proof system f . In this paper we extend these notions to any promise class
with a propositionally expressible promise. It results in the notion of a language
representable in a given proof system f and in the notion of a promise class rep-
resentable in f . O. Beyersdorff proved [2] that the class of all disjoint NP-pairs
has a complete pair if and only if there exists a proof system for TAUT in which
every disjoint NP-pair is representable. We prove the analogous theorem for the
class UP. Namely, UP has a complete language if and only if there exists a
proof system for TAUT such that UP is p-representable in it.
It turns out that there is a close connection between optimal proof systems
and complete languages for promise classes, namely, the existence of optimal
proof systems implies the existence of complete languages for various promise
classes (see [9]). Let us mention two exemplary results of this type. A. Razborov
[14] observed that the existence of an optimal proof system suffices to guarantee
the existence of complete disjoint NP-pairs. J. Messner and J. Tora´n showed
[12] that a complete language for UP exists in case there is a p-optimal proof
system for TAUT . The converses of these implications probably do not hold [6]
and in this paper we address the question of just why it is so.
It seems that the promise that a Turing machine computes a proof system
for TAUT , or more precisely it produces only propositional tautologies, is the
hardest one among those promises which are propositionally expressible. There-
fore, the sufficient condition for the existence of an optimal proof system should
be as strong as the existence of a complete language for every promise class
with a propositionally expressible promise. At present, this intuition is only
supported by the result of J. Messner [11] which states that a p-optimal proof
system for TAUT exists if and only if every promise function class with a test
set polynomial-time reducible to TAUT has a complete function (see also [9]).
The analogous theorem in the setting of promise language classes instead of
promise function classes is missing. The main result from this paper, that the
existence of an optimal proof system for TAUT is equivalent to the existence of
a proof system for TAUT in which any promise class with the test set in co-NP
is representable, may be treated as the first step in this direction.
2 Preliminaries
We assume some familiarity with basic complexity theory and refer the reader
to [1] and [13] for standard notions and for definitions of complexity classes
appearing in the paper. The class of all disjoint pairs (A, B) of NP-languages is
denoted by DisNP.
The symbol Σ denotes a certain fixed finite alphabet throughout the paper.
The set of all strings over Σ is denoted by Σ?. For a string x, |x| denotes the
length of x.
Optimal Proof Systems and Complete Languages 409
Given two languages L1 and L2 (L1, L2 ⊆ Σ?), we say that L1 is polynomial-
time many-one reducible to L2 if and only if there exists a polynomial-time
computable function f : Σ? −→ Σ? such that x ∈ L1 if and only if f(x) ∈ L2
holds for any x ∈ Σ?.
We use Turing machines (acceptors and transducers) as our basic compu-
tational model. We will not distinguish between a machine and its code. For a
Turing machine M the symbol L(M) denotes the language accepted by M .
We consider deterministic and nondeterministic polynomial-time clocked Tur-
ing machines (PTM and NPTM for short) with uniformly attached standard
clocks that stop their computations in polynomial time (see [1]). We impose
some restrictions on our encoding of these machines. From the code of any
polynomial-time clocked Turing machine we can easily detect (in polynomial
time) the polynomial pN which is its polynomial-time bound.
Let D1, D2, D3, ... and N1, N2, N3, ... be respectively standard enumer-
ations of all deterministic and nondeterministic polynomial-time clocked Tur-
ing machines. For any class of languages C, we say that C has an uniform
enumeration if and only if there exists a recursively enumerable list of nonde-
terministic polynomial-time clocked Turing machines Ni1 , Ni2 , Ni3 , ... such that
{L(Nik): k ≥ 1 } = C.
We consider only languages over the alphabet Σ (this means that, for exam-
ple, boolean formulas have to be suitably encoded). The symbol TAUT denotes
the set (of encodings) of all propositional tautologies over a fixed adequate set
of connectives. Finally, 〈., . . . , .〉 denotes some standard polynomial-time com-
putable tupling function.
3 Propositional proof systems
The concept of an abstract propositional proof system, subsuming all proposi-
tional proof systems used in practice, was introduced by S. Cook and R. Reckhow
[5] in the following way:
Definition 1. A proof system for TAUT is a polynomial-time computable func-
tion f : Σ? onto−→ TAUT .
A string w such that f(w) = α we call an f -proof of a formula α. We write
f `∗ αn if and only if {αn: n ≥ 1} is a sequence of tautologies with polynomial-
size f -proofs. A polynomially bounded proof system for TAUT (which allows
short proofs to all tautologies) exists if and only if NP=co-NP (see [5]).
Proof systems are compared according to their strength using the notion of
simulation and the presumably stronger notion of p-simulation.
Definition 2. (Kraj´ıcˇek, Pudla´k) Let h, h′ be two proof systems for TAUT .
We say that h simulates h′ if there exists a polynomial p such that for any
x ∈ TAUT , if x has a proof of length n in h′, then x has a proof of length
≤ p(n) in h.
410 Zenon Sadowski
Definition 3. (Cook, Reckhow) Let h, h′ be two proof systems for TAUT . We
say that h p-simulates h′ if there exists a polynomial-time computable function
γ : Σ? −→ Σ? such that for every x ∈ TAUT and every w ∈ Σ?, if w is a proof
of x in h′, then γ(w) is a proof of x in h.
In other words, γ translates h′-proofs into h-proofs of the same formula.
The notions of an optimal proof system for TAUT and a p-optimal proof
system for TAUT were introduced by J. Kraj´ıcˇek and P. Pudla´k [10].
Definition 4. A proof system for TAUT is optimal (p-optimal) if and only if
it simulates (p-simulates) any proof system for TAUT .
We will study the problem of the existence of an optimal proof system and
the problem of the existence of a p-optimal proof system from computational-
complexity perspective.
4 Promise classes representable in a proof system
A nondeterministic polynomial-time clocked Turing machine which is the com-
putational model of a given promise (semantic) class should obey the special
condition, called the promise of the class. It can be illustrated by an example
of the class UP. We call a nondeterministic Turing machine categorical or un-
ambiguous if it has the following property: for any input x there is at most one
accepting computation. We define UP={L(Ni): Ni is categorical}.
Let T be any formal theory whose language contains the language of arith-
metic. We say that T is ”reasonable” if and only if T is sound (that is, in T
we can prove only true theorems) and the set of all theorems of T is recursively
enumerable. Let N be any NPTM . The notation T ` ”N is categorical” means
that the first order formula expressing the categoricity of N is provable in T . We
say that UP is representable in T if and only if for any A ∈ UP there exists an
NPTM N such that T ` ”N is categorical”. J. Hartmanis and L. Hemachandra
[8] proved that UP has a complete language if and only if it has an uniform
enumeration (see also [4]). It follows from Naming Lemma [7] that, the existence
of a uniform enumeration of UP is equivalent to the existence of a ”reasonable”
theory T such that UP is representable in T (see also [8]). Therefore, the prob-
lem of the existence of a complete language forUP can be characterized in terms
of a uniform representability of UP in a first order arithmetic theory T .
In this section we show that this problem can be also characterized in terms of
a nonuniform representability ofUP in a propositional proof system. In this case
the promise of the class is expressed as the sequence of propositional tautologies
with short proofs. We begin with the introduction of the necessary machinery.
Following J. Messner’s approach [11], we define promise classes in a very
general way. A promise R is described as a binary predicate on the Cartesian
product of the set of all NPTMs and the set of all strings, i. e. , R(N,x) means
that N obeys a promise R on input x. An NPTM N is called an R-machine if
and only if N obeys R on any input x ∈ Σ?. For a given promise predicate R
Optimal Proof Systems and Complete Languages 411
we define the class of languages CR = {L(N): N is an R-machine} and call it
the promise class generated by R.
Definition 5. (Messner) A class of languages C is called a promise class if and
only if C = CR for some promise predicate R.
The following notion of the test set for a promise class CR serves as a tool
for estimating the complexity of the promise R of this class.
Definition 6. By the test set for a promise class CR we mean the set
TR = {〈N, 0n, 0pN (n)〉: n is a natural number, N is an NPTM such that R(N,x)
holds for any x such that |x| = n}
The notion of the test set for CR corresponds to the notion of the generic
and length-only dependent test set from [9].
We are especially interested in the situation when the fact that a given
NPTM is an R-machine can be expressed propositionally, as a sequence of
propositional tautologies. It can be done only when the promise R has an ap-
propriate complexity.
To any NPTM N we will assign the set DNR = { αN1 , αN2 , αN3 ,... } of proposi-
tional formulas such that αNn is a propositional tautology if and only if R(N,x)
holds for any x such that |x| = n. So, for any NPTM N it holds: N is an
R-machine if and only if DNR ⊂ TAUT .
It should be possible to construct the formulas αNn for different sets D
N
R ,
corresponding to different NPTMs, easily and in an uniform manner. This leads
to the following definitions:
Definition 7. By a propositional description of a promise R we mean a set
DR = {αNn : N is an NPTM , n is a natural number} of propositional formulas
fulfilling conditions (1) – (3):
(1) Adequacy:
αNn is a propositional tautology if and only if R(N,x) holds for any x such
that |x| = n.
(2) Uniform constructibility:
There exists a polynomial time computable function f such that for any
NPTM N and for any n natural
f(〈N, 0n, 0pN (n)〉) = αNn
(3) Local recognizability:
For any fixed NPTM N , the set DNR = {αN1 , αN2 , αN3 , ...} is in P.
Definition 8. We say that a promise R is propositionally expressible if and only
if there exists a propositional description of R.
The next lemma will be needed in Section 5 in the proof of the main result
of the paper.
412 Zenon Sadowski
Lemma 1. Any promise R such that the class CR possesses the test set TR in
co-NP is propositionally expressible.
Let R be a propositionally expressible promise and let DR = {αNn : N is an
NPTM , n is a natural number} be its propositional description. Let h be a
proof system for TAUT .
Definition 9. A language A is weakly DR-representable in h if and only if there
exists an NPTM K such that conditions (1) – (2) are fulfilled:
(1) L(K) = A
(2) h `∗ αKn
Definition 10. A language A is strongly DR-representable in h if and only if
there exists an NPTM K such that conditions (1) – (2) are fulfilled:
(1) L(K) = A
(2) There exists a polynomial time algorithm that on input 0n produces an h-
proof of αKn , for any n natural.
Finally, we have the following definitions:
Definition 11. A promise class CR is representable in h if and only if there
exists a propositional description DR of R such that any language A ∈ CR is
weakly DR-representable in h.
Definition 12. A promise class CR is p-representable in h if and only if there
exists a propositional description DR of R such that any language A ∈ CR is
strongly DR-representable in h.
Now we present the above mentioned characterization of the problem of the
existence of a complete language for UP.
Theorem 1. There exists a complete language for UP if and only if there exists
a propositional proof system h such that UP is p-representable in h.
Proof. (Sketch) (i) → (ii) The existence of the desired propositional proof sys-
tem h follows from the existence of a uniform enumeration of the class UP.
(ii)→ (i) Assume that there exists a propositional proof system h such that
UP is p-representable in it. There exists a propositional description DR = {αNn :
N is an NPTM , n is a natural number} of the promise of UP such that any
language A ∈ UP is strongly DR-representable in h.
The following language L is the desired UP complete language.
L = {〈N,x, αN|x|, P roof, 0pN (|x|)〉 : x ∈ L(N)}
where N is an NPTM , pN is the polynomial that bounds the running time of N ,
x is a string, αN|x| is a propositional formula from the propositional description
of the promise of UP, Proof is an h-proof of αN|x|, 0
pN (|x|) is the sequence of
zeros (padding).
Optimal Proof Systems and Complete Languages 413
5 Main results
J. Messner [11] considered the family of all proof systems for TAUT as a promise
function class. He proved that a p-optimal proof system exists if and only if any
promise function class with a test set polynomial-time reducible to TAUT pos-
sesses a complete function. Our intention was to find an analogous theorem
in the setting of promise language classes. In our results from this section we
characterize the existence of optimal proof systems in terms of a nonuniform
presentability of promise classes in a proof system. For most promise classes,
having a complete language and a uniform enumeration (a uniform representa-
tion in an arithmetic theory) are equivalent. Similarly, it seems that for promise
classes a nonuniform p-presentability in a proof system is close to the possession
of a complete language.
In our chracterization of the problem of the existence of optimal and p-
optimal proof systems the families of all easy and all NP-easy subsets of TAUT
play a very important role.
Definition 13. By an easy subset of TAUT we mean a set A such that A ⊂
TAUT and A ∈ P.
Definition 14. By an NP-easy subset of TAUT we mean a set A such that
A ⊂ TAUT and A ∈ NP.
The next lemma shows that for every easy subset of TAUT there exists
a proof system in which tautologies from this set have short and easily con-
structible proofs.
Lemma 2. (Messner, Tora´n [12]) If A is an easy subset of TAUT then there
exists a proof system f : Σ? onto−→ TAUT and a polynomial-time computable
function t that on input α produces f-proof of α, for any tautology α in A. That
is for every α ∈ A, f(t(α)) = α.
Let us proceed to the main results of the paper.
Theorem 2. Statements (i) - (iii) are equivalent:
(i) There exists an optimal propositional proof system.
(ii) There exists a propositional proof system in which any promise class with
the test set in co-NP is representable.
(iii) There exists a propositional prof system in which the class of all NP-easy
subsets of TAUT is representable.
The previous result can be translated to the deterministic case in the follow-
ing way:
Theorem 3. Statements (i) - (iii) are equivalent:
(i) There exists a p-optimal propositional proof system.
(ii) There exists a propositional proof system in which any promise class with
the test set in co-NP is p-representable.
414 Zenon Sadowski
(iii) There exists a propositional prof system in which the class of all easy subsets
of TAUT is p-representable.
We proved [15] that there exists a p-optimal proof system for TAUT if and
only if the class of all easy subsets of TAUT is uniformly enumerable. It can be
otherwise stated thus: there exists a p-optimal proof system for TAUT if and
only if there exists a ”reasonable” arithmetic theory T such that the class of all
easy subsets of TAUT is representable in it (see [7]). In this paper we replaced
representability in an arithmetic theory by representability in a proof system, in
the characterization of the existence of a p-optimal proof system in terms of easy
subsets of TAUT . It is typical for proof complexity that an arithmetic theory
coincides with a proof system for TAUT and the latter is a nonuniform version
of the former.
References
1. J. Balcazar, J. Dı´az and J. Gabarro´, Structural Complexity I (Springer-Verlag,
Berlin, 1995).
2. O. Beyersdorff, Tuples of disjoint NP-sets, Thechnical Report TR 05-123, Elec-
tronic Colloquium on Computational Complexity, 2005.
3. O. Beyersdorff, Disjoint NP-Pairs and Propositional Proof systems, PhD Thesis,
Humbold-Universita¨t zu Berlin, July 2006.
4. H. Buhrman, S. Fenner, L. Fortnow and D. van Melkebeek, Optimal proof systems
and sparse sets, Proc. Symposium on Theoretical Aspects of Computer Science,
Lecture Notes in Computer Science 1770, Springer - Verlag, Berlin, 2000.
5. S. Cook and R. Reckhow, The relative efficiency of propositional proof systems,
Journal of Symbolic Logic 44 (1979) 36-50.
6. C. Glasser, A. L. Selman, S. Sengupta and L. Zhang, Disjoint NP-pairs, SIAM
Journal on Computing, 33(6), (2004) 1369 –1416.
7. J. Hartmanis, Independence Results about Context-Free Languages and Lower
Bounds, Technical Report, TR 84-606, Department of Computer Science Cornell
University, May 1984.
8. J. Hartmanis and L. Hemachandra, Complexity classes without machines: On com-
plete languages for UP, Theoretical Computer Science 58(1988) 129-142.
9. J. Ko¨bler, J. Messner and J. Tora´n, Optimal proof systems imply complete sets
for promise classes, Information and Computation 184 (2003) 71 – 92.
10. J. Kraj´ıcˇek and P. Pudla´k, Propositional proof systems, the consistency of first
order theories and the complexity of computations, Journal of Symbolic Logic 54
(1989) 1063-1079.
11. J. Messner, On the simulation order of proof systems, PhD Thesis, Universita¨t
Ulm, December 2000.
12. J. Messner and J. Tora´n, Optimal proof systems for Propositional Logic and com-
plete sets, in: Proc. 15th Symposium on Theoretical Aspects of Computer Science,
Lecture Notes in Computer Science 1373 (Springer,Berlin,1998) 477-487.
13. C. Papadimitriou, Computational Complexity, (Addison - Wesley, 1994).
14. A. Razborov, On provably disjoint NP-pairs, Technical Report 94–006, Electronic
Colloquium on Computational Complexity, 1994.
15. Z. Sadowski, On an optimal propositional proof system and the structure of easy
subsets of TAUT, Theoretical Computer Science, 288 (1), 2002, 181 – 193.
On the Complexity of Computing Winning
Strategies for Finite Poset Games
Michael Soltys and Craig Wilson
Department of Computing and Software
McMaster University
1280 Main Street West, Hamilton, Ontario, L8S 4K1, Canada
soltys@mcmaster.ca
Abstract. We study the complexity of computing winning strategies for
poset games; these are two player games on finite partially ordered sets.
Given a poset game, we consider the corresponding language of those
“board configurations” from which the first player has a winning strategy.
While it is reasonably clear that such a language is in PSPACE (as the
answer can be determined by evaluating a quantified boolean formula),
we give a simple and direct proof of this fact by reducing general poset
games to the game of geography. We also show how to reason about poset
games in Skelley’s theory W11 for PSPACE reasoning—we show that this
theory can formalize the “strategy stealing argument” and show that the
first player has a winning strategy in the poset game Chomp. More than
anything, this paper is an invitation to consider the problem whether
poset games are PSPACE complete, and/or show under what conditions
computing a winning strategy can be done efficiently.
Key words: Finite games, posets, PSPACE, proof complexity.
1 Introduction
A partially ordered set (a poset) is a set U together with an ordering relation 
on its elements, where  is a subset of U × U . The relation  must satisfy the
following conditions: (1) if a  b, then b 6 a (anti-symmetry), and (2) if a  b
and b  c, then a  c (transitivity). Not all elements are necessarily comparable,
in that there may be elements a, b such that a 6= b, where a 6 b and b 6 a. When
two elements are incomparable, we write a||b.
Given a poset (U,), a poset game (A,) on (U,) is played as follows:
at first A := U . Then, two players take turns making moves. On each move, a
player picks an element x ∈ A, and removes all the elements y ∈ A such that
x  y. The player who is unable to move because A = ∅ loses.
An example of a poset game is Chomp, first detailed by Gale in [1]. A game
of Chomp is played on a “chocolate bar” divided into individual squares, with
the bottom-left square being “poisoned”. This is usually represented by a grid of
m rows and n columns, with the poisoned square residing at position (1, 1) (see
Fig. 1). Two players take turns breaking off pieces of the chocolate by selecting a
416 Michael Soltys and Craig Wilson
square (i, j) from among the remaining squares and deleting all the squares (k, l)
such that i ≤ k and j ≤ l. The game ends when a player selects the poisoned
square. The player who does so loses. It is easy to show, by a “strategy-stealing
argument”, that player 1 always wins ([1]). To transform Chomp to a poset
Fig. 1. An example 4× 5 Chomp grid
game we delete the lower-left square, so now the player who ends up without any
chocolate to chomp loses. Let Barm,n := {(i, j)|1 ≤ i ≤ m, 1 ≤ j ≤ n}−{(1, 1)},
and (i, j)  (k, l) if and only if (i ≤ k ∧ j < l)∨ (i < k ∧ j ≤ l). So, for any m,n
we have the poset game (Barm,n,).
Recall that PSPACE is the class of languages decidable in polynomial space in
the length of the input on a Turing machine. A language L is PSPACE complete if
it is in PSPACE, and for every language L′ in PSPACE, there exists a polynomial
time function f : Σ∗ −→ Σ∗, such that x ∈ L′ ⇐⇒ f(x) ∈ L. See [2] or [3] for
more background on PSPACE.
2 From Posets to Geography
In this section we present a polynomial time reduction from poset games to
Geography. In order to study the complexity of the reduction, we assume that
the posets are finite (i.e., U is finite). However, the reduction itself still works
on infinite posets, and yields infinite instances of the game of Geography.
The game of Geography is played as follows: a directed graph is given (it
does not have to be acyclic), and a starting node s is specified. The first player
selects s, then the second player selects an outgoing edge of s, and takes that
edge to another node. The two players then traverse the graph, alternatively
selecting outgoing edges of the current node. The player who is forced to revisit
a node, or has no outgoing edges to select from, loses. It is a well known fact
that Geography is PSPACE-complete (see [2] or [3]).
Let Posetgames = {〈(U,)〉 : player 1 has a winning strategy} and let
Geography = {〈(G, s)〉 : player 1 has a winning strategy}. Here 〈(U,)〉 and
〈(G, s)〉 are the encodings of (U,) and (G, s), respectively. Note that s is the
specified starting node for the game of Geography on the graph G, so the game
starts by player 1 selecting node s, then player 2 selects an edge out of s to a
new node n. This is in contrast to poset games, where any element x ∈ U can
be selected on the first turn.
On the Complexity of Computing Winning Strategies 417
In the following paragraphs we will show how to transform (U,) into (G =
(V,E), s). More precisely, in what follows we describe a function f such that
〈(U,)〉
f
7→ 〈(G = (V,E), s)〉, where  is described as a list of pairs. Note
that such a list has length at most O(|U |2 log(|U |)), and f can be computed in
logarithmic space in |〈(U,)〉|.
For every element x of U we have a node in V , and for every pair x, y ∈ U
such that x  y, we put the directed edge (y, x) in E. For every pair x, z ∈ U
where x||z, we put (x, z) and (z, x) in E. This is not enough to simulate the
poset game on G, however, because we have to keep track of all the elements of
U that have been eliminated, that is, we need to keep track of A.
We are going to accomplish keeping track of A as follows: every time player 2
makes a move to a comparable node, player 1 can challenge that move. What is
this challenge? It is player 1’s claim that player 2 moved to a node y such that x 
y and node x has already been visited. As well, we must ensure that challenging
a legal move, or even one’s own move yields no benefit to the challenger.
To be able to do these challenges, we are going to add four different auxiliary
nodes {x1, x2, x3, x4} to each node x ∈ V , as represented in Fig. 2. The edges
are placed as follows: for each outgoing edge of x to a node z (comparable or
incomparable to x), we have the edge (x3, z). For each incoming edge from a
comparable node y such that x  y, we add the edge (y, x1). Finally, we have
the edges (x1, x), (x, x2), (x2, x3), (x2, x4), and (x4, x1).
Fig. 2. Reduction gadget: each x ∈ U is represented by five nodes in G.
We repeat this process for every x ∈ U . Finally, we add two more nodes,
s, s′, with an edge from s to s′, and edges from s′ to every other non-auxiliary
node. These nodes are required because in a poset game (U,) we can select
any x ∈ U to start the game, whereas in Geography the first player must select
node s on their first turn. To emulate the free choice of the poset game, we have
s as the initial node, which means player 2 must move to s′, then player 1 can
select any non-auxiliary node in V . It is clear that this simple construction can
be carried out in polynomial time (in fact, logarithmic space).
It is obvious that the Geography game on G mirrors a poset game on (U,) if
it is played correctly, in the sense that we never go to a node that corresponds to
an element in U that was removed at a previous step. As mentioned previously,
418 Michael Soltys and Craig Wilson
the auxiliary nodes enable players to challenge each other’s moves. We examine
below what occurs during these challenges.
Legitimate Challenge: Here player 2 moves to a node y where x  y and
node x was visited in the past. This means that player 2 moves to a y /∈ A. Then
player 1 challenges player 2 as follows: he moves from y to the challenging node
x1 of x. Player 2 is now forced to revisit x, and loses. This ensures that players
can only move to legal nodes.
False Challenge: What if a player challenges a legal move? That is, if
player 2 moved to node y which was in A? Then the following happens: player 1
challenges player 2 by moving to node x1, and now player 2 moves to x and
nothing happens as x has not been visited before. Then player 1 is forced to
move to x2, and then player 2 moves to x4, and player 1 is now forced to move
to x1, and thereby revisit a node, and player 1 loses. This not only shows that
challenges to legal nodes yield no benefit, but also challenges to incomparable
nodes.
Self-Challenge: The final type of challenge occurs when a player challenges
their own move. Suppose player 2 arrives at node x without a challenge, i.e.,
directly through one of the incoming edges, then player 1 moves to x2. Now
player 2 has a choice to move to x4, as if player 2 were going to do a counter-
challenge. If indeed player 2 moves to x4, then player 1 moves to x1, and player
2 is forced to revisit node x, and player 2 loses. Thus, no gains are made if a
player challenges their own move.
Therefore, if player 2 arrives at node x directly (without a challenge) he must
continue (after the move of player 1) from x2 to x3, and then player 1 is free to
select an outgoing edge from x3 to another node z. The above shows that there
is a (correct) polynomial time reduction from Posetgames to Geography. As
Geography itself is in PSPACE, this gives us that Posetgames ∈ PSPACE
as well. Of course, this was expected as the existence of a winning strategy
for a given “board configuration” can be expressed as the satisfiability of a
quantified Boolean formula. On the other hand, we have given a simple and
direct proof by reducing to Geography. The interesting (open) question is
whether Posetgames is PSPACE-complete, and to characterize the poset games
for which a winning strategy can be computed efficiently1.
3 W1
1
proves the existence of a strategy for Chomp
In this section we show how to extract a PSPACE algorithm for computing a
winning strategy for Chomp from a W11-proof
2 of the existence of a winning
1 For example, consider a chomp configuration in the shape of an “L”. That is, we
have the first column and last row, with their intersection containing the poisoned
square. Then as long as the two arms of the “L” have a different length, the first
player has the following (simple) winning strategy: the first player chomps the longer
arm to be the same length as the shorter, and then copies symmetrically the moves
of the second player on the opposite arm, to force the second player to be left with
the poisoned square.
2 The system W11 was introduced in [4].
On the Complexity of Computing Winning Strategies 419
strategy3. We hope that a program consisting in formalizing proofs of existence
of winning strategies in weaker and weaker fragments of Bounded Arithmetic
can yield better algorithms (i.e., algorithms of lower complexity) for computing
winning strategies for poset games.
W11 is a logical theory that captures PSPACE reasoning. It extends V
1 (which
captures polynomial time, the class P) presented by Cook and Nguyen in [5],
but W11 is designed to reason over sets of strings, which themselves encode
sets of sets of elements. W11 is a three-sorted (“third-order”) predicate calculus
with free and bound variables of three sorts (the bound variables are given in
parenthesis): a, b, c, . . . (x, y, z, . . .) for the first sort, intended to denote natural
numbers encoded in unary, A,B,C, . . . (X,Y,Z, . . .) for the second sort, intended
to denote finite binary strings, and A,B,C, . . . (X,Y,Z, . . .) for the third sort,
intended to denote sets of strings, and in particular functions from strings to
strings.
The third order language we use is L3A = [0, 1,+, ·, | · |2,∈2,∈3,≤,=] where
0, 1 are numbers, +, · is addition and multiplication of numbers, | · |2 denotes
length of strings (note that size of third sort objects is not there), i ∈2 X is
equivalent to X(i) (i.e., it is true if the i-th bit is turned on), X ∈3 X denotes
the string X in X (we may omit the subscript 2 or 3 if it is clear from the context
which type it is), and ≤,= are used to compare numbers (there is no equality
symbol for the second and third sort; equality for these objects can be defined
in L3A). We are interested in sets of pairs defining functions, and so we use the
notation X(X) = Y , which formally denotes 〈X,Y 〉 ∈ X. Our functions are
going to be finite, thus they shall be defined for X,Y such that |X|, |Y | ≤ n for
some number term n. Of course, the intention is that we shall have a “strategy
function” S(C1) = C2 which on configuration C1 produces a configuration C2.
Table 1. The set 2-BASIC
B1. x + 1 6= 0 B7. (x ≤ y ∧ y ≤ x)→ x = y
B2. (x + 1 = y + 1)→ x = y B8. x lex + y
B3. x + 0 = x B9. 0 ≤ x
B4. x + (y + 1) = (x + y) + 1 B10. x ≤ y ∨ y ≤ x
B5. x× 0 = 0 B11. x ≤ y ↔ x < y + 1
B6. x× (y + 1) = (x× y) + x B12. x 6= 0→ ∃y ≤ x(y + 1 = x)
L1. X(y)→ y < |X| L2. y + 1 = |X| → X(y)
SE. [|X| = |Y | ∧ ∀i < |X|(X(i)↔ Y (i))]→ X = Y
The theory W11 consists of axiom B1–B12 and L1, L2 (listed in table 1, from
[5]) as well as two comprehension axioms (or rather axiom schemes) given below.
We let ΣBi be the set of formulas over L
3
A containing formulas with arbitrarily
3 In fact, our method works for any poset game to which we can apply the “strategy
steeling argument,” namely to any poset game with a supremum.
420 Michael Soltys and Craig Wilson
many bounded first and second-order quantifiers, and exactly i alternations of
third-order quantifiers. The purpose of the comprehension axioms is to allow the
definition of new strings and functions (from strings to strings):
(
∃Y ≤ t
(
x,X
)) (
∀z ≤ s
(
x,X
)) [
φ
(
x,X,X, z
)
↔ Y (z)
]
ΣB0 -2COMP
(∃Y)
(
∀Z ≤ s
(
x,X
)) [
φ
(
x,X,X, Z
)
↔ Y (Z)
]
ΣB0 -3COMP
In each of these schemes φ ∈ ΣB0 is subject to the restriction that neither Y nor
Y (as appropriate) occurs free in φ. Y (Z) abbreviates Z ∈3 Y, and similarly for
Y (z).
Following [4, theorem 4] we know that if we can show that W11 proves the
existence of a winning strategy for the first player, where the formula asserting
the existence of this strategy is in a proper syntactic form (i.e., it is a ΣB1 -
formula), then we can conclude directly that the strategy can be computed in
PSPACE. This is what we do next.
Any configuration of chomp on an n ×m board can be represented with a
binary string X ∈ {0, 1}n+m. Following a suggestion of [6], a configuration can
be represented with a string of 0s and 1s of length (n + m), where there are
exactly n 1s and m 0s. In this scheme, a board configuration can be seen as a
path from the upper-left corner to the lower-right corner (the poisoned square
is in the lower-left corner), where we are only allowed to move right or down.
Reading the string left to right, every time we see a 0 we move right, and
every time we see a 1 we move down. The original configuration would be
000000011111, and the final configuration would be the same but with all the 1s
moved to the left. Whoever moves the game into the final configuration loses. A
possible intermediate configuration is given by Fig. 3. Each move is a “chomp”,
Fig. 3. A chomp board; it is represented by 011001000101.
which consists of picking a square, from among the surviving squares (i.e., to
the left of the thick line in the picture above), and chomping off all the squares
to the right and up. This means, that each move of chomp consists in moving
one or more 1s one or more positions to the left, without ever overtaking the 1s
in front.
All this can be expressed easily in W11. We are going to provide a formula
Φ(X,n,m) which will assert that X is a valid chomp game on a n ×m board.
Here X is a long string, consisting of n×m many segments (the longest possible
On the Complexity of Computing Winning Strategies 421
chomp game, where each player takes just one square at a time) of length (n+m)
each. We denote the i-th segment by X [i] (all valid notation in the language of
W11). Φ is the conjunction of three formulas: φinit, φfinal, and φmove. φinit asserts
that X [1] is the initial configuration, i.e.
φinit(X
[1], n,m) = ∀i ≤ (n+m)((i > m)→ X [1](i) = 1)
φfinal asserts that X
[n×m] is the final configuration:
φfinal(X
[n×m], n,m) = ∀i ≤ (n+m)((i > n)→ X [n×m](i) = 0)
φmove asserts that each segment of X can be obtained from one legal move on
the previous segment:
φmove(X,n,m) = ∀i < (n×m)(X
[i] “yields” X [i+1])
In order to define the “yields” portion of φmove, we first take the high-level
approach of determining what square was played between two consecutive con-
figurations X [i] and X [i+1], and then express the conditions with W11 formulas.
Note that for the remainder of this paper we will “invert” our coordinate sys-
tem for Chomp configurations, as now the “origin” (1, 1) is the top-right square,
while (m,n) is the bottom-right square.
First, we notice that in our string representation of configurations, a 0 occur-
ring to the left of at least a single 1 corresponds to a column of playable squares,
the height of which is the number of 1s to the right of the 0. Thus, a square
(j, k) can be played on X [i] if and only if in reading X [i] from left to right, the
kth 0 is encountered before the jth 1. To express this formally, we introduce two
functions F0 and F1. F0 is defined as
F0 (a, b,X) = c↔ numones (1,X(a : c)) = b
or, the position in the string X where, starting from (and including) position a,
there are b 0s. F1 is defined analogously for b 1s:
F1 (a, b,X) = c↔ |X(a : c)| − numones (1,X(a : c)) = b
Note that we define F0 (a, 0,X) and F1 (a, 0,X) to be 0 for any a,X. Also,
if we reach the end of X before encountering b0s or 1s, we define F0 (a, b,X)
and F1 (a, b,X) to be |X|. We now restate the conditions for the existence of
playable square as a formula using these functions. This formula is the first of
many sub-formulas which will comprise the final “yields” formula. Each of these
sub-formulas will be marked with a label ψi.
Lemma 1 A square (j, k) can be played on a configuration X if and only if
F0(1, k,X) < F1(1, j,X) (ψ1)
422 Michael Soltys and Craig Wilson
Proof. We will argue by contradiction. Suppose square (j, k) exists on configu-
ration X, but F0(1, k,X) > F1(1, j,X) - note that we cannot have F0(1, k,X) =
F1(1, j,X) as a position in X
[i] can’t simultaneously be 0 and 1. This means that
the kth 0 occurs after that jth 1, which in turn means that row j terminated
before column k. Thus, there are no squares beyond column k − 1 in row j, so
square (i, j) does not exist in X, and cannot be played. ⊓⊔
Now that we can identify playable squares, we need to determine the configura-
tions which result from playing these squares. In general, playing a square (j, k)
shifts at least a single 1 behind the 0 corresponding to the move, to delimit the
row being shortened or eliminated. If a move affects multiple rows, then mul-
tiple 1s will be shifted over. In order to figure out the substring of X which is
affected by playing a square (j, k), we calculate two values p and q which mark
the beginning and ending of the substring (inclusive):
p = F0(1, k − 1,X
[i]) + 1 (ψ2)
q = F1(p, j,X
[i]) (ψ3)
Intuitively, (p − 1) marks the location of the (k − 1)th 0, so p forms the left
boundary of the substring. q is the position of X [i] in which we have seen j 1s
starting from p, so it designates the right boundary. Finally, to arrive at the new
configuration X [i+1] we replace X [i](p : q) with 1j0(q−p−k+1), i.e., the number of
1s corresponding to the number of rows affected, then the number of 0s indicating
the new difference in row lengths. In order to express this new substring as a
formula we break it down into several sub-formulas. First, positions that are
outside the range of p and q remain unchanged:
(r < p)→ (X [i+1](r) = X [i](r)) (ψ4)
(r > q)→ (X [i+1](r) = X [i](r)) (ψ5)
Next, for positions between p and q we assign values based on their placement
relative to the 0 associated with the move. Positions before the 0 hold 1s affected
by the move, while positions after contain 0s.
(r < p+ j)→ (X [i+1](r) = 1) (ψ6)
(r ≥ p+ j)→ (X [i+1](r) = 0) (ψ7)
(p ≤ r ≤ q)→ (ψ6 ∧ ψ7) (ψ8)
We now put formulas ψ4, ψ5 and ψ8 together to create one formula describing
the changes between configurations X [i] and X [i+1]:
(∀r ≤ |X|)(ψ4 ∧ ψ5 ∧ ψ8) (ψ9)
Finally, we combine formulas ψ2 and ψ3 with existential quantification to de-
scribe the existence of valid p and q:
(∃p < |X [i]|)(∃q ≤ |X [i]|)(ψ2 ∧ ψ3) (ψ10)
On the Complexity of Computing Winning Strategies 423
Thus, if we take the formula ψ9 for describing legal changes between configura-
tions and combine it with formula ψ1 for the existence of a playable square, as
well as ψ10 for the existence of values for p and q, we have the following formula
for “yields”:
(∃j ≤ NumOnes)(∃k ≤ NumZeros) [ψ1 ∧ ψ10 ∧ ψ9]
where
NumOnes = |X [i]| − numones(1,X [i])
NumZeroes = numones(1,X [i])
Having completed the formula Φ(X,n,m) we can now restate the existence of a
winning strategy for the first player in W11.
A winning strategy in chomp is just a function S : X → Y , which maps
configurations to configurations, so that whoever plays by S, wins. It is well
known that the first player has a winning strategy. How can this be stated?
As follows: ∀Y , such that Y has nm/2 many segments of length (n +m) each,
there exists an X as above, such that every other segment of X is Y , and
X [i+1] = S(Y [i]), and Φ(X,n,m), and also the first time the final configuration
occurs, it is in an even segment (so second player loses).
We want to say that given any configuration C as an initial configuration,
either player 1 or player 2 has a winning strategy. This can be stated as follows:
∀C∃S[WinP1(S, C) ∨WinP2(S, C)], (1)
where WinPi(S, C) (i ∈ {0, 1}, and i¯ will denote the value in the set {0, 1}−{i})
asserts that if player i plays by strategy S, then player i wins. This can be easily
state as follows: ∀Y (where Y is a sequence of moves of player i¯), if it is player
i’s move on configuration C, and player i plays C ′ = S(C), then player i¯ will
end up with the poisoned square. Note that (1) is a ΣB1 formula since the “∀C”
is bounded by the size of the chomp grid—we omit the bound for clarity, and
WinPi is a Σ
B
0 formula.
Theorem 1. W11 ⊢ ∀C∃S[WinP1(S, C) ∨WinP2(S, C)].
Proof. We prove it by induction on |C|, where we let |C| is the number of squares
in configuration C (in particular, if C consists of the poisoned square only, then
|C| = 1). The basis case is simple: if |C| = 1, then P1 loses, so in particular
∃SWinP2(S, C) holds (and it does not matter what S is, since P2 will not use it
anyways).
For the induction step, suppose that the claim holds for all C such that
|C| ≤ n. Consider some C ′ such that |C ′| = n+ 1. We want to show that
∃S[WinP1(S, C
′) ∨WinP2(S, C
′)]. (2)
Let C ′′ be the result of P1 making a move; since P1 must select some square,
it follows that |C ′′| < |C ′|, and so we can apply the induction hypothesis to
424 Michael Soltys and Craig Wilson
it; in other words, ∃SWinP1(S, C
′′) ∨ ∃SWinP2(S, C
′′) (we used the fact here
that ∃x(α ∨ β) is equivalent to ∃xα ∨ ∃xβ). If no matter what first move P1
makes (to obtain C ′′) it is always the case that ∃SWinP2(S, C
′′), then we can
conclude that ∃SWinP2(S, C
′). If, on the other hand, for some move of P1 it is
the case that ∃SWinP1(S, C
′′), then define (using comprehension) S′ to be the
same as S, except S′(C ′) = C ′′, and we can conclude that WinP1(S
′, C ′), and so
∃SWinP1(S, C
′). Therefore, in either case we obtain (2). ⊓⊔
Once we know that W11 proves (1), we can prove that P1 has a winning
strategy for the full rectangle.
Theorem 2. W11 ⊢ “C is full rectangle”→ ∃SWinP1(S, C).
Proof. Suppose that C is a full rectangle (an r × c chomp grid). We know that
either P1 or P2 has a winning strategy. Suppose that it is P2; so P2 (playing
by some S) will win no matter what first move P1 makes. So let P1 select the
top-right square (i.e., the square (r, c)), and let C ′ be the resulting configuration
(i.e., C ′ consists of all squares but it has a dent in the top-right square).
Now P2 makes a move according to S, i.e., P2 plays to obtain C
′′ = S(C ′).
Now observe that whoever starts playing from configuration C ′′ loses, i.e., P1
loses on C ′′, (by our assumptions). Also observe that P1 could have played to
obtain C ′′ directly, since no matter what C ′′ is, it must contain the top-right
corner, as it is the supremum of the grid. Thus, by taking the strategy S′ to
be S(C) = C ′′ and otherwise S′ = S, P1 can win (note that we have used
comprehension to define S′ from S). Contradiction; so P2 cannot have a winning
strategy, and so P1 must have a winning strategy. ⊓⊔
Using the Witnessing Theorem for W11 ([4]) we can conclude from theorem 2
that the winning stragegy S can be computed in PSPACE (which, of course, we
know from the previous section—and the fact that there is a simple way of com-
puting the strategy by quering repeatedly whether one exists). The interesting
question is: can we prove the same in a weaker theory than W11; in particular,
is V1 too much to ask for?
References
1. Gale, D.: A Curious Nim-Type Game. The American Mathematical Monthly 81(8)
(October 1974) 876–879
2. Papadimitriou, C.H.: Computational Complexity. Addison Wesley Longman (1995)
3. Sipser, M.: Introduction to the Theory of Computation. Second edn. Thomson
Course Technology (2006)
4. Skelley, A.: A Third-Order Bounded Arithmetic Theory for PSPACE. In: CSL.
(2004) 340–354
5. Cook, S., Nguyen, P.: Foundations of Proof Complexity: Bounded Arithmetic and
Propositional Translations. http://www.cs.toronto.edu/∼sacook/csc2429h/book.
Last checked April 29, 2008 (2006)
6. Herman, G.: Private communication (2006)
A Statistical Mechanical Interpretation of
Algorithmic Information Theory
Kohtaro Tadaki
Research and Development Initiative, Chuo University
1–13–27 Kasuga, Bunkyo-ku, Tokyo 112-8551, Japan.
tadaki@kc.chuo-u.ac.jp
Abstract. We develop a statistical mechanical interpretation of algo-
rithmic information theory by introducing the notion of thermodynamic
quantities, such as free energy, energy, statistical mechanical entropy,
and specific heat, into algorithmic information theory. We investigate
the properties of these quantities by means of program-size complexity
from the point of view of algorithmic randomness. It is then discovered
that, in the interpretation, the temperature plays a role as the com-
pression rate of the values of all these thermodynamic quantities, which
include the temperature itself. Reflecting this self-referential nature of
the compression rate of the temperature, we obtain fixed point theorems
on compression rate.
Key words: algorithmic information theory, algorithmic randomness,
Chaitin’s Ω, compression rate, fixed point theorem, statistical mechanics,
temperature
1 Introduction
Algorithmic information theory is a framework to apply information-theoretic
and probabilistic ideas to recursive function theory. One of the primary concepts
of algorithmic information theory is the program-size complexity (or Kolmogorov
complexity) H(s) of a finite binary string s, which is defined as the length of the
shortest binary program for the universal self-delimiting Turing machine U to
output s. By the definition, H(s) can be thought of as the information content of
the individual finite binary string s. In fact, algorithmic information theory has
precisely the formal properties of classical information theory (see Chaitin [3]).
The concept of program-size complexity plays a crucial role in characterizing
the randomness of a finite or infinite binary string. In [3] Chaitin introduced the
halting probability Ω as an example of random infinite binary string. His Ω is
defined as the probability that the universal self-delimiting Turing machine U
halts, and plays a central role in the metamathematical development of algorith-
mic information theory. The first n bits of the base-two expansion of Ω solves
the halting problem for a program of size not greater than n. By this property,
the base-two expansion of Ω is shown to be a random infinite binary string.
In [7, 8] we generalized Chaitin’s halting probability Ω to ΩD by
ΩD =
∑
p∈domU
2−
|p|
D , (1)
426 Kohtaro Tadaki
so that the degree of randomness of ΩD can be controlled by a real number D
with 0 < D ≤ 1. Here, domU denotes the set of all programs p for U . As D
becomes larger, the degree of randomness of ΩD increases. When D = 1, ΩD
becomes a random real number, i.e., Ω1 = Ω. The properties of ΩD and its
relations to self-similar sets were studied in Tadaki [7, 8].
Recently, Calude and Stay [2] pointed out a formal correspondence between
ΩD and a partition function in statistical mechanics. In statistical mechanics,
the partition function Z(T ) at temperature T is defined by
Z(T ) =
∑
x∈X
e−
Ex
kT ,
where X is a complete set of energy eigenstates of a statistical mechanical sys-
tem and Ex is the energy of an energy eigenstate x. The constant k is called the
Boltzmann Constant. The partition function Z(T ) is of particular importance in
equilibrium statistical mechanics. This is because all the thermodynamic quanti-
ties of the system can be expressed by using the partition function Z(T ), and the
knowledge of Z(T ) is sufficient to understand all the macroscopic properties of
the system. Calude and Stay [2] pointed out, in essence, that the partition func-
tion Z(T ) has the same form as ΩD by performing the following replacements
in Z(T ):
Replacements 1
(i) Replace the complete set X of energy eigenstates x by the set domU of all
programs p for U .
(ii) Replace the energy Ex of an energy eigenstate x by the length |p| of a program
p.
(iii) Set the Boltzmann Constant k to 1/ ln 2, where the ln denotes the natural
logarithm. uunionsq
In this paper, inspired by their suggestion above, we develop a statistical me-
chanical interpretation of algorithmic information theory, where ΩD appears as
a partition function.
Generally speaking, in order to give a statistical mechanical interpretation to
a framework which looks unrelated to statistical mechanics at first glance, it is
important to identify a microcanonical ensemble in the framework. Once we can
do so, we can easily develop an equilibrium statistical mechanics on the frame-
work according to the theoretical development of normal equilibrium statistical
mechanics. Here, the microcanonical ensemble is a certain sort of uniform proba-
bility distribution. In fact, in the work [9] we developed a statistical mechanical
interpretation of the noiseless source coding scheme in information theory by
identifying a microcanonical ensemble in the scheme. Then, in [9] the notions
in statistical mechanics such as statistical mechanical entropy, temperature, and
thermal equilibrium are translated into the context of noiseless source coding.
Thus, in order to develop a statistical mechanical interpretation of algorith-
mic information theory, it is appropriate to identify a microcanonical ensemble
in the framework of the theory. Note, however, that algorithmic information
A Statistical Mechanical Interpretation of Algorithmic Information Theory 427
theory is not a physical theory but a purely mathematical theory. Therefore,
in order to obtain significant results for the development of algorithmic infor-
mation theory itself, we have to develop a statistical mechanical interpretation
of algorithmic information theory in a mathematically rigorous manner, unlike
in normal statistical mechanics in physics where arguments are not necessarily
mathematically rigorous. A fully rigorous mathematical treatment of statistical
mechanics is already developed (see Ruelle [6]). At present, however, it would
not as yet seem to be an easy task to merge algorithmic information theory with
this mathematical treatment in a satisfactory manner.
On the other hand, if we do not stick to the mathematical strictness of an
argument and make an argument on the same level of mathematical strictness as
statistical mechanics in physics, we can develop a statistical mechanical interpre-
tation of algorithmic information theory while realizing a perfect correspondence
to normal statistical mechanics. In the physical argument, we can identify a mi-
crocanonical ensemble in algorithmic information theory in a similar manner
to [9], based on the probability measure which gives Chaitin’s Ω the meaning
of the halting probability actually.1 In consequence, for example, the statistical
mechanical meaning of ΩD is clarified.
In this paper, we develop a statistical mechanical interpretation of algorith-
mic information theory in a different way from the physical argument mentioned
above.2 We introduce the notion of thermodynamic quantities into algorithmic
information theory based on Replacements 1 above.
After the preliminary section on the mathematical notion needed in this pa-
per, in Section 3 we introduce the notion of the thermodynamic quantities at any
given fixed temperature T , such as partition function, free energy, energy, statis-
tical mechanical entropy, and specific heat, into algorithmic information theory
by performing Replacements 1 for the corresponding thermodynamic quantities
in statistical mechanics. These thermodynamic quantities in algorithmic infor-
mation theory are real numbers which depend only on the temperature T . We
prove that if the temperature T is a computable real number with 0 < T < 1
then, for each of these thermodynamic quantities, the compression rate by the
program-size complexity H is equal to T . Thus, the temperature T plays a role
as the compression rate of the thermodynamic quantities in this statistical me-
chanical interpretation of algorithmic information theory.
Among all thermodynamic quantities in thermodynamics, one of the most
typical thermodynamic quantities is temperature itself. Thus, based on the re-
sults of Section 3, the following question naturally arises: Can the compression
rate of the temperature T be equal to the temperature itself in the statistical
mechanical interpretation of algorithmic information theory ? This question is
rather self-referential. However, in Section 4 we answer it affirmatively by prov-
1 Due to the 10-page limit, we omit the detail of the physical argument in this paper.
It will be included in a full version of this paper, and is also available in Section 6 of
an extended and electronic version of this paper at URL: http://arxiv.org/abs/
0801.4194v1
2 We make an argument in a fully mathematically rigorous manner in this paper.
428 Kohtaro Tadaki
ing Theorem 9. One consequence of Theorem 9 has the following form: For every
T ∈ (0, 1), if ΩT =∑p∈domU 2− |p|T is a computable real number, then
lim
n→∞
H(Tn)
n
= T,
where Tn is the first n bits of the base-two expansion of T . This is just a fixed
point theorem on compression rate, which reflects the self-referential nature of
the question.
The works [7, 8] on ΩD might be regarded as an elaboration of the technique
used by Chaitin [3] to prove that Ω is random. The results of this paper may
be regarded as further elaborations of the technique. Due to the 10-page limit,
we omit most proofs. A full paper describing the details of the proofs is in
preparation.3
2 Preliminaries
We start with some notation about numbers and strings which will be used in
this paper. N = {0, 1, 2, 3, . . . } is the set of natural numbers, and N+ is the set
of positive integers. Q is the set of rational numbers, and R is the set of real
numbers. {0, 1}∗ = {λ, 0, 1, 00, 01, 10, 11, 000, 001, 010, . . . } is the set of finite
binary strings, where λ denotes the empty string. For any s ∈ {0, 1}∗, |s| is the
length of s. A subset S of {0, 1}∗ is called a prefix-free set if no string in S is a
prefix of another string in S. {0, 1}∞ is the set of infinite binary strings, where
an infinite binary string is infinite to the right but finite to the left. For any
α ∈ {0, 1}∞ and any n ∈ N+, αn is the prefix of α of length n. For any partial
function f , the domain of definition of f is denoted by dom f . We write “r.e.”
instead of “recursively enumerable.”
Normally, o(n) denotes any function f : N+ → R such that limn→∞ f(n)/n =
0. On the other hand, O(1) denotes any function g : N+ → R such that there is
C ∈ R with the property that |g(n)| ≤ C for all n ∈ N+.
Let T be an arbitrary real number. T mod 1 denotes T − bT c, where bT c is
the greatest integer less than or equal to T . Hence, T mod 1 ∈ [0, 1). We identify
a real number T with the infinite binary string α such that 0.α is the base-two
expansion of T mod 1 with infinitely many zeros. Thus, Tn denotes the first n
bits of the base-two expansion of T mod 1 with infinitely many zeros.
We say that a real number T is computable if there exists a total recursive
function f : N+ → Q such that |T − f(n)| < 1/n for all n ∈ N+. We say that
T is right-computable if there exists a total recursive function g : N+ → Q such
that T ≤ g(n) for all n ∈ N+ and limn→∞ g(n) = T . We say that T is left-
computable if −T is right-computable. It is then easy to see that, for any T ∈ R,
T is computable if and only if T is both right-computable and left-computable.
See e.g. Weihrauch [12] for the detail of the treatment of the computability of
real numbers and real functions on a discrete set.
3 The details of the proofs are also available in an extended and electronic version of
this paper at URL: http://arxiv.org/abs/0801.4194v1
A Statistical Mechanical Interpretation of Algorithmic Information Theory 429
2.1 Algorithmic information theory
In the following we concisely review some definitions and results of algorithmic
information theory [3, 4]. A computer is a partial recursive function C : {0, 1}∗ →
{0, 1}∗ such that domC is a prefix-free set. For each computer C and each
s ∈ {0, 1}∗, HC(s) is defined by HC(s) = min
{ |p| ∣∣ p ∈ {0, 1}∗ & C(p) = s}. A
computer U is said to be optimal if for each computer C there exists a constant
sim(C) with the following property; if C(p) is defined, then there is a p′ for
which U(p′) = C(p) and |p′| ≤ |p|+sim(C). It is easy to see that there exists an
optimal computer. Note that the class of optimal computers equals to the class
of functions which are computed by universal self-delimiting Turing machines
(see Chaitin [3] for the detail). We choose a particular optimal computer U as
the standard one for use, and define H(s) as HU (s), which is referred to as the
program-size complexity of s or the Kolmogorov complexity of s.
Chaitin’s halting probability Ω is defined by
Ω =
∑
p∈domU
2−|p|.
For any α ∈ {0, 1}∞, we say that α is weakly Chaitin random if there exists c ∈ N
such that n− c ≤ H(αn) for all n ∈ N+ [3, 4]. Then Chaitin [3] showed that Ω is
weakly Chaitin random. For any α ∈ {0, 1}∞, we say that α is Chaitin random
if limn→∞H(αn) − n = ∞ [3, 4]. It is then shown that, for any α ∈ {0, 1}∞, α
is weakly Chaitin random if and only if α is Chaitin random (see Chaitin [4] for
the proof and historical detail). Thus Ω is Chaitin random.
In the works [7, 8], we generalized the notion of the randomness of an infinite
binary string so that the degree of the randomness can be characterized by a
real number D with 0 < D ≤ 1 as follows.
Definition 1 (weak Chaitin D-randomness and D-compressibility). Let
D ∈ R with D ≥ 0, and let α ∈ {0, 1}∞. We say that α is weakly Chaitin
D-random if there exists c ∈ N such that Dn − c ≤ H(αn) for all n ∈ N+.
We say that α is D-compressible if H(αn) ≤ Dn + o(n), which is equivalent to
limn→∞H(αn)/n ≤ D. uunionsq
In the case of D = 1, the weak Chaitin D-randomness results in the weak
Chaitin randomness. For any D ∈ [0, 1] and any α ∈ {0, 1}∞, if α is weakly
Chaitin D-random and D-compressible, then
lim
n→∞
H(αn)
n
= D. (2)
Hereafter the left-hand side of (2) is referred to as the compression rate of an
infinite binary string α in general. Note, however, that (2) does not necessarily
imply that α is weakly Chaitin D-random.
In the works [7, 8], we generalized Chaitin’s halting probability Ω to ΩD by
(1) for any real number D > 0. Thus, Ω = Ω1. If 0 < D ≤ 1, then ΩD converges
and 0 < ΩD < 1, since ΩD ≤ Ω < 1.
430 Kohtaro Tadaki
Theorem 2 (Tadaki [7, 8]). Let D ∈ R.
(i) If 0 < D ≤ 1 and D is computable, then ΩD is weakly Chaitin D-random
and D-compressible.
(ii) If 1 < D, then ΩD diverges to ∞. uunionsq
Definition 2 (Chaitin D-randomness, Tadaki [7, 8]). Let D ∈ R with D ≥
0, and let α ∈ {0, 1}∞. We say that α is Chaitin D-random if limn→∞H(αn)−
Dn =∞. uunionsq
In the case of D = 1, the Chaitin D-randomness results in the Chaitin ran-
domness. Obviously, for any D ∈ [0, 1] and any α ∈ {0, 1}∞, if α is Chaitin
D-random, then α is weakly Chaitin D-random. However, in 2005 Reimann
and Stephan [5] showed that, in the case of D < 1, the converse does not nec-
essarily hold. This contrasts with the equivalence between the weakly Chaitin
randomness and the Chaitin randomness, each of which corresponds to the case
of D = 1.
For each real numbers Q > 0 and D > 0, we define W (Q,D) by
W (Q,D) =
∑
p∈domU
|p|Q 2− |p|D .
As the first result of this paper, we can show the following theorem.
Theorem 3. Let Q and D be positive real numbers.
(i) If Q and D are computable and 0 < D < 1, then W (Q,D) converges to a
left-computable real number which is Chaitin D-random and D-compressible.
(ii) If 1 ≤ D, then W (Q,D) diverges to ∞. uunionsq
Thus, we see that the weak Chaitin D-randomness in Theorem 2 is replaced
by the Chaitin D-randomness in Theorem 3 in exchange for the divergence at
D = 1.
3 Temperature as a compression rate
In this section we introduce the notion of thermodynamic quantities such as
partition function, free energy, energy, statistical mechanical entropy, and spe-
cific heat, into algorithmic information theory by performing Replacements 1
for the corresponding thermodynamic quantities in statistical mechanics.4 We
investigate their convergence and the degree of randomness. For that purpose,
we first choose a particular enumeration q1, q2, q3, . . . of the countably infinite
set domU as the standard one for use throughout this section.5
4 For the thermodynamic quantities in statistical mechanics, see Chapter 16 of [1]
and Chapter 2 of [11]. To be precise, the partition function is not a thermodynamic
quantity but a statistical mechanical quantity.
5 The enumeration {qi} is quite arbitrary and therefore we do not, ever, require {qi}
to be a recursive enumeration of domU .
A Statistical Mechanical Interpretation of Algorithmic Information Theory 431
In statistical mechanics, the partition function Zsm(T ) at temperature T is
given by
Zsm(T ) =
∑
x∈X
e−
Ex
kT , (3)
Motivated by the formula (3) and taking into account Replacements 1, we in-
troduce the notion of partition function into algorithmic information theory as
follows.
Definition 3 (partition function). For each n ∈ N+ and each real number
T > 0, we define Zn(T ) by
Zn(T ) =
n∑
i=1
2−
|qi|
T .
Then, the partition function Z(T ) is defined by Z(T ) = limn→∞ Zn(T ), for each
T > 0. uunionsq
Since Z(T ) = ΩT , we restate Theorem 2 as in the following form.
Theorem 4 (Tadaki [7, 8]). Let T ∈ R.
(i) If 0 < T ≤ 1 and T is computable, then Z(T ) converges to a left-computable
real number which is weakly Chaitin T -random and T -compressible.
(ii) If 1 < T ,then Z(T ) diverges to ∞. uunionsq
In statistical mechanics, the free energy Fsm(T ) at temperature T is given
by
Fsm(T ) = −kT lnZsm(T ), (4)
where Zsm(T ) is given by (3). Motivated by the formula (4) and taking into
account Replacements 1, we introduce the notion of free energy into algorithmic
information theory as follows.
Definition 4 (free energy). For each n ∈ N+ and each real number T > 0, we
define Fn(T ) by Fn(T ) = −T log2 Zn(T ). Then, for each T > 0, the free energy
F (T ) is defined by F (T ) = limn→∞ Fn(T ). uunionsq
Theorem 5. Let T ∈ R.
(i) If 0 < T ≤ 1 and T is computable, then F (T ) converges to a right-computable
real number which is weakly Chaitin T -random and T -compressible.
(ii) If 1 < T ,then F (T ) diverges to −∞. uunionsq
In statistical mechanics, the energy Esm(T ) at temperature T is given by
Esm(T ) =
1
Zsm(T )
∑
x∈X
Exe
−ExkT , (5)
where Zsm(T ) is given by (3). Motivated by the formula (5) and taking into
account Replacements 1, we introduce the notion of energy into algorithmic
information theory as follows.
432 Kohtaro Tadaki
Definition 5 (energy). For each n ∈ N+ and each real number T > 0, we
define En(T ) by
En(T ) =
1
Zn(T )
n∑
i=1
|qi| 2−
|qi|
T .
Then, for each T > 0, the energy E(T ) is defined by E(T ) = limn→∞En(T ). uunionsq
Theorem 6. Let T ∈ R.
(i) If 0 < T < 1 and T is computable, then E(T ) converges to a left-computable
real number which is Chaitin T -random and T -compressible.
(ii) If 1 ≤ T , then E(T ) diverges to ∞. uunionsq
In statistical mechanics, the entropy Ssm(T ) at temperature T is given by
Ssm(T ) =
1
T
Esm(T ) + k lnZsm(T ), (6)
where Zsm(T ) and Esm(T ) are given by (3) and (5), respectively. Motivated by
the formula (6) and taking into account Replacements 1, we introduce the notion
of statistical mechanical entropy into algorithmic information theory as follows.
Definition 6 (statistical mechanical entropy). For each n ∈ N+ and each
real number T > 0, we define Sn(T ) by Sn(T ) = 1T En(T ) + log2 Zn(T ). Then,
for each T > 0, the statistical mechanical entropy S(T ) is defined by S(T ) =
limn→∞ Sn(T ). uunionsq
Theorem 7. Let T ∈ R.
(i) If 0 < T < 1 and T is computable, then S(T ) converges to a left-computable
real number which is Chaitin T -random and T -compressible.
(ii) If 1 ≤ T , then S(T ) diverges to ∞. uunionsq
Finally, in statistical mechanics, the specific heat Csm(T ) at temperature T
is given by
Csm(T ) =
d
dT
Esm(T ), (7)
where Esm(T ) is given by (5). Motivated by this formula (7), we introduce the
notion of specific heat into algorithmic information theory as follows.
Definition 7 (specific heat). For each n ∈ N+ and each real number T > 0,
we define Cn(T ) by Cn(T ) = E′n(T ), where E
′
n(T ) is the derived function of
En(T ). Then, for each T > 0, the specific heat C(T ) is defined by C(T ) =
limn→∞ Cn(T ). uunionsq
Theorem 8. Let T ∈ R.
(i) If 0 < T < 1 and T is computable, then C(T ) converges to a left-computable
real number which is Chaitin T -random and T -compressible, and moreover
C(T ) = E′(T ) where E′(T ) is the derived function of E(T ).
(ii) If T = 1, then C(T ) diverges to ∞. uunionsq
A Statistical Mechanical Interpretation of Algorithmic Information Theory 433
Thus, the theorems in this section show that the temperature T plays a role
as the compression rate for all the thermodynamic quantities introduced into
algorithmic information theory in this section.
These theorems also show that the values of the thermodynamic quantities:
partition function, free energy, energy, and statistical mechanical entropy diverge
in the case of T > 1. This phenomenon might be regarded as some sort of phase
transition in statistical mechanics.6
4 Fixed point theorems on compression rate
In this section, we show the following theorem and its variant.
Theorem 9 (fixed point theorem on compression rate). For every T ∈
(0, 1), if Z(T ) is a computable real number, then the following hold:
(i) T is right-computable and not left-computable.
(ii) T is weakly Chaitin T -random and T -compressible.
(iii) limn→∞H(Tn)/n = T . uunionsq
Theorem 9 follows immediately from the following three theorems.
Theorem 10. For every T ∈ (0, 1), if Z(T ) is a right-computable real number,
then T is weakly Chaitin T -random. uunionsq
Theorem 11. For every T ∈ (0, 1), if Z(T ) is a right-computable real number,
then T is also a right-computable real number. uunionsq
Theorem 12. For every T ∈ (0, 1), if Z(T ) is a left-computable real number
and T is a right-computable real number, then T is T -compressible. uunionsq
Theorem 9 is just a fixed point theorem on compression rate, where the
computability of the value Z(T ) gives a sufficient condition for a real number
T ∈ (0, 1) to be a fixed point on compression rate. Note that Z(T ) is a strictly
increasing continuous function on (0, 1). In fact, Tadaki [7, 8] showed that Z(T )
is a function of class C∞ on (0, 1). Thus, since the set of all computable real
numbers is dense in R, we have the following for this sufficient condition.
Theorem 13. The set {T ∈ (0, 1) | Z(T ) is computable } is dense in [0, 1]. uunionsq
We thus have the following corollary of Theorem 9.
Corollary 1. The set {T ∈ (0, 1) | limn→∞H(Tn)/n = T} is dense in [0, 1]. uunionsq
From the point of view of the statistical mechanical interpretation introduced
in the previous section, Theorem 9 shows that the compression rate of temper-
ature is equal to the temperature itself. Thus, Theorem 9 further confirms the
role of temperature as the compression rate, which is observed in the previous
section.
In a similar manner to the proof of Theorem 9, we can prove another version
of a fixed point theorem on compression rate as follows. Here, the weak Chaitin
T -randomness is replaced by the Chaitin T -randomness.
6 It is still open whether C(T ) diverges or not in the case of T > 1.
434 Kohtaro Tadaki
Theorem 14 (fixed point theorem on compression rate II). Let Q be
a computable real number with Q > 0. For every T ∈ (0, 1), if W (Q,T ) is a
computable real number, then the following hold:
(i) T is right-computable and not left-computable.
(ii) T is Chaitin T -random and T -compressible. uunionsq
For the sufficient condition of Theorem 14, in a similar manner to the case
of Theorem 9, we can show that, for every Q > 0, the set {T ∈ (0, 1) |
W (Q,T ) is computable } is dense in [0, 1].
Acknowledgments
This work was supported both by KAKENHI, Grant-in-Aid for Scientific Re-
search (C) (20540134) and by SCOPE (Strategic Information and Communica-
tions R&D Promotion Programme) from the Ministry of Internal Affairs and
Communications of Japan.
References
1. H. B. Callen, Thermodynamics and an Introduction to Thermostatistics, 2nd ed.
John Wiley & Sons, Inc., Singapore, 1985.
2. C. S. Calude and M. A. Stay, “Natural halting probabilities, partial randomness,
and zeta functions,” Inform. and Comput., vol. 204, pp. 1718–1739, 2006.
3. G. J. Chaitin, “A theory of program size formally identical to information theory,”
J. Assoc. Comput. Mach., vol. 22, pp. 329–340, 1975.
4. G. J. Chaitin, Algorithmic Information Theory. Cambridge University Press, Cam-
bridge, 1987.
5. J. Reimann and F. Stephan, On hierarchies of randomness tests. Proceedings of
the 9th Asian Logic Conference, World Scientific Publishing, August 16-19, 2005,
Novosibirsk, Russia.
6. D. Ruelle, Statistical Mechanics, Rigorous Results, 3rd ed. Imperial College Press
and World Scientific Publishing Co. Pte. Ltd., Singapore, 1999.
7. K. Tadaki, Algorithmic information theory and fractal sets. Proceedings of 1999
Workshop on Information-Based Induction Sciences (IBIS’99), pp. 105–110, August
26-27, 1999, Syuzenji, Shizuoka, Japan. In Japanese.
8. K. Tadaki, “A generalization of Chaitin’s halting probability Ω and halting self-
similar sets,” Hokkaido Math. J., vol. 31, pp. 219–253, 2002. Electronic Version
Available: http://arxiv.org/abs/nlin/0212001v1
9. K. Tadaki, A statistical mechanical interpretation of instantaneous codes. Proceed-
ings of 2007 IEEE International Symposium on Information Theory (ISIT2007),
pp. 1906–1910, June 24-29, 2007, Nice, France.
10. K. Tadaki, The Tsallis entropy and the Shannon entropy of a universal proba-
bility. To appear in the Proceedings of 2008 IEEE International Symposium on
Information Theory (ISIT2008), July 6-11, 2008, Toronto, Canada.
11. M. Toda, R. Kubo, and N. Saitoˆ, Statistical Physics I. Equilibrium Statistical Me-
chanics, 2nd ed. Springer, Berlin, 1992.
12. K. Weihrauch, Computable Analysis. Springer-Verlag, Berlin, 2000.
Solving Tripartite Matching by Interval-valued
Computation in Polynomial Time⋆
A´kos Tajti and Benedek Nagy⋆⋆
Faculty of Informatics, University of Debrecen
Hungary H-4010 Debrecen, P.O. Box 12
akos.tajti@gmail.com, nbenedek@inf.unideb.hu
Abstract. New computing paradigms are usually legitimated by show-
ing their computing power. Hard, usually NP-complete problems are
shown to be solved in efficient way. One of the well known NP-complete
problem is Tripartite Matching. In this paper this problem is solved by
a polynomial Interval-valued computation.
Key words: new computing paradigms, interval-valued computing,
NP-complete problems
1 Introduction
There are several problems shown to be computationally hard. Karp presented
several computational problems that are proved to be NP-complete ([Karp 1972]).
These intractable problems cannot be solved efficiently by the traditional com-
puting way (deterministic Turing machines or similar devices) unless P=NP. The
problem P=NP is one of the most important challenges of the (Theoretical) Com-
puter Science. The fact that there is no known method to solve efficiently these
problems by traditional computations leads to the phenomenon of developing
several new paradigms of computation. Usually in these new paradigms one or
more of the basic properties of the classical computing methodology are dropped.
The traditional computation is sequential, uses discrete time-steps, the amount of
data that can be processed in a step is finitely limited, etc. ([Tanenbaum 1984])
DNA and membrane computing, inspired by molecular biology, drop the se-
quential mode of computation. In his famous paper ([Adleman 1994]) Adleman
showed a method to solve the NP-complete Hamiltonian Path problem in polyno-
mial time by DNA computation. Several various methods are known to solve var-
ious NP-complete problems efficiently by membrane computing ([Paun 2002]), as
well. The ways of parallelism of these (bio)computations are essentially different
([Loos–Nagy 2007]).
Interval-valued computation is another computing paradigm initiated by B.
Nagy in [Nagy 2005b,Nagy 2005c,Nagy–Va´lyi 2008] based on an interval-valued
⋆ The work is party supported by the O¨veges programme of NKTH, Hungary.
⋆⋆ Corresponding author
436 A´kos Tajti and Benedek Nagy
fuzzy logic ([Nagy 2005a]) instead of the the binary logic of traditional archi-
tectures ([Tanenbaum 1984]). In this paradigm the amount of data that can be
processed at a step has not restriction. The paradigm keeps some features of the
traditional computations and of classical computers. Classical computers work on
finite sequences of bits, called bytes or memory cells. Instead of finite sequences
of bits the interval-valued computations work on specific subsets of the interval
[0, 1), more specifically, on finite unions of [)-type subintervals. In this way it is
a straightforward extension of the classical paradigm. It also can be considered
as a 1-dimensional version of the optical computing [Woods–Naughton 2005].
In this paper, after the formulation of the problem of tripartite matching
(Section 2) and a formulation of the computing model (Section 3), a method
is shown that solves the problem in the frame of the model in an efficient way
(Section 4). An example is also presented (Section 5), finally Conclusions close
the paper.
2 Formulation of the Problem
Now the formal definition of the proposed problem is presented (based on
[Papadimitriou 1994]).
Definition 1. ( The Tripartite Matching problem )
Let B = {b1, ..., bm}, G = {g1, ..., gm} and H = {h1, ..., hm} be three disjoint
sets such that | B |=| G |=| H |= m (m > 1) and let T ⊆ B × G × H be a
relation. The task is to find a relation U ⊆ T containing each element of B,
G and H exactly once. In other words no two distinct triplets in T contain the
same bi ∈ B, gj ∈ G and hk ∈ H, moreover every element bi ∈ B, gj ∈ G, and
hk ∈ H appears in one triplet of T .
One generalization of the problem is the simple d-partite matching problem.
An instance of this problem consists of the sets A1, A2, ..., Ad (d ≥ 3), and the
T ⊆ A1×A2× ...×Ad relation. The task is (similarly to Tripartite Matching) to
find a relation U ⊆ T containing each element of the sets Ai(i = 1, ..., d) exactly
once. Further generalizing the problem we get the d-partite matching problem
in which a weight ca is associated with every d-tuple and the task is to find a
matching with the minimal cost.
Our approach can be extended to solve these more general problems but in
the remaining of this paper we are only concerned with the original Tripartite
Matching problem.
The d-partite matching problem is also important in – mainly biological –
applications. In [Singh–Xu–Berger 2008] the authors used the problem in an
algorithm for global alignment of multiple protein-protein inter-action (PPI)
networks. This is the first known algorithm about the problem. A new computa-
tional method is invented for recognition of binding patterns common to a set of
protein structures in [Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005a]. The d-
partite matching is also used in that algorithm. By the same authors a method
Solving Tripartite Matching 437
of recognition of a set of common physico-chemical properties is presented in
[Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005b].
In 1972 Karp proved that the Tripartite Matching problem is NP-complete
([Karp 1972], see also [Papadimitriou 1994]). Since there are not known efficient
algorithms to solve this problem in traditional computer, in our approach a new
computing paradigm will be used. In the next section we briefly describe the
interval-valued computing.
3 A Brief Description of Interval-valued Computations
In this section we briefly describe the model. For full description we refer to
[Nagy 2005b] and [Nagy–Va´lyi 2008].
First we present an inductive definition of the interval-values.
Base of induction:
Every interval of the form [a, b) with 0 ≤ a < b ≤ 1 is an atomic interval.
The unit interval [0, 1) is an atomic interval. The interval-values can be consid-
ered as sets of real numbers (points) of the unit interval [0, 1).
The atomic intervals are interval-values.
Induction steps:
If A and B are interval-values, then A ∪ B and A \ B are interval-values (the
operations union and difference can be applied on interval-values as sets).
Every interval-value can be obtained by finitely many applications of the induc-
tive steps from some atomic interval-values.
In this way, the interval-values are finite unions of atomic intervals. The
empty interval-value is also allowed since it can be obtained as the difference of
any atomic-interval and the unit interval.
The characteristic function of the interval-values A is a function [0, 1) →
{0, 1} giving value 1 at exactly those points of the unit interval which belong to
A.
The operations defined on these interval-values are motivated by operations
of the traditional computers. There are logical and non-logical operators.
The logical operators are naturally extended for interval-values. They work
on the values of the characteristic functions. Table 1 shows the interval opera-
tors corresponding to some well known logical operators. All Boolean operators
can be defined and, as it was shown in [Nagy 2005b], the set of interval-values
are closed under these operations. Usually the operations negation, disjunction,
conjunction and implication are defined. By the expressiveness of the operators
using each other, it is sufficient to define only nand or the Sheffer stroke. These
operators are well-known, they have theoretical importance, the number of de-
fined operators on interval-values can be reduced by using only one of them as
logical operator.
Besides in this paper we need mostly the logical operators, we mention here
that some non-logical operators such as shift and product are also defined in
438 A´kos Tajti and Benedek Nagy
Table 1. Definition of the logical operators on interval-values
(A and B are interval-values)
Operator Negation Disjunction Conjunction Implication
Sign ¬A A ∨B A ∧B A ⊃ B
Value [0, 1) \A A ∪B A ∩B ¬A ∪B
[Nagy 2005b] and used also in [Nagy–Va´lyi 2008]. For the completeness of this
paper we recall the definition of product:
Let A =
k⋃
i=1
[ai1 , ai2) and B =
l⋃
j=1
[bj1 , bj2) be two interval values with k
and l atomic intervals, respectively. The value of C = A ∗ B is defined as fol-
lows: the number of its atomic intervals is k · l. Their indices are given in the
form (i, j) from (1, 1) to (k, l). Then the atomic interval with index (i, j) is
[ ai1 + bj1(ai2 − ai1), ai1 + bj2(ai2 − ai1) ).
An interval-valued computation is a deterministic sequence of operator ap-
plications starting by some atomic intervals. The operands of the operators can
be any initially defined atomic interval and any previously computed interval-
value. In decision problems one may check if the interval-value obtained by the
computation is the empty interval-value. Note, that since the negation is al-
lowed operation, it has the same efficiency than the checking whether the result
is the unit interval [0, 1). The complexity is measured by the length of the com-
putation (i.e., the length of the computational sequence: the sum of the initial
atomic intervals and operator applications). We note here, that the interval-
valued computations proved to be very powerful using only 1 initially defined
atomic interval, the
[
0, 1
2
)
, for details see [Nagy–Va´lyi 2008].
4 Theory: Solution in Polynomial Time
NP-completeness means that we cannot solve the problem in polynomial time
by computers with traditional architecture, unless P=NP. At the same time, the
paradigm of interval-valued computations makes it possible to execute the steps
of the algorithms in a parallel way. More precisely, the real power comes from
the fact that each logical operation is performed (in parallel) on every interval
of the interval-value, independent of the number of these intervals. Due to this
property of this computing architecture we can construct an algorithm to solve
the Tripartite Matching problem in polynomial time using the inner parallelism.
For this reason, we must build an equivalent specification of the problem
using interval-values instead of triplets. To do this, we will engage the theory of
logic: we will transform the elements of set theory to elements of logic. We know
the solution of the SAT problem on interval computers ([Nagy 2005b]) and so,
if we reduce our problem to the SAT problem, then we will be able to solve it
efficiently.
Solving Tripartite Matching 439
4.1 Reduction to the SAT problem
We will present a method of constructing a logical formula for each instance of
the Tripartite Matching problem. These logical formulas are satisfiable if and
only if the original problem instance is solvable; moreover if a solution exists, we
can use these formulas to construct a solution.
We define the Xbgh Boolean variable as the following:
Xbgh =
{
true, if the (b, g, h) triplet is in the set U
false, otherwise,
where b ∈ B, g ∈ G and h ∈ H. This assignment can be done efficiently. (In a
cubic complexity of m.)
Next, using Definition 1, the following logical formulas are constructed using
these variables:
EB =


∧
b∈B


∨
g, h
(b, g, h) ∈ T


Xbgh ∧
∧
g′, h′
(b, g′, h′) ∈ T
g 6= g′ ∧ h 6= h′
¬Xbg′h′




,
if every
b ∈ B
appears in a
triplet of T ;
false, otherwise.
EG =


∧
g∈G


∨
b, h
(b, g, h) ∈ T


Xbgh ∧
∧
b′, h′
(b′, g, h′) ∈ T
b 6= b′ ∧ h 6= h′
¬Xb′gh′




,
if every
g ∈ G
appears in a
triplet of T ;
false, otherwise.
EH =


∧
h∈H


∨
b, g
(b, g, h) ∈ T


Xbgh ∧
∧
b′, g′
(b′, g′, h) ∈ T
b 6= b′ ∧ g 6= g′
¬Xbg′h′




,
if every
h ∈ H
appears in a
triplet of T ;
false, otherwise.
440 A´kos Tajti and Benedek Nagy
The signs of conjunction and disjunction can be used as n-ary operators. Since
both the conjunction and disjunction are associative operations, their n-ary
forms are well-defined. For this reason it is allowed to use these operations as
∧
and
∨
in the way we used above. These formulas also can be built in a polynomial
complexity.
The interpretation of the above formulas is as follows:
1. EB is true if and only if there are no two distinct triplets in the set U such
that the two triplets are equal in their first elements and b ∈ B is included
in a triplet.
To show this let b ∈ B, g1, g2 ∈ G and h1, h2 ∈ H such that g1 6= g2 or
h1 6= h2. Suppose we have (b, g1, h1), (b, g2, h2) ∈ U , where U is a solution of
the problem. In each of the m direct subformulas of EB there is exactly one
Boolean variable which is not negative (i.e., not prefixed by the operator ¬).
So, if both (b, g1, h1) and (b, g2, h2) are in U , then both Xbg1h1 and Xbg2h2 are
true and so the whole subformula is false, thus EB is false. (The presented
atomic conjunctions are true if and only if exactly one of the used Xbgh’s
has value true.)
On the other side, since EB is a conjunction, it can only be false if one of
its conjunctional subformulas is false or there exists a b ∈ B such that it
doesn’t appear in any triplets of T . In the first case, from the definition of
EB it follows that such a subformula is false if and only if there exists a
b ∈ B such that (b, g′, h′) and (b, g′′, h′′) (g′, g′′ ∈ G, h′, h′′ ∈ H, g′ 6= g′′ or
h′ 6= h′′) are in U , that is, the problem instance has no solution.
2. EG is true if and only if there are no two triplets in the set U such that the
two triplets are equal in their second elements and every g ∈ G is included
in a triplet.
The correctness of this statement can be seen similarly as above.
3. EH is true if and only if there are no two triplets in the set U such that the
two triplets are equal in their third elements and every h ∈ H is included in
a triplet.
This statement can be proved in the same way as in the previous cases.
If the above formulas are satisfiable in the same evaluation, then the actual
problem instance has a solution. That is, the logical formula assigned to the
instance is the following:
EB ∧ EG ∧ EH (1)
If the formula (1) is satisfiable, then the problem is solvable and a solution
can be extracted from the satisfying evaluation.
The formula can be built in complexity O(m5).
4.2 Constructing a solution
The solution can be divided into two phases. In the first phase we construct a
logical formula and check if it is satisfiable. If it is satisfiable, then in the next
phase we can construct a solution.
Solving Tripartite Matching 441
Solution to SAT The solution of the SAT problem using interval-valued com-
putations can be found in [Nagy 2005b]. We recall the main steps.
Let n be the number of the Boolean variables in the formula. Let the interval-
value assigned to the i-th variable be given in the following form
Ai =
2
i−1
−1⋃
j=0
[
2j
2i
,
2j + 1
2i
)
, 1 ≤ i ≤ n.
These values can be computed in a linear way (with respect to number of vari-
ables). One can easily construct the above interval-values, for instance, using the
product operator:
A1 =
[
0,
1
2
)
and Ai+1 = (Ai ∗A1) ∨ (¬Ai ∗A1).
After constructing the n interval-values the logical expression (the given for-
mula) must be evaluated using these interval-values and the definition of the
Boolean operators on the interval-values. This is also a linear computations
(with respect to the length of the formula). If the result interval-value is non-
empty, then the formula is satisfiable. Every non-empty interval-value contains
a point x ∈ [0, 1). One can find a satisfying evaluation by evaluating the logical
variables according to the characteristic function of their interval-values at x.
Constructing the solution to Tripartite Matching Through the descrip-
tion of the algorithm we are referring to the notion of characteristic functions of
interval-values.
If the actual problem instance is solvable, then the following algorithm builds
a solution:
1. let U be the empty set.
2. let x be a point of the result interval-value (which satisfies all the formulas)
where its characteristic function is true.
3. for all Xbgh: if the characteristic function of the interval assigned to Xbgh on
x is true (i.e., cbgh(x) = true, where cbgh is the characteristic function of the
appropriate interval value) then U = U ∪ (b, g, h), b ∈ B, g ∈ G and h ∈ H.
The set U is a solution of the Tripartite Matching problem instance.
5 Example
Let the sets be the following:
B = {b1, b2}, G = {g1, g2} and H = {h1, h2},
and define the relation T on these sets as follows:
442 A´kos Tajti and Benedek Nagy
T ⊆ B ×G×H
and
T = {(b1, g1, h1), (b1, g1, h2), (b2, g2, h2), (b1, g2, h2), (b2, g1, h2)}
Now we define five Boolean variables, Xb1g1h1 ,Xb1g1h2 ,Xb2g2h2 , Xb1g2h2 and
Xb2g1h2 , one for each triplet in T . For each variable we construct an appropriate
interval-value. These values are the following:
Xb1g1h1
Xb1g1h2
Xb2g2h2
Xb1g2h2
and
Xb2g1h2
We build the logical formulas:
EB = ( (Xb1g1h1 ∧ ¬Xb1g1h2 ∧ ¬Xb1g2h2)∨
∨(¬Xb1g1h1 ∧Xb1g1h2 ∧ ¬Xb1g2h2)∨
∨(¬Xb1g1h1 ∧ ¬Xb1g1h2 ∧Xb1g2h2) ) ∧
∧( (Xb2g2h2 ∧ ¬Xb2g1h2)∨
∨(¬Xb2g2h2 ∧Xb2g1h2) )
EG = ( (Xb1g1h1 ∧ ¬Xb1g1h2 ∧ ¬Xb2g1h2)∨
∨(¬Xb1g1h1 ∧Xb1g1h2 ∧ ¬Xb2g1h2)∨
∨(¬Xb1g1h1 ∧ ¬Xb1g1h2 ∧Xb2g1h2) ) ∧
∧( (Xb2g2h2 ∧ ¬Xb1g2h2)∨
∨(¬Xb2g2h2 ∧Xb1g2h2) )
EH = Xb1g1h1∧
∧( (Xb1g1h2 ∧ ¬Xb1g2h2 ∧ ¬Xb2g2h2 ∧ ¬Xb2g1h2)∨
∨(¬Xb1g1h2 ∧Xb1g2h2 ∧ ¬Xb2g2h2 ∧ ¬Xb2g1h2)∨
∨(¬Xb1g1h2 ∧ ¬Xb1g2h2 ∧Xb2g2h2 ∧ ¬Xb2g1h2)∨
∨(¬Xb1g1h2 ∧ ¬Xb1g2h2 ∧ ¬Xb2g2h2 ∧Xb2g1h2) )
Based on the interval-values assigned to the Boolean variables, we calculate
the interval-values corresponding to the formulas EB , EG and EH .
For example, to calculate the interval-value of EB we calculate
Solving Tripartite Matching 443
(Xb1g1h1 ∧ ¬Xb1g1h2 ∧ ¬Xb1g2h2)
then compute
¬Xb1g1h1 ∧Xb1g1h2 ∧ ¬Xb1g2h2
and calculate
¬Xb1g1h1 ∧ ¬Xb1g1h2 ∧Xb1g2h2
Now, if we apply disjunction on the above values, then we get first part of
EB :
Then, as above, we compute the second part of the formula EB :
and applying conjunction on these two subformulas, EB is obtained:
Similarly, the computation of EG will result:
and for EH we get:
Now, by computing EB ∧ EG ∧ EH the result is:
As we can see the formula is satisfiable. Thus, there exists a set U ⊆ T with
the properties mentioned in Definition 1. Evaluating the variables with the result
interval-value will give us the following:
Xb1g1h1 = true, Xb2g2h2 = true, Xb1g1h2 = false Xb1g2h2 = false
and Xb2g1h2 = false
Therefore the set U is the following:
U = {(b1, g1, h1), (b2, g2, h2)}
One can easily check that U is a solution of the problem.
6 Conclusions
It was known ([Nagy 2005b]) that SAT can be solved in a linear interval-valued
computation. Since the computing model is based on logic, it was natural to
solve problems with strong relation to logic. In this paper another NP-complete
problem was considered; the Tripartite Matching proved to be solvable by an
algorithm of a polynomial length in the frame of interval-valued computation.
This paper also proves that not only logical problems can be solved efficiently
444 A´kos Tajti and Benedek Nagy
by the interval-valued computations. The method presented here is easily ex-
tendible to solve the simple d-partite matching problem. The solution will also
be polynomial for any value of d ∈ N: O(m2d−1).
We note here that by the method presented in [Jackson–Sheridan 2005] the
logical formulas can be transformed to conjunctive normal form in complexity
O(m3), in this way polynomial solution exists to the Tripartite Matching prob-
lem in other computing paradigms in which the SAT of formulas of conjunctive
normal form is solvable in polynomial time.
References
[Adleman 1994] Adleman, L.: Molecular Computation of Solutions To Combinatorial
Problems, Science 266 (1994) 1021–1024.
[Jackson–Sheridan 2005] Jackson, P. and Sheridan, D.: Clause Form Conversions for
Boolean Circuits. in: SAT 2004, LNCS 3542, Springer, (2005) pp. 183–198.
[Karp 1972] Karp, R.: Complexity of Computer Computations. PlenumPress, New-
York, (1972).
[Loos–Nagy 2007] Loos, R. and Nagy, B.: Parallelism in DNA and Membrane Comput-
ing, in: “Computability in Europe 2007: Computation and Logic in the Real World”,
(S. B. Cooper, T. F. Kent, B. Lo¨we, A. Sorbi eds.), Siena, Italy, pp. 283–287.
[Nagy 2005a] Nagy, B.: A general fuzzy logic using intervals, in: Proceedings of the 6th
International Symposium of Hungarian Researchers on Computational Intelligence,
Budapest, Hungary, (2005), pp. 613–624.
[Nagy 2005b] Nagy, B.: An Interval-valued Computing Device, in: “Computability in
Europe 2005: New Computational Paradigms”, (S. B. Cooper, B. Lo¨we, L. Toren-
vliet eds.), ILLC Publications X-2005-01, Amsterdam, pp. 166–177.
[Nagy 2005c] Nagy, B.: U´j elvu˝ sza´mı´to´ge´pek. (New computational paradigms) Lecture
Notes, University of Debrecen, Hungary, (2005).
[Nagy–Va´lyi 2008] Nagy, B. and Va´lyi, S.: Interval-valued computations and their con-
nection with PSPACE, Theoretical Computer Science 394 (2008) 208–222.
[Papadimitriou 1994] Papadimitriou, C. H.: Computational Complexity, Addison-
Wesley, (1994).
[Paun 2002] Paun, Gh.: Membrane Computing. An Introduction, Springer-Verlag,
Berlin (2002).
[Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005a] Shatsky, M.; Shulman-Peleg, A.;
Nussinov, R. and Wolfson, H.J.: Recognition of Binding Patterns Common to a
Set of Protein Structures, in: Research in Computational Molecular Biology, LNCS
3500, Springer, (2005) pp. 440–455.
[Shatsky–Shulman-Peleg–Nussinov–Wolfson 2005b] Shatsky, M.; Shulman-Peleg, A.;
Nussinov, R. and Wolfson, H.J.: MAPPIS: Multiple 3D Alignment of Protein-
Protein Interfaces, in: Computational Life Sciences, LNCS 3695, Springer, (2005)
pp. 91–103.
[Singh–Xu–Berger 2008] Singh, R.; Xu, J. and Berger, B.: Global Alignment of Multi-
ple Protein Interaction Networks, in: Proceedings 13th Pacific Symposium on Bio-
computing (2008) pp. 303–314.
[Tanenbaum 1984] Tanenbaum, A. S.: Structured Computer Organization, Prentice-
Hall, 1984.
[Woods–Naughton 2005] Woods, D. and Naughton, T.: An optical model of computa-
tion, Theoretical Computer Science 334 (2005) 227-258.
Probabilistic Machines vs. Relativized
Computation
Hayato Takahashi1,2? and Kazuyuki Aihara1,2??
1 Aihara complexity modelling project, ERATO, JST,
2 Institute of Industrial Science, University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan.
Abstract. Computational power of probabilistic machines (Turing ma-
chines with random input) is studied. In particular, we extend the classi-
cal result of K. de Leeuw et al. (1956) to various distributions then, apply
our result to analyze the computational power of analog machines.
Key words: probabilistic machine, relativized computation, analog
computation, ergodic process
1 Introduction
There are two kinds of computational models of noisy environment: Turing
machine with random input [1] and machine models that allow the transi-
tion of states to be stochastic, e.g., probabilistic (analog) automata [2, 3] and
computable-stochastic machine [1]. In this paper, we call the former model (Tur-
ing machines with random input) as probabilistic machine. The difference of these
two models is that the former is a model of Turing machine with external random
input and the transition of states is deterministic; on the other hand the latter
model does not have external random input but allows the transition of states to
be stochastic (not deterministic). In [2], it is shown that a probabilistic automa-
ton (the number of states is finite) is equivalent to a finite automaton under the
condition isolated cut-off. In [1], a machine is called computable-stochastic if it
has countably many states and the probability of the transition is computable,
then it is shown that the class that is computable by computable-stochastic ma-
chines with positive probability is equivalent to that of Turing machines. Thus
under these conditions, the former model includes the latter model.
In this paper, we study the former models (probabilistic machines). In [1],
they showed that if the distribution of random input is Bernoulli with parameter
p, then a set is enumerable with positive probability from random inputs iff
it is enumerable with oracle p. In Section 1,2, we extend the above result to
various distributions and show some examples and counter-examples. We also
study the class that probabilistic machines can compute with probability one
? Present address: The Institute of Statistical mathematics, 4-6-7 Minami-Azabu,
Minato-ku, Tokyo 106-8569, Japan. takahasi@ism.ac.jp
?? E-mail: aihara@sat.t.u-tokyo.ac.jp
446 Hayato Takahashi and Kazuyuki Aihara
and show an interesting example (Sturmian sequence). In Section 3, we propose
analog machine models with noise and analyze their computational power. In
particular, we show that it is possible to increase its computational power by
simple feedback.
Let N and Q be the set of natural numbers and the set of rational numbers,
respectively. Let A be a finite alphabet; and let A∗ and A∞ be the set of finite
strings and the set of infinite sequences of A, respectively. For x, y ∈ A∗, xy is
the concatenation of x and y, and x v y if x is a prefix of y. For x ∈ A∗, |x|
is the length of x and x¯ := 0|x|1x. Let q : Q → A∗ be a computable bijection.
Throughout the paper, ξ and lowercase bold letters (e.g. p) denote elements of
A∞.
We say that S is ξ-enumerable if S is recursively enumerable (r.e.) relative to
the set {(i, ξi)|i ∈ N}, where ξ = ξ1ξ2 · · · ,∀i ξi ∈ A, and ξ′ is called ξ-computable
if {(i, ξ′i)|i ∈ N} is ξ-enumerable. Let M ⊆ N × A∗ be a r.e. set. For x ∈ A∗,
we write M(x) := {n|y v x, (n, y) ∈ M} and M(ξ) := ∪x@ξM(x). Then S is
ξ-enumerable iff there is a r.e. M such that S = M(ξ). We write the class of
ξ-enumerable sets as M(ξ), i.e.,
M(ξ) = {S ⊆ N|∃M, S = M(ξ)}.
Throughout the paper, M denotes a r.e. set of N ×A∗.
Let P be a probability on (B, A∞), where B is the Borel-field generated by the
cylinder sets ∆(x) = {xξ|ξ ∈ A∞}, x ∈ A∗. For x ∈ A∗, let P (x) := P (∆(x)),
then ∀x ∈ A∗ P (x) = ∑y∈A P (xy). P is called computable relative to ξ (ξ-
computable) if there is a ξ-computable total function p : A∗ × N → Q such
that
∀x, k, |P (x)− p(x, k)| < 1/k. (1)
Let p := q(p(n(1))) q(p(n(2))) · · · ∈ A∞, where n : N→ A∗ ×N is a computable
bijection. Then the graph of p is p-enumerable and vice versa, so that a set S is
r.e. relative to p iff it is p-enumerable. p is called approximation of P if p satisfies
(1). Note that 1) an approximation of P is not unique and 2) P is computable
relative to its approximation. Let AP be the set of approximations of P and
M(AP ) := ∩p∈APM(p).
Let M−1(S) := {ξ ∈ A∞|M(ξ) = S} for S ⊆ N. Note that M−1(S) is
measurable for all S. S is called P-enumerable if there is a r.e. M such that
P (M−1(S)) > 03. We write the class of P -enumerable sets as
M(P ) := {S ⊆ N|∃M P (M−1(S)) > 0}.
P is called effectively estimated if there are computable functions e : A×A∗ →
Q and r : N× N×A∗ → Q such that
∀x ∈ A∗∀k ∈ N, P (|P (x)− e(x,X1, X2, . . . , Xn)| > 1/k) < r(n, k, x) (2)
and r(n, k, x)→ 0 as n→∞ for each k, x. The following is the theorem of K. de
Leeuw et al.[1].
3 In [1], it is called strongly enumerable.
Probabilistic Machines vs. Relativized Computation 447
Theorem 1 (K. de Leeuw et al.[1]). Let P be a probability on A∞.
a) M(P ) ⊆M(AP ).
b) If P is effectively estimated then, M(P ) =M(AP ).
Note thatM(P ) always includes the class of r.e. sets (ignore the random input),
so that part a) implies that if P is computable, M(P ) is the class of r.e. sets.
For example, let P be the Bernoulli process with parameter p, i.e.,
P (x1x2 · · ·xn) = p
P
xi(1− p)n−
P
xi , (3)
where p ∈ [0, 1]. Let p = 0.p1p2 · · · be the representation of base A and p :=
p1p2 · · · ∈ A∞. Then P is p-computable. Since p = P (1), we see that for any
approximation p′ of P , p is p′-computable. Conversely, from (3), there is an
approximation p′ such that p′ is p-computable. Therefore M(AP ) = M(p).
Since P is effectively estimated, we have M(P ) =M(AP ) =M(p).
In general, the converse of a) of Theorem 1 does not hold. For example, let
p ∈ [0, 1] be a non-computable real and P (100 · · ·) = p and P (00 · · ·) = 1 − p,
where 100 · · · consists of 0s following 1 and 00 · · · consists of 0s. Then possible
sequences generated by P are 100 · · · and 00 · · ·, so that M(P ) coincides with
the class of r.e. sets. On the other hand AP consists of approximations of non-
computable p, we see M(P ) 6= M(AP ). In the same way, if P (11 · · ·) = p and
P (00 · · ·) = 1−p and p is not computable, thenM(P ) 6=M(AP ). Note that the
first example is not stationary and the second one is stationary but not ergodic.
Example 1. If P is ergodic, by ergodic theorem, there is a function r such that
r(n, k, x)→ 0 as n→∞ for each k, x and
∀x ∈ A∗∀k ∈ N, P (|P (x)−
n−|x|+1∑
i=1
I
X
i+|x|−1
i =x
/n| > 1/k) < r(n, k, x), (4)
where I is the indicator function. If r is computable, then P is effectively esti-
mated and M(P ) =M(AP ). For example, in case of ergodic Markov processes,
r decreases with exponential rate so that it is computable. However it is possible
that r is not computable. Indeed, in pp.171 [4], it is shown that for any given
decreasing function r, there is an ergodic process that satisfies
∀n, P (|P (1)−
n∑
i=1
IXi=1/n| ≥ 1/2) > r(n).
In particular if r is chosen such that r decreases to 0 asymptotically slower than
any computable function then P is not effectively estimated from sample mean.
For a similar example for a stationary process (not ergodic), see [5]. Note that
this fact does not imply that M(P ) 6= M(AP ) for that P . The authors of the
paper do not know whether M(P ) = M(AP ) for all ergodic processes or not.
We study the weakened form of the equivalence in the next section.
448 Hayato Takahashi and Kazuyuki Aihara
In order to study the converse of a) of Theorem 1 for arbitrary distributions,
we introduce some notations. Let xji := xixi+1 · · ·xj for i ≤ j, and f : N → N.
P ′ is called independent blocking process of (P, f) if
P ′(xn1 ) = P (x
f(1)
1 )P (x
f(1)+f(2)
1+f(1) ) · · ·P (xn1+f(1)+···+f(k)) (5)
for xn1 ∈ A∗, where k is the number such that 1 +
∑k
i f(i) ≤ n ≤
∑k+1
i f(i). P
′
is a probability on A∞.
Theorem 2. Let P be a probability on A∞ and f be an unbounded computable
function. Let P ′ be the independent blocking process of (P, f). Then M(P ′) =
M(AP ′) =M(AP ).
2 Ergodic process
First we study a weakened form of Theorem 1 for ergodic process. Let
rˆ(n, k, x) := P (|P (x)−
n−|x|+1∑
i=1
I
X
i+|x|−1
i =x
/n| > 1/k), (6)
for x ∈ A∗, k ∈ N. Let r : N × N × A∗ × N → Q be a total function such that
∀n, k, x, t, |r(n, k, x, t) − rˆ(n, k, x)| < 1/t. Let m : N → N × N × A∗ × N be a
computable bijection, and set r := q(r(m(1))) q(r(m(2))) · · ·. Then the graph of
r is r-enumerable and vice versa. r is called approximation of rˆ. LetMr ⊆ N×A∗
be r-enumerable. Let Arˆ be the set of approximations of rˆ and
M(P ∪Arˆ) := ∩r∈Arˆ{S ⊆ N|∃Mr P (M−1r (S)) > 0}.
Roughly speaking, M(P ∪Arˆ) is the class of subsets of natural numbers that is
computable with positive probability from rˆ and random inputs.
Corollary 1. If P is ergodic, M(P ∪Arˆ) =M(AP ).
Next, we study the class of subsets of natural numbers that probabilistic
machines can compute with probability one. Let
M1(P ) := {S ⊆ N|∃M P (M−1(S)) = 1},
SP := {x|P (x) > 0},
and M(SP ) be the class of r.e. sets relative to SP .
Theorem 3. If P is ergodic, M1(P ) =M(SP ).
Remark 1. Let ξ′ be the shifted sequence of ξ, i.e., ∀i, ξ′i = ξi+1. M is called
shift invariant if M(ξ) = M(ξ′) for almost all ξ with respect to P . Then for
a shift invariant M , the set {ξ|M(ξ) = S} is shift invariant with respect to
P , so that if P is ergodic, we have P (M−1(S)) > 0 ⇔ P (M−1(S)) = 1, and
M1(P ) = {S|∃ shift invariant M, P (M−1(S)) > 0}. Therefore if 1) SP is a
r.e. set, 2)M(P ) =M(AP ), and 3) P is not computable, thenM that computes
an approximation of P with positive probability cannot be shift invariant.
Probabilistic Machines vs. Relativized Computation 449
For example if P is a finite state Markov process then SP is a r.e. set, and
hence M1(P ) coincide with the class of r.e. sets. However, there is an example
such that M1(P ) is beyond the class of r.e. sets.
Example 2 (Sturmian sequence). Let xn+1 = xn + θ mod 1 for θ ∈ [0, 1] and
yn := 0 if xn ∈ [0, θ) else 1. If the initial distribution of x0 is uniform then
y0, y1, . . . is an ergodic process. Let Pθ be such the process. Then Pθ is com-
putable iff θ is computable. In [6, 7], it is shown that there are a computable func-
tion e and a computable decreasing (to 0) function r such that |e(x)−θ| ≤ r(|x|),
for all x generated by Pθ. Therefore we have M(Pθ) = M(APθ ) = M1(Pθ) =
M(SPθ ).
3 Analog machines with noise
Certain kind of analog machines with noise can be modeled as follows (Model A):
f : [0, 1]→ [0, 1], Yn := f(Yn−1) + n−1
D : R→ A, Xn := D(Yn)
· · ·X2X1 →M → s1s2 · · · ,
where {n} are i.i.d. random variables (noise) and S = {s1, s2, . . .} is an output
of the machine. Then X1, X2, . . . are stochastic process on A∞, and if the distri-
butions of Y0 and {n} are given, the distribution of X1, X2, . . . is determined.
This machine is a combination of a dynamical system with noise and a Turing
machine, and it is considered to be a probabilistic machine, see Fig. 1 and Re-
mark 2. Thus the results of the previous section can be applied to this machine.
For example, if 1. f : [0, 1]→ [0, 1] is a polynomial with rational coefficient (e.g.
logistic map), 2. distribution of Y0 is computable, 3. distributions of {n} are
i.i.d. and computable, and 4. D(x) = 0 if x ∈ [0, a) and D(x) = 1 if x ∈ [a, 1],
where a is a rational number, then the distribution of X1, X2, . . . is computable.
In such a case, Model A is equivalent to a Turing machine, which follows from
the first statement of Theorem 1.
In general, let P be the probability that X1, X2, . . . obey and AP be the
set of approximations of P . Then M(P ) ⊆M(AP ). However, unless P is effec-
tively estimated, in general, the converse does not hold. In order to increase the
computational power of Model A whenM(P ) 6=M(AP ), let us consider the fol-
lowing transformation (Model B): Let f : N→ N be an unbounded computable
function. If n = f(k) + 1, k = 1, 2, . . ., then Yn in Model A is reset i.e., Yn is
drawn according to the initial distribution and it is independent from the past
sequence, see Fig. 1. Model B corresponds to the independent blocking process
P ′ of (P, f), and we have M(P ′) =M(AP ) from Theorem 2.
Remark 2. Physically realizable analog machines always contain noise (uncer-
tainty of initial value, noise effect of transition, and so on), and some of them
are modeled by discrete-time dynamical systems with noise, see [8]. {Yn} (and
450 Hayato Takahashi and Kazuyuki Aihara
Model A
s1, s2, . . .
M
Xn = D(Yn)
Yn = f(Yn−1) + n−1
Y0
?
?
?
?
Model B
s1, s2, . . .
M
Xn = D(Yn)
Yn = f(Yn−1) + n−1
Y0
?
?
?
?
ff
Fig. 1. machine models
{Xn}) in Fig. 1 is a discrete-time dynamical system with noise and it is consid-
ered to be an analog machine model with noise. Thus our model in Fig. 1 is a
combination of noisy analog machine and Turing machine. In this paper, we are
mainly interested in the distribution of {Xn}, and do not explore the dynamics
of analog machines. For example, analog neural nets might be modeled by a
transformation of a higher dimensional space, however it is not essential in this
paper.
4 Proof
We need Lebesgue density theorem.
Lemma 1 (Lebesgue). Let D ⊆ A∞ be measurable and ID be the characteris-
tic function of D. Let f(x) := P (ID ∩∆(x))/P (∆(x)). Then limx→ξ f(x) = ID
for almost all ξ with respect to P .
Proof of Theorem 1)
a: Assume that S is P -enumerable. Then there is a r.e. M such that P ({ξ ∈
A∞|M(ξ) = S}) > 0. From Lemma 1, we have
∀ > 0∃x ∈ A∗ P ({xξ|M(xξ) = S})/P (x) > 1− .
Let x be a finite string that satisfies the above inequality for  = 1/2. We see
that
s ∈ S ⇔ P ({∆(xy)|s ∈M(xy), y ∈ A∗})/P (∆(x)) > 1/2.
Since {(s, y)|s ∈ M(y)} is recursively enumerable and P is computable relative
to p, S is enumerable from the finite string x and p.
Probabilistic Machines vs. Relativized Computation 451
b: By assumption, P is effectively estimated and (2) holds. Let e and r are
computable functions in (2). Let N(x, k) be the least integer such that
r(N(x, k), k, x) < 6/(pin−1(x, k))2, where n : N → A∗ × N is a computable
bijection. Since r is computable and r(n, k, x) → 0 as n → ∞ for each k, x, we
see that N is computable for all x, k. Let
A(x, k) := {∆(y)| |P (x)− e(x, y)| ≤ 1/k, |y| = N(x, k), y ∈ A∗},
where |y| is the length of y. Then P (A(x, k)c) < r(N(x, k), k, x), where Ac is the
complement of A. Since
∑
x,k 6/(pin
−1(x, k))2 = 1, we have
P (∩x,kA(x, k)) ≥ 1 −
∑
x,k P (A(x, k)
c) > 1 −∑x,k 6/(pin−1(x, k))2 = 0. Let
px,k := e(x,X1, X2, . . . , XN(x,k)) for all x, k and p := q(pn(1)) q(pn(2)) · · ·. Then,
p is an approximation of P with positive probability and it is P -enumerable.
Thus, if S ∈M(AP ) then S ∈M(P ). uunionsq
Proof of Theorem 2) First we show thatM(AP ) ⊆M(P ′). In order to show
this, it is enough to show that P is effectively estimated from random sampling
according to P ′. Let X1, X2, . . . be random variables according to P ′ and X
j
i :=
Xi · · ·Xj for i ≤ j. Then blocks of random variables Xf(1)1 , Xf(1)+f(2)f(1)+1 , . . . are
independent. Thus we see that P is effectively estimated from X∞1 . Formally let
N(l) := {k|f(k) ≥ l}. Since f is an unbounded computable function, N(l) is
computable and infinite for all l. For each fixed l, Yk := X
f(1)+···+f(k−1)+l
1+f(1)+···+f(k−1), k ∈
N(l) are independent, where f(0) = 0. Let N(l) = {l1, l2, . . . , }, l1 < l2 < · · ·
and e(x,X1, X2, . . . , Xn) :=
∑m
i=1 IYli=x/m for f(1) + · · ·+ f(lm − 1) + l ≤ n ≤
f(1) + · · ·+ f(lm+1). Then for x ∈ A∗, |x| = l, we have E(IYk=x) = P (x), where
expectation is average with respect to P ′, and from Chebyshev inequality, we
have P ′(|e(x,X1, X2, . . . , Xn)− P (x)| > 1/k) ≤ k2/4m. Since m is computable
from n and m → ∞ as n → ∞, we see that P is effectively estimated from
random sampling according to P ′. Thus we have M(AP ) ⊆ M(P ′). M(P ′) ⊆
M(AP ′) follows from Theorem 1. Since f is computable, by (5), we see that for
any approximation p of P there is an approximation p′ of P ′ such that p′ is
p-computable, and hence M(AP ′) ⊆M(AP ). uunionsq
Proof of Corollary 1) From (6), we see that M(Arˆ) ⊆ M(AP ). From Theo-
rem 1 (a), we haveM(P∪Arˆ) ⊆M(AP ). Since approximation of rˆ is given, as in
the same way of proof of Theorem 1 (b), we can show the converse inclusion. uunionsq
Proof of Theorem 3) Let Sξ be the set of strings that appear in ξ, i.e.,
Sξ = {x|∃y∃ξ′ yxξ′ = ξ}. Since P is ergodic, by ergodic theorem, we have
Sξ = {x|P (x) > 0} almost surely. Thus M1(P ) ⊇ M(SP ). Conversely if
P (M−1(S)) = 1 then s ∈ S ⇔ ∃x, P (x) > 0, s ∈ M(x). Thus we have
M1(P ) ⊆M(SP ). uunionsq
References
1. de Leeuw, K., Moore, E.F., Shannon, C.E., Shapiro, N.: Computability by proba-
bilistic machines. In Shannon, C.E., McCarthy, J., eds.: Automata Studies. Prince-
ton Univ. Press (1956) 183–212
452 Hayato Takahashi and Kazuyuki Aihara
2. Rabin, M.O.: Probabilistic automata. Information and Control 6 (1963) 230–245
3. Ben-Hur, A., Roitershtein, A., Siegelmann, H.T.: On probabilistic analog automata.
Theoret. Comput. Sci. 320 (2004) 449–464
4. Shields, P.: The ergodic theory of discrete sample paths. AMS (1996)
5. V’yugin, V.V.: Ergodic theorems for individual random sequences. Theoret. Com-
put. Sci. 207 (1998) 343–361
6. Takahashi, H., Aihara, K.: Algorithmic analysis of irrational rotations in a sigle
neuron model. J. Complexity 19 (2003) 132–152
7. Kamae, T., Takahashi, H.: Statistical problems related to irrational rotations.
Ann. Inst. Statis. Math. 58 (2006) 573–593
8. Maass, W., Orponen, P.: On the effect of analog noise in discrete-time analog
computations. Neural Computation 10 (1998) 1071–1095
Quantum Query Algorithms for AND and OR
Boolean Functions ⋆
Alina Vasilieva
Institute of Mathematics and Computer Science, University of Latvia
Raina bulvaris 29, Riga, LV-1459, Latvia
alina.vasilieva@gmail.com
Abstract. Quantum algorithms can be analyzed in a query model to
compute Boolean functions where input is given in a black box and the
aim is to compute function value for arbitrary input using as few queries
as possible. We concentrate on quantum query algorithm designing tasks
in this paper. The main aim of the research was to find new efficient algo-
rithms and develop general algorithm designing techniques. In this article
we propose quantum query algorithm constructing methods. Given algo-
rithms for the set of sub-functions, our methods use them to build a more
complex one, based on algorithms described before. Methods are applica-
ble to input algorithms with specific properties and preserve acceptable
error probability and number of queries. Methods offer constructions for
computing AND and OR kinds of Boolean functions.
1 Introduction
Let f(x1, x2, ..., xn) : {0, 1}
n
→ {0, 1} be a Boolean function. We have studied
the query model, where a black box contains the input (x1, x2, ..., xn) and can
be accessed by questioning xi values. The goal is to compute the value of the
function. The complexity of a query algorithm is measured by number of ques-
tions it asks. The classical version of this model is known as decision trees [1].
Quantum algorithms can solve certain problems faster than classical algorithms.
The best-known exact quantum algorithm was designed for XOR function with
n/2 questions vs. n questions required by classical algorithm [2, 3]. No analogous
efficient algorithms exist for AND and OR Boolean functions.
Quantum query model differs from quantum circuit model [2, 5] and algo-
rithm construction techniques for this model are less developed. The problem
of quantum query algorithm construction is not that easy. Although there is
a large amount of lower and upper bound estimations of quantum query algo-
rithm complexity [2, 6, 7], examples of non-trivial and original quantum query
algorithms are very few. Moreover, there is no special technique described to
build a quantum query algorithm for an arbitrary function with complexity de-
fined in advance.
In our work we have tried to develop general constructions and approaches
for computing Boolean functions efficiently in a quantum query settings.
⋆ Research supported by the European Social Fund
454 Alina Vasilieva
2 Notation and Definitions
Let f(x1, x2, ..., xn) : {0, 1}
n
→ {0, 1} be a Boolean function. We use ⊕ to denote
XOR operation. We use abbreviation QQA for ”quantum query algorithm”.
2.1 Quantum Computing
We apply the basic model of quantum computing. For more details see textbooks
by Gruska [4] and Nielsen and Chuang [5]. An n-dimensional quantum pure state
is a unit vector in a Hilbert space. Let |0〉 , |1〉 , ..., |n− 1〉 be an orthonormal basis
for Cn. Then, any state can be expressed as |ψ〉 =
∑n−1
i=0 ai |i〉 for some ai ∈ C.
Since the norm of |ψ〉 is 1, we have
∑n−1
i=0 |ai|
2
= 1. States |0〉 , |1〉 , ..., |n− 1〉 are
called basis states. Any state of the form
∑n−1
i=0 ai |i〉 is called a superposition of
basis states. The coefficient ai is called an amplitude of |i〉. The state of a system
can be changed by applying unitary transformation. Unitary transformation U
is a linear transformation on Cn that maps vector of unit norm to vector of unit
norm. The simplest case of quantum measurement is used in our model. It is
the full measurement in the computation basis. Performing this measurement
on a state |ψ〉 = a0 |0〉 + ... + an−1 |n− 1〉 gives the outcome i with probability
|ai|
2. The measurement changes the state of the system to |i〉 and destroys the
original state.
2.2 Quantum Query Model
Query algorithm is the model for computing Boolean functions. A black box
contains the input (x1, x2, ..., xn) and can be accessed by questioning xi values.
Algorithm must be able to determine the value of a function correctly for ar-
bitrary input. The complexity of the algorithm is measured by the number of
queries to the black box. We consider computing Boolean functions in the quan-
tum query model. For more details, see the survey by Ambainis [6] and textbooks
by Gruska [4] and de Wolf [2]. A quantum computation with T queries is a se-
quence of unitary transformations:
U0 → Q0 → U1 → Q1 → ...→ UT−1 → QT−1 → UT .
U ′is can be arbitrary unitary transformations that do not depend on the input
bits. Q′is are query transformations. Computation starts in the state
∣∣∣→0〉. Then
we apply U0, Q0, ..., QT−1, UT and measure the final state.
We use the following definition of query transformation: if input is a state
|ψ〉 =
∑
i ai |i〉 , then the output is |φ〉 =
∑
i(−1)
xkai |i〉 , where we can arbi-
trarily choose a variable assignment xk for each basis state |i〉.
Each quantum basis state corresponds to the algorithm output. We assign
a value of a function to each output. The probability of obtaining result j after
executing an algorithm on input X equals the sum of squares of all amplitudes,
which corresponds to outputs with value j.
Very convenient way of QQA representation is a graphical picture and we
will use this style when describing our algorithms.
Quantum Query Algorithms for AND and OR Boolean Functions 455
2.3 Query Algorithm Complexity
The complexity of a query algorithm is based on the number of questions it uses
to determine the value of a function on worst-case input.
The deterministic complexity of a function f, denoted by D(f), is the max-
imum number of questions that must be asked on any input by a deterministic
algorithm for f [1].
The sensitivity of f on input (x1, x2, ..., xn) is the number of variables xi
with the following property: f(x1, ..., xi, ..., xn) 6= f(x1, ..., 1 − xi, ..., xn). The
sensitivity of f is the maximum sensitivity of all possible inputs. It has been
proved that D(f) ≥ s(f) [1].
A quantum query algorithm computes f exactly if the output equals f(x) with
a probability 1, for all x ∈ {0, 1}
n
. Complexity is denoted by QE(f) [1].
A quantum query algorithm computes f with bounded-error if the output
equals f(x) with probability p > 1/2, for all x ∈ {0, 1}
n
. Complexity is denoted
by Qp(f) [1].
3 Basic Exact Quantum Query Algorithms
In this paper we will describe quantum query algorithm constructing methods,
which use existing algorithms to construct more complex examples. To demon-
strate methods we need at least few basic algorithms. The following exact QQAs
were presented in [9] and we will use them as a base.
Algorithm 1. Exact QQA with 2 queries is presented in Fig. 1.
Fig. 1. Exact QQA for EQUALITY3 with two queries
Boolean function: EQUALITY3(X) = ¬(x1 ⊕ x2) ∧ ¬(x2 ⊕ x3).
Deterministic complexity: D(EQUALITY3) = 3, by sensitivity on any ac-
cepting input.
To make reader more familiar with QQA model we show the computation
for X = 111 (bra notation is used for convenience):
〈ψ| = (1, 0, 0, 0)U0Q0U1Q1U2 =
(
1
2
, 1
2
, 1
2
, 1
2
)
Q0U1Q1U2 =
=
(
− 1
2
,− 1
2
,− 1
2
,− 1
2
)
U1Q1U2 =
(
− 1
2
,− 1√
2
, 0,− 1
2
)
Q1U2 =
(
1
2
, 1√
2
, 0, 1
2
)
U2 =
= (1, 0, 0, 0)⇒ [ACCEPT ]
456 Alina Vasilieva
Algorithm 2. Exact QQA for PAIR EQUALITY4 is presented in Fig. 2.
Fig. 2. Exact QQA for PAIR EQUALITY4 with two queries
Boolean function: PAIR EQUALITY4(X) = ¬(x1 ⊕ x2) ∧ ¬(x3 ⊕ x4).
Deterministic complexity: D(PAIR EQUALITY4) = 4, by sensitivity on
accepting input.
3.1 Exact Quantum Query Algorithm Classification
Two described exact QQAs have useful specific properties. They are designed
in such a way that the final amplitude distribution satisfies certain condition on
any possible input. To describe these properties we introduce QQA classification
and define the following algorithm classes.
Class 1. Exact QQA belongs to Class 1 IFF on any input system state before a
measurement is such that for exactly one amplitude αi it is true that |ai|
2
= 1.
For other amplitudes it is true that |aj |
2
= 0, for ∀j 6= i.
Algorithm 1 and Algorithm 2 both belong to Class 1.
Class 2+. Exact QQA belongs to Class 2+ IFF there is exactly one accepting
basis state and on any input for its amplitude α ∈ C only two values are possible
before the final measurement: either α = 0 or α = 1.
Algorithm 1 belongs to Class 2+.
Class 2-. Exact QQA belongs to Class 2- IFF there is exactly one accepting
basis state and on any input for its amplitude α ∈ C only two values are possible
before the final measurement: either α = 0 or α = −1.
Lemma 1. It is possible to transform algorithm that belongs to Class 2- to al-
gorithm that belongs to Class 2+ by applying additional unitary transformation.
Proof. Let’s assume that we have QQA that belongs to Class 2- and k is the
number of accepting output. To transform algorithm to Class 2+ instance apply
the following quantum gate:
U = (uij) =


0, if i 6= j
1, if i = j 6= k
−1, if i = j = k
Quantum Query Algorithms for AND and OR Boolean Functions 457
Class 3. Exact QQA belongs to Class 3 IFF it belongs to Class 1; and there
is exactly one accepting basis state; and on any input accepting state amplitude
value before measurement is α ∈ {−1, 0, 1}.
Algorithm 1 and Algorithm 2 both belong to Class 3.
3.2 Algorithm Transformation Methods
QQA transformation methods were introduced in [9]. These methods are useful
for enlarging a set of exactly computable functions. By applying transformation
methods to Algorithm 1 and Algorithm 2 it is possible to obtain two sets:
– QAlg3 - which consists of exact QQAs that compute 3-variable Boolean
functions using 2 queries. |QAlg3| = 8.
– QAlg4 - which consists of exact QQAs that compute 4-variable Boolean
functions using 2 queries. |QAlg4| = 24.
QAlg3 contains two algorithms that belong to Class 2+, two algorithms that
belong to Class 2- and four algorithms that belong to Class 3. QAlg4 contains
12 algorithms that belong to Class 3. All these algorithms can be used as a base
for constructions methods described in the next section.
We will use these results when calculating total number of algorithms that
can be constructed by methods presented in this paper.
4 Algorithm Constructing Methods
In this section we will present two quantum query algorithm constructing meth-
ods. Each method requires explicitly specified exact QQAs on input, but as
a result a bounded-error QQA for more complex function is constructed. The
most important behavior is that methods do not increase overall algorithm com-
plexity. No additional queries are necessary to compute complex function; in-
put algorithms just have to be combined in a specific way. Methods maintain
low quantum query complexity for complex function in comparison to increased
deterministic complexity, thus enlarging a gap between classical and quantum
complexities of an algorithm.
We offer a general constructions for computing AND and OR kinds of Boolean
functions.
4.1 Bounded-error QQA for 6-Variable Function
We consider composite Boolean function, where two instances of EQUALITY3
are joined with logical AND operation:
EQUALITY ∧2
3
(x1, ..., x6) = (¬(x1⊕x2)∧¬(x2⊕x3))∧(¬(x4⊕x5)∧¬(x5⊕x6))
Deterministic complexity. D(EQ-LITY ∧2
3
)=6, by sensitivity on X=111111.
458 Alina Vasilieva
Algorithm 3. Our approach in designing an algorithm for EQUALITY ∧2
3
is
to employ quantum parallelism and superposition principle. We execute algo-
rithm pattern defined by original algorithm for EQUALITY3 in parallel for
both blocks of EQUALITY ∧2
3
variables. Finally, we apply additional quantum
gate to correlate amplitude distribution. Algorithm flow is depicted in Fig. 3.
Fig. 3. Bounded-error QQA for EQUALITY ∧23
Quantum complexity. Algorithm 3 computes EQUALITY ∧2
3
using 2 queries
with correct answer probability p = 3/4: Q3/4(EQUALITY
∧2
3
) = 2.
4.2 First Constructing Method - AND(f1, f2)
It is possible to generalize approach demonstrated in the previous section. It
is evident that complex algorithm behaviour is based on a structure of sub-
algorithms. However, it does not depend on internal structure, but just on a
properties of final amplitude distributions. Therefore, described construction is
not limited to use algorithm for EQUALITY3 as a base, but can be applied to
a partucular class of exact QQAs. Generalized method is described in Table 1.
4.3 Bounded-error QQA for 8-Variable Function
Next step is to realize similar approach for OR operation. This time we take
Algorithm 2 for PAIR EQUALITY4 function as a base. We consider composite
Boolean function, where two instances of PAIR EQUALITY4 are joined with
OR operation:
PAIR EQLTY ∨
4
(X) = (¬(x1 ⊕ x2) ∧ ¬(x3 ⊕ x4)) ∨ (¬(x5 ⊕ x6) ∧ ¬(x7 ⊕ x8))
We succeeded in constructing quantum algorithm for PAIR EQUALITY ∨
4
and this time algorithm structure is more complex than in AND operation case.
However, this structure allows us to formulate generalized version of method,
which will be applicable to QQAs that belong to Class 3.
Quantum Query Algorithms for AND and OR Boolean Functions 459
Table 1. First Constructing Method - AND(f1, f2)
Input. Two exact QQAs Alg1 and Alg2 that belong to Class 2+ or Class 2- and
compute Boolean functions f1(X1) and f2(X2).
Constructing steps.
1. If any algorithm belongs to Class 2-, transform it to Class 2+ algorithm as
described in Lemma 1.
2. If Alg1 and Alg2 utilize quantum systems of different size, extend the smallest one
with auxiliary space to obtain equal number of amplitudes. We denote the dimension
of obtained Hilbert spaces with m.
3. For new algorithm utilize a quantum system with 2m amplitudes.
4. Combine unitary transformations and queries of Alg1 and Alg2 in the following
way: Ui =
(
U1i O
O U2i
)
, where O’s are m × m zero-matrices, U1i and U2i are either
unitary transformations or query transformations of Alg1 and Alg2.
5. Start computation from the state: 〈ψ| = ( 1√
2
, 0, ..., 0, 1√
2
, 0, ..., 0).
6. Before the final measurement apply additional unitary gate. Let us denote the
positions of accepting outputs of Alg1 and Alg2 by acc1 and acc2. Then the final
gate is defined as follows:
U = (uij) =


1, if (i = j) AND (i 6= acc1) AND (i 6= (m+ acc2))
1/
√
2, if (i = j = acc1)
1/
√
2, if ((i = acc1) AND (j = (m+ acc2)))
OR ((i = (m+ acc2)) AND (j = acc1))
−1/√2, if (i = j = (m+ acc2))
0, otherwise
7. Define as accepting output exactly one basis state |acc1〉.
Output. A bounded-error QQA A for computing a function F (X) = f1(X1)∧f2(X2)
with probability p = 3/4 and complexity Q3/4(A) = max(QE(Alg1), QE(Alg2)).
Algorithm 4. This time we use 4 qubit quantum system, so in total there are 16
amplitudes. First, we execute PAIR EQUALITY4 algorithm pattern in parallel
on first 8 amplitudes, and then apply two additional quantum gates.
First gate USWAP swaps state amplitudes in the following way: 2
nd ↔ 5th
and 6th ↔ 9th.
USWAP = (uij) =


1, if (i = 2 & j = 5) OR (i = 5 & j = 2)
1, if (i = 6 & j = 9) OR (i = 9 & j = 6)
1, if (i = j) & (i /∈ {2, 5, 6, 9})
0, otherwise
Second gate UOR is defined as follows:
UOR =


H2 O2×4 O2×4 O2×6
O4×2 H2 ⊗H2 O4×4 O4×6
O4×2 O4×4 H2 ⊗H2 O4×6
O6×2 O6×4 O6×4 I6

,
where H2 =
1√
2
(
1 1
1 −1
)
, I6 is 6× 6 identity matrix; Oi×j are zero matrices.
460 Alina Vasilieva
Complete algorithm structure is presented in Fig. 4.
Fig. 4. Bounded-error QQA for PAIR EQUALITY ∨4
Quantum complexity. Algorithm 4 computes PAIR EQUALITY ∨
4
using 2
queries with probability p = 5/8: Q5/8(PAIR EQUALITY
∨
4
) = 2.
4.4 Second Constructing Method - OR(f1, f2)
In this section we generalize approach for computing composite Boolean func-
tions matching OR(f1, f2) pattern. The next lemma will be useful during method
application.
Lemma 2. For any QQA on any computation step it is possible to swap ampli-
tude values in arbitrary order by applying specific quantum gate.
Proof. Assume we need to swap amplitude values according to permutation σ =(
α1α2...αn
β1β2...βn
)
Then we can define quantum gate USWAP = (uij) elements as:
– ∀k ∈ {1..n} : uαkβk = 1;
– uij = 0, in all other cases.
Now we are ready to formulate a method for computing OR(f1, f2) kind
of functions. For simplicity we consider only input algorithms, which employ
2 qubit system. However, approach can be generalized for quantum systems of
arbitrary size.
Quantum Query Algorithms for AND and OR Boolean Functions 461
Table 2. Second Constructing Method - OR(f1, f2)
Input. Two exact QQAs Alg1 and Alg2 that belong to Class 3, employ 2 qubit
quantum systems and compute Boolean functions f1(X1) and f2(X2).
Constructing steps.
1. Use 4 qubit quantum system for a new algorithm, in total 24 = 16 basis states.
2. Start from the state: 〈ψ| =
(
1√
2
, 0, 0, 0, 1√
2
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
)
.
3. Combine Alg1 and Alg2 unitary and query transformations in the following way:
Ui =
(
U1i O4×4 O4×8
O4×4 U
2
i O4×8
O8×4 O8×4 I8
)
, where I8 is 8×8 identity matrix; Oi×j - zero matrices.
4. Apply amplitude swapping gate USWAP , which was defined in the proof of Lemma
2, to arrange amplitudes in the following order:
− 1st amplitude ↔ accepting amplitude of the first sub-algorithm;
− 2nd amplitude ↔ accepting amplitude of the second sub-algorithm;
− 3rd, 4th, 5th amplitudes ↔ rejecting amplitudes of the first sub-algorithm;
− 7th, 8th, 9th amplitudes ↔ rejecting amplitudes of the second sub-algorithm.
5. Apply the last quantum gate UOR that was precisely defined in previous section.
6. Assign function values to an algorithm according to Fig. 4.
Output. A bounded-error QQA A for computing a function F (X) = f1(X1)∨f2(X2)
with probability p = 5/8 and complexity Q5/8(A) = max(QE(Alg1), QE(Alg2)).
5 Results of Applying Methods
We have applied constructing methods to the basic exact algorithms from sets
QAlg3 and QAlg4 (see Sect. 3.2). In total we obtained 272 bounded-error QQAs.
Each algorithm computes different Boolean function and uses only 2 queries.
We have demonstrated, that by employing quantum computation features
it is possible to calculate composite functions by combining algorithms for sub-
functions, without additional queries. It is doubtful that similar effect can be
achieved in a classical model.
The important point is that invention of each brand-new exact QQA with re-
quired properties will at once significantly increase a set of efficiently computable
functions.
Table 3. Results of transformation and constructing methods application
Basic exact quantum algorithms
Set Size Number of function arguments Queries Probability
QAlg3 8 3 2 1
QAlg4 24 4 2 1
Constructed sets of algorithms
QAlg AND 16 6 2 3/4
QAlg OR 256 6,7,8 2 5/8
Total: 272
462 Alina Vasilieva
6 Conclusion
In this work we consider quantum query algorithm constructing problems. Main
goal of research is to develop a framework for building efficient ad-hoc quantum
algorithms for arbitrary Boolean functions. We have tried to develop general
approaches for designing algorithms for computing Boolean functions defined by
logical formula. In this paper we describe general algorithm constructions for
computing AND and OR kinds of Boolean functions.
Suggested approaches allow building bounded-error quantum algorithms for
complex functions based on already known algorithms. Significant behavior is
that overall algorithm complexity does not increase. Additional queries are not
required to compute composite function. However, error probability is the cost
for efficient computing.
Further work in that direction is to improve general quantum query algorithm
constructing techniques. Regarding already existing methods we would like to
decrease existing limitations, such as restrictions on input basic algorithms. An-
other goal is to increase correct answer probability. From the other side, we need
new methods to compute Boolean functions of different logical structure, to raise
overall framework flexibility. Ultimate goal is to invent new efficient quantum
algorithms that exceed already known separation from classical algorithms.
7 Acknowledgments
This research is supported by the European Social Fund.
References
[1] H. Buhrman and R. de Wolf: Complexity Measures and Decision Tree Complexity:
A Survey. Theoretical Computer Science, v. 288(1): 2143 (2002).
[2] R. de Wolf: Quantum Computing and Communication Complexity. University of
Amsterdam (2001).
[3] R. Cleve, A. Ekert, C. Macchiavello, et al. Quantum Algorithms Revisited. Pro-
ceedings of the Royal Society, London, A454 (1998).
[4] J. Gruska: Quantum Computing. McGraw-Hill (1999).
[5] M. Nielsen, I. Chuang: Quantum Computation and Quantum Information. Cam-
bridge University Press (2000).
[6] A. Ambainis: Quantum query algorithms and lower bounds (survey article). Pro-
ceedings of FOTFS III.
[7] A. Ambainis and R. de Wolf: Average-case quantum query complexity. Journal of
Physics A 34, pp 67416754 (2001).
[8] A. Ambainis. Polynomial degree vs. quantum query complexity. Journal of Com-
puter and System Sciences 72, pp. 220238 (2006).
[9] A. Dubrovska, ”Quantum Query Algorithms for Certain Functions and General
Algorithm Construction Techniques,” Quantum Information and Computation V,
Proc. of SPIE Vol. 6573 (SPIE, Bellingham, WA, 2007) Article 65730F.
Space Complexity in Ordinal Turing Machines
Joost Winter
1 Introduction
In [Ko], Peter Koepke introduced a model for transfinite computation that goes
beyond the earlier infinite time Turing machine model described by Joel Hamkins
and Andy Lewis in [HaLe]. Unlike ITTMs, which use a number of tapes of size ω,
these ordinal Turing machines have a class-sized tape of length Ord. Also unlike
ITTMs, these machines also give rise to a intuitive notion of space complexity,
allowing us to define a variety of space complexity classes.
As it turns out, these machines are strictly more powerful than the earlier
Hamkins-Kidder model of ITTMs: it has been shown in [Lo¨] that the Hamkins-
Kidder weak halting problem h is decidable by one of these Ordinal machines.
In this article, we will look at some issues of space complexity for these ma-
chines. Although in principle, here it is possible to look at classes of decidable
ordinals (which need not even be sets!), in this article, the time and space com-
plexity classes will be defined as containing only sets of reals (where ‘reals’ are
taken to be subsets of N) decidable within a certain time and space complexity.
2 Definitions
Ordinal Turing machines can be defined as follows:
Definition 2.1. An ordinal Turing machine, or OTM, is defined by a tuple of
the form T = (Q, δ, qs, qf ), where Q is a set of internal states, δ is a transition
function from (Q\{qf})×{0, 1}3 to Q×{0, 1}3×{L,R} and qs, qf ∈ Q are two
special states called the initial and halting state, such that qs 6= qf holds.
In the next definition, we will recursively define three operations: (1) qTα (x)
denotes the state a machine T is in at stage α, having started from the input
x; (2) hTα(x) denotes the position of the head at stage α, having started from
the input x; and (3) cTi,α(x), where i ∈ {1, 2, 3}, denotes the content of tape i at
stage α, having started from the input x. Also, we will define another operation,
cTi,n,α(x), which will be defined as 1 if n ∈ cTi,α(x), and 0 otherwise.
Of the three tapes we have, the first one will be called, and play the role
of input tape, and the second one will be used as output tape. We will
regularly use the term the snapshot at α to refer to the tuple of values
(qTα (x), h
T
α(x), c
T
1,α(x), c
T
2,α(x), c
T
3,α(x)).
Definition 2.2. For any ordinal Turing machine T and any input x ∈ R, we
define the operations qTα (x), h
T
α(x), c
T
i,α(x) as follows for all ordinals α, using
transfinite recursion:
464 Joost Winter
– If α = 0, we set qTα (x) = qs, h
T
α(x) = 0, c
T
1,α(x) = x, and c
T
i,α(x) = ∅ for
i ∈ {2, 3}.
– If there is some ordinal β < α such that qTβ (x) = qf , then q
T
α (x), h
T
α(x), and
cTi,α(x) for i ∈ {1, 2, 3} are undefined.
– Otherwise, if α is a successor ordinal β + 1, and hTβ (x) = h, we look at the
value of the transition function
(q′, (a1, a2, a3),∆) = δ(qTβ (x), (c
T
1,h,β(x), c
T
2,h,β(x), c
T
3,h,β(x)))
and set:
• qTα (x) = q′
• hTα(x) = h + 1 if ∆ = R; hTα(x) = h − 1 if ∆ = L and h is a successor
ordinal; and hTα(x) = 0 if ∆ = L and h is either 0 or a limit ordinal.
• For each i ∈ {1, 2, 3}, cTi,α(x) is defined as cTi,β(x)\{h} if ai = 0, and as
cTi,β(x) ∪ {h} if ai = 1.
– Otherwise, if α is a limit ordinal, we set qTα (x) = lim sup{qTβ (x) : β < α},
hTα(x) = lim inf{hTβ (x) : β < α}, and for each i ∈ {1, 2, 3}, cTi,β(x) = {γ ∈
Ord : lim supα<β cTi,γ,α(x) = 1}.
As a result of defining the position of the head at limit stages as the lim inf
of the earlier head positions, it follows directly that the head position may, at
some point, be an infinite ordinal. This, in turn implies that infinite ordinals
may be written to, or again deleted from the tapes: as a result, for an arbitrary
ordinal α, any x ∈ R, and any i ∈ {1, 2, 3}, cTi,α(x) need not be a real anymore,
but can in principle be any set of ordinals. In principle, we could also remove the
restriction that the input of a function be a real, and replace it with the weaker
requirement that it be a set of ordinals. However, in this article we will not do
this, and focus only on computations by ordinal Turing machines starting from
reals.
Now we have defined the ordinal Turing machines and their computation, we
continue by defining the complexity classes P and PSPACE1. The P classes
for ITTMs were originally defined in [Sc], and the P and PSPACE classes for
OTMs both were originally defined in [Lo¨].
Definition 2.3. If T is a machine that eventually reaches the halting state qf ,
and α is the smallest ordinal such that qTα (x) = qf , then we say that time(x, T ) =
α.
Definition 2.4. For any ordinal Turing machine T , we say that T is a time
f machine if, for all x ∈ R, we have time(x, T ) ≤ f(x). For any ordinal ξ,
we say that T is a time ξ machine if T is a time f machine for the constant
function f such that f(x) = ξ for all x ∈ R.
1 It should be noted that is very little if anything at all polynomial about these classes:
the P notation has been introduced in [Sc] originally, and later on the PSPACE
was introduced in [Lo¨]. The fact that these notions have stuck around might be
regrettable, but is true nonetheless. The present author has decided to stick with
the familiar, but strange, notation in this paper.
Space Complexity in Ordinal Turing Machines 465
Using these definitions, we can now define the classes PKf and P
K
α for func-
tions f and ordinals α. Note that a set of reals A is decidable, if there is a OTM
that terminates on all inputs r ∈ R, and which outputs 1 on exactly the reals
r ∈ A. Also, following [Lo¨], we will use the notations PHK and PSPACEHK
for the corresponding classes for Hamkins-Kidder style infinite Time Turing ma-
chines.
Definition 2.5. For any function f , we let PKf denote the class of all sets of
reals that are decidable by a time f machine. For any ordinal ξ, we let PKξ
denote the class of all sets of reals that are decidable by a time η machine for
some η < ξ.2 Likewise, we use the similar notations PHKf and P
HK
ξ for the
corresponding complexity classes for ITTMs.
For the space complexity classes, we can likewise proceed by simply defining
space(x, T ) as the largest α that occurs as the position of the head at some
stage of the computation:
Definition 2.6. If T is a machine that, at one point reaches the halting state
qf , and time(x, T ) = α, we define space(x, T ) = sup{hTβ (x) : β ≤ α}.
Definition 2.7. For any OTM T , we say that T is a space f machine if, for
all x ∈ R, we have space(x, T ) ≤ f(x). For any ordinal ξ, we say that T is a
space ξ machine if T is a space f machine for the constant function f such
that f(x) = ξ for all x ∈ R.
Definition 2.8. For any function f , we let PSPACEKf denote the class of all
sets of reals that are decidable by a space f machine. For any ordinal ξ, we let
PSPACEKξ denote the class of all sets of reals that are decidable by a space η
machine for some η < ξ.
An immediate result of these definitions (see [Lo¨]), is that a computation of
a ordinal Turing machine can never take more space than it can take time:
Proposition 2.9. For all f and all α, we have PKf ⊆ PSPACEKf and PKα ⊆
PSPACEKα respectively.
We will now introduce the notions of clockable, writable, eventually writable,
and accidentally writable ordinals. All these notions were originally defined in
[HaLe], from page 581 onward. Note that the these notions refer to writability
by ITTMs, rather than by OTMs!
Definition 2.10. We call an ordinal α x-clockable if there is an ITTM that
terminates at time α starting from input x. An ordinal α is x-writable if there
is an ITTM that terminates with a code for α on the output tape, starting from
input x. An ordinal α is eventually x-writable if there is an ITTM that, on
2 The < here and in Definition 2.8 is correct, and should not be a ≤. A justification
for this is found in the fact that using < instead of ≤ here allows for a larger variety
of classes–a broader discussion on the exact nature of these definitions can be found
in Appendix A of [Wi].
466 Joost Winter
input x, at one point writes a code for α on the out tape, never to change it
afterwards, without necessarily terminating. An ordinal α is accidentally x-
writable if there is an ITTM that, on input x, writes α on any of the tapes at
some stage of the computation.
With clockable, writable, etc., we mean ∅-clockable, -writable, etc.
We call the supremum of x-writable ordinals λx, the supremum of x-clockable
ordinals γx, the supremum of eventually x-writable ordinals ζx, and the supre-
mum of accidentally x-writable ordinals Σx.
It turns out that the notions of clockability, writability, eventually and acci-
dentally writability are related in the following way:
Proposition 2.11. For all reals x, γx = λx, and λx < ζx < Σx.
Proof. See [We, Corollary 2.1], and for a corrected and more detailed version,
[Wi, Proposition 3.16].
3 Variants on the ordinal Turing machine architecture
It turns out that the exact number of tapes used by an OTM is generally irrel-
evant, and that, as long as there is just one head used for these tapes, OTMs
with more tapes can be simulated by OTMs with less tapes, and vice versa.
Proposition 3.1. For any n ∈ N, with n > 2, a set A ⊆ R is decidable by an
OTM with n tapes if and only it is decidable by a ordinal Turing machine with
2 tapes.
Proof. Because we only are considering sets of reals here, we only have to deal
with inputs that are real numbers. Assume that A is decidable by a ordinal
Turing machine T with n tapes. We can now construct a ordinal Turing machine
T ′ with 2 tapes as follows:
– T ′ starts out by stretching the input such that, if x is the input, after the
stretching operation we find {(n − 1) · k : k ∈ x} on the first tape. We can
use a signal state to recognize when we are done with this operation, and
ensure this signal state only reads 1s at successor ordinals; at stage ω we will
find ourselves with the head at position ω, in this signal state, and read a 0,
and move left so that the head position will again be 0.
– After this, we simulate the original n tape machine on our 2 tape machine,
using the second tape to simulate the output tape, and the first tape to
simulate the n−1 other tapes. We make sure that we have a fixed number k
such that each step by T is simulated by k steps of T ′, and moreover ensure
that, for any ordinal α such that α = k · β for some k, hTα(x) = (n − 1) · η
for some η.
Because the only tape being simulated on the second tape is the original
second tape, and because first cell of the second tape of T corresponds to the
first cell of the second tape of T ′, it follows that T and T ′ will, on the same
input values, always give the same output values.
Space Complexity in Ordinal Turing Machines 467
This gives us the following corollary:
Corollary 3.2. For any m,n ≥ 2, a set A is decidable by a ordinal Turing
machine with m tapes if and only if it is decidable by a ordinal Turing machine
with n tapes.
Furthermore, it has been conjectured that this is also true in the case where
n = 1.
4 Time complexity for ordinal Turing machines
We can prove a number of theorems about time complexity for ordinal Turing
machines: to start with, it turns out that for many recursive ordinals α, a ordinal
Turing machine can compute exactly the same sets in time α that an ITTM
machine can. The first theorem is a modification of [Lo¨, Proposition 4].
It should be noted that, with sufficiently closed α and/or f , PHKα ⊆ PKα and
PHKf ⊆ PKf always follow. To simulate a Hamkins-Kidder machine by a Koepke
machine, we only need to take care of finding a way to recognize when we are
at a limit stage of the computation (which can be achieved through the use of
flags on an auxiliary tape), then move left, and move to a special limit state.
This ‘moving left’ might take at most ω steps, and hence, the inequality holds
for any ordinal α such that ω × α = ω.
Proposition 4.1. For any recursive ordinal α larger than ω2, where α is the
successor of a limit ordinal β, and ω × β = β, we have PHKα = PKα .
Proof. Assume that A ∈ PKα . This means that A is decidable by a time η machine
for some η < α, which, because α = β+1, here can be restated as: A is decidable
by a time β OTM. Then A is, by Proposition 2.9, decidable by a time β OTM
that uses β or less cells. Because β is a recursive ordinal, we can let an infinite
time Turing machine write a code for β on the tape in ω steps. After that, we
can simulate the algorithm of the ordinal Turing machine, which needs β cells,
on our ω-sized tape. Because every step of the simulated computation will be
simulated by a finite number of steps of the simulating computation, and because
the simulated computation takes less than β steps (the halting state cannot be
reached at β itself, because β is a limit ordinal), the simulating computation
will also take less than β steps. At limit stages, we still need to find which the
lim inf of the visited tape positions, which might take ω steps: hence, a single
step might be simulated by ω steps in the simulation.
Because β ≥ ω2, the complete algorithm will take less than ω + (ω × β) = β
steps in total.
This equivalence between the time complexity classes of OTMs and ITTMs
also holds at the level λ, the supremum of all writable ordinals:
Proposition 4.2. PHKλ = P
K
λ .
468 Joost Winter
Proof. Assume that A is decidable by a time α OTM T , where α < λ (or, in
other words, where α is a writable ordinal). This machine T is also a space α
machine, so the computation uses at most α cells. Say, α is writable by an ITTM
in time β: this implies that β is clockable and hence also writable. After writing
a code for α on the tape in β steps, we can continue by simulating the OTM
algorithm by simulating a tape of size α, in time less than α. Thus, we can
decide A with an ITTM in time β+α, which is again a writable ordinal. Hence,
A ∈ PHKλ .
It turns out, that in fact, an equality of the above type still holds up to the
level Σx, the supremum of all ordinals accidentally writable on input x. Note
here that PKΣx is taken to mean P
K
f for the function f such that f(x) = Σ
x for
all x.
Proposition 4.3. PKΣx = P
HK
λx .
Proof. Assume that a set A is in PKΣx . Then it follows from Proposition 2.9
that A is in PSPACEKΣx , and we know that there is an accidentally x-writable
ordinal α such that, if we can write α on the tape, we can compute A on an
ITTM, simulating an α-sized fragment of the tape of the OTM within space ω,
based upon the just-found coding of α.
The tactic now is to construct an ITTM, that simulates all other ITTMs on
input x, and in doing so, performs an algorithm that, one by one, writes all reals
that are accidentally writable from x on one of the scratch tapes. We can be sure
that every real that is accidentally writable from x will be written on one of the
scratch tapes at some point; and, each time a real is written on the scratch tape
this way, we check if it codes an ordinal. If it does, we continue simulating the
computation using this ordinal as tape length: eventually we will either run out
of space, in which case we continue the algorithm writing accidentally writable
reals on the tape, or it will turn out that we have sufficient space, in which case
the computation will finish.
This computation will eventually halt, because a real coding an ordinal equal
to or larger than α will be written on the tape at some point and, as all halting
computations will halt before λx, we know that the computation will halt before
λx. So, it follows that the set A is in PHKλx .
This gives us the following corollary:
Corollary 4.4. PKΣx = Dec
HK
Proof. This follows directly from DecHK = PHKλx and P
K
Σx = P
HK
λx .
5 Space complexity for OTMs
Now we will take a quick look look at space complexity issues for Koepke-type
Ordinal machines.
Proposition 5.1. DecHK ⊆ PSPACEKω+2
Space Complexity in Ordinal Turing Machines 469
Proof. Assume that A ∈ DecHK. We now can simulate an algorithm deciding A
on an OTM with tape length ω + 1 by moving left towards position 0 at limit
states, and besides this, simply performing the algorithm used by the ITTM.
Proposition 5.2. PSPACEKΣx ⊆ DecHK
Proof. In the proof Proposition 4.3, it was shown that PSPACEKΣx ⊆ PHKλx . In
combination with the fact that PHKλx = Dec
HK, this gives the desired result.
These two results combined give an interesting result: together they imply
that it does not really matter how much extra space you use on an OTM, as
long as it does not go beyond Σx. In particular, we have:
Corollary 5.3. For any ordinal α, such that α ≥ ω+1 and α ≤ Σ+1, and for
any function f such that we have f(x) ≥ ω and f(x) ≤ Σx for all x, we have
respectively that PSPACEKα = Dec
HK or PSPACEKf = Dec
HK.
So, it turns out that, even with space Σx at our disposal, we are unable to
compute any functions that an ITTM cannot compute. Moreover, we can now
establish a relationship between the PSPACEK-classes and the PK-classes in a
few cases.
Proposition 5.4. For any ordinal α, such that α ≥ ω + 1 and α ≤ λ, we have
that PKα ( PSPACE
K
α .
Proof. On one side, we know from Corollary 5.3 that PSPACEKα = Dec
HK,
and on the other side, we know that PKα ( Dec
HK.
Proposition 5.5. For functions f such that, for all x, Σx > f(x) > λx, we
have PSPACEKf = P
K
f = Dec
HK.
Proof. On one side, from Corollary 5.3 it follows that PSPACEKf = Dec
HK.
On the other side, we know that PKf ⊇ PKλx , and from Proposition 4.3 we know
that PKλx = P
HK
λx . From the fact that P
HK
λx = Dec
HK, it thus follows that PKf ⊇
DecHK. So we know that PSPACEKf = Dec
HK, as well as that PKf ⊇ DecHK.
Finally, from Proposition 2.9 we know that PKf ⊆ PSPACEKf , so that we must
have PKf = Dec
HK, completing the proof.
However, we will shortly show that Koepke machines are in fact more pow-
erful than ITTMs, even when it comes to computing sets of reals. To see this,
we will let ourselves be guided by the following question: are there any count-
able functions or ordinals, such that there are functions computable by an OTM,
bounded in space by respectively that ordinal or function, that are not decidable
by any ITTM? It turns out that this, indeed is the case. With Proposition 5.6,
we can show that the weak halting problem h is indeed in PSPACEKΣ+2:
Proposition 5.6. For all reals x, and for any computation by an ITTM T from
x that does not halt by time Σx, the snapshot at time ζx is the same as that at
time Σx.
470 Joost Winter
Proof. See [Wi, Proposition 3.16] for a modified version of an earlier proof by
Philip Welch.
Proposition 5.7. h ∈ PSPACEKΣ+2.
Proof. We will sketch a possible way to compute h in PSPACEKΣ+2 space. First
we will simply check if the input codes a natural number n; if it does not, we
simply output 0.
If it does, we know that either the computation of ITTM n on input n will
finish before time λ, or, as a result of Proposition 5.6 the contents of the tape
at stage ζ will be identical to that at stage Σ.
Now, we will simulate the (non-halting) computation of h on input n, but,
using a scratch tape, we will keep track of all the ω-sized tapes at any stage. If
we start a new step in the computation, we will first check if the configuration
at this step is equal to one at an earlier step. If this is the case, we halt. Because
the tape of the simulated computation has size ω, at any time α, we will have
used at most ω · α tape. This leaves us with the following two possibilities:
– The ITTM n, on input n, finishes before time λ. In this case, n ∈ h, and we
output 1, and we have used less than ω · λ < Σ space.
– The ITTM n, on input n, never finishes, and the content of stage Σ will be
equal to that at time ζ due to Proposition 5.6. We will realize this during
the checking stage at step Σ, and at that point the used part of the tape
has size Σ.
In either case, we will use at most Σ space during the computation, and thus
we have obtained a computation of h in PSPACEKΣ+2 space.
As a result, up to the level of Σx, OTMs bounded in space up to level Σx
cannot compute any sets that ITTMs cannot compute. However, just above the
level Σ this changes: the weak halting problem h is contained in PSPACEKΣ+2.
6 Relating the space classes to the ITTM degrees
The above results caused a few questions to be raised (initially by Benedikt Lo¨we
and Peter Koepke) about other variations on the infinite time Turing machine
architecture, with different space constraints. One of these questions was whether
the supremum of ordinals writable by OTMs that use at most Σ + 2 space is
equal to the set λh, that is, the supremum of ordinals writable from the weak
ITTM halting problem h. In this section a negative answer to this question
will be provided. We will furthermore extend this question a bit further, and
consider whether some connection can be made between OTMs with certain
space constraints, and the ITTM degrees.
Definition 6.1. We let λκ denote the supremum of ordinals (on input 0) writable
by an OTM that, on any real input, uses at most κ space. In particular, in this
article we will be concerned with OTM computations constrained in length by
Σ + 2.
Space Complexity in Ordinal Turing Machines 471
Following [HaLe, p. 588], we define the weak jump A∇ of a set A ∈ R as
follows:
Definition 6.2. We define
A∇ = A⊕ hA = A⊕ {p : φAp (0) ↓}
We let A∇
(n)
denote the αth iteration of the weak operation. For successor ordi-
nals this simply means that A∇
(α+1)
= A∇
(α)∇. For limit ordinals α, in [HaLe]
A∇
(α)
is defined as A ⊕ wα, where wα =
⊕
β<α wβ (here, for ordinals β < α,
wβ is the set such that A∇
(β)
= A ⊕ wβ), using some code z for α to organize
the information.
It is immediate from the definition that 0∇ = 0 ⊕ h, which is computable
from h and vice versa.
Proposition 6.3. If x is eventually writable, and y is eventually x-writable,
then y is eventually writable.
Proof. We perform the algorithm that eventually writes x. In parallel, we per-
form the algorithm that writes y from x on another set of tapes, using the tape
on which x will eventually be written as the input. During each step of the first
computation, we perform a single step of the second computation if x remains
unchanged during that step; and we restart, again copying the input from the
tape on which x will eventually be output, if x has changed during that step.
As x will be eventually written, from some point onward the second com-
putation will never be restarted, and continue forever, and eventually give y as
output. This is the desired algorithm which eventually writes y.
Proposition 6.4. For all eventually writable ordinals α, we have ζ = ζ0
∇(α)
,
and hence ζ > λ0
∇(α)
.
Proof. By the Eventual Jump Theorem, [HaLe, Theorem 6.14], if z is an even-
tually writable real, and α is an eventually writable ordinal, then the iterated
jump z∇
(α)
is also writable. Applying this theorem to iterated jumps of 0, we
obtain that 0∇
(α)
is eventually writable for every eventually writable ordinal α.
Assume for some ordinal α and some eventually writable ordinal β, that
α < ζ0
∇(β)
. This means that some code for α is eventually writable from 0∇
(β)
,
which is itself eventually writable.
But now, by Proposition 1, α itself is eventually writable, and hence α < ζ.
So we have established that ζ0
∇(α) ≤ ζ. The other direction, ζ ≤ ζ0∇(α) is trivial.
By [Wi, Proposition 3.14], which gives us the unequality of λx and ζx for all x,
the result ζ > λ0
∇(α)
now follows.
Proposition 6.5. ζ < λΣ+2
472 Joost Winter
Proof. We can compute ζ by a machine with tape length Σ+2 the following way:
for every natural number n, we can, using the construction in the proof of [Wi,
Proposition 6.21] decide if the computation by machine n on input 0 finishes,
and moreover, we are able to decide if machine n on input 0 will eventually write
some ordinal. The latter can be done by inspecting the repeating part once we
have recognized it: if it remains unchanged and codes some ordinal, then the
machine eventually writes some ordinal.
This way we can, using a pairing function, write down all codes for eventually
writable ordinals together in a single real. Once we have done this, we can reduce
this collection to a single ordinal by iteratively identifying the least elements from
the combined set. The ordinal thus obtained is the supremum of all eventually
writable ordinals, ζ. Because this is writable by a space Σ+2 machine, we obtain
ζ < λΣ+2.
We not only showed that λΣ+2 is not equal to λh, but indeed that it is not
equal to λ0
∇(α)
for any eventually writable ordinal α.
References
[HaLe] Joel David Hamkins and Andy Lewis: infinite time Turing machines, Jour-
nal Of Symbolic Logic 65 (2000), pp. 567-604
[Ko] Peter Koepke, Turing computations on ordinals, Bulletin of Symbolic Logic
11 (2005), pp. 377-397
[Lo¨] Benedikt Lo¨we: Space bounds for infinitary computation, in Arnold Beck-
mann, Ulrich Berger, Benedikt Lo¨we, John V. Tucker (eds.), Logical Ap-
proaches to Computational Barriers, Second Conference on Computabil-
ity in Europe, CiE 2006, Swansea, UK, July 2006, Proceedings, Springer-
Verlag, Berlin [Lecture Notes in Computer Science 3988] (2006), pp. 319-
329
[DeHaSc] Vinay Deolalikar, Joel David Hamkins, Ralf-Dieter Schindler, P 6= NP ∩
co−NP for infinite time Turing machines, Journal of Logic and Compu-
tation 15 (2005), pp. 577-592
[Wi] Joost Winter: Space Complexity in infinite time Turing Machines, ILLC
Scientific Publications, MoL-2007-14, 2007
[Sc] Ralf Schindler, P 6= NP for infinite time Turing machines, Monatshefte
der Mathematik 139 (2003), pp. 335-340
[We] Philip D. Welch, The Length of infinite time Turing machine Computa-
tions, Bulletin of the London Mathematical Society 32 (2000), pp. 129-136
Reverse Mathematics for Fourier Expansion
Keita Yokoyama
Department of Mathematics, Tokyo Institute of Technology
2-12-1 Oh-okayama, Meguro-ku, Tokyo 152-8551, JAPAN.
yokoyama@math.titech.ac.jp or k yoko tautology@infoseek.jp
Abstract. This research is motivated by the program of Reverse Math-
ematics. We investigate some theorems for the convergence of Fourier
series within some weak subsystems of second order arithmetic, in or-
der to determine which set existence axioms are needed to prove these
theorems. We show that uniform convergence of Fourier series for C1-
functions and L2-convergence of Fourier series for continuous functions
are equivalent to WKL0 over RCA0. We also show that L
2-convergence of
Fourier series for bounded continuous functions is equivalent to WWKL0
over RCA0.
Key words: Reverse Mathematics, second order arithmetic, Fourier
expansion, weak Ko¨nig’s lemma
1 Introduction
This paper is a contribution to the program of Reverse Mathematics, whose ul-
timate goal is to determine which set existence axioms are needed to prove theo-
rems of ordinary mathematics. The Reverse Mathematics program was initiated
by Friedman and carried forward most notably by Simpson. It is now known
that many of theorems of analysis, algebra and other branches of mathematics
are either proved in the system RCA0 or equivalent over RCA0 to particular set
existence axioms.
In this paper, we deal with subsystems RCA0, WKL0 and WWKL0 of second
order arithmetic. RCA0 is the system of recursive comprehension, which guaran-
tees the existence of recursively definable sets, and Σ01 induction. WKL0 consists
of RCA0 and a particular set existence axiom called weak Ko¨nig’s lemma, which
asserts that every infinite tree of sequences of 0’s and 1’s has an infinite path.
WWKL0 consists of RCA0 and weak weak Ko¨nig’s lemma, which asserts that if
a tree T has no path, then
lim
n→∞
|{σ ∈ T | lh(σ) = n}|
2n
= 0.
WWKL0 is weaker than WKL0. For details of the definitions of these subsystems,
see [1]. For WWKL0, see also [4].
Reverse Mathematics for various parts of analysis were investigated by sev-
eral people. We study Reverse Mathematics for Fourier expansions. In the next
474 Keita Yokoyama
section, we prepare basic parts of differential calculus for Fourier expansions. In
section 3, we study some theorems for Fourier expansions within RCA0, WKL0
and WWKL0. We show that uniform convergence of Fourier series for periodic
C1-functions and L2-convergence of Fourier series for periodic continuous func-
tions are equivalent to WKL0 over RCA0. We also show that L2-convergence of
Fourier series for bounded continuous functions is equivalent to WWKL0 over
RCA0. Moreover, we give a local approximation for a continuous function within
WWKL0.
2 Derivative and integral
Within RCA0, we can define the real number system R by Cauchy sequences, and
can also define continuous functions on R, see [1]. We first define C1-functions.
Definition 1 (C1-functions). The following definitions are made in RCA0. Let
U be an open subset of R, and let f and f ′ be continuous functions from U to
R. Then a pair (f, f ′) is said to be C1 if and only if
∀a ∈ U lim
x→a
f(x)− f(a)
x− a = f
′(a).
A continuous function f is said to be C1 if we can find a (code for a) continuous
function f ′ which is the derivative of f .
To prove basic properties of C1-functions in RCA0, we construct differen-
tiable condition functions. A differentiable condition function for a C1-function
f expresses the condition of differentiability at each point of dom(f). It also ex-
presses the continuity of the derivative f ′. Hence using a differentiable condition
function, we can easily prove basic properties of C1-functions in RCA0.
Theorem 1. The following is provable in RCA0. Let U be an open subset of R,
and let f be a C1-function from U to R. Then, there exists a continuous function
ef from U × U to R such that
∀x ∈ U ef (x, x) = 0;
∀x, y ∈ U f(y)− f(x) = (y − x)(f ′(x) + ef (x, y)).
We call this ef a differentiable condition function for f .
Proof. See [3].
Note that we can effectively find a differentiable condition function for a C1-
function.
To integrate continuous functions effectively, we introduce a modulus of in-
tegrability. Let f be a continuous function and let ∆ be a partition of [a, b], i.e.
∆ = {a = x0 ≤ ξ1 ≤ x1 ≤ · · · ≤ ξn ≤ xn = b}. Then, we define S∆[a,b](f) as
S∆[a,b](f) =
n∑
k=1
f(ξk)(xk − xk−1)
and define |∆| as |∆| = max{xk − xk−1 | 1 ≤ k ≤ n}.
Reverse Mathematics for Fourier Expansion 475
Definition 2 (modulus of integrability). The following definition is made in
RCA0. Let f be a continuous function from [a, b] to R. A modulus of integrability
on [a, b] for f is a function h from N to N such that for all n ∈ N and for all
partitions ∆1,∆2 of [a, b],
|∆1| < 2
−h(n)
b− a ∧ |∆2| <
2−h(n)
b− a → |S
∆1
[a,b](f)− S∆2[a,b](f)| < 2−n+1.
f is said to be effectively integrable if f has a modulus of integrability.
In RCA0, there might exist an unbounded continuous function on a closed
interval. Note that an unbounded continuous function on a closed interval is
not Riemann integrable and does not have a modulus of integrability. The next
theorem shows that to integrate continuous functions requires WKL0.
Theorem 2. The following assertions are pairwise equivalent over RCA0.
1. WKL0.
2. Every continuous function on [a, b] is Riemann integrable.
3. Every continuous function on [a, b] has a modulus of integrability.
Proof. See [1, Theorem IV.2.7].
To integrate bounded functions, we only need WWKL0.
Theorem 3. The following assertions are pairwise equivalent over RCA0.
1. WWKL0.
2. Every bounded continuous function on [a, b] is Riemann integrable.
3. Every bounded continuous function on [a, b] has a modulus of integrability.
Proof. Within WWKL0, we can prove a weak version of Heine-Borel theorem
appeared in [4]: if {Un}n∈N is an open covering of [0, 1], then there exists a
sequence of finite sequences of intervals 〈{[cni, dni]}i<ln | n ∈ N〉 such that
∀n ∈ N [0, 1] ⊆
⋃
k<n
Uk ∪
⋃
i<ln
[cni, dni];
lim
n→∞
∑
i<ln
(dni, cni) = 0.
By this theorem, we can prove 1 → 3 easily. The implication 3 → 2 is trivial.
To show 2 → 1, we define some notation. For a tree T ⊆ 2<N, define a set
ST ⊆ 2<N and λTn ∈ N as
ST := {σ ∈ 2<N | σ /∈ T ∧ ∀τ ⊆ σ(τ 6= σ → τ ∈ T )};
λTn := |{σ ∈ T | lh(σ) = n}|.
For a finite sequence σ ∈ 2<N, define aσ, bσ ∈ Q as
aσ :=
∑
i<lh(σ)
σ(i)
2i+1
;
bσ := aσ +
1
2lh(σ)
.
476 Keita Yokoyama
Thus, if σ, τ ∈ ST , then, bτ ≤ aσ or bσ ≤ aτ . Note that a tree T has a path if
and only if [0, 1] 6⊆ ⋃σ∈ST [aσ, bσ].
Now, we show ¬1 → ¬2. We reason within RCA0. Assume ¬WWKL0. Then,
there exist q > 0 and a tree T which has no path such that λTn/2
n > q for all
n ∈ N. Since [0, 1] ⊆ ⋃σ∈ST [aσ, bσ], we can define a continuous function f from
[0, 1] to [0, 1] as
f(x) :=
{
x−aσ
cσ−aσ x ∈ [aσ, cσ] ∧ σ ∈ ST ,
bσ−x
bσ−cσ x ∈ [cσ, bσ] ∧ σ ∈ ST
where cσ := (bσ + aσ)/2.
We show that this f is not Riemann integrable. Define partitions of [0, 1] ∆k
as
∆k :=
{
0 ≤ 1
2k
≤ 2
2k
≤ · · · ≤ 2
k − 1
2k
≤ 1
}
= {[aη, bη] | η ∈ 2<N ∧ lh(η) = k}.
Note that we can easily take Mσ := max{f(x) | x ∈ [aσ, bσ]} and mσ :=
min{f(x) | x ∈ [aσ, bσ]}. We show that for all k ∈ N,∑
η∈2<N∧lh(η)=k
(Mη −mη)2−k > q.
If η ∈ T , then, there exists σ ∈ ST such that σ ⊇ η, thus, [aη, bη] ⊇ [aσ, bσ].
Therefore, η ∈ T implies Mη −mη = 1. Hence, for all k ∈ N,∑
η∈2<N∧lh(η)=k
(Mη −mη)2−k ≥
∑
η∈T∧lh(η)=k
2−k ≥ λTn2−n > q.
This completes the proof of 2 → 1.
Next, we construct series of continuous functions.
Theorem 4. The following is provable in RCA0. Let {αn}n∈N be a sequence of
nonnegative real numbers whose series
∑∞
n=0 αn is convergent. Let {fn}n∈N be
a sequence of continuous functions on [a, b] such that ∀x ∈ [a, b] |fn(x)| ≤ αn
for all n ∈ N. Then, there exists a continuous function f such that
∀x ∈ [a, b] f(x) =
∞∑
n=0
fn(x).
Proof. See [1, Lemma II.6.5].
We can show termwise differentiation and integration by using a differentiable
condition function and a modulus of integrability. For the proof of the following
theorems, see [3].
Reverse Mathematics for Fourier Expansion 477
Theorem 5 (termwise differentiation). The following is provable in RCA0.
Let U be an open interval of R, and let
∑∞
n=0 an and
∑∞
n=0 bn be nonnegative
convergent series. Let {(fn, f ′n)}n∈N be a sequence of C1-functions from U to R
such that ∀x ∈ U |fn(x)| ≤ an and ∀x ∈ U |f ′n(x)| ≤ bn for all n ∈ N. Then,
there exists a C1-function (f, f ′) from U to R such that
∀x ∈ U f(x) =
∞∑
n=0
fn(x) ∧ f ′(x) =
∞∑
n=0
f ′n(x).
Theorem 6 (termwise integration). The following is provable in RCA0. Let∑∞
n=0 αn be nonnegative convergent series, and let {fn}n∈N be a sequence of
effectively integrable continuous functions from [a, b] to R such that ∀x ∈ [a, b]
|fn(x)| ≤ αn for all n ∈ N. By Theorem 4, we define f =
∑∞
n=0 fn. Then, f is
effectively integrable and∫ b
a
f(x) dx =
∞∑
n=0
∫ b
a
fn(x) dx.
(We say that {fn}n∈N is a sequence of effectively integrable continuous functions
if there exists a sequence of functions {hn}n∈N such that each hn is a modulus
of integrability for fn.)
By Theorems 4 and 5, we can construct trigonometric functions sinx and cosx
as C1-functions within RCA0. We can also define pi as a unique zero-point of sinx
on [2, 4]. Note that we can prove basic properties of sinx and cosx in RCA0.
3 Fourier expansion
In this section, we show some results on Reverse Mathematics for some basic
theories of Fourier expansions. See e.g. [2] for the usual theory of Fourier analysis.
We write f ∈ P2pi if f is a continuous periodic function with period 2pi. Let
{ak}k∈N, {bk}k∈N be real sequences. Then, define Sn as
Sn[{ak}{bk}](x) = a02 +
n∑
k=1
ak cos kx+ bk sin kx.
If f ∈ P2pi is effectively integrable, then, define
Sn[f ](x) = Sn[{ak}{bk}](x)
where ak and bk are Fourier coefficients, i.e.,
ak =
1
pi
∫ pi
−pi
f(x) cos kxdx;
bk =
1
pi
∫ pi
−pi
f(x) sin kxdx.
We first prepare some lemmas.
478 Keita Yokoyama
Lemma 1. The following assertions are pairwise equivalent over RCA0.
1. WKL0.
2. Every periodic C1-function is bounded.
3. Every periodic C1-function is uniformly continuous.
Proof. Easy modification of the proof of [1, Theorem IV.2.3] (by means of piece-
wise parabolic functions).
Lemma 2 (Bessel inequality). The following is provable in RCA0. Let f ∈
P2pi be effectively integrable and let ak and bk be Fourier coefficients of f . Then,
2pi
n∑
i=0
(|ai|2 + |bi|2) ≤
∫ pi
−pi
f(x)2dx
for all n ∈ N.
Proof. Straightforward imitation of the usual proof.
Lemma 3. The following is provable in RCA0. Let f ∈ P2pi be effectively inte-
grable. If ∫ pi
−pi
f(x) cos kxdx = 0 ∧
∫ pi
−pi
f(x) sin kxdx = 0
for all n ∈ N, then f ≡ 0.
Proof. Straightforward imitation of the usual proof.
Lemma 4. The following is provable in RCA0. Let f ∈ P2pi be a C1-function
and let f and f ′ be effectively integrable. Then the Fourier series Sn[f ] uniformly
converges to f .
Proof. We reason within RCA0. Let ak and bk be Fourier coefficients of f , let a′k
and b′k be Fourier coefficients of f
′ and let K =
∫ pi
−pi f
′(x)2dx. Then,
piak =
∫ pi
−pi
f(x) cos kxdx =
1
k
∫ pi
−pi
f ′(x) sin kxdx =
pib′k
k
;
pibk =
pia′k
k
.
By Schwarz inequality and Lemma 2,
m∑
k=n
|ak|+ |bk| ≤
√√√√2 m∑
k=n
1
k2
√√√√ m∑
k=n
|b′k|2 + |a′k|2 ≤
√√√√K
pi
m∑
k=n
1
k2
.
Thus,
∑∞
k=0 |ak|+ |bk| converges. Then, by Theorem 4, there exists g ∈ P2pi such
that
g(x) = lim
n→∞Sn[f ](x) =
a0
2
+
∞∑
k=1
ak cos kx+ bk sin kx.
Let f¯ = g−f . Then, by Lemma 3, f¯ ≡ 0. This means Sn[f ] uniformly converges
to f . This completes the proof.
Reverse Mathematics for Fourier Expansion 479
The first theorem is concerned with the uniform convergence of Fourier series.
Theorem 7. The following assertions are equivalent over RCA0.
1. WKL0.
2. If f ∈ P2pi is a C1-function, then there exist real sequences {ak}k∈N and
{bk}k∈N such that Sn[{ak}{bk}] uniformly converges to f .
Proof. By Theorem 2 and Lemma 4, 1 → 2 holds. For the converse, we assume
2. By Lemma 1, we only need to show that every periodic C1-function (with
period 2pi) is uniformly continuous. Let f ∈ P2pi be a C1-function. Then, there
exist {ak}k∈N and {bk}k∈N such that Sn[{ak}{bk}] uniformly converges to f .
Since sinx and cosx are uniformly continuous, we can easily show that f is also
uniformly continuous. This completes the proof.
Next, we argue about L2-convergence of Fourier series.
Definition 3 (L2-convergence). The following definition is made in RCA0.
Let {fn}n∈N be a sequence of functions in P2pi. Then, we say that {fn}n∈N L2-
converges to f if for all i ∈ N there exists k ∈ N such that for all m ≥ k there
exists a continuous function g such that g2 is effectively integrable and
|fm(x)− f(x)| ≤ g(x),∫ pi
−pi
g(x)2dx < 2−i.
Lemma 5. The following is provable in RCA0. Let f ∈ P2pi be effectively inte-
grable. Then, the Fourier series Sn[f ] L2-converges to f .
Proof. We reason within RCA0. Let h : N → N be a modulus of integrability
for f on [−pi, pi]. We can construct a sequence of continuous functions on [−pi, pi]
{f˜i}i∈N (by means of piecewise parabolic functions) which satisfies the following:
– f˜i is C1 and f˜i and f˜i
′
are effectively integrable;
– f˜i(tij) = f(tij);
– f˜i is monotone on [tj , tj+1]
where tij = −pi + 2pij/2h(i) (j ≤ 2h(i)). Then, {f˜i}i∈N L2-converges to f . By
Lemma 2,∫ pi
−pi
(f(x)− Sn[f ](x))2dx
≤
∫ pi
−pi
{(f(x)− f˜i(x))2 + (f˜i(x)− Sn[f˜i](x))2 + (Sn[f˜i](x)− Sn[f ](x))2}dx
≤ 2
∫ pi
−pi
(f(x)− f˜i(x))2dx+
∫ pi
−pi
(f˜i(x)− Sn[f˜i](x))2dx.
480 Keita Yokoyama
Since {f˜i}i∈N L2-converges to f ,
lim
i→∞
∫ pi
−pi
(f(x)− f˜i(x))2dx = 0.
By Lemma 4, {Sn[f˜i]}n∈N uniformly converges to f˜i. Thus,
lim
n→∞
∫ pi
−pi
(f˜i(x)− Sn[f˜i](x))2dx = 0.
Hence,
lim
n→∞
∫ pi
−pi
(f(x)− Sn[f ](x))2dx = 0,
and this completes the proof.
Theorem 8. The following assertions are equivalent over RCA0.
1. WKL0.
2. If f ∈ P2pi, then there exist real sequences {ak}k∈N and {bk}k∈N such that
Sn[{ak}{bk}] L2-converges to f .
Proof. We reason within RCA0. By Theorem 2 and Lemma 5, 1 → 2 holds. For
the converse, we show ¬1→ ¬2. Let ¬WKL0. Then, by Lemma 1, there exists an
unbounded function f ∈ P2pi. Thus, for any real sequences {ak}k∈N and {bk}k∈N,
|Sn[{ak}{bk}] − f | is unbounded. Therefore, if |Sn[{ak}{bk}] − f | ≤ g, then g2
is not integrable. This means that ¬2 and this completes the proof of 2 → 1.
Theorem 9. The following assertions are equivalent over RCA0.
1. WWKL0.
2. If f ∈ P2pi and |f | ≤ K for some K ∈ Q, then there exist real sequences
{ak}k∈N and {bk}k∈N such that Sn[{ak}{bk}] L2-converges to f .
Proof. We reason within RCA0. By Theorem 3 and Lemma 5, 1 → 2 holds. For
the converse, we show ¬1 → ¬2. We use the notation ST and λTn defined in the
proof of Theorem 3. Let ¬WWKL0. Then, there exist q > 0 and a tree T which
has no path such that λTn/2
n > q for all n ∈ N. For a finite sequence σ ∈ 2<N,
define aσ, bσ ∈ R as
aσ := −pi + 2pi
∑
i<lh(σ)
σ(i)
2i+1
;
bσ := aσ +
2pi
2lh(σ)
.
Since T has no path, [−pi, pi] = ⋃σ∈ST [aσ, bσ]. Define a function f ∈ P2pi as
f(x) :=

8(x−aσ)
bσ−aσ x ∈ [aσ, cσ] ∧ σ ∈ ST ,−8{x−(aσ+bσ)/2}
bσ−aσ x ∈ [cσ, dσ] ∧ σ ∈ ST ,
8(x−bσ)
bσ−aσ x ∈ [dσ, bσ] ∧ σ ∈ ST
Reverse Mathematics for Fourier Expansion 481
where cσ := (bσ + 3aσ)/4 and dσ := (3bσ + aσ)/4. Then, f(cσ) = 2, f(dσ) = −2
for any σ ∈ ST and |f | ≤ 2.
Now, we show that for any real sequences {ak}k∈N and {bk}k∈N and for any
n ∈ N, if |Sn[{ak}{bk}] − f | ≤ g, then
∫ pi
−pi g(x)
2dx > piq. Let {ak}k∈N and
{bk}k∈N be real sequences, let n ∈ N and let g be a continuous function such
that g2 is effectively integrable and |Sn[{ak}{bk}] − f | ≤ g. Take M0 ∈ Q such
that M0 ≥ max{|a0|, . . . , |an|, |b0|, . . . , |bn|} and define M := (n+ 1)2M0. Then,
|Sn[{ak}{bk}]′(x)| ≤ M for any x ∈ [−pi, pi]. Thus, if σ ∈ ST and 2pi/2lh(σ) ≤
1/M , then, |Sn[{ak}{bk}](x)| < 1 for all x ∈ [aσ, bσ] or |Sn[{ak}{bk}](x)| > −1
for all x ∈ [aσ, bσ]. Therefore, g(cσ) > 1 or g(dσ) > 1 for any σ ∈ ST such that
2pi/2lh(σ) ≤ 1/M . Let h be a modulus of integrability for g2 on [−pi, pi]. Take
N ∈ N such that 2pi/2N ≤ 1/M and 2−N < 2−h(i)/2pi where i = min{j ∈ N |
2−j+2 < piq}. As in the proof of Theorem 3, if η ∈ T and lh(η) = N , then there
exists x ∈ [aη, bη] such that g(x)2 > 1. Take 〈αη ∈ [aη, bη] | η ∈ 2<N∧ lh(η) = N〉
such that g(αη) > 1 if η ∈ T . Then, as in the proof of Theorem 3,∫ pi
−pi
g(x)2dx ≥
∑
η∈2<N∧lh(η)=N
g(αη)2(bη − aη)− 2−i+2
≥
∑
η∈T∧lh(η)=N
(bη − aη)− 2−i+2
≥ 2piλ
T
N
2N
− piq
> piq.
This means that ¬2 and this completes the proof of 2 → 1.
Imitating the usual arguments for Fourier expansions in RCA0, we can show
the following theorems.
Theorem 10 (Parseval equality). The following is provable in RCA0. Let
f ∈ P2pi be effectively integrable, and let ak and bk be Fourier coefficients of f .
Then,
2pi
∞∑
i=0
(|ai|2 + |bi|2) =
∫ pi
−pi
f(x)2dx.
Theorem 11 (Riemann-Lebesgue lemma). The following is provable in
RCA0. Let f be an effectively integrable continuous function on R. Then, for
all a, b ∈ R,
lim
n→∞
∫ b
a
f(x) cosnxdx = 0.
Theorem 12 (pointwise convergence). The following is provable in RCA0.
Let f ∈ P2pi be of bounded variation on [−pi, pi], i.e., there exist monotone in-
creasing functions g0, g1 such that f = g0 − g1. Then, f is effectively integrable
and Sn[f ] pointwise converges to f .
482 Keita Yokoyama
Theorem 13. Let f1, f2 ∈ P2pi be effectively integrable and let x0 ∈ R. Let
f1 ≡ f2 on some neighborhood of x0. Then, Sn[f1](x0) converges if and only if
Sn[f2](x0) converges. Moreover, if Sn[f1](x0) converges, then,
lim
n→∞Sn[f1](x0) = limn→∞Sn[f2](x0).
Finally, we argue about local approximation for continuous functions by
trigonometric functions.
Theorem 14. The following assertions are equivalent over RCA0.
1. WWKL0.
2. If f is a continuous function on R and x0 ∈ R, then there exist real se-
quences {ak}k∈N and {bk}k∈N such that Sn[{ak}{bk}] L2-converges to f on
a neighborhood of x0.
Proof. We reason within RCA0. 1 → 2 is a straightforward consequence of The-
orem 9. For the converse, we show ¬1 → ¬2. We reason within RCA0. By
¬WWKL0, define a continuous function f on [−pi, pi] as in the proof of 2 →
1 of Theorem 9. Then, define continuous functions fi on [0, pi/2i] as fi(x) =
2−if(2i+1x−pi). Note that |fi| ≤ 2−i+1. Thus, we can define a continuous func-
tion f¯ on [−pi, pi] as
f¯(x) :=

fi+1(x+ pi2i ) x ∈ [−pi2i , −pi2i+1 ],
fi+1(x− pi2i+1 ) x ∈ [ pi2i+1 , pi2i ],
0 x = 0.
Let U be a neighborhood of 0. Then, there exists i ∈ N such that [ pi2i+1 , pi2i ] ⊆ U .
As in the proof of Theorem 9, there are no real sequences {ak}k∈N and {bk}k∈N
such that Sn[{ak}{bk}] L2-converges to f¯ on [ pi2i+1 , pi2i ]. This means that ¬2 and
this completes the proof of 2 → 1.
References
1. S. G. Simpson. Subsystems of Second Order Arithmetic. Springer-Verlag, 1999.
2. Elias M. Stein and Rami Shakarchi. Fourier Analysis. Princeton Lectures in Anal-
ysis. Princeton University Press, 2003.
3. Keita Yokoyama. Standard and Non-standard Analysis in Second Order Arithmetic.
Doctoral thesis, Tohoku University, December 2007.
4. X. Yu and S. G. Simpson. Measure theory and weak Ko¨nig’s lemma. Archive for
Mathematical Logic, 30:171–180, 1990.
Induced Matchings
in Graphs of Maximum Degree Three
Graz˙yna Zwoz´niak
Institute of Computer Science, Wrocław University, Poland
grazyna@ii.uni.wroc.pl
Abstract. An induced matching is a matchingM where each two edges e1, e2 ∈
M are at distance greater than one. In this paper we consider the problem of
finding maximum induced matchings in graphs of maximum degree three. The
problem is NP-hard. We present an algorithm which finds the maximum induced
matching of size at least n/6+O(1), where n is the number of vertices in a graph.
Using this bound we achieve an approximation ratio of 1.8 in cubic graphs.
Key words: induced matching, graphs, cubic, approximation algorithm
1 Introduction
Given a connected undirected graph G = (V, E), the maximum induced matching
problem is to find a maximum set of edges M which fulfils the following conditions:
(a) no two edges of M share a common vertex (b) no two edges of M are joined by
an edge of G. Induced matchings are also referred to as strong matchings [6, 7]. The
problem has received considerable attention in the discrete mathematics community,
since finding large induced matchings is a subtask of finding a strong edge-colouring in
a graph [4, 5, 11, 14], a proper colouring of the edges, where no edge is incident to two
edges of the same colour. There is also immediate connection between the size of an
induced matching and the irredundancy number of a graph [7]. Stockmeyer and Vazi-
rani [15] motivate the problem as the risk-free marriage problem, where each married
person is compatible with no married person other than the one he (or she) is married to.
On practical side, induced matchings have the applications for secure communication
channels and network flow problems [8].
Cameron [2] showed that the maximum induced matching problem is NP-hard for
bipartite graphs. Ko and Shepherd [12] proved the NP-hardness for cubic planar graphs.
The problem was shown to be polynomial for chordal graphs and for interval graphs by
Cameron [2]. Golumbic and Laskar [7] gave a polynomial time algorithm for circular
arc graphs. Golumbic and Lewenstein [8] constructed polynomial time algorithms for
trapezoid graphs, interval-dimension graphs and cocomparability graphs, and a linear
time algorithm for the maximum induced matching problem in interval graphs. The
linear time algorithms for trees were presented by Fricke and Laskar [6], Zito [16],
Golumbic and Lewenstein [8].
There are several papers that focus on finding maximum induced matching in d-
regular graphs. Zito [16] showed that for every k ≥ 1 there is a constant c > 1 such that
the problem of approximating maximum induced matching within a factor of c on 4k-
regular graphs is NP-hard. He also showed that the problem is approximable within d.
484 Graz˙yna Zwoz´niak
Duckworth, Manlove and Zito [3] proved that for any ǫ > 0 it is NP-hard to approximate
maximum induced matching within a factor of 12601259 − ǫ in graphs of maximum degree
three and within a factor of 74207419 − ǫ in cubic graphs. They also presented a greedy
algoritm for approximating a maximum induced matching in d-regular graphs with a
factor of d − 1. Gotthilf and Lewenstein [9] provided a simple greedy approach that
yields an 0.8d approximation factor.
In our paper we concentrate on graphs of maximum degree three. We present an
algorithm which finds IM(G) – an induced matching of a graph G of size n/6+O(1).
The approach to the problem is quite novel. Firstly a forest F with large number of
leaves is constructed. Then the algorithm processes small subtrees of the trees T ∈ F
and adds some edges to IM(G). If after this step IM(G) is not maximal then some
edges e ∈ E(G)\E(F ) are added to IM(G). The properties of the forest F let us
significantly reduce the number of possible cases which we have to consider looking at
the subgraphs of G. It was shown [16] that the size of an optimum induced matching in
a cubic graph is at most 3n/10, so using our bound we achieve an approximation ratio
of 1.8 for the maximum induced matching problem in cubic graphs.
The paper is organized as follows. In Section 2 we introduce some notation. In
Section 3 we show how to construct the forest F and give some of its properties. In
Section 4 we present the second part of the algorithm – adding edges to IM(G). In
Section 5 we give the main contribution of our paper. The proofs of the facts, lemmata
and the theorem can be found in the full version of the paper.
2 Preliminaries
Let G be a connected undirected graph. We use V (G) to denote the set of vertices in G
and E(G) to denote the set of edges in G. For a vertex v ∈ V (G) let ΓG(v) denote the
set of vertices {w : (v, w) ∈ E(G)}. The degree of v in G, degG(v), is the number
of edges incident to v in G. If T is a rooted tree, then we use LCAT (u, w) to denote
the lowest common ancestor of vertices u, w in T , f(v) to denote the father of a vertex
v ∈ V (T ), and h(T ) to denote the height of the tree T . By L(T ) we denote the set of
leaves of T .
If u, v ∈ V (G) then dist(u, v) denotes the shortest distance between u, v. If
e1, e2 ∈ E(G) then dist(e1, e2) denotes the shortest distance between e1, e2.
3 Construction of the forest F
In this section we consider a connected undirected graph G of maximum degree three,
where at least one vertex v ∈ V (G) has degree three. We present the algorithm which
constructs the forest F for G. This algorithm was also used in [13] and [17], were the
forest F was the base structure for the presented algorithms.
In the first step our algorithm builds successive trees T0, . . . , Tk of a forest F .
Rule 1 puts to the tree Ti two vertices u, w 6∈ V (F ) adjacent to a leaf v ∈ L(Ti), see
Fig. 1(a). Rule 2 puts to Ti a vertex u 6∈ V (F ) adjacent to a leaf v ∈ L(Ti) together
with both further neighbours w1, w2 of u, where w1, w2 6∈ V (F ), see Fig. 1(b). We
name the two leaves added by Rules 1 and 2 the left and the right son of their father.
Induced Matchings in Graphs of Maximum Degree Three 485
(a) (b) (c) (d)
TiTiTi Ti
vvvv
u
u
w
w
w
w1 w2
x
x
y
y
u1u1
u2
us
us
Fig. 1. (a) Rule 1. (b) Rule 2. (c), (d) Rule 3.
Rule 3 initiates a new tree Tj , j > i, see Fig. 1(c)-(d). Let P be a path which starts
at v ∈ L(Ti), goes through vertices u1, . . . , us 6∈ V (F ), s ≥ 1 and ends at the first
vertex w 6∈ V (F ), where degG(w) = 3, ΓG(w) = {us, x, y} and x, y 6∈ V (F ). The
vertex us precedes w on P . Rule 3 starts to build Tj rooted at us and adds us, w, x, y
to V (Tj) and (us, w), (w, x), (w, y) to E(Tj). We refer to Ti as the father of Tj . This
relation determines a partial order in F , so we use some other related terms as e.g.
ancestor of the tree.
Let F = {T0, . . . , Tk}. In our algorithm and analysis we use the following notions:
Definition 1. A vertex v ∈ V (G) is an exterior vertex if v 6∈ V (F ). Let EX denote the
set of all exterior vertices in G.
Definition 2. Let r0 be a root of T0. Let R be the set of roots of the trees T1, . . . , Tk.
The algorithm works as follows.
CONSTRUCT F(G).
1. F ← ∅
2. V (T0) ← {r0, v1, v2, v3}, where r0 is any degree three vertex of G and
v1, v2, v3 ∈ ΓG(r0); E(T0) ← {(r0, v1), (r0, v2), (r0, v3)}; let r0 be a root
of T0; i← 0
3. if it is possible: find the leftmost leaf in Ti that can be expanded by the
Rule 1 and expand it; go to step 3;
else: go to step 4;
4. if it is possible: find the leftmost leaf in Ti that can be expanded by the
Rule 2 and expand it; go to step 3
else: F ← F ∪ Ti and go to step 5
5. if it is possible: find the leftmost leaf v in Ti such that Rule 3 can be applied
to v and apply this rule to v; i ← i + 1; let Ti be a new tree created in this
step; go to step 3 with Ti
else: if Ti has a father: go to step 5 with the father of Ti
else: return F .
The properties of the forest F are described by Fact 1. The possibilities for edges
e ∈ E(G)\E(F ) are presented in Fig. 1(c),(d) and 2.
486 Graz˙yna Zwoz´niak
(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)
TiTiTiTiTi Ti Ti Ti TjTj
Tj TjTjTjTjTj Tj Tj
Fig. 2. The edges (x, y) ∈ E(G)\E(F ), x, y ∈ EX ∪ L(F ) (dashed lines). Either Ti = Tj or
Ti is an ancestor of Tj . Dotted lines denote the paths where all interior vertices have degree two
in G.
Fact 1 Let v ∈ V (T ), T ∈ F .
1. If degT (v) = 2 and w ∈ ΓT (v) then degT (w) = 3.
2. If degT (v) = 2 and w ∈ ΓG(v) then w ∈ V (T ).
3. If v is adjacent to the vertex w ∈ EX ∪R, u ∈ ΓG(v) and u 6= w then u ∈ V (T ).
The following facts let us consider the subtrees of the trees T ∈ F in some order
described precisely in the next section.
Fact 2 Let degT (u) = 2, degT (w) = 2, T ∈ F , (u, w) ∈ E(G)\E(F ), a =
LCAT (u, w) and let u be on the left of w in T . Then w is the right son of a, and
there are no vertices of degree 2 on the path from u to a in T .
Fact 3 Let r be a root of a tree T ∈ F . Let u ∈ L(T ) and let w be the first vertex of
degree 2 on the path from u to r in T . If there is v ∈ V (T ) such that degF (v) = 2 and
(u, v) ∈ E(G)\E(T ) then w is an ancestor of v in T .
4 Induced matching
Let G
′ denote a current graph, F ′ = G′ ∩ F , EX ′ = V (G′)\V (F ′). At the beginning
G
′
= G, F
′
= F and EX ′ = EX . In successive steps of our algorithm we add
some edges (u, v) to IM(G) and remove from E(G′) these added edges with all edges
(w, z) where (u, v), (w, z) are at distance at most one. A vertex u is removed from
V (G
′
) when the last edge incident to u is removed from E(G′).
Now we present the idea of our algorithm, precise description is given later.
There are two main phases of our algorithm. After the first one all edges E(F ′)
and some edges E(G′)\E(F ′) are deleted from E(G′). Moreover, after this phase all
vertices in V (G′) have degree at most two. After the second phase the remaining edges
E(G
′
)\E(F
′
) are removed from E(G′).
Each phase consists of the steps. In a single step some edges E¯ are added to IM(G)
and appropriate edges Ed and vertices Vd are deleted from G
′
. Our goal is to choose E¯
in such a way that |Vd| ≤ 6|E¯|.
Since after the first phase every connected component of G′ is either a path or a
simple cycle, the way of adding some of their edges to IM(G) is obvious. Now we will
describe more precisely the first phase.
Induced Matchings in Graphs of Maximum Degree Three 487
Fig. 3. Vertices v1, v2, v3 and z would be removed from V (G
′
) if the edge (u, w) was added to
IM(G). In the cases (b)-(d) there exist edges (vi, f(vi)), (f(vi), f(f(vi))) ∈ E(F ′).
Let T
′
= G
′
∩ T , where T ∈ F . Let S′x denote the subtree of T
′
rooted at some
vertex x ∈ V (T
′
). We define S′+x as a tree, where E(S
′+
x ) = E(S
′
x) ∪ E
+ and E+ ⊆
E(G
′
)\E(F
′
) denotes some edges incident to V (S′x) (precise definition of S
′+
x is given
later). We process bottom-up small subtrees S′x of T
′
and we look for the edges E¯ =
{(u, w) : u, w ∈ V (S
′+
x ), u, w 6= x} such that if we added E¯ to IM(G) then
1. the set E(S′x) would be empty;
2. the equality E(T ′) = E(S′r) would hold, where r is the root of T ;
3. the vertices v ∈ EX ′ with degG′ (v) = 3 and some neighbours in V (S
′
x) would be
removed or would change degree into 1 or 2.
In the special cases when x ∈ {r0} ∪ R and h(S
′+
x ) = 1 we add to IM(G) an edge
incident to x.
The special care is needed when u ∈ V (S′x), v 6∈ V (S
′
x), degT ′ (v) = 2 and
(u, v) ∈ E(G
′
), because if we added an edge incident to u to IM(G) then the equality
E(T
′
) = E(S
′
r) might not hold (there could be two connected components: S
′
r and
the tree rooted at the son of v). That is why in our algorithm the vertices y, where
degT ′ (y) = 2 and degG′ (y) = 3, are treated as ”milestones”. More precisely, we
consider in post-order subtrees S′y of T
′
, where (degT ′ (y) = 2 and degG′ (y) = 3) or
(y is the root of T , T ′ = G′ ∩ T ), and process bottom-up subtrees S′x of S
′
y .
When we process S′+x some vertices v 6∈ V (S
′+
x ) may be removed. To reduce
the number of possible cases which we have to consider looking at these vertices we
make some preprocessing and add to IM(G) some edges e ∈ E(G′)\E(F ′) which are
close to S′x. Later we will define precisely the sets C1(S
′
x) and C2(S
′
x) of such edges.
After this operation we process S′+x and add some edges E¯ to IM(G). Now a vertex
z 6∈ V (S
′+
x ) is removed if either z ∈ N or ΓG′ (z) ⊆ N , where N = {v : (u, v) ∈
E(G
′
), u ∈ V (S
′+
x ), v 6∈ V (S
′+
x ) and (u, w) ∈ E¯ for some w ∈ V (S
′+
x )}, see Fig. 3.
In our algorithm we use some kind of accounting analysis. We add some edges E¯
to IM(G) if they remove at most 6|E¯| vertices from V (G′). But in some cases we let
the number of removed vertices be larger, let us say 6|E¯|+ x. In these cases we choose
x vertices which are still in V (G′) and mark them. Later these marked vertices will be
double counted in the process of adding edges to IM(G). Every chosen vertex may be
marked at most once. It is possible to mark a vertex a ∈ V (G′) if
488 Graz˙yna Zwoz´niak
(a) (b)
S
′
xS
′
xS
′
xS
′
x
a
a
a
a
a a a
a
b1 b1
b1
b1
b1b1b1b1b2
b2
b2
b2
b2b2b3 b3c1 c1 c1c2 c2c3
d1
d1
d1
d1
d1 d1
d1
d1
d2
d2
d2
d2
d2
d2
d3 d3
f1
f1
f2
f2
Fig. 4. A vertex a may be marked. (a) situations before adding E¯ to IM(G), (b) situations after
adding E¯ to IM(G). It is possible that ci1 = ci2 , i1, i2 ∈ {1, 2, 3}.
1. before adding edges E¯ to IM(G) there are edges (ci, bi), (bi, a) ∈ E(G
′
),
(bi, di) ∈ E(F
′
), such that degG′ (a) = j, 1 ≤ j ≤ 3, 1 ≤ i ≤ j, and after adding
edges E¯ to IM(G) all edges (ci, bi) would be removed, a ∈ V (G
′
), at least one
bi ∈ V (G
′
), and if bi ∈ V (G
′
) then degG′ (bi) = 2, or
2. before adding edges E¯ to IM(G) there are edges (a, bi), (bi, fi) ∈ E(G
′
),
(bi, di) ∈ E(F
′
), degG′ (a) = 3, degG′ (fi) = 1, where i = 1, 2 and adding edges
E¯ to IM(G) degG′ (a) = 2 and (a, bi), (bi, fi), (bi, di) ∈ E(G
′
), see Fig. 4.
The choice of vertices which can be marked is intentional. In most of these cases we
do not need to propagate the marking process, because we are sure that marked vertices
are in the trees, where it is possible to add y edges to IM(G) and remove less than 6y
vertices from V (G′).
The algorithm starts with the procedure CONSTRUCT IM(G′), which realizes two
main phases of our algorithm: the first one which removes E(F ′) (step 1 of the proce-
dure) and the second one which removes the remaining edges of G′ (steps 2 and 3 of
the procedure).
The procedure MATCHING(T ′) applies the procedure MATCH() to the subtrees of
T
′
.
The procedure MATCH(S′y) starts with preprocessing steps. Then it tries to remove
the edges of the trees S′+x , where x ∈ V (S
′+
y ) and h(S
′+
x ) = 2. If it is not possible,
trees S
′+
x of height three are considered. If it is not possible, trees S
′+
x of height four
are considered. If it is impossible to remove the edges of any tree of height two, three
or four and there is a tree S′+x of height five then some edges are added to IM(G) and
E(S
′+
x ) are removed. In the special case when x = r0, degG′ (x) = 3 and h(S
′+
x ) > 1
we split S′+x into two trees and successively process them.
y1
y2 y3
y4y5
y6
y7
y8
Fig. 5. The tree T
′
∈ F
′
rooted at y8. The algorithm considers successively subtrees rooted at
y1, y2, y3, y4, y5, y6, y7, y8.
Induced Matchings in Graphs of Maximum Degree Three 489
CONSTRUCT IM(G
′
)
1. apply the procedure MATCHING() in post-order to T ′ ;
2. while there is a path {(v1, v2), . . . , (vk−1, vk)}: ADD((v3i+1, v3i+2)),
i = 0, . . . , k−2
3
;
3. while there is a cycle {(v1, v2), . . . , (vk, v1)}: ADD((v3i+1, v3i+2)),
i = 0, . . . , k−3
3
.
MATCHING(T ′ )
1. apply the procedure MATCH() in post-order to the subtrees S′y of T ′ such that
(deg
T
′ (y) = 2 and deg
G
′ (y) = 3) or (y is the root of T , T ′ = G′ ∩ T ).
MATCH(S′y)
1. if h(S′y) > 0 apply the procedure MATCH() to the trees rooted at the sons of y;
2. while there is an edge (a, b) ∈ C1(S
′
y): ADD((a, b));
3. while there is an edge (a, b) ∈ C2(S
′
y): ADD((a, b));
4. for (i = 2; i ≤ 5; i++):
5. if there is a tree S′+x , where x ∈ V (S
′
+
y ), x 6= r0 or degF ′ (x) 6= 3, h(S
′
+
x ) = i
and M(S′+x ) 6= ∅: MAKE MATCH(S
′
+
x ) and goto 2;
6. if h(S′+y ) = 1 and y ∈ {r0}∪R or |E(G
′
)| = 1: ADD((y, u)) where u is a son of
y;
7. if y = r0 and degF ′ (y) = 3:
let S′∗y be a part of S
′
+
y where E(S
′
∗
y ) = E(S
′
+
u1
) ∪ E(S
′
+
u2
) ∪ (y, u1) ∪ (y, u2)
and h(S′+u1 ) ≥ h(S
′
+
u2
) ≥ h(S
′
+
u3
);
(a) MAKE MATCH(S′∗y );
(b) if E(S′+y ) 6= ∅: MAKE MATCH(S′+u3 ).
MAKE MATCH(S
′
+
x )
1. ADD(M(S′+x ));
2. while there is an edge e ∈ I(T ′): ADD(e).
Now we will describe notations used in our procedures.
Let Vu(E¯) (Vm(E¯)) denote the set of unmarked (marked) vertices which would be
removed from V (G′) if the edges E¯ were added to IM(G). The process of adding
edges to IM(G) is realized by the procedure ADD(E¯) which adds the edges E¯ to
IM(G) and removes appropriate vertices and edges from G′ . Moreover, if |Vu(E¯)| +
2|Vm(E¯)| = 6|E¯| + x and x > 0 then the procedure ADD(E¯) marks x vertices which
are still in V (G′). The only exception is the situation when the last tree is processed. In
490 Graz˙yna Zwoz´niak
this case we have x = 0, but there are some cases when the number of removed vertices
may be larger than 6|E¯|. That is why the size of IM(G) is n/6 +O(1).
In the procedure MATCH(S′y) subtrees S
′
y of T
′
∈ F
′
are considered. Let C1(S
′
y)
and C2(S
′
y) be the sets of the edges which are removed in preprocessing phase (steps 2
and 3 of MATCH(S′y)). C1(S
′
y) contains the edges (v1, v2) ∈ E(G
′
)\E(F
′
) such that
v1, v2 ∈ EX
′
, degG′ (v1) = 3 and v1 is at distance at most two from some vertex
w ∈ V (S
′
y).
C2(S
′
y) contains the edges e ∈ E(G
′
)\E(F
′
) such that
1. e is at distance one from some leaf of S′y or e is at distance two from some interior
vertex of S′y;
2. for every S′z ⊆ F
′
: e is at distance at least one from any leaf of S′z;
3. for every S′z ⊆ F
′
: e is at distance at least two from any interior vertex of S′z .
Let P
′ be the set of the edges which can be processed together with S′y . P
′
contains
the following paths:
1. P = {(v1, v2)}, where v1 ∈ L(S
′
y), and v2 is not the endpoint of any edge e ∈
E(F
′
).
2. P\(vk−1, vk), where P = {(v1, v2), . . . , (vk−1, vk)}, k ≥ 3, v1, vk ∈ V (S
′
y),
(vi−1, vi) ∈ E(G
′
)\E(F
′
) where i = 2, . . . , k, and degG′ (vj) = 2 where j =
2, . . . , k − 1.
We define a tree S′+y as S
′
y ∪ P
′′
, where P ′′ ⊆ P ′ and if e ∈ P ′\P ′′ then S′+y ∪ {e}
contains a cycle.
When we consider some tree S′+x , where x ∈ V (S
′+
y ) then we select a set of the
edges M(S′+x ) which can be added to IM(G). This set fulfil the following conditions:
1. if h(S′+x ) ≥ 2 and (a, b) ∈M(S
′+
x ) then a, b ∈ V (S
′+
x )\{x};
2. if h(S′+x ) = 1 and f(x) does not exist or is not in V (F
′
) then a, b ∈ V (S′+x );
3. if e1, e2 ∈M(S
′+
x ) then dist(e1, e2) > 1;
4. if |Vu(M(S
′+
x ))| + 2|Vm(M(S
′+
x ))| = 6|M(S
′+
x )| + z and z > 0 then at least z
vertices can be marked;
5. if we added M(S′+x ) to IM(G) then the set E(S
′+
x ) would be empty.
If it is possible to find such a set (M(S′+x ) 6= ∅) then we add M(S
′+
x ) to IM(G). The
choice of M(S′+x ) depends on the shape of S
′+
x and the edges incident to S
′+
x . When
h(S
′+
x ) = 2 then in most of the cases it is possible to choose the edges presented in
Fig. 7. Technical details are presented in the full version of the paper.
Let us introduce one more definition. We define I(T ′) as E(T ′)\E(S′r), where r is
the root of T and T ′ = T ∩G′ . Such edges may appear only if before removing E(S′+x )
some vertex u of S′+x was connected by an edge with a vertex y of degree two in F
′
,
and the tree rooted at y was not completely removed by MATCH(S′y), see Fig. 6.
In our procedures we use the following notions: vertices ui are the sons of x, vertices
vi, j are the sons of ui, vertices wi, j, k are the sons of vi, j , i ∈ {1, 2, 3} j, k ∈ {1, 2}.
Induced Matchings in Graphs of Maximum Degree Three 491
Fig. 6. (a) After MATCH(S′y) one edge (y, u1) ∈ E(S
′
y) may be still in E(G
′
). In such a case
deg
′
G(u1) = 1. (b) After MATCH(S
′
y) two edges (y, u1), (u1, v1,1) ∈ E(S
′
y) may be still in
E(G
′
). In such a case deg
′
G(u1) = 2 and there are edges (v1,1, bi), (bi, ai), (bi, ci),∈ E(G
′
),
i ∈ {1, 2} where deg
′
G(ai) = 1 and (bi, ci),∈ E(F
′
). (c) If we add (u, z) to IM(G) then
(y, v) will be removed and (v, w) will be in E(T
′
)\E(S
′
r)
5 Main theorem
The main contribution of this paper is the following theorem.
Theorem 1. Let G be a connected graph of maximum degree three where |V (G)| = n.
Our algorithm constructs forG an induced matching IM(G) of size at least n/6+O(1)
in linear time.
The following lemmata are used in the proof of Theorem 1.
Lemma 1. Let us consider the procedure CONSTRUCT IM(G′).
1. After step 1 E(F ′) = ∅ and ∀v∈V (G′ )degG′ (v) ≤ 2.
2. After step 2 ∀v∈V (G′ )degG′ (v) = 2.
3. After step 3 V (G′) = ∅.
Lemma 2. After step 1 of the procedure MATCHING(T ′) E(T ′) = ∅.
Lemma 3. Let us consider the procedure MATCH(S′y).
1. If y ∈ R ∪ {r0} then after MATCH(S′y) E(S
′
y) = ∅.
2. If degT ′ (y) = 2 and degG′ (y) = 3 then after MATCH(S
′
y) at most two edges of S
′
y
may be still in E(G′), see Fig. 6(a)-(b).
3. If e ∈ C1(S′y) and the procedure ADD(e) is called in step 2 of the procedure
MATCH(S
′
y) then it removes at most 6 vertices from V (G
′
). All these vertices are
unmarked.
4. If e ∈ C2(S′y) and the procedure ADD(e) is called in step 3 of the procedure
MATCH(S
′
y) then it removes at most 6 vertices from V (G
′
). All these vertices are
unmarked.
Lemma 4. If h(S′+y ) ≥ 5 then it is possible to find a tree S
′+
x , where x ∈ V (S
′+
y ) and
M(S
′+
x ) 6= ∅.
492 Graz˙yna Zwoz´niak
Fig. 7. Basic shapes of S
′
+
x where h(S
′
+
x ) = 2. The edges added to IM(G) are bold.
References
1. P. Alimonti, V. Kann Some APX-completeness results for cubic graphs, Theoretical Com-
puter Science, 237, pages 123–134, 2000.
2. K. Cameron Induced matchings, Discrete Applied Mathematics, 24, pages 97-102, 1989.
3. W. Duckworth, D. F. Manlove, M. Zito On the approximability of the maximum induced
matching problem, The Journal of Discrete Algorithms, 3(1), pages 79-91, 2005.
4. P. Erdo˝s Problems and results in combinatorial analysis and graph theory, Discrete Mathe-
matics, 72, pages 81-92, 1988.
5. R. J. Faudree, A. Gya´rfas, R. H. Schelp, Z. Tuza Induced matchings in bipartite graphs,
Discrete Mathematics, 78, pages 83-87, 1989.
6. G. Fricke, R. Laskar Strong matchings in trees, Congressus Numerantium, 89, pages 239-
244, 1992.
7. M. C. Golumbic, R. C. Laskar Irredundancy in circular arc graphs, Discrete Applied Math-
ematics, 44, pages 79-89, 1993.
8. M. C. Golumbic, M. Lewenstein New results on induced matchings, Discrete Applied Math-
ematics, 101, pages 157-165, 2000.
9. T. Gotthilf, M. Lewenstein Tighter Approximations on Greedy for Maximum Induced Match-
ings in Regular Graphs, Third Workshop on Approximation and Online Algorithms, LNCS
3879, pages 270-281, 2005.
10. R. Greenlaw, R. Petreschi Cubic graphs, ACM Computing Surveys, 27, pages 471-495,
1995.
11. P. Hora´k, H. Qing, W. T. Trotter Induced matchings in cubic graphs, Journal of Graph The-
ory, 17(2), pages 151-160, 1993.
12. C. W. Ko, F. B. Shepherd Adding an identity to a totally unimodular matrix, London School
of Economics Operations Research Working Paper LSEOR.94.14, 1994.
13. K. Lorys´, G. Zwoz´niak Approximation algorithm for the maximum leaf spanning tree prob-
lem for cubic graphs, Proceedings of the 10th Annual European Symposium on Algorithms,
LNCS 2461, pages 686–697, 2002.
14. A. Steger, M. Yu On induced matchings, Discrete Mathematics, 120, pages 291-295, 1993.
15. L. J. Stockmeyer, V. V. Vazirani NP-completeness of some generalizations of the maximum
matxhing problem, Information Processing Letters, 15(1), pages 14-19, 1982.
16. M. Zito Maximum induced matchings in regular graphs and trees, The 25th International
Workshop on Graph-Theoretic Concepts in Computer Science, LNCS 1665, pages 89-100,
1999.
17. G. Zwoz´niak Small independent edge dominating sets in graphs of maximum degree three,
Proceedings of the 32nd Conference on Current Trends in Theory and Practice of Computer
Science, LNCS 3831, pages 556-564, 2006.
Subsystems of Iterated Inductive Definitions
Bahareh Afshari and Michael Rathjen
University of Leeds, Leeds UK
In 1963, G. Kreisel [5] initiated the study of formal theories featuring induc-
tive definitions. Subsystems of the theories of iterated inductive definitions (IDn)
such as the fixed point theories ˆIDn where investigated by Aczel and Feferman
in connection with Hancock’s conjecture about the strength of Martin-Lo¨f type
theories with universes. Another interesting type of theory lying between ˆIDn
and the usual IDn is ID∗n. To illustrate this in the case n = 1, in contrast to
ˆID1, ID∗1 has an induction principle for the fixed points but it is restricted to
formulas in which other fixed points occur only positively. Results about the the-
ories ID∗n were obtained by Friedman, Feferman [4], and Cantini [2]. However,
they did not settle the proof-theoretic strength of the theories ID∗n. I would like
to talk about our recent results revealing the strength of these theories.
References
1. Wilfried Buchholz, Solomon Feferman, Wolfram Pohlers and Wilfried Sieg. Iterated
inductive definitions and subsystems of Analysis: Recent Proof-Theoretical Studies
theories, Springer-Verlag, Berlin, Heidelberg, 1981.
2. Andrea Cantini. A note on a predicatively reducible theory of elementary iterated
induction, Bollettino U.M.I., pp. 413-430, 1985.
3. Andrea Cantini. On the relation between choice and comprehension principles in
second order arithmetic, Journal of Symbolic Logic 51, pp. 360–373, 1986.
4. Solomon Feferman. Iterated inductive fixed-point theories: Application to Han-
cock’s conjecture, Patras Logic Symposion, pp. 171–196, North-Holland, Amster-
dam, 1982.
5. G. Kreisel. Generalized inductive definitions. Tech. rep., Stanford University, 1963.
6. Michael Rathjen. Auwahl und Komprehension in Teitsystemen der Analysis, M.Sc.
thesis, University of Mu¨nster, Germany, 1985.
7. K. Schu¨tte. Proof Theory, Springer-Verlag, Berlin, Heidelberg, 1977.
8. Helmut Schwichtenberg. Proof Theory: Some Applications of Cut-Elimination,
Handbook of Mathematical Logic, pp. 868–895, North-Holland, 1977.
9. Stephen G. Simpson. Subsystems of Second Order Arithmetic, Springer-Verlag,
Berlin, Heidelberg, 1999.
Query Algorithms for Detecting Hamming and
Reed-Solomon Codes ?
Rubens Agadzˇanjans
Institute of Mathematics and Computer Science
University of Latvia, Rain¸a bulv. 29, R¯ıga, LV-1459, Latvia.
ruben.agadzanyan@gmail.com
In this talk we will compare quantum and classical query complexity of some
Boolean functions. This is the model where the Boolean function is known, but
its arguments are unknown. So, to compute the function, the query algorithm
asks for values of particular arguments. The complexity of the algorithm is the
number of queries. Up to now there have been discovered some, though few,
Boolean functions whose quantum query algorithm is better than classical. Here
we will talk about another group of such functions, based on Hamming and Reed-
Solomon error-correcting codes. There is a 25% improvement with the quantum
algorithm for the Hamming codes and a 50% improvement for the Reed-Solomon
codes (which repeats the previously best known achievement).
Key words: Boolean functions, polynomial degree, query algorithms, Ham-
ming code, Reed-Solomon code.
References
1. R. Agadzanjans, J. Smotrovs. Efficient Quantum Query Algorithms Detecting Ham-
ming and Reed-Solomon Codes. In Proc. of the SOFSEM 06, 2006
2. A. Ambainis. Polynomial degree vs. quantum query complexity. In Proc. of the
44th IEEE FOCS, 2003.
3. H. Buhrman and R. de Wolf. Complexity Measures and Decision Tree Complexity
: A Survey. Theoretical Computer Science, v. 288(1): 21–43, 2002
4. R. Freivalds, M. Miyakawa, H. Tatsumi. An Exact Quantum Query Algorithm for
a Specific Boolean Function. In Proc. of the EQIS 04, 2004.
5. J. Gruska. Quantum Computing. McGraw-Hill, 1999.
6. R. W. Hamming. Error detection and error correcting codes. Bell System Technical
Journal, 26(2):147–160, 1950.
7. M. Nielsen, I. Chuang. Quantum Computation and Quantum Information. Cam-
bridge University Press, New York, 700pp., 2000.
8. C. Papadimitriou. Computational Complexity. Addison-Wesley, Reading, 500pp.,
1994.
9. I. S. Reed,G. Solomon. Polynomial codes over certain finite fields. J. Soc. Indust.
Appl. Math., v.8, p.300–304, 1960.
? Research supported by the European Social Fund.
Expressive Power of Graph Logic
Timos Antonopoulos and Anuj Dawar
University of Cambridge
We present results on the expressive power of Graph Logic, a spatial logic for
querying graphs introduced by Cardelli et al. [1] and studied further by Dawar
et al. [2]. Graph Logic is an extension of First Order Logic with a second order
quantifier over edges of restricted form, and a first order quantifier over labels
of edges. In particular, if G is some graph, all one can do with the second order
quantifier is express that there exists a set of edges X of the graph G such that
some formula φ is satisfied by the subgraph of G containing exactly the edges
in X, and some formula ψ is satisfied by the subgraph of G with exactly the
remaining edges not in X.
It has been observed that GL is a sublogic of Monadic Second Order Logic
with quantification over edges and additional first order quantification over edge
labels (MSO for short). Although it seems that GL is strictly less expressive
than MSO, many interesting properties have been shown to be expressible in GL.
Furthermore it was shown in [2] that GL is able to express complete problems on
any level of the Polynomial Hierarchy and that GL and MSO are equi-expressive
when restricted to words. Marcinkowski [3] showed that this richer form of MSO
with quantification over edge labels, is more expressive than GL.
As this richer form of MSO is not the one we usually deal with, the case
where we omit the first order quantification over edge labels from both logics is
a more interesting one. We show that this restriction of GL is indeed strictly less
expressive than the one of MSO. Moreover we show that this is the case even
when restricted to the class of forests.
References
1. L. Cardelli, P. Gardner and G. Ghelli, A spatial logic for querying graphs, ICALP
2002, Springer LNCS, 2380, 597–610.
2. A. Dawar, P. Gardner and G. Ghelli, Expressiveness and Complexity of Graph Logic,
Information and Computation, 205 (2007) 263–310.
3. J. Marcinkowski, On The Expressive Power of Graph Logic, CSL 2006, Springer
LNCS, 4207, 486–500.
The Lost Melody Theorem:
Infinite Time Register Machines
Merlin Carl and Peter Koepke
Mathematisches Institut, Universita¨t Bonn, Germany
In ([1]) it is shown that, for ITTMs there are sets which are computable,
but not writeable. We ask whether an analogous theorem holds for ITRMs as
introduced in ([2]). We start with some definitions.
P x indicates that register machine program P is run with x as an oracle. A real r
(a subset of ω) is ITRM-decidable (decidable) iff there is a program P such that
P x(0) = 1↔ x = r and the former is halts for every x. It is ITRM-computable
(computable) iff there is P such that P ∅(n) = 1↔ n ∈ r, P(n) a total function.
A countable ∈-model M can be coded by a real by fixing a surjection s : ω→ |M |
and setting: r := {i ∈ ω : ∃m,n ∈ ω[p(m,n) = i ∧ (b(m) ∈ b(n))M ]}, where p
is the Cantor pairing function. Jα is the α-th level of the Jensen hierarchy of
constructible sets.
Now we can formulate our result:
Theorem: There is a real r which is decidable but not computable. In fact, we can
take r to be the <L-minimal code in the sense defined above for the ∈-minimal
model Jα of ZF−.
Proof: Let r be as specified in the theorem statement. Then r is not computable,
for if P computes r, the computation can be simulated in Jα and since ITRM-
computations are absolute between models ZF−, there is an ∈-formula φ such
that φ(n) ↔ n ∈ r, so r ∈ Jα. But then r codes an element of Jα, and α is
not minimal. But r is decidable: First, we check whether the ∈-relation given
by r is wellfounded, as described in [2]. From now on, we suppose that this is
the case. The first surjection s : ω>1 → Jα is an element of Jα+2. So r ∈ Jα+2
and the question whether a real x satisfies the above conditions is equivalent
to Jα+2 |= φ(x) for some φ. Now, elements of Jα+2 can be coded by terms of
the form f(k1, ..., kn), where f is a composition of Goedel functions and ki ∈ ω
for 0 < i < n+ 1, which again can be compressed into a single natural number
by an appropriate use of p. Questions of this form are then solved recursively
by unfolding the constructions given by the f’s, ending up with questions whose
answers can be directly read from r.
References
[1] Hamkins,J.D. and Lewis,A.: Infinite Time Turing machines, J.of Symbolic Logic
65(2), (2000),567-604
[2] Koepke.P. and Miller,R.: An Enhanced Theory of Infinite Time Register Machines.
Logic and Theory of Algorithms, Fourth Conference on Computability in Europe,
CiE 2008 (June 2008)
Comparing Notions of Fractal Dimension?
Chris J. Conidis
The University of Chicago, Chicago, IL USA
We construct a countable Π01 -class X ⊂ 2ω with effective packing dimension
1. This answers a question of Athreya, Hitchcock, Lutz, and Mayordomo [1] who
asked if there is a correspondence principle for effective packing dimension, as
in the case of effective Hausdorff dimension [4, 3].
References
[1] Athreya, K.B., Hitchcock, J.M., Lutz, J.H., Mayordomo, E.: Effective strong di-
mension in algorithmic information and computational complexity. SIAM Journal
on Computing 37, 671–705 (2007)
[2] Conidis, C.J.: Effective Packing Dimension ofΠ01 -classes. Proceedings of the Amer-
ican Mathematical Society (to appear).
[3] Hitchcock, J.M.: Correspondence Principles for effective dimensions. Theory of
Computing Systems 38, 559–571 (2005)
[4] Lutz, J.H.: The Dimensions of Individual Strings and Sequences. Information and
Computation 187, 49–79 (2003)
? This work has been published in The Proceedings of the American Mathematical
Society [2].
Clockable Ordinals for
Infinite Time Register Machines
Tim Fischbach, Peter Koepke, Miriam Nasfi, and Gregor Weckbecker
Mathematisches Institut, Universita¨t Bonn, Germany
We do ordinal computability with the Infinite Time Register Machine (ITRM)
model of Miller and Koepke ([1]). In analogy with the theory of Infinite Time
Turing Machines (ITTM) ([2]) we introduce a notion of ITRM-clockable ordinals
corresponding to the running time of a computation.
Definition 1. Let α ∈ Ord. α is ITRM-clockable iff there is an ITRM program
P and a halting computation on input (0, . . . , 0) with zero oracle Z
I : α+ 2→ ω, R : α+ 2→ (ωω)
All natural numbers and all ordinals of the form ωn−1 · cn−1+ . . .+ω · c1+ c0
for some n ∈ ω and a sequence (ci)i∈n are clockable. Using finite vectors of bits
we can also show that ωω is ITRM-clockable. In contrast to the ITTM situation
there are no gaps in the ITRM-clockable ordinals:
Theorem 1. CLOCK := {α | α ITRM-clockable} is a transitive initial segment
of the ordinals and is properly contained in the ITTM-clockable ordinals.
The proof involves a halting criterion for ITRMs:
Lemma 1. Let
I : θ → ω, R : θ → (ωω)
be a infinite time register computation by P with input (0, . . . , 0) and oracle Z.
This computation does not stop iff there is some constellation (I ′, R′) such that
otp({t < θ | (I(t), R(t)) = (I ′, R′)}) ≥ ωω
Furthermore, a finite speed-up lemma is used:
Lemma 2 (Speed-up lemma). Let α + n be a clockable ordinal for some
n ∈ ω, then α itself is clockable.
From theorem 1 we derive some closure properties of CLOCK and the fol-
lowing
Theorem 2. The class of ITRM-clockable ordinals coincides with the class of
ITRM-computable ordinals.
References
1. Koepke, P., Miller, R.: An enhanced theory of infinite time register machines. Logic
and Theory of Algorithms, Fourth Conference on Computability in Europe, CiE
2008 (June 2008)
2. Hamkins, J.D., Lewis, A.: Infinite time Turing machines. J. Symbolic Logic 65(2)
(2000) 567–604
Embedding the Enumeration Degrees in the
ω-Enumeration Degrees
Hristo Ganchev
Sofia University, Faculty of Mathematics and Informatics
5 James Bourchier blvd., 1165 Sofia, Bulgaria
ganchev@fmi.uni-sofia.bg
The structure of the ω-enumeration degrees Dω = (Dω,≤ω) is defined in [1].
It is shown, that Dω is an upper semi-lattice and a jump operation is defined.
Furthermore it is shown, that there is a natural embedding of the structure of the
enumeration degrees De = (De,≤) in Dω, preserving l.u.b and jump operation.
The image of the enumeration degrees under the natural embedding is denoted
by D1 and it is shown in [2] that D1 is first order definable in (Dω;∪; ′).
We shall be concerned with embeddings of De in Dω, not realizable as the
composition of an endomorphism of De with the natural embedding. We prove
that there are at least 2ℵ0 different embeddings, preserving the least upper bound
operation, and at least ℵ0 — preserving the jump operation. We prove a neces-
sary and sufficient condition for the existence of an embedding preserving both
operations. From it, we conclude that the algebraic closure of (De;∪; ′), with
respect to the least jump-invert operation (see [2]), is only emebeddable in the
algebraic closure
⋃Dn of (D1;≤ω;∪; ′). So, ⋃Dn may play an important role
in the structural properties of Dω. Finally, we show that
⋃Dn is first order
definable substructure of (Dω;∪; ′).
References
1. I. N. Soskov, The ω-enumeration degrees, Journal of Logic and Computation 17
(2007), 1193–1214.
2. I. N.Soskov H. Ganchev, The jump operator on the ω-enumeration degrees, to ap-
pear.
Computable Models Spectras of Ehrenfeucht
Theories?
Alexander N. Gavryushkin
Novosibirsk State University, Russia
This paper is connected with next common problem. To characterize the set
of models of an Ehrenfeucht theory (a complete theory with finite number of
countable models up to isomorphism) having computable presentation. Types
of isomorphism of a prime model and of a saturated model are well known in
classical model theory. In [3] there are examples of Ehrenfeucht theories the
only computably presentable model of which is the prime one. In [2] there is
an example of Ehrenfeucht theory such that only saturated model of the theory
has computable presentation. In [1] there is an example of Ehrenfeucht theory
such that only one model of the theory has a computable presentation, and that
model is neither prime nor saturated. Also there exist an Ehrenfeucht theory T,
whose prime and saturated models have computable presentations, and a model
of T which lacks in such. The example is in [1].
In [4] there is a syntactic characterization for the class of Ehrenfeucht the-
ories. In [5] there are examples of Ehrenfeucht theories guaranteeing that all
possible parameters given in the characterization theorem in [4] are realizable.
The result I want to present is formulated in terms of mentioned character-
ization and it couldn’t be described using 1-page abstract. Then I have only to
state that there are two groups of examples of Ehrenfeucht theories: 1) there
are two models of a theory at the same level up to classification from [4] one of
which has computable presentation and another is not computably presentable;
2) there are levels of the classification all models from which have computable
presentations and another levels are without computably presentable models.
References
1. Gavryushkin,A.N., Spectra of Computable Models for Ehrenfeucht Theories, Al-
gebra and Logic, 46, No. 3, 149–157, 2007.
2. Khoussainov, B., Nies, A., Shore, R., Computable Models of Theories with Few
Models, Notre Dame Journal of Formal Logic, 38, 165–178, 1997.
3. Peretyat’kin,M.G., On Complete Theories with Finite Number of Countable Mod-
els, Algebra i Logika, 12, No. 5, 550-576, 1973.
4. Sudoplatov, S.V., Complete Theories with Finitely Many Countable Models. I,
Algebra and Logic, 43, No. 1, 62-69, 2004.
5. Sudoplatov, S.V., Complete Theories with Finitely Many Countable Models. II,
Algebra and Logic, 45, No. 3, 180-200, 2006.
? The work was partially supported by President Grant – 335.2008.1
Deterministic Subsequential Transducers with
Additional FIFO-memory
S. V. Gerdjikov
Department of Mathematical Logic and Applications
Sofia University
st gerdjikov@abv.bg
Regular rewriting rules play a significant role in text-processing techniques.
It is well known that they can be expressed in terms of rational functions and
consequently for each such rule one can efficiently construct a transducer or,
equivalently a bimachine. Unfortunately the transducers, in general cannot be
determinized and the bimachines do not allow to stream a text, and thus one
cannot process a text on-line.
We propose a new formalism, which aims at preserving the determinism and
still to be able to process the input sequentially, i.e. whithout disposing on the
entire text in advance. To this end we incorporate an additional infinite memory,
represented as FIFO. We model it in a way to garantee the deterministic, linear
traversal of an input text.
We call these machines FIFO-transducers and study their properties with
respect to rational functions. Our main efforts concern the composition problem.
We show that we can efficiently compose FIFO-transducers with subsequentional
transducers, but in general they are not closed under composition and they are
unable to describe the class of all rational function. We also show a simple
example of a function that is not rational but can be represented by such a
machine.
Finally, we define a subclass of rational functions that can be recognized by
FIFO-transducers but (in the general case) not by a subsequential transducer.
We state sufficient conditions for a regular rule in order to be represented by a
FIFO-transducer and show how to check these properties algorithmically.
Proof Fragments and Cut-Elimination
Stefan Hetzl
Institute of Computer Languages
Vienna University of Technology
Favoritenstraße 9, 1040 Vienna, Austria
hetzl@logic.at
Cut-elimination is a proof transformation of fundamental importance. Origi-
nally it was introduced by Gentzen together with the sequent calculus [2] and it
builds the core of his consistency proof of Peano arithmetic [3]. It also plays an
important role in the analysis of mathematical proofs and has deep connections
to computation in functional programming languages.
Cut-elimination is usually presented as a set of local rewrite rules with some
terminating strategy thus showing the existence of cut-free proofs for all provable
sequents. The changes to the global structure of the proof that are caused by
these local rewrite steps have traditionally been less investigated.
In this talk we consider proof skeletons (see e.g. [4]) which are abstract rep-
resentations of the structure of proofs. We will describe the changes the skeleton
of a proof undergoes during cut-elimination: Based on its skeleton, a proof with
cuts can be split into several pieces (fragments) which are not broken up further
by the local rewrite rules. The global effect of cut-elimination on the structure
of a proof is therefore shown to be a re-composition of instances of these frag-
ments. The proof is carried out by relying on methods based on cut-elminiation
by resolution [1].
This result allows to describe a certain kind of redundancy whose presence
is a necessary condition for a cut-free proof to allow strong compression by
introduction of cuts. From this characterization follows a lower bound on cut-
introduction.
References
1. Matthias Baaz and Alexander Leitsch. Cut-elimination and Redundancy-
elimination by Resolution. Journal of Symbolic Computation, 29(2):149–176, 2000.
2. Gerhard Gentzen. Untersuchungen u¨ber das logische Schließen. Mathematische
Zeitschrift, 39:176–210,405–431, 1934–1935.
3. Gerhard Gentzen. Die Widerspruchsfreiheit der reinen Zahlentheorie. Mathematis-
che Annalen, 112:493–565, 1936.
4. Jan Kraj´ıcˇek and Pavel Pudla´k. The Number of Proof Lines and the Size of Proofs
in First Order Logic. Archive for Mathematical Logic, 27:69–84, 1988.
q-Overlaps in the Random Exact Cover Problem
Gabriel Istrate1? and Romeo Negrea2
1 e-Austria Institute, V.Paˆrvan 4, cam. 045B, Timis¸oara RO-300223, Romania
gabrielistrate@acm.org.
2 Department of Mathematics, Universitatea Politehnica din Timis¸oara
Victoriei 2, 300006, Timis¸oara, Romania
negrea@math.uvt.ro
We prove lower and upper bounds for the threshold of the following prob-
lem: given q ∈ (0, 1) and c > 0 what is the probability that a random
instance of the k-Exact Cover problem [KM05] has two solutions of
overlap qn±o(n) ? This problem is motivated by the study of phase transitions
in Combinatorial Optimization problems has recently motivated (and brought to
attention) the geometric structure of the solution space of a combinatorial prob-
lem. A remarkable recent advance in this area is due to Me´zard et al. [MMZ05],
[DMMZ08]. These papers have provided rigorous evidence that for the random
k-satisfiability problem (with sufficiently large k) the intuitions concerning the
geometry of the solution space provided by the 1-RSB approach are correct.
The approach in this paper is similar. We study the overlap distribution of the
random k-Exact Cover problem. The phase transition in this problem has been
studied in [KM05]. Zdeborova´ et al. [RSZ07],[MMR+07] have applied nonrigor-
ous methods from Statistical Physics (the cavity approach) and have suggested
that the 1-step Replica Symmetry Breaking assumption is valid. This motivates
us to study the problem q-overlap k-Exact Cover and prove lower and upper
bounds on its satisfiability threshold.
References
[DMMZ08] H. Daude´, M. Me´zard, T. Mora, and R. Zecchina. Pairs of SAT assignments
and clustering in random boolean formulae. Theoretical Computer Science,
393(1-3):260–279, 2008.
[KM05] Vamsi Kalapala and Cris Moore. The phase transition in exact cover.
Technical Report cs/0508037, arXiv.org, 2005.
[MMR+07] E. Maneva, T. Meltzer, J. Raymond, A. Sportiello, and L. Zdeborova´. A
hike in the phases of the 1-in-3 satisfiability problem. In J.P. Bouchaud,
M. Me´zard, and J. Dalibard, editors, Lecture Notes of the Les Houches
Summer School 2006, pages 491–498. Elsevier, 2007.
[MMZ05] M. Me´zard, T. Mora, and R. Zecchina. Clustering of solutions in the random
satisfiability problem. Physical Review Letters, 94(197205), 2005.
[RSZ07] J. Raymond, A. Sportiello, and L. Zdeborova´. The phase diagram of ran-
dom 1-in-3 satisfiability. Phys. Rev. E, 76(011101), 2007.
? Corresponding author. Supported by a Marie Curie International Reintegration
Grant within the 6th European Community FP.
Inductive Definitions over Domain
Representable Spaces
Petter Kristian Køber
Department of Mathematics, University of Oslo, Norway
petterk@math.uio.no
One of the main constructions in domain theory is the solution of recursive
domain equations. Of particular interest are the positive equations, which can
be solved iteratively within set theory. The domains we consider are separable
Scott domains.
The class of topological spaces with an admissible domain representation has
been characterised ([1,2]) as the T0 quotients of countably based spaces (qcb0
spaces). In this talk, we look at how canonical fixed points can be constructed
for strictly positive operators over qcb0 spaces, with the least solution of the
equation
X = P unionmulti [Q⇒ X]
(where P and Q are fixed qcb0 spaces) as the most natural example.
The most useful approach to this problem is to consider partial equivalence
relations on domains (domain-pers). We generalise the strictly positive operators
as well as the inductive limit construction to domain-pers, and consider an im-
portant subclass of domain-pers which is closed under these operations. A strictly
positive operator over qcb0 spaces can be represented by a strictly positive op-
erator over domain-pers and the latter admits a least fixed point (w.r.t. certain
well-behaved domain embeddings). Under certain natural initial conditions on
the operator, this least fixed point gives rise to an admissible representation.
A different approach is to consider the operator over qcb0 spaces more gen-
erally as an operator over weak limit spaces. For the main example, this gives a
least fixed point (w.r.t. continuous, injective maps) and the sequential topologi-
cal space associated turns out to be homeomorphic to the one derived from the
least fixed point of the operator over domain-pers. Combining these results, we
obtain a least fixed point for the operator on qcb0 spaces.
The method used works only in the case of strictly positive inductive def-
initions, thus it remains open whether qcb0 spaces can be defined by positive
induction in general. We also discuss whether it is possible to generalise the
results by allowing the use of free algebra constructions in the defining equation.
References
1. I.Battenfeld, M.Schro¨der, A.Simpson, A Convenient Category of Domains. Elec-
tronic Notes in Theoretical Computer Science 172, 69-99, Elsevier 2007.
2. G.Hamrin, Admissible Domain Representations of Topological Spaces.
U.U.D.M.Report 16, Uppsala University 2005.
The Computable Dimension
of Free Projective Planes
Nurlan Kogabaev
Sobolev Institute of Mathematics
Koptyug Prospect 4, Novosibirsk 630090, Russia.
kogabaev@math.nsc.ru
Shirshov (cf. [1]) suggested to treat projective planes as partial algebraic
systems. In the framework of this approach a projective plane is a structure
〈A, (A0, 0A), ·〉 with a disjunction of A into two subsets A0∪ 0A = A, A0∩ 0A = ∅
and commutative partial operation “·” which satisfy the following properties:
(1) a·b is defined iff a6=b and a, b ∈ A0 (or a, b ∈ 0A) with the product a·b ∈ 0A
(a·b ∈ A0 respectively);
(2) for all a, b, c ∈ A if a·b, a·c, (a·b)·(a·c) are defined, then (a·b)·(a·c) = a;
(3) there exist distinct a, b, c, d ∈ A such that products a·b, b·c, c·d, d·a are
defined and pairwise distinct.
Any free projective plane is freely generated by the set of “points” {b1, b2} ∪
{ai|i ∈ I} and unique “line” c which is incident with ai for each i. From the
results of [1] it follows that any countable free projective plane has a computable
presentation.
In the present paper we investigate the question of possible computable di-
mension of free projective planes and the existence problem of computable list
for the class of all projective planes (up to computable isomorphism). Applying
the Unbounded Models Theorem (cf. [2]) we obtained the following results:
Theorem 1. Every countable free projective plane has computable dimension 1
or ω. Furthermore, such a plane is computably categorical if and only if it has
finite rank.
Theorem 2. The class of all projective planes is not computable (up to com-
putable isomorphism).
References
1. Shirshov, A.I., Nikitin, A.A.: On the theory of projective planes. Algebra and Logic.
20, 330–356 (1981)
2. Goncharov, S.S.: Autostability of models and Abelian groups. Algebra and Logic.
19, 13–27 (1980)
A Classification of Theories of Truth
Graham Leigh and Michael Rathjen
University of Leeds, Leeds UK
We augment the first-order language of Peano Arithmetic with a unary pred-
icate T with T (x) intended to mean “x is the Go¨del number of a true sentence
of arithmetic”. In [2] Friedman and Sheard constructed a list of twelve axioms
and rules of inference concerning the predicate T , each expressing some desir-
able property of truth, and classified all subsets of the list as either consistent or
inconsistent. This gave rise to a collection of nine theories of truth, two of which
have been treated in the literature (see [2], Halbach [3], Sheard [4] and Cantini
[1] for more details). We uncover the proof-theoretic strength of the remaining
seven and in the process construct a proof-theory of truth allowing the systems
to be subject to an ordinal analysis.
References
1. A. Cantini, A Theory of Formal Truth Arithmetically Equivalent to ID1, Journel of
Symbolic Logic, 55, 1, 244–259, 1990.
2. H. Friedman and M. Sheard, An Axiomatic Approach to Self-referential Truth,
Annals of Pure and Applied Logic, 33 1–21, 1987.
3. V. Halbach, A System of Complete and Consistent Truth, Notre Dame Journel of
Formal Logic, vol. 35, 3, 311-327, 1994.
4. M. Sheard, Weak and Strong Theories of Truth, Studia Logica, 68, 89–101, 2001.
Monotonicity Conditions over Characterisations
of PSPACE
Bruno Loff1,3 and Isabel Oitavem2,3
1 Dept. of Mathematics, Instituto Superior Te´cnico, Technical Univ. of Lisbon
2 Dept. of Mathematics, Faculdade de Cieˆncias e Tecnologia, Univ. Nova de Lisboa
3 Centro de Matema´tica e Aplicac¸o˜es Fundamentais (CMAF), Univ. of Lisbon
It is an open problem whether P = PSPACE. An algorithm working in
polynomial space is allowed to erase previously used cells in order to reuse space,
and if we observe the well-known algorithms for PSPACE-complete problems,
they always seem to rely heavily on this possibility. Our intuition then indicates
that this is the crucial difference between P and PSPACE. In this presentation,
we begin by making this intuition rigourous, proving that the additional power
of PSPACE arises exactly from the fact that we are allowed to reuse space.
We consider a “write-only” Turing machine, given polynomial space, which is
not allowed to erase (or re-write over) cells which have been previously written
on; we then prove that under this restriction, polynomial-space-bounded Turing
machines decide exactly P.
This will lead us to consider weaker forms of this “write-only” restriction.
Consider the natural partial order over binary strings , where w  v if |w| < |v|
or if |w| = |v| and every symbol of w is less or equal to the corresponding symbol
of v in the same position. Then by a detailed observation of the tape configu-
rations of a write-only machine we find that consecutive tape configurations are
monotonically increasing according to this order. It turns out that this mono-
tonicity ensures that any set decided by a monotone polynomial-space bounded
computation can also be decided in P. Then P vs. PSPACE is equivalent to ask-
ing whether a polynomial-space-bounded computation can be made monotone.
So in the second part of our presentation we apply these results to implicit
characterisations of PSPACE. We introduce two multi-sorted function algebras
A and B. A is, essentially, the characterisation of PSPACE given by Isabel
Oitavem in 97, and B is A with the input-sorted primitive recursion replaced by
input-sorted primitive iteration. We show that, analogously to A, B characterises
PSPACE. When we restrict the recursion and iteration operators to functions
obeying a monotonicity condition for , we obtain two algebras Aˆ and Bˆ. Aˆ
is obtained by restricting input-sorted primitive recursion, and Bˆ by restricting
input-sorted iteration. Our results are that Bˆ characterises P, Aˆ characterises the
polynomial hierarchy, and if we consider a hierarchy Aˆ1, ..., Aˆn, ... by counting
the rank of the operator of restricted input-sorted primitive recursion, then Aˆn
corresponds to the functions computable with oracles in Σn ∩ Πn. The whole
work suggests a new machine-based characterisation of PH and of each level
Σn ∩ Πn by polynomial-space write-only deterministic machines coupled with
clocks.
Some Results on Local LR-degree Structures
Anthony Morphett?
University of Leeds
A natural direction of study arising from recent work in algorithmic random-
ness is to investigate connections between the information content of a set (in
the sense of its Turing degree) and the notion of relative randomness that is
obtained by adding the set as an oracle. One approach to this is the LR(low for
random)-reducibility: an set A is LR-reducible to B if the class of reals Martin-
Lo¨f random relative to oracle B is contained in the class of randoms relative to
A. The associated degree structure is the LR-degrees. An LR-degree is c.e. (∆02
respectively) if it contains a c.e. (∆02) set.
Although many questions exist about the global and local LR-degree struc-
tures, some results have been obtained. For instance, Barmpalias, Lewis and
Soskova [1] prove a splitting theorem for c.e. sets, and Barmpalias, Lewis and
Stephan [2] prove a weak density theorem for the c.e. LR-degrees. It is not known
if full density holds for the c.e. or ∆02 LR-degrees. We describe some additional
results about the c.e. and ∆02 LR-degrees, including upward density results. We
will also discuss some similarities and differences between these structures and
the c.e. or ∆02 Turing degrees.
References
1. George Barmpalias, Andrew E. M. Lewis, Mariya Soskova, Randomness, Lowness
and Degrees, Journal of Symbolic Logic vol.73, Issue 2, pp. 559-577 (2008)
2. George Barmpalias, Andrew E. M. Lewis, Frank Stephan, Π01 classes, LR degrees
and Turing degrees, to appear in Annals of Pure and Applied Logic
? Supported by MEST-CT-2004-504029 MATHLOGAPS Marie Curie Host Fellow-
ship.
The Axiomatic Derivation of Absolute Lower
Bounds
Yiannis N. Moschovakis1,2
1 Department of Mathematics, University of California in Los Angeles
2 Graduate Program in Logic, Algorithms and Computation (MPLA)
Department of Mathematics, University of Athens
I will outline a method for deriving lower bounds for the complexity of prob-
lems (especially in arithmetic) which are absolute (universal), i.e., they apply
to all algorithms; the key idea is to derive lower bounds from three axioms for
algorithms, which are natural and easily shown to apply to all known models of
computation.
Exploring the Computational Contribution of a
Non-constructive Combinatorial Principle
Diana Ratiu and Trifon Trifonov?
Mathematics Institute, University of Munich, Germany
{ratiu,trifonov}@math.lmu.de
We regard Π02 -formulas as specifications of the sort “Given an input x satis-
fying some property D, is there an algorithm producing an output y from x so
that a given requirement G is met?” and look at proofs by contradiction of such
statements. Here we will consider a particular combinatorial problem expressed
as ∀x(D → ∀y(G(x, y)→ ⊥)→ ⊥) and analyze the algorithms that we extract
from its classical proof by two distinct methods — the refined A-Translation and
Go¨del’s functional (Dialectica) interpretation.
A-Translation is a combination of Go¨del-Gentzen double negation translation
and Friedman’s trick [2], which substitutes ⊥, viewed as a predicate variable,
by ∃y G(x, y). [1] proposed a refinement of the method, such that unnecessary
double negations are avoided for certain classes of formulasD andG. By coupling
this with Kreisel’s (modified) realisability, one can now associate with the A-
translated proof an extracted term in a Curry-Howard fashion.
Go¨del’s Dialectica interpretation [3] presents an alternative for collecting
computational content from non-constructive proofs, namely by tracking coun-
terexamples, i.e., terms instantiating universal assumptions. By employing sim-
ple finite types we can translate every formula A to a decidable relation AD(z, y)
between a solution z and a counterexample y. Furthermore, from a derivation of
A we can extract a term t, which satisfies AD(t, y) for every counterexample y.
We investigate a combinatorial result used to prove Ramsey’s foundational
theorem, namely the Infinite Pigeon Hole (IPH) principle. IPH states that any
finitely colored infinite sequence has an infinite monochromatic subsequence.
In general, there is no computable functional producing such an infinite subse-
quence. However, IPH can be used to give a simple classical proof of the Π02 -
statement that a finite monochromatic subsequence of any given length exists.
We compare the programs extracted from this corollary using the aforemen-
tioned methods and discuss how they reflect the computational meaning of IPH.
References
1. Berger, U. and Buchholz, W. and Schwichtenberg, H., Refined Program Extraction
from Classical Proofs, Annals of Pure and Applied Logic, 114, pp 3–25, (2002)
2. Friedman, H., Classically and intuitionistically provably recursive functions, D.S.
Scott and G.H. Mu¨ller, 669, Lecture Notes in Mathematics, pp 21–28, (1978)
3. Go¨del, K, U¨ber eine bisher noch nicht benu¨tzte Erweiterung des finiten Standpunk-
tes, Dialectica, 12, pp 280–287, (1958)
? The authors gratefully acknowledge financial support by MATHLOGAPS (MEST-
CT-2004-504029), a Marie Curie Early Stage Training Site.
Autostability of Automatic Linear Orders?
Alexandra Revenko
Novosibirsk State University, Russia
The class of automatic structures is the easiest class of computable structures
from an algorithmic point of view.
Our investigations concern the problem of existence a computable isomor-
phism between two automatic presentations of a structure. We say that a struc-
ture is autostable under automatic presentations if there is a computable iso-
morphism between any two automatic presentations of this structure [1]. Here
we deal with linear orders.
Some description of automatic linear orders was found by B.Khoussainov,
S.Rubin and others. Let L = (L,≤) be a countable linear order. We say that two
elements x, y ∈ L are equivalent if there is a finite number of elements between x
and y. The quotient structure with respect to this equivalence relation is a linear
order L1 (ordering is induced from the original structure). One can continue this
procedure by transfinite induction. So the least ordinal α for which Lα = Lα+1
is a FC-rank of L. Then the FC-rank of automatic linear order is finite [2].
It was earlier showed that all automatic ordinals and automatic scattered
linear orders with FC-rank less than 3 are autostable under automatic presen-
tations. We proved that moreover there exist an algorithm which, given two
automatic presentations of a scattered linear order with FC-rank less than 3,
constructs the computable isomorphism between them. Then every two auto-
matic presentations of scattered linear order with FC-rank 3 are computable
isomorphic.
References
1. Ershov,Yu. L., Goncharov, S. S., Constructive Models, Siberian School of Algebra
and Logic, 1999.
2. S. Rubin. Automatic Structures. A thesis submitted in partial fulfilment of the
requirements for the Degree of Doctor of Philosophy. The University of Auckland,
2004.
? The work was partially supported by President Grant – 335.2008.1
Quantifiers on Automatic Structures
Sasha Rubin
Department of Computer Science, University of Auckland
rubin@cs.auckland.ac.nz
An automatic structure is one that has a presentation consisting of finite
or infinite words or trees so that the coded domain and atomic operations are
computable by finite automata operating synchronously on their inputs [KN95]
[Blu99]. The fundamental fact about these structures is that they are effectively
closed under first-order interpretations [BG00]. This result has been strength-
ened by extending first-order logic with certain generalised quantifiers (for in-
stance, ‘there exist infinitely many’ and ‘there exist k modulo m many’) [BG00]
[KRS04] [Col04] [KL05] [BKR07] [BKR08] [KL08].
Say that a generalised quantifier Q preserves regularity for a class of auto-
matic structures C if C is closed under FO + Q interpretations. In this talk I will
present some steps towards a fuller understanding of these quantifiers.
I will introduce a natural collection Q of quantifiers, each preserving regular-
ity for the finite-word automatic structures Cfw. The collection Q includes the
known quantifiers that preserve regularity for Cfw; and every unary quantifier
that preserves regularity for Cfw is in Q.
This is part of ongoing work with Valentin Goranko and Moshe Vardi.
References
[BG00] A. Blumensath and E. Gra¨del. Automatic structures. In 15th Symposium on
Logic in Computer Science (LICS), pages 51–62, 2000.
[BKR07] V. Ba´ra´ny, L. Kaiser, and A. Rabinovitch. Eliminating cardinality quantifiers
from MLO, 2007. manuscript.
[BKR08] V. Ba´ra´ny, L. Kaiser, and S. Rubin. Cardinality and counting quantifiers on
omega-automatic structures. In STACS ’08: Proceedings of the 25th Annual
Symposium on Theoretical Aspects of Computer Science, 2008.
[Blu99] A. Blumensath. Automatic Structures. Diploma thesis, RWTH Aachen, 1999.
[Col04] T. Colcombet. Properties and representation of infinite structures. PhD the-
sis, University of Rennes I, 2004.
[KL05] D. Kuske and M. Lohrey. First-order and counting theories of omega-
automatic structures. Technical Report Fakulta¨tsbericht Nr. 2005/07, Univer-
sita¨t Stuttgart, Fakulta¨t Informatik, Elektrotechnik und Informationstechnik,
2005.
[KL08] D. Kuske and M. Lohrey. Hamiltonicity of automatic graphs. FIP TCS 2008,
2008.
[KN95] B. Khoussainov and A. Nerode. Automatic presentations of structures. Lec-
ture Notes in Computer Science, 960:367–392, 1995.
[KRS04] B. Khoussainov, S. Rubin, and F. Stephan. Definability and regularity in
automatic structures. In STACS 2004, volume 2996 of LNCS, pages 440–451.
Springer, Berlin, 2004.
The Almost Zero ω-Enumeration Degrees
Ivan N. Soskov
Faculty of Mathematics and Computer Science, Sofia University
5 James Bourchier Blvd., 1164 Sofia, Bulgaria,
soskov@fmi.uni-sofia.bg
The jump operator “′” on the ω-enumeration degrees possesses a surprising
inversion property that for every n and every degree a ≥ 0(n)ω there exists a least
degree In(a) among the degrees x such that x(n) = a, see [4].
A degree x is almost zero (a.z.) if x ≤ 0′ω and for all n, In(x(n)) = x.
There exist non-zero a.z. degree. Moreover, there exist incomparable a.z.
degrees. Another nice property of the a.z. degrees is that for every pair a and b
of a.z. degrees we have (∀n)((a⊕ b)(n) = a(n) ⊕ b(n)). The a.z. degrees form an
ideal called Az which has no minimal upper bound below 0′ω.
Suppose that C = (C,≤) is a degree structure and let “′”be a jump operator
defined on the elements of C. Given elements a and b of C, let a  b if for some
n, a(n) ≤ b(n). Let a ∼ b if a  b and b  a. Finally for every a ∈ C, set
a∗ = {b : a ∼ b}.
Let a∗  b∗ if a  b and denote by JC the partial ordering ({a∗ : a ∈ C},).
The ordering JR based on the r.e. Turing degrees is studied by Lempp [2]
who shows that it contains a minimal pair over 0∗ and a splitting of (0′)∗.
Jockusch, Lerman, Soare and Solovay [1] show that this ordering is dense.
The ordering JG based on the Σ02 enumeration degrees is of no particular interest
since by a result of McEvoy [3] it is isomorphic to JR.
In the talk we shall present some results about the orderings JAz and JGω
based on the almost zero degrees and on the Σ02 ω-enumeration degrees respec-
tively. We shall show that JAz is isomorphic to (Az,≤ω) and that the orderings
JAz and JGω are dense.
References
1. R. I. Soare R. M. Solovay C. G. Jockusch, M.Lerman, Recurseively enumerable
sets modulo iterated jumps and extensions of Arslanov’s completeness criterion, J.
Symbolic Logic 54 (1989), 1288–1323.
2. S. Lempp, Topics in recursively enumerable sets and degrees, Ph.D. thesis, University
of Chicago, 1986.
3. K. McEvoy, Jumps of quasi-minimal enumeration degrees, J. Symbolic Logic 50
(1985), 839–848.
4. I. N. Soskov and H. Ganchev, The jump operator on the ω-enumeration degrees,
Ann. Pure Appl. Logic, to appear.
A Sequent Calculus for
Intersection and Union Logic
Anastasia Veneti1 and Yiorgos Stavrinos2
1 Department of Computer Science, National Technical University of Athens
GR-15773 Zografou, Greece, tassiana98@gmail.com
2 MPLA, Department of Mathematics, University of Athens
GR-15784 Zografou, Greece, g.stavrinos@math.ntua.gr
We present a logical formalism for the intersection and union type assignment
system IUT [1]. A first attempt to this end is the intersection and union logic IUL,
a logic whose main structures are binary trees called kits [3, 4]. The rules of IUL
are in natural deduction style and are categorized as global or local according
to whether they affect all leaves of the kits involved or not. While rules for
the intersection and union are meant to be local, the complex notation of kits
conceals a certain kind of globality inherent in the union elimination rule. This
becomes explicit if we abandon kits and resort to the linear structures employed
in [2]. These structures are multisets of judgements (atoms) called molecules. A
logic for IUT using molecules, but still in natural deduction style, incorporates
a union elimination rule
[(Γi ` ϕi)]i<n
⋃
[(Γ ` σ ∪ τ)] [(Γi, ϕi ` ψi)]i<n
⋃
[(Γ, σ ` ρ), (Γ, τ ` ρ)]
[(Γi ` ψi)]i<n
⋃
[(Γ ` ρ)] (∪E)
which has both a global and a local behaviour. Here globality and locality refer
to the number of atoms modified by the rule in the premise molecules.
In the present work we describe IUL with molecules in sequent calculus style.
The main advantage of this formulation is that globality and locality of the union
elimination rule are separated in the cut rule and left union rule, respectively.
[(Γi ` ϕi)]i<n [(Γi, ϕi ` ψi)]i<n
[(Γi ` ψi)]i<n (cut)
M⋃ [(Γ, σ ` ρ), (Γ, τ ` ρ)]
M⋃ [(Γ, σ ∪ τ ` ρ)] (L∪)
These improvements lead the way in yielding the relation of IUT to the logic
IUL.
References
1. Barbanera F., Dezani-Ciancaglini M., and de’Liguoro U., Intersection and Union
Types: Syntax and Semantics, Information and Computation 119, 202–230 (1995)
2. Pimentel E., Ronchi Della Rocca S., and Roversi L., Intersection Types from a proof-
theoretic perspective, 4th Workshop on Intersection Types and Related Systems,
Torino (2008)
3. Ronchi Della Rocca S. and Roversi L., Intersection Logic, Proceedings of CSL’01,
LNCS 2142, 414–428 (2001)
4. Veneti A. and Stavrinos Y., Towards an intersection and union logic, 4th Workshop
on Intersection Types and Related Systems, Torino (2008)
Anhomomorphic Logic:
The Logic of Quantum Realism
Petros Wallden
Raman Research Institute, Sadashivanagar, Bangalore 560-080, India
petros.wallden@gmail.com or petros@rri.res.in
Anhomomorphic logic, is a novel interpretation of Quantum Theory (initiated
by Sorkin) that comes as a development of the consistent histories approach and
is an attempt to retain realism. When using logic to describe (classical) physics,
we have a set of possible histories Ω, a set of truth values ( e.g. {True, False})
and the possible maps (φi) that are homomorphisms between the Boolean al-
gebra of subsets of Ω and of the truth values. These maps give rise to different
realizations (here is where the measure and thus the dynamics enter the picture).
It is well known that the above picture cannot hold in quantum theory. One can
either restrict the subsets of Ω allowed (standard consistent histories) or change
the set of truth values to some Heyting algebra (Isham) or finally, weaken the
requirement that the map is homomorphism. This latter approach is taken in
“Anhomomorphic Logic”. The weakening of the requirement to be a homomor-
phism is replaced by other conditions, that guarantee that most structure is
preserved and the basic inference law (modus ponens) still aplies. Thus we have
a deductive logic (which is not the case in what is usually referred to as “quan-
tum logic”). In this talk, we will first introduce anhomomorphic logic in some
detail. Then we will deal with some recent developments on the emergence of
classicality and the closely related issue of recovery of probabilistic predictions.
The former, essentially means that in some scale of coarse graining, we know that
classical (boolean) logic applies. Thus we have to show that the anhomomorphic
logic, upon some coarse grainings, results to homomorphic logic (i.e. classical).
Finally, the issues of how probabilities arise, is present in several attempts to
retain realism in quantum theory (such as many worlds), and in this approach
it is resolved with the use of the concept of “approximate preclusion” and by
taking a frequentist’s view on probability rather than treating it as propensity.
Author Index
Afshari, Bahareh . . . . . . . . . . . . . . . . . 493
Agadzˇanjan, Ruben . . . . . . . . . . . . . . 494
Aihara, Kazuyuki . . . . . . . . . . . . . . . . 445
Almeida, Marco . . . . . . . . . . . . . . . . . . . . 3
Aman, Bogdan . . . . . . . . . . . . . . . . . . . . 15
Antonopoulos, Timos . . . . . . . . . . . . 495
Antunes, Luis . . . . . . . . . . . . . . . . . . . . . 25
Ayala-Rinco´n, Mauricio . . . . . . . . . . 137
Azevedo, Tiago . . . . . . . . . . . . . . . . . . . 35
Beggs, Edwin . . . . . . . . . . . . . . . . . . . . . 45
Benevides, Mario . . . . . . . . . . . . . . . . . .35
Benferhat, Salem . . . . . . . . . . . . . . . . .205
Bonet, Isis . . . . . . . . . . . . . . . . . . . . . . . 284
Caldwell, James . . . . . . . . . . . . . . . . . .254
Calhoun, William . . . . . . . . . . . . . . . . . 55
Carl, Merlin . . . . . . . . . . . . . . . . . . . . . 496
Chiarabini, Luca . . . . . . . . . . . . . . . . . . 64
Cho, Sung-Jin . . . . . . . . . . . 77, 165, 367
Choi, Un-Sook . . . . . . . . . . . . . . . 77, 165
Ciobanu, Gabriel . . . . . . . . . . . . . . . . . . 15
Conidis, Chris . . . . . . . . . . . . . . . . . . . 497
Costa, Antoˆnio Carlos . . . . . . . . . . . . .87
Dawar, Anuj . . . . . . . . . . . . . . . . . . . . . 495
de Miguel Casado, Gregorio . . . . . . . 97
Devlin, Keith . . . . . . . . . . . . . . . . . . . . . . .1
Dimuro, Grac¸aliz . . . . . . . . . . . . . . . . . . 87
Durand-Lose, Je´roˆme . . . . . . . . . . . . 107
Eleftheriou, Pantelis . . . . . . . . . . . . . 117
Elouedi, Zied . . . . . . . . . . . . . . . . . . . . 205
Fischbach, Tim . . . . . . . . . . . . . . . . . . 498
Fokina, Ekaterina . . . . . . . . . . . . . . . . 127
Galdino, Andre´ Luiz . . . . . . . . . . . . . 137
Ganchev, Hristo . . . . . . . . . . . . . . . . . .499
Garc´ıa, Zenaida . . . . . . . . . . . . . . . . . . 284
Garc´ıa Chamizo, Juan Manuel . . . . 97
Gavryushkin, Alexander . . . . . . . . . 500
Gaßner, Christine . . . . . . . . . . . . . . . . 147
Gerber, Annelies . . . . . . . . . . . . . . . . . . 45
Gerdjikov, Stefan . . . . . . . . . . . . . . . . 501
Hetzl, Stefan . . . . . . . . . . . . . . . . . . . . . 502
Horihata, Yoshihiro . . . . . . . . . . . . . . 157
Hwang, Yoon-Hee . . . . . . . . . . . . 77, 165
Irrgang, Bernhard . . . . . . . . . . . . . . . .175
Istrate, Gabriel . . . . . . . . . . . . . . . . . . 503
Jain, Sanjay . . . . . . . . . . . . . . . . . . . . . 185
Jansen, Maurice . . . . . . . . . . . . . . . . . 195
Jenhani, Ilyes . . . . . . . . . . . . . . . . . . . . 205
Jervell, Herman Ruge . . . . . . . . . . . . 215
Kahle, Reinhard . . . . . . . . . . . . . . . . . 224
Kara´dais, Basil . . . . . . . . . . . . . . . . . . 234
Kim, Han-Doo . . . . . . . . . . . . . . . 77, 165
Kim, Jin-Gyoung . . . . . . . . . . . . . . . . . .77
Kim, Seok-Tae . . . . . . . . . . . . . . . . . . . 367
Koepke, Peter . . . . . . . . . . . . . . . 496, 498
Kogabaev, Nurlan . . . . . . . . . . . . . . . .505
Korovina, Margarita . . . . . . . . . . . . . 246
Kothari, Sunil . . . . . . . . . . . . . . . . . . . 254
Koutras, Costas . . . . . . . . . . . . . . . . . . 117
Køber, Petter Kristian . . . . . . . . . . . 504
Lafitte, Gre´gory . . . . . . . . . . . . . . . . . .264
Le Roux, Ste´phane . . . . . . . . . . . . . . . 274
Leo´n, Maikel . . . . . . . . . . . . . . . . . . . . . 284
Leigh, Graham . . . . . . . . . . . . . . . . . . . 506
Li, Chung-Chih . . . . . . . . . . . . . . . . . . 294
Loff, Bruno . . . . . . . . . . . . . . . . . . . . . . 507
Makowsky, Johann . . . . . . . . . . . . . . . 304
Manousaridis, Angelos . . . . . . . . . . . 324
Mora Mora, Higinio . . . . . . . . . . . . . . . 97
Moreira, Nelma . . . . . . . . . . . . . . . . . . . . 3
Morphett, Anthony . . . . . . . . . . . . . . 508
Moschovakis, Yiannis . . . . . . . . . . . . 509
Mostowski, Marcin . . . . . . . . . . . . . . . 332
Nagy, Benedek . . . . . . . . . . . . . . . . . . . 435
Nasfi, Miriam . . . . . . . . . . . . . . . . . . . . 498
Negrea, Romeo . . . . . . . . . . . . . . . . . . .503
Nigam, Vivek . . . . . . . . . . . . . . . . . . . . 344
Nomikos, Christos . . . . . . . . . . . . . . . 117
Author Index 517
Oitavem, Isabel . . . . . . . . . . . . . . . . . . 507
Papakyriakou, Michalis . . . . . . . . . . 324
Papaspyrou, Nikolaos . . . . . . . . . . . . 324
Pelupessy, Florian . . . . . . . . . . . . . . . .354
Petrova, Katya . . . . . . . . . . . . . . . . . . . 361
Piao, Yongri . . . . . . . . . . . . . . . . . . . . . 367
Potgieter, Petrus . . . . . . . . . . . . . . . . . 377
Protti, Fa´bio . . . . . . . . . . . . . . . . . . . . . . 35
Prunescu, Mihai . . . . . . . . . . . . . . . . . 387
Qiu, Daowen . . . . . . . . . . . . . . . . . . . . . 397
Rathjen, Michael . . . . . . . . . . . . 493, 506
Ratiu, Diana . . . . . . . . . . . . . . . . . . . . . 510
Reis, Roge´rio . . . . . . . . . . . . . . . . . . . . . . . 3
Revenko, Alexandra . . . . . . . . . . . . . .511
Rubin, Sasha . . . . . . . . . . . . . . . . . . . . 512
Sadowski, Zenon . . . . . . . . . . . . . . . . . 407
Seyfferth, Benjamin . . . . . . . . . . . . . . 175
Sihman, Marcelo . . . . . . . . . . . . . . . . . . 35
Solon, Boris . . . . . . . . . . . . . . . . . . . . . .361
Soltys, Michael . . . . . . . . . . . . . . . . . . .415
Soskov, Ivan N. . . . . . . . . . . . . . . . . . . 513
Souto, Andre´ . . . . . . . . . . . . . . . . . . . . . .25
Stavrinos, Yiorgos . . . . . . . . . . . . . . . 514
Stephan, Frank . . . . . . . . . . . . . . . . . . 185
Tadaki, Kohtaro . . . . . . . . . . . . . . . . . 425
Tajti, A´kos . . . . . . . . . . . . . . . . . . . . . . 435
Takahashi, Hayato . . . . . . . . . . . . . . . 445
Trifonov, Trifon . . . . . . . . . . . . . . . . . . 510
Vardi, Moshe Y. . . . . . . . . . . . . . . . . . . . .2
Vasilieva, Alina . . . . . . . . . . . . . . . . . . 453
Veneti, Anastasia . . . . . . . . . . . . . . . . 514
Vorobjov, Nicolai . . . . . . . . . . . . . . . . 246
Wallden, Petros . . . . . . . . . . . . . . . . . . 515
Weckbecker, Gregor . . . . . . . . . . . . . . 498
Weiermann, Andreas . . . . . . . . . . . . . 354
Weiss, Michael . . . . . . . . . . . . . . . . . . . 264
Wilson, Craig . . . . . . . . . . . . . . . . . . . . 415
Winter, Joost . . . . . . . . . . . . . . . . . . . . 463
Ye, Nan . . . . . . . . . . . . . . . . . . . . . . . . . .185
Yokoyama, Keita . . . . . . . . . . . . 157, 473
Zwoz´niak, Graz˙yna . . . . . . . . . . . . . . 483

12
Induced Sorting Suffixes in External Memory
GE NONG, Sun Yat-sen University, SYSU-CMU Shunde International Joint Research Institute
WAI HONG CHAN, The Hong Kong Institute of Education
SHENG QING HU and YI WU, Sun Yat-sen University
We present in this article an external memory algorithm, called disk SA-IS (DSA-IS), to exactly emulate
the induced sorting algorithm SA-IS previously proposed for sorting suffixes in RAM. DSA-IS is a new disk-
friendly method for sequentially retrieving the preceding character of a sorted suffix to induce the order of
the preceding suffix. For a size-n string of a constant or integer alphabet, given the RAM capacity ((nW )0.5),
where W is the size of each I/O buffer that is large enough to amortize the overhead of each access to disk,
both the CPU time and peak disk use of DSA-IS are O(n). Our experimental study shows that on average,
DSA-IS achieves the best time and space results of all of the existing external memory algorithms based on
the induced sorting principle.
Categories and Subject Descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnu-
merical Algorithms and Problems—Sorting and searching; G.2.1 [DiscreteMathematics]: Combinatorics—
Combinatorial algorithms; H.3.4 [Information Storage and Retrieval]: Systems and Software—
Performance evaluation (efficiency and effectiveness)
General Terms: Algorithms, Performance
Additional Key Words and Phrases: Suffix array, sorting algorithm, external memory
ACM Reference Format:
Ge Nong, Wai Hong Chan, Sheng Qing Hu, and Yi Wu. 2015. Induced sorting suffixes in external memory.
ACM Trans. Inf. Syst. 33, 3, Article 12 (February 2015), 15 pages.
DOI: http://dx.doi.org/10.1145/2699665
1. INTRODUCTION
For a size-n input string x[0,n− 1] over an ordered alphabet of characters in [0, σ − 1],
the substring starting from x[i] and running to x[n− 1] is called a suffix and denoted
by suf(x, i). The problem of sorting suffixes is to lexicographically sort all of the suffixes
of x into increasing order. The result of sorting is generally stored in an integer array
sa[0,n− 1], called a suffix array [Manber and Myers 1993], in which each item is a
logn-bit integer storing the index for the start position of a suffix in x. Hence, sorting
all of the suffixes of x is also widely known as constructing the suffix array of x.
The suffix array is a fundamental data structure in many applications, and its con-
struction, in either internal memory or external memory, is crucial to the applications’
overall efficiencies. The recent book by Ohlebusch [2013] has a sophisticated survey of
G. Nong was supported by projects DEGP 2012KJCX0001, NCET-10-0854, 11lgzd04, and 11lgpy93. W. H.
Chan was supported by GRF (810012), Research Grant Council, Hong Kong SAR.
Authors’ addresses: G. Nong, S. Q. Hu, and Y. Wu, Computer Science Department, Sun Yat-sen Univer-
sity, Guangzhou 510275, China; emails: issng@mail.sysu.edu.cn, hu-sheng-qing@163.com, wu.yi.christian@
gmail.com; W. H. Chan, Department of Mathematics and Information Technology, The Hong Kong Institute
of Education, Kowloon, Hong Kong; email: waihchan@ied.edu.hk.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display alongwith the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c© 2015 ACM 1046-8188/2015/02-ART12 $15.00
DOI: http://dx.doi.org/10.1145/2699665
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
12:2 G. Nong et al.
the applications of enhanced suffix arrays (the original suffix array enhanced by other
auxiliary data structures such as lcp-table) in bioinformatics. Suffix arrays are used
in many information processing applications, and their construction is a problem that
has received intensive research attention.
Many suffix array construction algorithms (SACAs) have been proposed on random-
access machine models. Puglisi et al. [2007] provide a comprehensive survey up to
2007. Recently, work has been done on external memory SACAs. Recent experimental
results have shown that algorithms based on the induced sorting principle, that is,
eSAIS [Bingmann et al. 2013] and EM-SA-DS [Nong et al. 2014], are competitive
against the previously best techniques such as DC3 [Dementiev et al. 2008] and bwt-
disk [Ferragina et al. 2012].1
Let SA(x) denote the suffix array of x. The key steps to compute SA(x) by induced
sorting are to (1) reduce x to the string x1[0,n1−1] with n1 ≤ n/2, (2) compute SA(x1),
and (3) induce SA(x) from SA(x1). The time complexity of induced sorting is linear
as given by the recursive formula T (n) = T (n/2) + O(n) = O(n). The induced sorting
method also facilitates time- and space-efficient designs in practice, for example, SA-IS
[Nong et al. 2011], its optimized implementation [Mori 2008], and a recent improvement
of SA-IS called SACA-K, which uses only a σ logn-bit workspace beyond the input string
and the output suffix array [Nong 2013].
If both x and SA(x) are held completely in the RAM, the process of inducing SA(x)
from SA(x1) consists of two scans of SA(x) [Nong et al. 2011]. The L-type suffixes are
sorted from the sorted leftmost S-type (LMS) suffixes and the S-type suffixes are sorted
from the sorted L-type suffixes (refer to Section 2.1 for the definitions of L-type, S-type,
and LMS suffixes). The details are described here, in which bkt[0, σ − 1] is an integer
array.
—Inducing L-type suffixes: Scan sa from left to right, and j = sa[i] − 1 for i increased
from 0 to n− 1. If j ≥ 0 and x[ j] is L-type, put suf(x, j) into the current head item of
its bucket in sa by setting sa[bkt[x[ j]] + +] = j and shift the bucket head one place
to the right.
—Inducing S-type suffixes: Scan sa from right to left, and j = sa[i] − 1 for i decreased
from n− 1 to 0. If j ≥ 0 and x[ j] is S-type, put suf(x, j) into the current end item of
its bucket in sa by setting sa[bkt[x[ j]]− −] = j and shift the bucket end one place to
the left.
These two passes are symmetrically analogous. If x, sa, and bkt are completely stored
in the RAM, these two steps can be done very quickly. However, random accesses of
x[ j], bkt[x[ j]], and sa[bkt[x[ j]]] in the external memory are problematic, because they
lead to slow disk seeks. Thus, a version of SA-IS that runs in the external memory
should be developed to avoid these random accesses. Retrieving x[ j] without random
accesses of x is still a challenge in the existing external memory designs for induced
sorting suffixes. The attempts to solve this problem in Bingmann et al. [2013] and Nong
et al. [2014] resulted in much more complicated designs than their counterparts in the
RAM.
Following our recent work [Nong et al. 2014] developing an external memory design
for the algorithm SA-DS [Nong et al. 2011], we propose a new solution called disk SA-IS
(DSA-IS) in this article. Pleasingly, unlike the solutions in Bingmann et al. [2013] and
Nong et al. [2014], DSA-IS emulates SA-IS merely by replacing random accesses of the
RAMwith sequential accesses of the external memory. We develop DSA-IS on the same
settings as the external memory model in Nong et al. [2014] with the following memory
parameters (in logn-bit words): RAM capacity M = ((nW)0.5), disk capacity E = O(n),
1bwt-disk computes the Burrows-Wheeler transform (BWT) and the others construct the suffix array.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
Induced Sorting Suffixes in External Memory 12:3
and the size of each I/O buffer W . W is large enough to amortize the overhead of each
disk access; given W , the I/O complexity in terms of the number of I/Os is defined as
the total I/O volume over W .
This work has a number of motivations. The suffix array is an indispensable data
structure for many applications such as full-text retrieval and sequence alignment.
In all applications involving a suffix array, constructing the suffix array is the first,
crucial step. Induced sorting is commonly recognized as a promising method for suffix
array construction in both the internal memory and external memory. Two external
memory algorithms, eSAIS and EM-SA-DS, have been designed based on induced
sorting. However, their designs are complicated and challenging for engineering by
third-party users, and their time-and-space performance must be further improved.
For such a fundamental problem, a better solution with a simpler design and better
performance is preferred and desired.
In this article, Section 2 gives the preliminaries for presenting DSA-IS, Section 3
the algorithm’s details, Section 4 the experiments for performance evaluation, and
Section 5 the summary.
2. OVERVIEW
2.1. Notations
Some basic notations for induced sorting are necessary for presenting this work:
L-type, S-type, and LMS suffixes/characters. The suffixes in x are classified into
two classes: suf(x, i) is S-type if (1) i = n − 1 or (2) suf(x, i) < suf(x, i + 1) for
i ∈ [0,n− 2]; otherwise, suf(x, i) is L-type. Moreover, suf(x, i) is LMS if suf(x, i) is
S-type and suf(x, i − 1) is L-type for i ∈ [1,n− 1]. A suffix and its head character
are considered to be the same type: x[i] is said to be L-type, S-type, or LMS if
suf(x, i) is L-type, S-type, or LMS, respectively.
L-type, S-type, and LMS substrings. For all i < j, if x[ j] is LMS and there is no LMS
character in x[i + 1, j − 1], then x[i, j] is L-type, S-type, or LMS as long as x[i] is
L-type, S-type, or LMS, respectively. Moreover, x[n− 1] itself is an S-type and an
LMS substring.
Bucket. In SA(x), all of the suffixes with the same head character, ch, are consec-
utively stored in a range called the bucket for ch, denoted by bucket(SA(x), ch).
The leftmost and the rightmost items of a bucket are called the start and the end
items of the bucket, respectively. If there are both L-type and S-type characters
in a bucket, all of the L-type characters are on the left side of the bucket and all
of the S-type characters are on the right.
Preceding character. The preceding character of x[i] for i ∈ [0,n− 1], denoted by
prec(x, i), is x[i − 1] for i > 0, or else x[n − 1]. The preceding character of a
suffix/substring starting at x[i] is also prec(x, i).
Reduced string. The reduced string x1 is formed by replacing all of the LMS sub-
strings in x with their integer names, which represent the order of the LMS
substrings.
We assume that x[n − 1] = 0 is the unique smallest character of x and that each
character is equal to the number of characters in x smaller than it. Under these as-
sumptions, x[n−1] is usually called the sentinel. It guarantees that any suffix is unique
and not a prefix of another. x[i] itself also gives the start position of bucket(SA(x), x[i]).
These assumptions are rather easy to satisfy in practice.2
2For an input string x of σ = O(1) or nO(1), all of the characters of x are sorted by an external memory integer
sorting algorithm and each character is renamed as the number of characters smaller than it. It is obvious
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
12:4 G. Nong et al.
2.2. DSA
For SA(x), we introduce its external memory alternative DSA(x) (i.e., disk-SA for x).
Each item of DSA(x) is a tuple 〈pos, ch, t〉 called DSAITEM, where each symbol is
defined as:
—pos: position index for suf(x, pos)
—ch: x[pos], that is, the head character of suf(x, pos)
—t: 0 or 1 for prec(x, pos) being L-type or S-type, respectively
For an instance e of DSAITEM, we say that e stores a suffix/substring in x if e.pos is
the start position of the suffix/substring in x.
Let DSA(x) and DSAB(x) be two size-n arrays of DSAITEM. DSA(x) is defined as
DSA(x)[i].pos = SA(x)[i] and DSA(x)[i].{ch, t}, set as specified in DSAITEM. Using
DSA(x), we define the disk BWT of x, denoted by DSAB(x), as DSAB(x)[i].pos set
as DSA(x)[i].pos− 1 or n− 1, for DSA(x)[i].pos greater than or equal to 0, respectively,
and DSAB(x)[i].{ch, t} set as specified in DSAITEM.
The key idea behind the design of DSA-IS is to split x into blocks and divide DSA(x)
and DSAB(x) for each block of x. Compute, in the RAM, DSA(x) and DSAB(x) of each
block in a block-by-block manner. Compute DSA(x) by sequentially accessing the data
of DSA(x) and DSAB(x) in each block via an I/O buffer of W words per block. SA-IS is
used to compute in the RAM the external representation DSA(x bi|x) of the suffix array
of each block x bi of x. The whole suffix array of x can then be efficiently induced by
merging all of the DSA(x bi|x) together in a scanning complexity. Prior to the merging,
each DSA(x bi|x) is augmented with the BWT of x bi, that is, DSAB(x bi|x), to enable
fast sequential access to the preceding character of each sorted suffix in x bi during the
inducing process, where a suffix suf(x, j) is said to be in x bi if x[ j] ∈ x bi.
2.3. Dividing x and dsa into Blocks
To follow are the general constraints for dividing x into k = n/m, where m = O(M),
consecutive blocks {x bi|i ∈ [0,k− 1]}:
(1) Each block starts with x[0] or an LMS character and ends with another LMS
character.
(2) Any pair of neighboring blocks overlaps on a common LMS character.
(3) A block x[g,h], 0 ≤ g < h ≤ n− 1, can have more than m characters only if there is
no LMS character in x[g + 1,h− 1].
There are many ways to divide x under these constraints. Here we initialize i = k−1
and x bi as empty. For all of the LMS substrings from right to left in x, a substring will
be added to x bi if the addition will not cause ‖x bi‖ > m, or else i is decreased by 1
and a new block, x bi, consisting of this substring at its rightmost is created. This block
division can be done by scanning x leftward. Similarly, we can also divide x by scanning
x rightward, but this will require a somewhat complicated method to check the type of
each character to on-the-fly detect each substring and hence is not used here.
The constraints and the division strategy lead to ‖x bi‖ = n+ k− 1 and ‖x bi‖ +
‖x bi+1‖ ≥ m+ 2 for i ∈ [0,k− 2]. Thus, ( k−12 )(m+ 2) < n+ k− 1 and hence k < 2nm + 1.
The number of blocks does not exceed 
 2nm .
For i ∈ [0,k−1], x[i ·m] is called a boundary character. Each character x[ j], i ·m≤ j ≤
(i+ 1) ·mmust belong to the block containing x[i ·m], the block containing x[(i+ 1) ·m],
or the block in between these two. The three blocks may be the same or all different.
that the new string of renamed characters satisfies the assumption and the suffix arrays for both strings are
identical.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
Induced Sorting Suffixes in External Memory 12:5
Hence, for each boundary character, we record the start and end positions of the block
containing it. In this way, given the position of a character, we can locate the block
containing this character in O(1) time and O(k) space.
In addition to dividing the input x into blocks, the output dsa[0,n−1] of nDSAITEM
tuples is also divided into blocks. We split dsa into dsa k blocks {dsa bi|i ∈ [0,dsa k−1]}
evenly (except that the last block may be smaller), where dsa bi = dsa[i ·dsa m, (i+1) ·
dsa m− 1], dsa k = 
n/dsa m and dsa m= O(M). It should be noted that mand dsa m
are two independent parameters for the division of x and dsa, respectively, under the
given RAM limit. They are generally different for optimal performance; nevertheless,
they can also be set as identical for engineering convenience in practice.
2.4. Algorithm Framework
The framework of DSA-IS remains similar to that of SA-IS and is sketched here:
Similar to SA-IS in RAM, with slight modifications, the algorithm for induced sorting
of suffixes in the externalmemory can also be reused for sorting LMS substrings. Hence,
the sorting algorithm for inducing the solution is described first. It is used to develop
the algorithm for sorting and naming LMS substrings to reduce the problem.
3. ALGORITHM DETAILS
Radix sort is frequently used in DSA-IS to sort fixed-size items of integer keys. Given
M = ((nW)0.5) in our external memory model, the sorting of each logn-bit integer
can be done in two passes using a multipass radix sort. The first pass sorts the lowest
0.5 logn bits and the second pass sorts the highest 0.5 logn bits.
3.1. Inducing Solution
We define the following sets for x bi = x[g,h] with g < h and i ∈ [0,k− 1], where the
symbol ′′|x′′ (i.e., on condition of x) in each set indicates that the ordering of the suffixes
in the set may be affected by parts of x outside x bi:
—SAlms(x bi|x) = {SA(x)[ j]|SA(x)[ j] ∈ [g + 1,h] and x[SA(x)[ j]] is LMS, j ∈ [0,n− 1]}.
—DSA(x bi|x) = {DSA(x)[ j]|DSA(x)[ j].pos ∈ [g,h− 1], j ∈ [0,n− 1]}.
—DSAB(x bi|x) = {DSAB(x)[ j]|DSA(x)[ j].pos ∈ [g,h− 1], j ∈ [0,n− 1]}.
—DSAB(x bi|x) and DSABs(x bi|x) contain only the items of L-type and S-type suffixes
in DSAB(x bi|x), respectively.
Computing DSA(x bi|x) from x bi and SAlms(x bi|x) is a key issue in inducing the
solution. We make two important observations with respect to whether x bi is a single-
substring block or not.
If x bi is not a single-substring block, the size of x bi is at most m and hence both
x bi and SAlms(x bi|x) can be stored in the RAM. The characters in x bi are sorted and
renamed as their ranks starting from 0 to form a new block x b′i. The LMS suffix array of
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
12:6 G. Nong et al.
x b′i, that is, SAlms(x b′i|x), can be derived from SAlms(x bi|x) trivially. Then DSA(x bi|x)
can be computed from x bi, x b′i, and SAlms(x b′i|x) using a method similar to that for
inducing SA(x) from SA(x1) as explained in the introduction. Hence, we have the first
observation.
OBSERVATION 3.1. Given x bi and SAlms(x bi|x), where x bi is not a single-substring
block, DSA(x bi|x) can be computed in linear CPU time and the RAM space O(x ni) by
adapting the algorithm for induced sorting in SA-IS with minor modifications.
When x bi is a single-substring block, the size of x bi may exceed m and hence it
cannot be put in the RAM. We thus make the second observation as follows.
OBSERVATION 3.2. DSA(x bi|x) for a single-substring block x bi can be computed in the
external memory.
In a single-substring block x bi, all of the L-type and S-type suffixes in the block are
already sorted from right to left into their increasing and decreasing orders, respec-
tively. DSA(x bi|x) can be computed by scanning and merging these two kinds of sorted
suffixes in CPU time O(x ni) and I/O complexity O(x ni/W), where x ni = ‖x bi‖.
As an example for demonstrating how to sort the suffixes in a single-substring block,
let us suppose that x bi = “abbcc...xxyzyxx....ccba” is a single-substring block with
all of the S-type characters marked in bold. Notice that the last suffix in x bi is not
contained inDSA(x bi|x). To computeDSA(x bi|x), we use two I/O buffers to sequentially
retrieve the L-type and S-type suffixes (excluding the last S-type suffix) in ascending
order, respectively. For any two suffixes being compared for merging, their order can be
determined immediately if their head characters are different; otherwise, the S-type
suffix must be greater (see Lemma 2 in Ko and Aluru [2005]). For instance, in this
example of the two suffixes starting with “y” in x bi, the one with a bold “y” is greater.
With these observations, the algorithm for induced sorting is given here:
ALGORITHM 1: Inducing a Solution in the External Memory
—Input: x, DSA(x1).
—Output: DSA(x).
—Procedure:
(1) Compute SAlms(x bi|x) from DSA(x1) for all i ∈ [0,k− 1].
(2) Make DSA(x bi|x), DSAB(x bi|x), and DSABs(x bi|x) from SAlms(x bi|x) for all i.
(3) Merge DSA(x bi|x) for all i to produce DSA(x).
More details are given next to each step in Algorithm 1.
Step 1.1
This step consists of three substeps:
(a) Radix sort j by DSA(x1)[ j].pos for all j ∈ [0,n1 − 1] to produce ISA(x1), that is, the
inverse suffix array of x1 defined as SA(ISA(x1)[ j]) = j for all j.
(b) Radix sort 〈PAlms(x)[ j], ch, t〉 by ISA(x1)[ j] for all j. The sorting result is DSAlms(x),
which contains all of the sorted LMS suffixes in x, where PAlms(x) is the position
array for all of the LMS characters in x. PAlms(x)[ j] gives the starting position of
the jth LMS character in x.
(c) Scan and decompose DSAlms(x) into SAlms(x bi|x) for all i ∈ [0,k − 1] as follows.
For each DSAlms(x)[ j] being scanned, denoted by e, we can determine in O(1) time
and O(k) space which x bi contains x[e.pos]. Once SAlms(x bi|x) is located, e.pos is
appended to the I/O buffer for SAlms(x bi|x). The I/O buffer will be flushed to disk
when it is full.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
Induced Sorting Suffixes in External Memory 12:7
Step 1.2
There are two cases with respect to whether x bi is a single-substring block or not for
computing DSA(x bi|x) from SAlms(x bi|x):
—Yes: DSA(x bi|x) can be computed in the disk (see Observation 3.2).
—No: DSA(x bi|x) can be computed in the RAM (see Observation 3.1), because x ni ≤ m.
Then, DSAB(x bi|x) and DSABs(x bi|x) are computed by scanning DSA(x bi|x).
Step 1.3
This step consists of three substeps, in which random access of x[p] in substeps (b) and
(c) is avoided because e1.ch = x[p]:
(a) Initialize each item of dsaas empty by setting all of itsmembers to 0. ScanDSAlms(x)
leftward. For each scanned item e, let p = e.pos and put suf(x, p) in the current
rightmost empty item in bucket(dsa, x[p]), say, dsa[ j], by copying e to dsa[ j].
(b) Scan dsa rightward. For each scanned item e with e.pos > 0 and e.t = 0, let
p = e.pos − 1. Determine the block in x containing x[e.pos], say, x br. Put suf(x, p)
in the current leftmost empty item in bucket(dsa, x[p]), say, dsa[ j], by copying the
current leftmost unvisited item in DSAB(x br|x), say, e1, to dsa[ j] and mark e1 as
visited.
(c) Scan dsa leftward. For each scanned item e with e.pos > 0 and e.t = 1, let p =
e.pos − 1. Determine the block in x containing x[e.pos], say, x br. Put suf(x, p) in
the current rightmost empty item in bucket(dsa, x[p]), say, dsa[ j], by copying the
current rightmost unvisited item in DSABs(x br|x), say, e1, to dsa[ j] and mark e1 as
visited.
Avoiding Random Access of dsa[j] in Step 1.3
In Algorithm 1, all of the steps except Step 1.3 adapt easily to disk. The obstacle for
making an external memory design for Step 1.3 is the need to “put suf(x, p) in the
current leftmost/rightmost empty item in bucket(dsa, x[p]).” dsa[ j] may be currently
on disk and will be accessed slowly. To overcome this difficulty, the blockwise-induced
sorting method reported in Beller et al. [2013] and Nong et al. [2014] can be applied to
avoid random access of dsa[ j].
Blockwise-induced sorting evenly splits dsa into a number of blocks (the last block
may be smaller). It repeats the following steps for all of the blocks of dsa sequentially
(from left to right for inducing L-type suffixes and from right to left for inducing S-type
suffixes), where the dsa block currently in the RAM is the active dsa block:
(1) After a block in dsahas been switched from the disk to the RAM and has become the
active block, the suffixes already in the block are stably radix sorted to the correct
positions in their buckets by their head characters. In each bucket, the L-type and
S-type suffixes are clustered at the left end and right end, respectively.
(2) For each suffix whose order is being induced from a sorted suffix in the active block,
determine the dsa block, say, dsa bi, that the suffix should be sorted into and put
the suffix into dsa bi as follows: if dsa bi is the active dsa block, put the suffix in its
correct position in dsa bi, or else append the suffix to the I/O buffer for dsa bi.
Figure 1 shows the main data structures used by the blockwise-induced sorting
algorithm with x and dsa divided into three and four blocks, respectively. In this
example, block 1 of x contains only a single substring and is much longer than the
other two blocks, whereas all of the blocks of dsa are the same length.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
12:8 G. Nong et al.
Fig. 1. The main data structures used in the blockwise-induced sorting method, where “DSAB blocks”
include {DSAB(x bi |x)} and {DSABs(x bi |x)} for all i for inducing L-type and S-type suffixes, respectively, in
Steps I.3.b and I.3.c of Algorithm 1. The I/O buffers and active dsa block are stored in the RAM, and DSAB
and dsa are in the disk.
3.2. Reducing the Problem
The main task for reducing the size of the problem is to sort and name all of the LMS
substrings. By replacing each substring in x with its new name, a new shorter string
can be produced. A naive method for sorting substrings in the external memory is to
use a k-way merge sort: sort all of the substrings in each block x bi and merge all of
the sorting results. This merge sort requires O(n log k) CPU time. However, a merge
sort works only when all of the k substrings in a comparison for merging are available
in the RAM. We should avoid using a merge sort for reducing the size of the problem
in our solution, because the k substrings may require storage exceeding M and each
substring may need to be read from the disk many times for comparisons. We reuse
the method for induced sorting of suffixes in Step 1.3 of Algorithm 1, similar to what
we did for sorting substrings in SA-IS.
Before presenting the algorithm for reducing the size of the problem, we first define
some symbols on x:
—DSTR(x): a size-n DSAITEM array stores all of the L-type and S-type substrings
sorted in lexicographical order.
—DSTRB(x): DSTRB(x)[i].pos = j and DSTRB(x)[i].{ch, t} are set as specified in the
definition of DSAITEM, where j = DSTR(x)[i].pos− 1 for DSTR(x)[i].pos > 0 or else
j = n− 1, for i ∈ [0,n− 1].
—DSTRlms(x) = {DSTR(x)[i].pos|x[DSTR(x)[i].pos] is LMS, i ∈ [0,n− 1]}.
Next, we let x bi = x[g,h] with g < h for all i ∈ [0,k − 1] and define some more
symbols on x bi:
—DSTR(x bi|x) = {DSTR(x)[ j]|DSTR(x)[ j].pos ∈ [g,h− 1], j ∈ [0,n− 1]}.
—DSTRB(x bi|x) = {DSTRB(x)[ j]|DSTR(x)[ j].pos ∈ [g,h− 1], j ∈ [0,n− 1]}.
—DSTRB(x bi|x) and DSTRBs(x bi|x) contain only the items of the L-type and S-type
substrings in DSTRB(x bi|x), respectively.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
Induced Sorting Suffixes in External Memory 12:9
—DSTRDATAlms(x bi|x) stores the characters for the sorted LMS substrings in x bi; that
is, each item of DSTRDATAlms(x bi|x) is a substring (not an index to the substring).
ALGORITHM 2: Reducing the Problem in the External Memory
—Input: x.
—Output: x1.
—Procedure:
(1) Compute DSTR(x bi|x) and DSTRDATAlms(x bi|x) for i ∈ [0, k− 1].
(2) Merge DSTR(x bi|x) for all i to produce DSTRlms(x).
(3) Scan DSTRlms(x) and DSTRDATAlms(x bi|x) for all i to compute the name for each LMS
substring in x, using DSTRlms(x) as the pivot for the order of sequentially retrieved
substrings from each DSTRDATAlms(x bi|x).
(4) Replace each LMS substring in x by its name to produce the reduced string x1.
More details of each step in Algorithm 2 are given here.
Step 2.1
Compute DSTR(x bi|x) using the algorithm for sorting LMS substrings in
SA-IS and scan DSTR(x bi|x) to produce DSTRB(x bi|x), DSTRBs(x bi|x), and
DSTRDATAlms(x bi|x).
Step 2.2
This step consists of three substeps:
(a) Scan x leftward to put all of the LMS suffixes into their buckets in dsa, from right
to left in each bucket.
(b) Reuse Steps I.3.b and I.3.c of Algorithm 1 to compute DSTR(x) in the disk by re-
placing DSAB(x bi|x) and DSABs(x bi|x) with DSTRB(x bi|x) and DSTRBs(x bi|x),
respectively.
(c) Scan DSTR(x) to produce DSTRlms(x).
In substep (c), we need to detect if a character x[DSTR(x)[i].pos] is LMS or not. This
can be done trivially as follows. In our program, each bucket in DSTR(x) is split into the
L-type and the S-type subbuckets for all of the L-type and S-type items, respectively,
in this bucket. All of the L-type subbuckets in DSTR(x) are consecutively stored in a
file and similarly all of the S-type subbuckets in another file. When DSTR(x)[i] is in an
S-type subbucket, we retrieve the type of its preceding character from DSTR(x)[i].t to
make a decision.
Step 2.3
Scan DSTRlms(x) rightward. For each item e being scanned, let x bh be the block con-
taining x[e]. The LMS substring starting at x[e] must be the current leftmost unvisited
substring in DSTRDATAlms(x bh|x). The sorted LMS substrings are sequentially re-
trieved and marked as visited from DSTRDATAlms(x bi|x), for all i. Hence, any pair of
neighboring substrings in DSTRlms(x) can be sequentially retrieved and compared once
to determine if they are equal or not. The name of each substring in DSTRlms(x), which
is defined as the number of all of the substrings less than it, can also be computed.
Step 2.4
DNAMElms(x) is an integer array storing the names of all of the sorted LMS substrings
in DSTRlms(x); that is, DNAMElms(x)[i] is the name for the LMS substring starting at
x[DSTRlms(x)[i].pos]. The reduced string x1 is produced by radix sorting DNAMElms(x)[i]
by DSTRlms(x)[i] for all i.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
12:10 G. Nong et al.
Table I. Corpora, n in GiB, 1 Byte per Character
Corpus n σ Specification
uniprot 2.42 96 UniProt Knowledgebase release 4.0, at
http://www.uniprot.org/news/2005/02/01/release.
genome 2.86 6 Human genome data, used in Dementiev et al. [2008], at
http://algo2.iti.kit.edu/dementiev/esuffix/instances.
guten 3.05 256 Gutenberg collection, used in Dementiev et al. [2008], at
http://algo2.iti.kit.edu/dementiev/esuffix/instances.
random2 4.00 256 A concatenation of two identical copies of a string with each
character randomly selected from [0, 255], with a maximum LCP
of 2.0 Gi. The exact size of this file is 232 − 2 bytes.
genome2 5.72 6 A concatenation of two copies of a corpus “genome,” with a
maximum LCP of 2.86 Gi.
enwiki 7.88 256 A dump for English Wikipedia, at http://download.wikimedia.org/
enwiki/latest/enwiki-latest-pages-articles.xml.bz2.
guten1209 22.44 256 The Gutenberg collection from September 2012, used in
Bingmann et al. [2013], at http://algo2.iti.kit.edu/bingmann/
esais-corpus/gutenberg-201209.24090588160.xz.
3.3. Analysis
The time and space complexities of DSA-IS are dominated by Algorithm 1 for induced
sorting; hence, it suffices to conduct the analysis on Algorithm 1.
ThemaximumRAM requirement is for storing the data structures shown in Figure 1,
where all of the I/O buffers for the blocks of DSAB and dsa must be simultaneously
maintained in the RAM. Thus, the maximum number of blocks is (k + dsa k) · W =
O(M), that is, k + dsa k = O(M/W). As dsa m = O(M), the maximum input size
is n = dsa k · dsa m ≤ (k + dsa k) · dsa m = O(M/W) · O(M) = O(M2/W), and so
M = ((nW)0.5). This meets the assumptions for our external memory model given
in Section 1. With these I/O buffers, accessing the data of each DSAB or dsa block is
done in an I/O complexity of the block’s size over W , yielding the algorithm’s total I/O
complexity as O(n/W).
DSA-IS thus exactly emulates SA-IS and, therefore, inherits the advantages of the
latter: both the CPU time and peak disk use are O(n) and the I/O complexity is O(n/W).
4. EXPERIMENTS
The time and space performance of DSA-IS are evaluated by comparing with eSAIS
[Bingmann et al. 2013] and EM-SA-DS [Nong et al. 2014] on the datasets listed in
Table I. All of the datasets except the last one were used in the experimental study
in Nong et al. [2014] for evaluating the performance of EM-SA-DS. The last dataset is
a recent version of the Gutenberg collection. It is chosen not only because it is large
enough for our experiments but also because its alphabet size of 256 is typical for texts
in full ASCII.
Our experimental platform has an identical configuration to that used in the experi-
mental study in Nong et al. [2014]: 1 CPU (Intel(R) Core(TM) i3 3.20GHz); 4GiB RAM
(1,333MHz DDR3); 1 Disk (2TiB, 7,200rpm, SATA2); Linux (Ubuntu 11.04). The pro-
gram for eSAIS is downloaded from the web page3 described in Bingmann et al. [2013]
and the programs for EM-SA-DS and DSA-IS are our own implementations. A package
containing our programs and SACA-K can be retrieved from our project site.4 For the
convenience of presentation, these programs are denoted by the lowercase symbols
esais, emsads, dsais, and sacak, respectively.
3http://panthema.net/2012/1119-eSAIS-Inducing-Suffix-and-LCP-Arrays-in-External-Memory/.
4http://code.google.com/p/ge-nong/.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
Induced Sorting Suffixes in External Memory 12:11
Table II. Running Times in μs/ch. The mean running time for each
program is the total time averaged over all of the corpora. The throughput
in μs per byte is the total time averaged over the total I/O volume.
Corpus esais emsads dsais
uniprot 2.998 3.124 2.502
genome 3.029 3.068 2.550
guten 3.662 3.900 3.250
random2 4.037 3.744 3.856
genome2 3.425 3.308 2.966
enwiki 3.827 4.610 4.185
mean 3.586 3.797 3.418
norm. 1.049 1.111 1.000
throughput 0.019 0.019 0.022
Two experiments are conducted to measure the time and space consumptions of each
algorithm. The program for each algorithm is set at 3-GiB RAM and 40-bit integers.
The performance metrics measured are the mean speed in microseconds per character
(μs/ch), peak RAM use in gigabytes, peak disk use and mean I/O volume in bytes
per character, and the recursion depth. To obtain the statistics for each program, the
running time and peak RAM use are collected using the shell commands “time” and
“memusage,” respectively, and the other metrics are collected by the program itself. For
accurate time results, the time of each program on a corpus is evaluated as the mean
of two runs.
For the recursion depth, esais counts all of the recursions as in SA-IS; that is,
the deepest recursion level considers all of the characters of the reduced string to
be different. However, both emsads and dsais count only the recursions down to the
recursion level where the suffix array of the reduced string can be computed in the RAM
by SA-IS. Hence, the metrics for esais should not be compared with the metrics of the
other two. The purpose of collecting these metrics is to see how many recursions are
needed for each individual program running on a corpus and to gain more information
about the program’s behavior.
4.1. Experiment I: Different Input Data
This experiment investigates the time and space performance of each algorithm on
different input data of varying size, alphabet, and LCP. The first six corpora in Table I
are used.
As a reference benchmark for the sequential I/O throughput of the machine in use,
we use the shell command “time cp” to evaluate the time to copy the file “guten1209.”
This file duplication job consists of reading and writing the file once. After three runs,
a mean running time of 673 seconds is recorded, yielding a saturated sequential I/O
throughput of 0.014μs per byte, that is, 68.3MiB/s. To calculate the speed gap between
DSA-IS and its internal memory counterpart SA-IS, we also run sacak, which is an
optimized SA-IS design requiring a 5n space for n≤ 232, on all 600MiB prefixes of each
corpus to obtain a total running time of 2,672 seconds, that is, 0.607μs per byte.
Table II shows the experimental results for the running time inμs/ch of each program
on a corpus, the mean running time, and the I/O throughput of each program on all of
the corpora, where the mean running time in μs/ch for each program is the total time
averaged over all of the corpora and the I/O throughput in μs per byte is the total time
averaged over the total I/O volume. In each row, the best result is marked in bold. The
row “norm.” gives the results in the row “mean” normalized by the best result. From
this table, we conclude the following:
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
12:12 G. Nong et al.
Table III. Peak RAM in GiB, Peak Hard-Disk (HD) Use and Mean I/O Volume in Bytes per Input Character, and
Recursion Depth (RD). The mean of each metric is the total of this metric averaged over all of the corpora.
Corpus esais emsads dsais
RAM HD I/O RD RAM HD I/O RD RAM HD I/O RD
uniprot 3.3 22.8 157.0 8 3.0 30.8 187.3 2 3.0 18.9 124.6 2
genome 3.7 22.5 153.2 11 3.0 29.3 159.0 3 3.0 18.8 120.4 2
guten 3.6 24.7 185.3 13 3.0 31.1 210.5 3 3.0 19.1 155.4 3
random2 3.2 25.4 201.4 18 3.0 31.0 220.7 3 3.0 19.9 172.8 3
genome2 3.7 22.6 169.1 18 3.0 28.6 157.9 3 3.0 18.8 133.8 3
enwiki 3.2 24.4 215.6 5 3.0 33.7 233.0 4 3.0 20.4 180.8 4
mean - 23.8 187.3 - - 31.1 199.4 - - 19.5 154.3 -
norm. - 1.2 1.2 - - 1.6 1.3 - - 1.0 1.0 -
—The mean running times of dsais and esais are more or less the same, although
dsais is observed to be marginally faster in most cases. As the speed of an I/O-
intensive program usually fluctuates about a certain range, a small difference of
10% may be negligible.
—The I/O throughput of each program is almost equal to 0.02μs per byte, which is
0.014/0.02 = 70% of the system’s saturated sequential I/O throughput.
—The mean running time of dsais is 3.418μs per byte, which is 3.418/0.607 = 5.6
times that of saca-k, indicating a speed gap of around six times between the internal
and external designs for the induced sorting method.
Table III shows the space metrics collected in this experiment, including the peak
RAM use, peak disk (HD) use, mean I/O volume, and recursion depth (RD), where the
mean of eachmetric is its total averaged over all of the corpora. The RAMuse for emsads
and dsais are observed to be well bounded by the given RAM limit of 3GiB, but that
for esais varies from 3.2 to 3.7GiB. In this study, the parameter “ram_use” in the file
“esactest.cc” of esais is set at 3GiB. As commented in “esactest.cc,” in addition to the
RAM set by this parameter, “all structures besides the external sorters need memory
as well.” This results in some more RAM being used in addition to the set RAM limit
of 3GiB. This should be regarded as an implementation choice instead.
Table III shows that the best results are achieved by dsais. Specifically:
—dsais has the smallest disk capacity requirement. On average, the disk use for esais
and emsads are, respectively, 1.2 and 1.6 times that of dsais.
—the smallest I/O volume is achieved by dsais. On average, the mean I/O volumes
for esais and emsads are, respectively, 1.2 and 1.3 times that of dsais. These results
agree with the ratios of the mean running times for esais and emsads against dsais,
that is, 1.049 and 1.111, respectively, and with the running time being proportional
to the I/O volume.
4.2. Experiment II: Increasing the Input Size
In this experiment, the scalability of each program is investigated by evaluating its
time and space performance on the prefixes of “guten1209” in lengths varying from
1 to 16GiB. The experiment results are shown in Figure 2 and Table IV. In Figure 2,
the running time of each program is plotted against the prefix length. The curves look
analogous: all of them increase at similar rates when the prefix length grows. The
curves for esais and dsais are so close that they twist together with only negligible
distances. However, there is a noticeable gap between the curve for emsads and the
other two curves. This gap is due to the larger I/O volume of emsads, as seen in Table IV.
Table IV shows that the best results for disk use and I/O volume are also achieved
by dsais. For each program, the peak disk use remains stable as the prefix length
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
Induced Sorting Suffixes in External Memory 12:13
Fig. 2. Running times in μs/ch for prefixes of “guten1209” in lengths from 1 to 16GiB.
Table IV. Peak RAM in GiB, Peak Hard-Disk (HD) Use and Mean I/O Volume in Bytes per Input Character,
and Recursion Depth (RD), for Prefixes of “guten1209” in Lengths from 1 to 16GiB. The mean
of each metric is the total of this metric averaged over all of the prefixes.
Prefix esais emsads dsais
RAM HD I/O RD RAM HD I/O RD RAM HD I/O RD
1 3.6 22.3 139.1 8 3.0 31.7 185.2 2 3.0 18.7 113.6 2
2 3.7 22.4 143.5 10 3.0 31.7 200.7 3 3.0 18.7 115.1 2
4 3.7 22.8 157.1 12 3.0 31.5 202.6 3 3.0 18.8 134.8 3
8 3.7 23.0 172.5 12 3.0 31.4 209.4 4 3.0 18.8 139.2 3
16 3.7 23.4 193.1 13 3.0 29.3 167.8 4 3.0 18.9 160.0 6
mean - 23.1 178.2 - - 30.3 185.7 - - 18.9 147.0 -
norm. - 1.2 1.2 - - 1.6 1.3 - - 1.0 1.0 -
increases. However, the I/O volume grows substantially as the prefix length increases.
For emsads and dsais (esais is not analyzed here because we do not know much about
its implementation details), the growth of the I/O volume is mainly due to the use of a
merge sort in the programs. In some steps of the algorithms EM-SA-DS and DSA-IS,
there are several tasks requiring sorting of fixed-size tuples of integer keys. Those
tasks are done using a merge sort instead of a radix sort in our programs. A merge sort
is used in the current revisions of the two programs as its external memory design is
much easier for us to code with limited time and manpower. In these two programs,
replacing merge sorting with radix sorting to achieve a linear I/O volume is a routine
engineering job and can be done with enough programming capacity. Due to the use
of a merge sort, given a fixed RAM limit, M is fixed and hence m = O(M) is fixed. At
each recursion level, k = n/m becomes larger as n increases and hence the I/O volume
O(n log k) also increases.
4.3. Implementation Issues
Given M and n, there are many possible choices formand W . In our program, the peak
RAM use is estimated as 91mbytes. Given M = 3GiB in our experiments, when n< 10
Gi, we simply set m = M/96 = 225, and hence k < 10 · 230/m = 5 · 26 and it is safe
to set W = (96 − 91)m/k = m/26 = 219. For bigger n, we choose m = M/91, and W is
dynamically set as m divided by the number of I/O buffers. For the program running
on the 16GiB prefix of “guten1209,” these parameters are recorded as W = 71.2KiB,
m= dsa m= 33.8MiB, and k = dsa k = 486. For a given M, if we keep choosing m and
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
12:14 G. Nong et al.
W in this way, then W will decrease when n increases and the I/O complexity O(n/W)
will proportionally increase to slow down the program. To solve this problem, we can
choose a smaller m to save RAM space for the I/O buffers when W is less than a given
threshold. In the current revision of our program, we fix m = M/91 and use 40-bit
integers, resulting in a peak disk use of around 20n bytes. If we do not take the issue
of W into account, the largest file that DSA-IS can handle on the experiment platform
is 2,048GiB/20 > 100GiB. However, as mentioned earlier, fixing m = M/91 in this
case is obviously not good enough and a better method must be used for improving the
performance.
Our programs dsais and emsads are natural implementations of the algorithms for
conducting our experiments. They can be further refined for better performance. Only
very basic integer sorting algorithms such as radix sorting and merge sorting are used.
The current routines for these sorts in our programs are plain and have not been
optimized. All the I/O jobs in our programs are executed via the operating system (by
calling functions fread and fwrite in C).
In their current implementations, dsais, emsads, and esais do not use compression
to reduce the I/O volume and disk use. To the best of our knowledge, bwt-disk is
a representative (and may be the only up-to-date) method for using compression to
improve the practical performance of sorting suffixes in the external memory. The study
in Ferragina et al. [2012] showed that compressing data can make disk use smaller
than the size of the input string. Both dsais and emsads perform I/Os in size-W blocks.
They can naturally be improved by using compression on each I/O block. Nevertheless,
due to the use of a priority queue provided by STXXL, adding a compression feature to
I/Os in esais may have to be done inside STXXL. As we are not experts in STXXL, we
are not sure if this can be done and leave it to be determined by interested readers.
5. SUMMARY
The core contribution of this article is to introduce a new disk-friendly method in DSA-
IS for sequentially retrieving the preceding character of a sorted suffix to induce the
order of the preceding suffix. There are several potential applications for this method.
For example, by modifying SA-IS, Fischer [2011] gave an excellent algorithm to induce
an LCP array. As the induced sorting process in DSA-IS exactly mimics its counter-
part in SA-IS, the method for inducing the LCP array can naturally be extended to
the external memory model with DSA-IS. Goto and Bannai [2013] utilized SACA-K
to design a space-efficient linear-time algorithm for computing LZ77 factorization on
constant alphabets. Ka¨rkka¨inen et al. [2014] proposed algorithms for computing the
LZ77 parsing efficiently using the external memory. This suggests a possibility for
extending DSA-IS for computing LZ77 factorization in the external memory. The ap-
proach for turning SA-IS into DSA-IS can be directly applied to make an external
memory design for the suffix array construction algorithm in Ko and Aluru [2005].
The algorithms eSAIS, EM-SA-DS, and DSA-IS contain three designs proposed for
using the induced sorting principle to sort suffixes in the external memory. For readers’
information, some comparisons between them are sketched here:
—Given the internal memory capacity M = ((nW)0.5) and the external memory ca-
pacity E = O(n), the I/O complexity of each is O(n/W).
—The best experimental times are achieved by DSA-IS and eSAIS. They are around
10% better than that of EM-SA-DS. Although they have similar speed, eSAIS uses
around 20% more disk space than DSA-IS.
—Random access of bkt[x[ j]] and sa[bkt[x[ j]]] (described in Section 1) is avoided by
dividing the suffix array into blocks in both DSA-IS and EM-SA-DS, whereas eSAIS
uses a priority queue managed by the external memory library STXXL.
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.
Induced Sorting Suffixes in External Memory 12:15
—Random access of x[ j] (described in Section 1) is avoided in different ways. The meth-
ods used in eSAIS and EM-SA-DS both divide long LMS substrings into fixed-size
substrings and move the substrings around during the inducing process. However,
DSA-IS uses a technique based on dividing x into blocks and constructing a separate
suffix array for each block, in which the main inducing phase can be regarded as a
multiway merging of the suffix arrays of the blocks.
DSA-IS is presented here to share the results of our current research on designing
efficient suffix sorting algorithms. We are approaching a favorable position for the
development of a distributed solution for building a big suffix array or suffix tree and
their compressed alternatives. We are currently investigating efficient methods for
extending our external memory algorithms to be distributed so as to further scale the
problem size by running the algorithms on a distributed system consisting of many
computing nodes.
ACKNOWLEDGMENTS
The authors wish to thank the reviewers who have given constructive and insightful suggestions for improv-
ing the presentation of this article.
REFERENCES
T. Beller, M. Zwerger, S. Gog, and E. Ohlebusch. 2013. Space-efficient construction of the burrows-
wheeler transform. In String Processing and Information Retrieval. Lecture Notes in Computer Science,
Vol. 8214. 5–16.
T. Bingmann, J. Fischer, and V. Osipov. 2013. Inducing suffix and LCP arrays in external memory. In
Proceedings of ALENEX. 88–102.
R. Dementiev, J. Ka¨rkka¨inen, J. Mehnert, and P. Sanders. 2008. Better external memory suffix array con-
struction. ACM Journal of Experimental Algorithmics 12 (Aug. 2008), 3.4:1–3.4:24.
P. Ferragina, T. Gagie, andG.Manzini. 2012. Lightweight data indexing and compression in externalmemory.
Algorithmica 63, 3 (2012), 707–730.
J. Fischer. 2011. Inducing the LCP-array. In Algorithms and Data Structures. Lecture Notes in Computer
Science, Vol. 6844. 374–385.
K. Goto and H. Bannai. 2013. Space Efficient Linear Time Lempel-Ziv Factorization on Constant Size
Alphabets. Retrieved January 6, 2014, from http://arxiv.org/abs/1310.1448.
J. Ka¨rkka¨inen, D. Kempa, and S. J. Puglisi. 2014. Lempel-ziv parsing in external memory. In Proceedings of
the Data Compression Conference (DCC’14). 153–162.
P. Ko and S. Aluru. 2005. Space-efficient linear time construction of suffix arrays. Journal of Discrete Algo-
rithms 3, 2–4 (2005), 143–156.
U. Manber and G. Myers. 1993. Suffix arrays: A new method for on-line string searches. SIAM Journal on
Computing 22, 5 (1993), 935–948.
Y. Mori. 2008. SAIS—An Implementation of the Induced Sorting Algorithm. Retrieved from http://yuta.256.
googlepages.com/sais.
G. Nong. 2013. Practical linear-time O(1)-workspace suffix sorting for constant alphabets. ACMTransactions
on Information Systems 31, 3 (July 2013), 15:1–15:15.
G. Nong, W. H. Chan, S. Zhang, and X. F. Guan. 2014. Suffix array construction in external memory using
D-critical substrings. ACM Transactions on Information Systems 32, 1 (Jan. 2014), 1:1–1:15.
G. Nong, S. Zhang, and W. H. Chan. 2011. Two efficient algorithms for linear time suffix array construction.
IEEE Transactions on Computers 60, 10 (Oct. 2011), 1471–1484.
E. Ohlebusch. 2013. Bioinformatics Algorithms: Sequence Analysis, Genome Rearrangements, and Phyloge-
netic Reconstruction. Oldenbusch Verlag.
S. J. Puglisi, W. F. Smyth, and A. H. Turpin. 2007. A taxonomy of suffix array construction algorithms. ACM
Computer Surveys 39, 2 (2007), 1–31.
Received October 2013; revised July 2014; accepted November 2014
ACM Transactions on Information Systems, Vol. 33, No. 3, Article 12, Publication date: February 2015.

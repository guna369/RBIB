Journal of Discrete Algorithms 18 (2013) 22–31
Contents lists available at SciVerse ScienceDirect
C
o
Ti
Ins
a
Ar
Av
Ke
Lo
Bu
W
Sh
Sh
1.
ex
on
ar
of
Fi
al
co
m
sp
th
st
th
w
ha
de
sh
st
of
*
Th
15
htJournal of Discrete Algorithms
www.elsevier.com/locate/jda
omputing the longest common prefix array based
n the Burrows–Wheeler transform
mo Beller ∗, Simon Gog, Enno Ohlebusch, Thomas Schnattinger
titute of Theoretical Computer Science, University of Ulm, 89069 Ulm, Germany
r t i c l e i n f o a b s t r a c t
ticle history:
ailable online 4 August 2012
ywords:
ngest common prefix array
rrows–Wheeler transform
avelet tree
ortest unique substrings
ortest absent words
Many sequence analysis tasks can be accomplished with a suffix array, and several of
them additionally need the longest common prefix array. In large scale applications, suffix
arrays are being replaced with full-text indexes that are based on the Burrows–Wheeler
transform. In this paper, we present the first algorithm that computes the longest common
prefix array directly on the wavelet tree of the Burrows–Wheeler transformed string. It
runs in linear time and a practical implementation requires approximately 2.2 bytes per
character.
© 2012 Elsevier B.V. All rights reserved.
Introduction
A suffix tree for a string S of length n is a compact trie storing all the suffixes of S (so it is a full-text index). It is an
tremely important data structure with applications in string matching, bioinformatics, and document retrieval, to mention
ly a few examples; see e.g. [14]. Suffix arrays can replace suffix trees and they use less memory than those. The suffix
ray SA of the string S is an array of integers in the range 1 to n specifying the lexicographic ordering of the n suffixes
the string S . To be precise, it satisfies SSA[1] < SSA[2] < · · · < SSA[n] , where Si denotes the i-th suffix S[i..n] of S; see
g. 1 for an example. In the last decade, much effort has gone into the development of efficient suffix array construction
gorithms (SACAs); see [38] for a survey.
However, the meteoric increase of DNA sequence information produced by next-generation sequencers demands new
mputer science approaches to data management because the data must be stored, analyzed, and mined. To analyze the
assive quantities of data, established data structures like the suffix array (and the suffix tree) are being replaced by less
ace consuming data structures like the wavelet tree of the Burrows–Wheeler transformed sequence. It goes as follows:
e sequence is subjected to the Burrows–Wheeler transform (BWT) [3], the Burrows–Wheeler transformed sequence is
ored in a wavelet tree (or, more generally, in an FM-index [9]), and the wavelet tree [13] supports backward search on
e original sequence. Let us recall the backward search technique in more detail. Let Σ be an ordered alphabet of size σ
hose smallest element is the so-called sentinel character $. In the following, S is a string (sequence) of length n over Σ
ving the sentinel character at the end (and nowhere else). The BWT transforms the string S into the string BWT[1..n]
fined by BWT[i] = S[SA[i] − 1] for all i with SA[i] = 1 and BWT[i] = $ otherwise; see Fig. 1. Ferragina and Manzini [9]
owed that it is possible to search a pattern backwards, character-by-character, in the suffix array SA of string S , without
oring SA. Let c ∈ Σ and ω be a substring of S . Given the ω-interval [i.. j] in the suffix array SA of S (i.e., ω is a prefix
SSA[k] for all i  k  j, but ω is not a prefix of any other suffix of S), backwardSearch(c, [i.. j]) returns the cω-interval
Corresponding author.
E-mail addresses: Timo.Beller@uni-ulm.de (T. Beller), Simon.Gog@uni-ulm.de (S. Gog), Enno.Ohlebusch@uni-ulm.de (E. Ohlebusch),
omas.Schnattinger@uni-ulm.de (T. Schnattinger).
70-8667/$ – see front matter © 2012 Elsevier B.V. All rights reserved.
tp://dx.doi.org/10.1016/j.jda.2012.07.007
T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31 23
[C
st
[2
M
ap
bo
se
th
m
m
th
dr
se
to
[3
on
20
ta
LC
de
go
it
H
Th
M
fo
an
2.
pr
1
ar
byFig. 1. Suffix array, LCP-array, and the Burrows–Wheeler-transformed string BWT of the string S = el_anele_lepanelen$.
[c] + Occ(c, i − 1) + 1 ..C[c] + Occ(c, j)], where C[c] is the overall number of occurrences of characters in S which are
rictly smaller than c, and Occ(c, i) is the number of occurrences of the character c in BWT[1..i].
The approach described above was used for example in the software-tools Bowtie [24], BWA [26], SOAP2 [27], and 2BWT
3] for short read alignment (mapping short DNA sequences to a reference genome); see [10] for an overview article.
ore recently, it was suggested to use it also in de novo sequence assembly [41]. In the field of genome comparisons, this
proach was first used by the software-tool bbwt [28], which uses k-mers (exact matches of fixed length k that appear in
th sequences) as a basis of the comparison. It should be stressed that all these software-tools rely on the BWT (backward
arch) but not on the LCP-array. However, there is at least one algorithm that uses both BWT and LCP [35], and we expect
at others will follow. The algorithm from [35] can be used in genome comparisons because it computes maximal exact
atches (exact matches that cannot be extended in either direction towards the beginning or end without allowing for a
ismatch) between two long strings (e.g. chromosomes).
In the last years, several algorithms have been proposed that construct the BWT either directly or by first constructing
e suffix array and then deriving the BWT in linear time from it; see [29,18,42,36,8,4]. The latter approach has a major
awback: all known SACAs require at least 5n bytes of main memory provided that n < 232 (note that some real world data
ts have a length that exceeds this bound; cf. Section 5). If one has to deal with large datasets, it is therefore advantageous
construct the BWT directly. For example, Okanohara and Sadakane [36] have shown that the SACA devised by Nong et al.
3] can be modified so that it directly constructs the BWT. Because it does not have to store the suffix array, it requires
ly O (n logσ log logσ n) bits to construct the BWT [36] (ca. 2.5n bytes in practice; Sadakane, personal communication,
11).
As described above, some sequence analysis tasks require the longest common prefix array (LCP-array): an array con-
ining the length of the longest common prefix between every pair of consecutive suffixes in SA; see Fig. 1. Formally, the
P-array is defined by LCP[1] = −1, LCP[n + 1] = −1, and LCP[i] = |lcp(SSA[i−1], SSA[i])| for 2  i  n, where lcp(u, v)
notes the longest common prefix between the strings u and v . There are several linear time LCP-array construction al-
rithms (LACAs); see Section 2. They all first construct the suffix array and then obtain the LCP-array in linear time from
. So these LACAs suffer from the same drawback as mentioned above: at least 5n bytes of main memory are required.
ere, we present the first LACA that acts directly on the Burrows–Wheeler transformed string and not on the suffix array.1
e algorithm has a worst-case time complexity of O (n logσ). Hence its run-time is linear for a constant size alphabet.
oreover, we provide a practical implementation that requires approximately 2.2n bytes (throughout the paper, log stands
r log2). Finally, we show that the approach can be used in other applications, namely finding shortest unique substrings
d shortest absent words. A preliminary version of this article appeared in [1].
Related work
In their seminal paper [30], Manber and Myers did not only introduce the suffix array but also the longest-common-
efix array. They showed that both the suffix array and the LCP-array can be constructed in O (n logn) time for a string of
The algorithms presented by Umar and Abdullah [44] do not really work on the BWT. They use the BWT to reconstruct data structures like the suffix
ray and then compute the LCP-array with the help of these data structures. Consequently, their algorithms are not space-efficient: they need at least 9n
tes.
24 T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31
Fig
are
le
th
th
an
di
9n
ov
co
th
a
th
po
co
by
O
th
re
re
ca
3.
da
or
su
st
co
nl
th
co
BW
an
i-
Σ
an
ra
in
bi
qu
th
M. 2. Conceptual illustration of the wavelet tree of the string BWT = nle_pl$nnlleee_eaae. Only the bit vectors are stored; the corresponding strings
shown for clarity. The shaded regions will be explained later.
ngth n. Kasai et al. [22] gave the first linear time algorithm for the computation of the LCP-array. Their algorithm uses
e string S , the suffix array, the inverse suffix array, and of course the LCP-array. Each of the arrays requires 4n bytes, thus
e algorithm needs 13n bytes in total (for an ASCII alphabet). The main advantage of their algorithm is that it is simple
d uses at most 2n character comparisons. But its poor locality behavior results in many cache misses, which is a severe
sadvantage on current computer architectures. Manzini [31] reduced the space occupancy of Kasai et al.’s algorithm to
bytes with a slow down of about 5%–10%. He also proposed an even more space-efficient (but slower) algorithm that
erwrites the suffix array. Recently, Kärkkäinen et al. [19] proposed another variant of Kasai et al.’s algorithm, which
mputes a permuted LCP-array (PLCP-array). In the PLCP-array, the lcp-values are in text order (position order) rather
an in suffix array order (lexicographic order). This algorithm is much faster than Kasai et al.’s algorithm because it has
much better locality behavior. However, in virtually all applications lcp-values are required to be in suffix array order, so
at in a final step the PLCP-array must be converted into the LCP-array. Although this final step suffers (again) from a
or locality behavior, the overall algorithm is still faster than Kasai et al.’s. Sirén [43] showed that the PLCP-array can be
mputed directly from a compressed suffix array. In a different approach, Puglisi and Turpin [39] tried to avoid cache misses
using the difference cover method of Kärkkäinen and Sanders [21]. The worst-case time complexity of their algorithm is
(nv) and the space requirement is n+O(n/√v+ v) bytes, where v is the size of the difference cover. Experiments showed
at the best run-time is achieved for v = 64, but their algorithm is still slower than Kasai et al.’s; see the experimental
sults in [19]. This is because it uses constant time range minimum queries, which take considerable time in practice. Just
cently, Gog and Ohlebusch [12] presented a very space efficient and fast LACA, which trades character comparisons for
che misses.
Wavelet tree
The wavelet tree introduced by Grossi et al. [13] supports one backward search step in O (logσ) time. To explain this
ta structure, we may view the ordered alphabet Σ as an array of size σ so that the characters appear in ascending
der in the array Σ[1..σ ], i.e., Σ[1] = $ < Σ[2] < · · · < Σ[σ ]. We say that an interval [l..r] is an alphabet interval if it is a
binterval of [1..σ ]. For an alphabet interval [l..r], the string BWT[l..r] is obtained from the Burrows–Wheeler transformed
ring BWT of S by deleting all characters in BWT that do not belong to the sub-alphabet Σ[l..r] of Σ[1..σ ]. As an example,
nsider the string BWT = nle_pl$nnlleee_eaae and the alphabet interval [1..4]. The string BWT[1..4] is obtained from
e_pl$nnlleee_eaae by deleting the characters l, n, and p. Thus, BWT[1..4] = e_$eee_eaae. The wavelet tree of
e string BWT over the alphabet Σ[1..σ ] is a balanced binary search tree defined as follows. Each node v of the tree
rresponds to a string BWT[l..r] , where [l..r] is an alphabet interval. The root of the tree corresponds to the string BWT =
T[1..σ ] . If l = r, then v has no children. Otherwise, v has two children: its left child corresponds to the string BWT[l..m]
d its right child corresponds to the string BWT[m+1..r] , where m =  l+r2 . In this case, v stores a bit vector B[l..r] whose
th entry is 0 if the i-th character in BWT[l..r] belongs to the sub-alphabet Σ[l..m] and 1 if it belongs to the sub-alphabet
[m + 1..r]. To put it differently, an entry in the bit vector is 0 if the corresponding character belongs to the left subtree
d 1 if it belongs to the right subtree; see Fig. 2. Moreover, each bit vector B in the tree is preprocessed so that the queries
nk0(B, i) and rank1(B, i) can be answered in constant time [17], where rankb(B, i) is the number of occurrences of bit b
B[1..i]. Obviously, the wavelet tree has height O (logσ). Because in an actual implementation it suffices to store only the
t vectors, the wavelet tree requires only n logσ bits of space plus o(n logσ) bits for the data structures that support rank
eries in constant time.
Instead of reviewing the implementation of one backward search step on a wavelet tree, we present a generalization
ereof: for an ω-interval [i.. j], the procedure getIntervals([i.. j]) presented in Algorithm 1 returns the list of all cω-intervals.
ore precisely, it starts with the ω-interval [i.. j] at the root and traverses the wavelet tree in a depth-first manner as
T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31 25
A
fo
ge
ge
fo
v
al
pr
in
ta
fo
is
th
vi
[1
4.
ex
lis
in
qu
th
1,
A
el
el
Th
Pr
c-
allgorithm 1 For an ω-interval [i.. j], the function call getIntervals([i.. j]) returns the list of all cω-intervals, and is defined as
llows.
tIntervals([i.. j])
list ← []
getIntervals′([i.. j], [1..σ ], list)
return list
tIntervals′([i.. j], [l..r], list)
if l = r then
c ← Σ[l]
add(list, [C[c] + i..C[c] + j])
else
(a0,b0) ← (rank0(B[l..r], i − 1), rank0(B[l..r], j))
(a1,b1) ← (i − 1− a0, j − b0)
m ←  l+r2 
if b0 > a0 then
getIntervals′([a0 + 1..b0], [l..m], list)
if b1 > a1 then
getIntervals′([a1 + 1..b1], [m + 1..r], list)
llows. At the current node v , it uses constant time rank queries to obtain the number b0 − a0 of zeros in the bit vector of
within the current interval. If b0 > a0, then there are characters in BWT[i.. j] that belong to the left subtree of v , and the
gorithm proceeds recursively with the left child of v . Furthermore, if the number of ones is positive (i.e. if b1 > a1), then it
oceeds with the right child in an analogous fashion. Clearly, if a leaf corresponding to character c is reached with current
terval [p..q], then [C[c]+ p ..C[c]+q] is the cω interval. In this way, Algorithm 1 computes the list of all cω-intervals. This
kes O (k logσ) time for a k-element list. Because the wavelet tree has less than 2σ nodes, O (σ ) is another upper bound
r the wavelet tree traversal. Consequently, Algorithm 1 has a worst-case time complexity of O (min{σ ,k logσ }), where k
the number of elements in the output list. As an illustration of Algorithm 1, we compute all intervals of the form ce in
e suffix array of Fig. 1 by invoking getIntervals([6..11]); note that [6..11] is the e-interval. In the wavelet tree of Fig. 2, the
sited nodes of the depth-first traversal are marked gray. The resulting list contains the three intervals [1..1], [13..15], and
7..18]. Algorithm 1 was developed by the fourth author [40] but others apparently had the same idea [6]; cf. also [11].
A LACA based on the BWT
Pseudo-code of our new LACA is given in Algorithm 2; it relies on Algorithm 1. We will illustrate the algorithm by an
ample. The first interval which is pulled from the queue is the ε-interval 〈[1..19],0〉, and getIntervals([1..19]) returns a
t of the seven ω-intervals [1..1], [2..3], [4..5], [6..11], [12..15], [16..18], and [19..19], where |ω| = 1 for each ω. For every
terval [ik.. jk], LCP[ jk + 1] is set to 0, except for the last one because LCP[20] = −1. These six intervals are pushed on the
eue, with the -value 1. Next, 〈[1..1],1〉 is pulled from the queue. The list returned by getIntervals([1..1]) just contains
e n$-interval [16..16]. Since LCP[17] has not been computed yet, 〈[16..16],2〉 is pushed on the queue, LCP[17] is set to
and so on.
lgorithm 2 Computation of the LCP-array in O (n logσ) time.
initialize the array LCP[1..n + 1] /* i.e., LCP[i] = ⊥ for all 1 i n + 1 */
LCP[1] ← −1; LCP[n + 1] ← −1
initialize an empty queue
enqueue(〈[1..n],0〉)
while queue is not empty do
〈[i.. j], 〉 ← dequeue()
list ← getIntervals([i.. j])
for each [lb..rb] in list do
if LCP[rb + 1] = ⊥ then
enqueue(〈[lb..rb],  + 1〉)
LCP[rb + 1] ← 
It is not difficult to see that Algorithm 2 maintains the following invariant: The set of the second components of all
ements of the queue has either one element  or two elements  and  + 1, where 0   < n. In the latter case, the
ements with second component  precede those with second component  + 1.
eorem 4.1. Algorithm 2 correctly computes the LCP-array.
oof. We proceed by induction on . In the base case, we have  = 0. For every character c = Σ[k] occurring in S , the
interval [lb..rb] is in the list returned by getIntervals([1..n]), where lb = C[c] + 1 and rb = C[d] with d = Σ[k + 1]. The
gorithm sets LCP[rb + 1] = 0 unless rb = n. This is certainly correct because the suffix SSA[rb] starts with the character c
26 T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31
an
th
m
se
|ω
in
On
th
w
ω
in
Be
LC
hy
th
p
im
po
LC
Th
Pr
IS
ge
th
in
th
LC
H
th
po
th
co
pr
la
m
no
th
an
fo
tw
to
(H
|ω
w
an
be
dod the suffix SSA[rb+1] starts with the character d. Clearly, because for every character c occurring in S the c-interval is in
e list returned by getIntervals([1..n]), all entries of LCP with value 0 are set. Let  > 0. By the inductive hypothesis, we
ay assume that Algorithm 2 has correctly computed all lcp-values < . After the last LCP-entry with value − 1 has been
t, the queue solely contains elements of the form 〈[i.. j], 〉, where [i.. j] is the ω-interval of some substring ω of S with
| = . Let the cω-interval [lb..rb] be in the list returned by getIntervals([i.. j]). If LCP[rb + 1] = ⊥, then we know from the
duction hypothesis that LCP[rb + 1] , i.e., the suffixes SSA[rb] and SSA[rb+1] have a common prefix of length at least .
the other hand, cω is a prefix of SSA[rb] but not of SSA[rb+1] . Consequently, LCP[rb+ 1] < + 1. Altogether, we conclude
at Algorithm 2 assigns the correct value  to LCP[rb + 1].
We still have to prove that all entries of the LCP-array with value  are really set. So let k, 0  k < n, be an index
ith LCP[k + 1] = . Since  > 0, the longest common prefix of SSA[k] and SSA[k+1] can be written as cω, where c ∈ Σ ,
∈ Σ∗ , and |ω| =  − 1. Consequently, ω is the longest common prefix of SSA[k]+1 and SSA[k+1]+1. Let [i.. j] be the ω-
terval, p be the index with SA[p] = SA[k] + 1, and q be the index with SA[q] = SA[k + 1] + 1. Clearly, i  p < q  j.
cause ω is the longest common prefix of SSA[p] and SSA[q] , there must be at least one index t with p < t  q so that
P[t] = |ω| = −1. If there is more than one index with that property, let t denote the smallest. According to the inductive
pothesis, Algorithm 2 assigns the value  − 1 to LCP[t]. Just before that, a pair 〈[s..t − 1], 〉 must have been pushed to
e queue, where [s..t − 1] is some ω′-interval with |ω′| = . By the definition of t , we have LCP[r] >  − 1 for all r with
< r  t − 1. Thus, p lies within the interval [s..t − 1]. In other words, ω′ is a prefix of SSA[p] . Moreover, BWT[p] = c
plies that the cω′-interval, say [lb..rb], is not empty. Since BWT[r] = c for all p < r < q, it follows that rb = k. At some
int, 〈[s..t − 1], 〉 is removed from the queue, and [lb..k] is in the list returned by getIntervals([s..t − 1]). Consequently,
P[k + 1] will be set to . 
eorem 4.2. Algorithm 2 has a worst-case time complexity of O (n logσ).
oof. In the proof, we use the ψ-function which is defined by ψ(i) = ISA[SA[i] + 1] for all i with 2  i  n and ψ(1) =
A[1], where ISA denotes the inverse of the permutation SA. Note that ψ is also a permutation of the suffix array.
We show that the algorithm creates at most 2n intervals. To this end, we first determine the maximum number of
nerated intervals 〈[lb..rb], 〉 for a fixed value rb. If  < LCP[rb + 1], then there exists no such interval because otherwise
e value LCP[rb + 1] would be set to . Moreover, for any  > LCP[ψ[rb] + 1] + 1 the algorithm does not create an
terval which ends at position rb. This can be seen as follows. There is only one interval 〈[x..y],  − 1〉 which can generate
e interval of length  ending at rb. Clearly, this interval must contain ψ[rb]. So x  ψ[rb]  y. It follows from  − 1 >
P[ψ[rb] + 1] that y = ψ[rb]. If this interval would be added to the queue, then LCP[ψ[rb] + 1] would be set to  − 1.
owever, this is impossible.
To sum up, all created intervals 〈[lb..rb], 〉 for a fixed value of rb satisfy LCP[rb+1]  LCP[ψ[rb]+1]+1. Therefore,
e algorithm generates at most LCP[ψ[rb] + 1] − LCP[rb + 1] + 2 intervals ending at rb. (Note that this number is always
sitive because LCP[ψ[rb] + 1] LCP[rb + 1] − 1.) Summing over all possible end positions rb gives an upper bound on
e number of created intervals:
n∑
rb=1
(
LCP
[
ψ[rb] + 1]− LCP[rb + 1] + 2)= 2n +
n∑
rb=1
LCP
[
ψ[rb] + 1]−
n∑
rb=1
LCP[rb + 1] = 2n
The function getIntervals takes O (logσ) time for each interval, whereas the remaining statements of Algorithm 2 take
nstant time per interval. Thus, the run-time of Algorithm 2 is in O (n logσ). 
Algorithm 2 uses only the wavelet tree of the BWT of S , a queue to store the ω-intervals and the LCP-array. Because a
actical implementation should use as little space as possible, we next show how to reduce the space consumption of the
tter two. The second components (-values) of the queue entries need not be stored because one can simply count how
any ω-intervals with |ω| =  enter the queue; see Algorithm 3 for details. For a fixed , the ω-intervals with |ω| =  do
t overlap. Thus, they can be stored in two bit vectors, say B and E of size n, and an ω-interval [i.. j] is stored by setting
e bits B[i] and E[ j] (due to singleton intervals, we actually need two bit vectors). By the invariant mentioned above, at
y point in time Algorithm 2 has to deal with at most two different -values. Therefore, we can replace the queue with
ur bit vectors of length n. The price to be paid for this is an increase in the worst-case time complexity. For each -value,
o bit vectors of length n are scanned to determine all ω-intervals with |ω| = . So the number of scans is proportional
the maximum lcp-value. Since this can be n − 2 (consider the string S = an−1$), the time complexity rises to O (n2).
owever, in practice the maximum lcp-value is only a fraction of n [25], and on average it is O (logn) [7].)
For this reason, we prefer the following hybrid approach. For a fixed , if there are more than n2 logn ω-intervals with
| =  (note that this can happen only O (logn) times), we use the bit vectors; otherwise we use the queue. More precisely,
e start with the queue, switch to the bit vectors when there are more than n2 logn ω-intervals with the current -value,
d switch back if there are less than n2 logn ω-intervals with the current -value. Note that the queue uses at most n bits
cause each queue entry is an interval that can be represented by two numbers using logn bits each. This hybrid approach
es not increase the worst-case time complexity as we show next. There are nlogn blocks of length logn in a bit vector of
T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31 27
le
w
sa
2n
Th
m
is
ca
su
go
m
or
lc
is
qu
oc
w
A
of
th
an
is
an
th
Fu
Be
in
ar
th
ne
ne
ve
in
tr
st
ti
ve
vi
id
W
th
5.
U
da
fo
w
w
go
(h
2
3ngth n. Since the two bit vectors have to be scanned at most logn times, we must handle at most 2n blocks. In each block
e determine the leftmost 1-bit. If we find such a 1-bit, we unset this bit and search again for the leftmost 1-bit in the
me block. Because every interval creates only two 1-bits, this happens at most 4n times (remember that there are at most
intervals). Altogether, we get all stored intervals by searching at most 6n times the leftmost 1-bit in a block of logn bits.
is search can be done in constant time under the word-RAM model with word size Ω(logn), thus the intervals can be
anaged in bit vectors with O (n) time complexity. Our experiments showed that this hybrid implementation of Algorithm 2
actually faster than a implementation which solely uses the queue (from the STL of C++). This can be attributed to fewer
che misses due to the fact that the bit vectors are accessed sequentially.
Up to now, our LACA needs n logσ bits for the wavelet tree of the BWT plus o(n logσ) bits for the data structures that
pport rank queries in constant time, 4n bits for the storage of the ω-intervals, and 4n bytes for the LCP-array itself. Our
al is to stay below 2.5n bytes because this is (currently) the space that is needed to build the BWT; cf. Section 1. To
eet this goal, we stream the LCP-array to disk. This is possible because Algorithm 2 calculates lcp-values in ascending
der. Clearly, the LCP-array requires only k · n bits to store all lcp-values less than 2k . During the computation of these
p-values, the i-th bit of a bit vector D of length n is set when a value is assigned to LCP[i]. Afterwards the LCP-array
written to disk, but the bit vector D tells us which LCP-entries are already done and we preprocess D so that rank
eries can be answered in constant time. Let m be the number of zeros in D . We use a new array A of length m that also
cupies k · n bits. In other words, each array element of A consists of b =  k·nm  bits, which are initially set to zero. Then,
e compute all lcp-values less than 2k +2b −1. When a value  is to be assigned to LCP[i], we store the value −2k +1 in
[rank0(D, i)]. After all lcp-values less than 2k + 2b − 1 have been computed, we further proceed as follows. During a scan
the bit vector D , we count the number of zeros seen so far. So when an index i with D[i] = 0 is encountered, we know
at this is, say, the j-th zero seen so far. Now we use a case analysis. If A[ j] = 0, then LCP[i] has not been computed yet
d there is nothing to do. Otherwise, the value 2k − 1 + A[ j] is written at index i to the LCP-array on the disk, and D[i]
set to one. When the scan of D is completed, the (updated) bit vector D is preprocessed so that rank queries can be
swered in constant time. This process is iterated (calculate the new values of m and b, initialize a new array A, etc.) until
e LCP-array is completely filled.
For example, for DNA sequences the wavelet tree takes 2n bits and the rank data structures take less than 2n bits.
rthermore, the storage of the ω-intervals takes 4n bits and the bit vector D plus its rank data structure take 1.25n bits.
cause we want to use at most 20n bits, k = 10 is the right choice for DNA sequences.
We can save even more space by splitting our algorithm in two phases. In the first phase, we do not keep the LCP-array
main memory. Instead, at each step the current lcp-value and the indices at which this value appears in the LCP-array
e written sequentially into a file. This phase needs only the wavelet tree, 4n bits for storing the intervals and n bits for
e D vector. In the second phase, we read lcp-values together with their indices from file and fill the LCP-array. This phase
eds the same memory as phase 1 provided that the parameter k is chosen appropriately.
In several applications, the access to the LCP-array is sequential, so it can be streamed from disk. If random access is
eded, one can get a compressed representation of the LCP-array by streaming it from disk, and then this compressed
rsion is kept in main memory. There are several compressed versions of the LCP-array which use about 1 byte per entry
practice, while the access time remains essentially the same as for the uncompressed version; see e.g. [2]. Moreover, the
ee topology of a compressed suffix tree can be constructed by streaming the LCP-array from disk and using a succinct
ack which uses only 2n bits of space; see [34].
A reviewer of the preliminary version of this article [1] came up with the following idea to improve the practical run-
me for finding the next 1-bit in the bit vector: “Instead of one bit vector (let us call it B0) one can keep a sequence of bit
ctors B0, B1, . . . The invariant of such a data structure is: Bi+1[ j] = 0 ⇔ Bi[8 j] = Bi[8 j+1] = · · · = Bi[8 j+7] = 0. One can
ew it as a bit-encoded balanced 8-ary tree. Using an additional array of constant size (256 bytes), the first bit set can be
entified in log8 n operations, and when reset to 0, the data-structure can be updated in at most log8 n another operations.”
e implemented this method with a 64-ary tree, but experiments showed that it is not faster than ours (probably because
e bit vector is not sparse enough).
Experimental results
All programs were compiled using the gcc version 4.4.3 with the options -O9 -msse4.2 -DNDEBUG on a 64 bit
buntu (Kernel 2.6.32) system equipped with a six-core AMD Opteron processor 2431 with 2.4 GHz and 32 GB of RAM. The
ta originates from the Pizza&Chili Corpus (http://pizzachili.dcc.uchile.cl/). For space reasons, we report only the results
r files of 200 MByte. Additionally, the genome of the house mouse (NCBIm36, http://www.ensembl.org/Mus_musculus)
as used. We compared our new algorithm and the space efficient version of it (called new algorithm 2 in Table 1)
ith the KLAAP-algorithm of Kasai et al. [22], the Φ and Φ64-semi2 algorithms of Kärkkäinen et al. [19], and the
-Φ algorithm of Gog and Ohlebusch [12]. The suffix array construction was done by Mori’s libdivsufsort-algorithm3
ttp://code.google.com/p/libdivsufsort/), while the direct BWT construction is due to Okanohara and Sadakane [36]. The
The semi-external version of Φ64 is one of the fastest algorithms of the Φ family.
Because the 32 bit version is limited to files of size  231, we had to use the 64 bit version for the mouse genome (which needs 9n bytes).
28 T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31
Ta
Ex
pe
co
ar
m
Of
slo
KL
im
us
it
ou
2.
co
tig
6.
th
6.1
pr
De
sh
th
th
it
it
co
be
st
S[
va
be
as
no
th
m
inble 1
perimental results: for each file, the first column shows the real run-time in seconds and the second column shows the maximum memory usage
r character. As an example, consider the dna 200 MB file. The construction of its suffix array takes 65 s and 5n bytes (1000 MB), whereas the direct
nstruction of its BWT takes 85 s and 1.9n bytes (380 MB). Rows 3–8 refer to the construction of the LCP-array under the assumption that the suffix
ray (the BWT, respectively) has already been built. In rows 9–14, the first column shows the overall run-time and the second shows the overall maximum
emory usage per character. For a fair comparison of the run-time, the data was chosen in such a way that all data structures fit in the main memory.
course, for very large files the space usage of 9n bytes is disadvantageous because then the data structures must reside in secondary memory, and this
ws down the algorithms.
dna 200 MB english 200 MB proteins 200 MB sources 200 MB xml 200 MB mouse 3242 MB
SA constr. 65 5.0 64 5.0 71 5.0 43 5.0 47 5.0 1689 8.9
BWT constr. 85 1.9 106 2.2 139 2.6 82 2.1 82 2.1 1480 2.0
KLAAP 48 9.0 46 9.0 45 9.0 28 9.0 29 9.0 1501 9.0
Φ 34 9.0 29 9.0 28 9.0 21 9.0 21 9.0 1117 9.0
Φ64-semi 74 1.0 67 1.0 67 1.0 53 1.0 68 1.0 1509 1.0
go-Φ 48 2.0 69 2.0 66 2.0 48 2.0 45 2.0 1297 2.2
new algorithm 45 1.9 105 2.2 114 2.3 94 2.2 66 2.2 814 2.0
new algorithm 2 53 1.0 110 1.3 116 1.3 100 1.5 72 1.4 962 1.0
KLAAP 114 9.0 110 9.0 116 9.0 71 9.0 77 9.0 3191 9.0
Φ 99 9.0 93 9.0 99 9.0 64 9.0 68 9.0 2806 9.0
Φ64-semi 140 5.0 131 5.0 138 5.0 97 5.0 115 5.0 3198 8.9
go-Φ 114 5.0 133 5.0 137 5.0 91 5.0 93 5.0 2986 8.9
new algorithm 131 1.9 211 2.2 253 2.6 176 2.2 149 2.2 2294 2.0
new algorithm 2 139 1.9 216 2.2 255 2.6 183 2.1 154 2.1 2442 2.0
AAP-algorithm is our own implementation, all other programs were kindly provided by the authors. Looking at the exper-
ental results in Table 1, one can see that the Φ algorithm is the fastest LACA. However, in large scale applications its space
age of 9n bytes is the limiting factor. The memory usage of algorithm go-Φ (row 6) is similar to that of our algorithm but
relies on the suffix array, so its overall space usage (row 12) is due to the suffix array construction (row 1). By contrast,
r new algorithm solely depends on the BWT, so that its overall maximum memory usage per character is approximately
2n bytes (row 13). It can be attributed to the usual space–time trade-off that our new algorithm is the slowest LACA in the
ntest for the 200 MB files (except for small alphabets). However, this changes in large scale applications when memory is
ht.
Other applications
In this section, we will sketch two other applications of our new approach. Because $ is solely used to mark the end of
e string S , we have to exclude it in the considerations below.
. Shortest unique substrings
As a first application, we will briefly describe how to find shortest unique substrings. This is relevant in the design of
imers for DNA sequences; see [14].
finition 6.1. A substring S[i.. j] is unique if it occurs exactly once in S . The shortest unique substring problem is to find all
ortest unique substrings of S .
Clearly, every suffix of S is unique because S is terminated by the special symbol $. Since we are not interested in
ese, we will exclude them. One can show that the ( + 1)-length prefix of SSA[i] , where  = max{LCP[i],LCP[i + 1]}, is
e shortest unique substring of S that starts at position SA[i]. Using this observation, we can modify Algorithm 2 so that
computes a shortest unique substring of S in O (n logσ) time. The resulting Algorithm 3 can easily be changed so that
computes all shortest unique substrings or even all unique substrings of S . We make use of the fact that Algorithm 2
mputes lcp-values in ascending order. So when Algorithm 2 executes the statement LCP[rb + 1] ←  and LCP[rb] has
en set before, then max{LCP[rb],LCP[rb + 1]} =  and S[SA[rb]..SA[rb] + ] is the shortest unique substring of S that
arts at position SA[rb]. Analogously, if LCP[rb + 2] has been set before, then max{LCP[rb + 1],LCP[rb + 2]} =  and
SA[rb + 1]..SA[rb + 1] + ] is the shortest unique substring of S that starts at position SA[rb + 1]. Because the current
lue of  is always available, all we have to know is whether or not LCP[rb] (LCP[rb+2], respectively) has been computed
fore. Consequently, we can replace the LCP-array with the bit vector D of length n, and D[i] is set to one instead of
signing a value to LCP[i]. However, there are two subtleties that need to be taken into account. First, the suffix array is
t at hand, so we have to find an alternative way to output the string S[SA[rb]..SA[rb] + ]. Second, we have to exclude
is string if it is a suffix. Fortunately, the wavelet tree provides the needed functionality, as we shall see next. The LF -
apping is defined by LF (i) = ISA[SA[i] − 1] for all i with SA[i] = 0 and LF (i) = 0 otherwise (where ISA denotes the
verse of the permutation SA). Its long name last-to-first column mapping stems from the fact that it maps the last column
T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31 29
L
M
th
ψ
ti
ex
et
O
be
su
th
rb
A
6.
sh
ge
al
an
D
w
n
de
ar
th
em
D
th
lis
up
po
th
sh
4= BWT to the first column F , where F contains the first character of the suffixes in the suffix array, i.e., F [i] = S[SA[i]].
ore precisely, if BWT[i] = c is the k-th occurrence of character c in BWT, then j = LF (i) is the index such that F [ j] is
e k-th occurrence of c in F . We recall that the ψ-function, defined by ψ(i) = ISA[SA[i] + 1] for all i with 2 i  n and
(1) = ISA[1], is the inverse permutation of LF . With the wavelet tree, both LF (i) and ψ(i) can be computed in O (logσ)
me; see [32]. Moreover, the character F [i] can be determined in O (logσ) time by a binary search on C ,4 or, using n+o(n)
tra bits, by a constant time rank query. Since S[SA[rb]] = F [rb], S[SA[rb] + 1] = F [ψ(rb)], S[SA[rb] + 2] = F [ψ(ψ(rb))]
c., it follows that the string S[SA[rb]..SA[rb] + ] coincides with F [rb] F [ψ(rb)] . . . F [ψ(rb)] (which can be computed in
( logσ) time). This solves our first little problem. The second problem was to exclude suffixes from the output. This can
done by keeping track of the suffix of length  + 1, where  is the current length. To be precise, initially  = 0 and the
ffix of length 1 is the character $, which appears at index idx = 1. Every time  is incremented, we obtain the index of
e suffix of length  + 1 by the assignment idx ← LF (idx). Consequently, a unique substring at index rb is output only if
= idx.
lgorithm 3 Computation of a shortest unique substring.
initialize a bit vector D[1..n + 1] /* i.e., D[i] = 0 for all 1 i n + 1 */
D[1] ← 1; D[n + 1] ← 1
initialize an empty queue
enqueue([1..n])
 ← 0; size ← 1; idx ← 1
while queue is not empty do
if size = 0 then
 ←  + 1
size ← current size of the queue
idx ← LF (idx)
[i.. j] ← dequeue()
size ← size− 1
list ← getIntervals([i.. j])
for each [lb..rb] in list do
if D[rb + 1] = 0 then
enqueue([lb..rb])
D[rb + 1] ← 1
if D[rb] = 1 and rb = idx then
return F [rb] F [ψ(rb)] . . . F [ψ(rb)] /* string S[SA[rb]..SA[rb] + ] */
if D[rb + 2] = 1 and rb + 1 = idx then
return F [rb + 1] F [ψ(rb + 1)] . . . F [ψ(rb + 1)]
2. Shortest absent words
As a second application, we show that our approach allows us to compute shortest absent words. This is relevant because
ort DNA sequences that do not occur in a genome are interesting to biologists. For example, the fact that the human
nome does not contain all possible DNA sequences of length 11 may be due to negative selection. For this reason, several
gorithms have been developed that compute shortest absent words; see [5,15,16,37,45]. Our approach allows us to give
alternative algorithm for this task.
efinition 6.2. Given string S , a string ω ∈ (Σ \ {$})+ is called an absent word if it is not a substring of S . The shortest absent
ords problem is to find all shortest absent words.
In the following, let σ be the size of Σ \{$}. If σ = 1, say Σ \{$} = {a}, then S consists solely of a’s. That is, S = an , where
is the length of S . In this case, the shortest absent word is obviously an+1. Therefore, we may assume that σ > 1. We
fine a tree structure as follows. The root of the interval-tree is the ε-interval [1..n], and the children of an ω-interval [i.. j]
e the cω-intervals returned by getIntervals([i.. j]), where c = $. Clearly, if the ω-interval [i.. j] has less than σ children,
en every non-occurring child corresponds to an absent word. In other words, if for some c ∈ Σ \ {$}, the cω-interval is
pty, then cω is not a substring of S . Our algorithm can be viewed as a depth-first traversal through this (virtual) tree.
uring the traversal, the shortest absent words that have been detected so far are stored in a list, while a variable len stores
eir length. For each absent word ω with |ω|  len, we proceed by case distinction. If |ω| = len, then ω is added to the
t. Otherwise, if |ω| < len, then len is set to |ω| and the list is set to [ω]. We next show that the initial value of len can be
per bounded without sacrificing correctness. Since there are at most n− k substrings of length k in S , while there are σ k
ssible strings of length k, it follows that there must be an absent word of length k provided that σ k > n−k. In particular,
ere must be an absent word of length logσ n. Therefore, if we initially set len to logσ n, the algorithm will find all
ortest absent words. For how many substrings ω of S must the procedure getIntervals be applied? Clearly, it need not be
As pointed out by an anonymous reviewer, F [i] can also be obtained by a range quantile query on the wavelet tree; see [11].
30 T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31
ap
fro
th
w
Br
st
Br
7.
Bu
m
Ac
tr
Re
[
[
[
[
[
[
[Fig. 3. For these 21 strings there is a procedure call to getIntervals if the algorithm gets the string aacagatccgctggtta$ as input.
plied if ω has length |ω|  logσ n. Thus, it must be applied at most
∑logσ n
i=0 σ
i = σ logσ n+1−1σ−1 times. Now it follows
m σ
logσ n+1−1
σ−1 <
σ logσ n+1
σ−1 = nσσ−1  2n that the procedure getIntervals is applied less than 2n times. Since every call to
is procedure takes O (σ ) time, the overall worst-case time complexity of the algorithm is O (nσ). As a matter of fact, the
orst-case occurs when the algorithm gets a de Bruijn sequence as input; see Fig. 3. In combinatorial mathematics, a de
uijn sequence (named after the Dutch mathematician Nicolaas Govert de Bruijn) of order m on the alphabet Σ ′ is a cyclic
ring containing every length m string over Σ ′ exactly once as a substring. For example, the string aacagatccgctggtt is a de
uijn sequence of order m = 2 on the alphabet Σ ′ = {a, c, g, t}.
Conclusions
We presented the first algorithm that computes the longest common prefix array directly on the wavelet tree of the
rrows–Wheeler transformed string (and not on the suffix array). Compared with [1], this article contains several improve-
ents:
• We were able to show that the algorithm runs in O (n logσ) time for a string of length n and an alphabet of size σ .
• Due to a reimplementation of the getInterval function, the algorithm is now faster in practice. In addition to that, it now
uses some SSE4.2 instructions, so that e.g. the calculation of the leftmost 1-bit in a 64 bit word can be done with an
assembler instruction. We believe that our algorithm will profit from further improvements on wavelet trees like the
one described in [20].
• We presented an alternative version of our algorithm which needs less space (about 1.5n byte) and is only slightly
slower.
• It was shown that the new approach can be used in two other applications, namely finding shortest unique substrings
and shortest absent words.
Our algorithm is available at http://www.uni-ulm.de/in/theo/research/seqana.html.
knowledgements
We thank the anonymous reviewers for their helpful comments. Special thanks go to Gonzalo Navarro, whose questions
iggered the improved worst-case complexity analysis of Algorithm 2.
ferences
1] T. Beller, S. Gog, E. Ohlebusch, T. Schnattinger, Computing the longest common prefix array based on the Burrows–Wheeler transform, in: Proc. 18th
International Symposium on String Processing and Information Retrieval, in: Lecture Notes in Computer Science, vol. 7024, Springer-Verlag, Berlin,
2011, pp. 197–208.
2] N.R. Brisaboa, S. Ladra, G. Navarro, Directly addressable variable-length codes, in: Proc. 16th International Symposium on String Processing and Infor-
mation Retrieval, in: Lecture Notes in Computer Science, vol. 5721, Springer-Verlag, Berlin, 2009, pp. 122–130.
3] M. Burrows, D.J. Wheeler, A block-sorting lossless data compression algorithm, Research Report 124, Digital Systems Research Center, 1994.
4] A. Cox, M. Bauer, G. Rosone, Lightweight BWT construction for very large string collections, in: Proc. 22nd Annual Symposium on Combinatorial Pattern
Matching, in: Lecture Notes in Computer Science, vol. 6661, Springer-Verlag, Berlin, 2011, pp. 219–231.
5] M. Crochemore, F. Mignosi, A. Restivo, Automata and forbidden words, Information Processing Letters 67 (3) (1998) 111–117.
6] J.S. Culpepper, G. Navarro, S.J. Puglisi, A. Turpin, Top-k ranked document search in general text databases, in: Proc. 18th Annual European Symposium
on Algorithms, in: Lecture Notes in Computer Science, vol. 6347, Springer-Verlag, Berlin, 2010, pp. 194–205.
7] L. Devroye, W. Szpankowski, B. Rais, A note on the height of suffix trees, SIAM Journal on Computing 21 (1) (1992) 48–53.
T. Beller et al. / Journal of Discrete Algorithms 18 (2013) 22–31 31
[8] P. Ferragina, T. Gagie, G. Manzini, Lightweight data indexing and compression in external memory, in: Proc. 9th Latin American Theoretical Informatics
Symposium, in: Lecture Notes in Computer Science, vol. 6034, Springer-Verlag, Berlin, 2010, pp. 697–710.
[9] P. Ferragina, G. Manzini, Opportunistic data structures with applications, in: Proc. IEEE Symposium on Foundations of Computer Science, 2000, pp. 390–
398.
[10] P. Flick, E. Birney, Sense from sequence reads: Methods for alignment and assembly, Nature Methods 6 (11 Suppl.) (2009) S6–S12.
[11] T. Gagie, S.J. Puglisi, A. Turpin, Range quantile queries: Another virtue of wavelet trees, in: Proc. 16th International Symposium on String Processing
and Information Retrieval, in: Lecture Notes in Computer Science, vol. 5721, Springer-Verlag, Berlin, 2009, pp. 1–6.
[12] S. Gog, E. Ohlebusch, Lightweight LCP-array construction in linear time, arXiv:1012.4263, 2011.
[13] R. Grossi, A. Gupta, J.S. Vitter, High-order entropy-compressed text indexes, in: Proc. 14th Annual ACM–SIAM Symposium on Discrete Algorithms, 2003,
pp. 841–850.
[14] D. Gusfield, Algorithms on Strings, Trees, and Sequences, Cambridge University Press, New York, 1997.
[1
[1
[1
[1
[1
[2
[2
[2
[2
[2
[2
[2
[2
[2
[2
[3
[3
[3
[3
[3
[3
[3
[3
[3
[3
[4
[4
[4
[4
[4
[45] G. Hampikian, T. Andersen, Absent sequences: Nullomers and primes, in: Proc. 12th Pacific Symposium on Biocomputing, 2007, pp. 355–366.
6] J. Herold, S. Kurtz, R. Giegerich, Efficient computation of absent words in genomic sequences, BMC Bioinformatics 9 (2008) 167.
7] G. Jacobson, Space-efficient static trees and graphs, in: Proc. 30th Annual Symposium on Foundations of Computer Science, IEEE, 1989, pp. 549–554.
8] J. Kärkkäinen, Fast BWT in small space by blockwise suffix sorting, Theoretical Computer Science 387 (3) (2007) 249–257.
9] J. Kärkkäinen, G. Manzini, S.J. Puglisi, Permuted longest-common-prefix array, in: Proc. 20th Annual Symposium on Combinatorial Pattern Matching,
in: Lecture Notes in Computer Science, vol. 5577, Springer-Verlag, Berlin, 2009, pp. 181–192.
0] J. Kärkkäinen, S.J. Puglisi, Fixed block compression boosting in FM-indexes, in: Proc. 18th International Symposium on String Processing and Information
Retrieval, in: Lecture Notes in Computer Science, vol. 7024, Springer-Verlag, Berlin, 2011, pp. 174–184.
1] J. Kärkkäinen, P. Sanders, Simple linear work suffix array construction, in: Proc. 30th International Colloquium on Automata, Languages and Program-
ming, in: Lecture Notes in Computer Science, vol. 2719, Springer-Verlag, Berlin, 2003, pp. 943–955.
2] T. Kasai, G. Lee, H. Arimura, S. Arikawa, K. Park, Linear-time longest-common-prefix computation in suffix arrays and its applications, in: Proc. 12th
Annual Symposium on Combinatorial Pattern Matching, in: Lecture Notes in Computer Science, vol. 2089, Springer-Verlag, Berlin, 2001, pp. 181–192.
3] T.-W. Lam, R. Li, A. Tam, S. Wong, E. Wu, S.-M. Yiu, High throughput short read alignment via bi-directional BWT, in: Proc. International Conference on
Bioinformatics and Biomedicine, IEEE Computer Society, 2009, pp. 31–36.
4] B. Langmead, C. Trapnell, M. Pop, S.L. Salzberg, Ultrafast and memory-efficient alignment of short DNA sequences to the human genome, Genome
Biology 10 (3) (2009), Article R25.
5] M. Léonard, L. Mouchard, M. Salson, On the number of elements to reorder when updating a suffix array, Journal of Discrete Algorithms 11 (2012)
87–99.
6] H. Li, R. Durbin, Fast and accurate short read alignment with Burrows–Wheeler transform, Bioinformatics 25 (14) (2009) 1754–1760.
7] R. Li, C. Yu, Y. Li, T.-W. Lam, S.-M. Yiu, K. Kristiansen, J. Wang, Soap2: an improved ultrafast tool for short read alignment, Bioinformatics 25 (15) (2009)
1966–1977.
8] R.A. Lippert, Space-efficient whole genome comparisons with Burrows–Wheeler transforms, Journal of Computational Biology 12 (4) (2005) 407–415.
9] R.A. Lippert, C.M. Mobarry, B. Walenz, A space-efficient construction of the Burrows–Wheeler transforms for genomic data, Journal of Computational
Biology 12 (7) (2005) 943–951.
0] U. Manber, E.W. Myers, Suffix arrays: A new method for on-line string searches, SIAM Journal on Computing 22 (5) (1993) 935–948.
1] G. Manzini, Two space saving tricks for linear time LCP array computation, in: Proc. 9th Scandinavian Workshop on Algorithm Theory, in: Lecture
Notes in Computer Science, vol. 3111, Springer-Verlag, Berlin, 2004, pp. 372–383.
2] G. Navarro, V. Mäkinen, Compressed full-text indexes, ACM Computing Surveys 39 (1) (2007), Article 2.
3] G. Nong, S. Zhang, W.H. Chan, Linear suffix array construction by almost pure induced-sorting, in: Proc. Data Compression Conference, IEEE Computer
Society, 2009, pp. 193–202.
4] E. Ohlebusch, J. Fischer, S. Gog, CST++, in: Proc. 17th International Symposium on String Processing and Information Retrieval, in: Lecture Notes in
Computer Science, vol. 6393, Springer-Verlag, Berlin, 2010, pp. 322–333.
5] E. Ohlebusch, S. Gog, A. Kügel, Computing matching statistics and maximal exact matches on compressed full-text indexes, in: Proc. 17th International
Symposium on String Processing and Information Retrieval, in: Lecture Notes in Computer Science, vol. 6393, Springer-Verlag, Berlin, 2010, pp. 347–
358.
6] D. Okanohara, K. Sadakane, A linear-time Burrows–Wheeler transform using induced sorting, in: Proc. 16th International Symposium on String Pro-
cessing and Information Retrieval, in: Lecture Notes in Computer Science, vol. 5721, Springer-Verlag, Berlin, 2009, pp. 90–101.
7] J. Pinho, P. Ferreira, S.P. Garcia, J. Rodrigues, On finding minimal absent words, BMC Bioinformatics 10 (2009) 137.
8] S.J. Puglisi, W.F. Smyth, A. Turpin, A taxonomy of suffix array construction algorithms, ACM Computing Surveys 39 (2) (2007) 1–31.
9] S.J. Puglisi, A. Turpin, Space–time tradeoffs for longest-common-prefix array computation, in: Proc. 19th International Symposium on Algorithms and
Computation, in: Lecture Notes in Computer Science, vol. 5369, Springer-Verlag, Berlin, 2008, pp. 124–135.
0] T. Schnattinger, Bidirektionale indexbasierte Suche in Texten, Diploma thesis, University of Ulm, Germany, 2010.
1] J.T. Simpson, R. Durbin, Efficient construction of an assembly string graph using the FM-index, Bioinformatics 26 (12) (2010) i367–i373.
2] J. Sirén, Compressed suffix arrays for massive data, in: Proc. 16th International Symposium on String Processing and Information Retrieval, in: Lecture
Notes in Computer Science, vol. 5721, Springer-Verlag, Berlin, 2009, pp. 63–74.
3] J. Sirén, Sampled longest common prefix array, in: Proc. 21st Annual Symposium on Combinatorial Pattern Matching, in: Lecture Notes in Computer
Science, vol. 6129, Springer-Verlag, Berlin, 2010, pp. 227–237.
4] I. Umar, R. Abdullah, Longest-common-prefix computation in Burrows–Wheeler transformed text, in: Proc. 2nd IMT–GT Regional Conference on Math-
ematics, Statistics and Applications, Universiti Sains Malaysia, Penang, 2006.
5] Z.-D. Wu, T. Jiang, W.-J. Su, Efficient computation of shortest absent words in a genomic sequence, Information Processing Letters 110 (14–15) (2010)
596–601.
